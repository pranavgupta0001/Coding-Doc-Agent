{"repository": "psf/requests", "commit_sha": "5f338446f9eabd956e7e132668f330d9de4fc368", "commit_message": "Update docstring for Session.verify to include string support (#6859) (#6865)\n\n\n---------\n\nCo-authored-by: Nate Prewitt <nate.prewitt@gmail.com>", "commit_date": "2026-02-13T02:52:04+00:00", "author": "m0d3v", "file": "src/requests/sessions.py", "patch": "@@ -422,6 +422,8 @@ def __init__(self):\n         #: expired certificates, which will make your application vulnerable to\n         #: man-in-the-middle (MitM) attacks.\n         #: Only set this to `False` for testing.\n+        #: If verify is set to a string, it must be the path to a CA bundle file\n+        #: that will be used to verify the TLS certificate.\n         self.verify = True\n \n         #: SSL client certificate default, if String, path to ssl client", "before_segments": [{"filename": "src/requests/sessions.py", "start_line": 61, "code": "def merge_setting(request_setting, session_setting, dict_class=OrderedDict):\n    if session_setting is None:\n        return request_setting\n    if request_setting is None:\n        return session_setting\n    if not (\n        isinstance(session_setting, Mapping) and isinstance(request_setting, Mapping)\n    ):\n        return request_setting\n    merged_setting = dict_class(to_key_val_list(session_setting))\n    merged_setting.update(to_key_val_list(request_setting))", "documentation": "    \"\"\"Determines appropriate setting for a given request, taking into account\n    the explicit setting on that request, and the setting in the session. If a\n    setting is a dictionary, they will be merged together using `dict_class`\n    \"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 91, "code": "def merge_hooks(request_hooks, session_hooks, dict_class=OrderedDict):\n    if session_hooks is None or session_hooks.get(\"response\") == []:\n        return request_hooks\n    if request_hooks is None or request_hooks.get(\"response\") == []:\n        return session_hooks\n    return merge_setting(request_hooks, session_hooks, dict_class)", "documentation": "    \"\"\"Properly merges both requests and session hooks.\n\n    This is necessary because when request_hooks == {'response': []}, the\n    merge breaks Session hooks entirely.\n    \"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 107, "code": "    def get_redirect_target(self, resp):\n        if resp.is_redirect:\n            location = resp.headers[\"location\"]\n            location = location.encode(\"latin1\")\n            return to_native_string(location, \"utf8\")\n        return None", "documentation": "        \"\"\"Receives a Response. Returns a redirect URI or ``None``\"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 127, "code": "    def should_strip_auth(self, old_url, new_url):\n        old_parsed = urlparse(old_url)\n        new_parsed = urlparse(new_url)\n        if old_parsed.hostname != new_parsed.hostname:\n            return True\n        if (\n            old_parsed.scheme == \"http\"\n            and old_parsed.port in (80, None)\n            and new_parsed.scheme == \"https\"\n            and new_parsed.port in (443, None)\n        ):", "documentation": "        \"\"\"Decide whether Authorization header should be removed when redirecting\"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 282, "code": "    def rebuild_auth(self, prepared_request, response):\n        headers = prepared_request.headers\n        url = prepared_request.url\n        if \"Authorization\" in headers and self.should_strip_auth(\n            response.request.url, url\n        ):\n            del headers[\"Authorization\"]\n        new_auth = get_netrc_auth(url) if self.trust_env else None\n        if new_auth is not None:\n            prepared_request.prepare_auth(new_auth)", "documentation": "        \"\"\"When being redirected we may want to strip authentication from the\n        request to avoid leaking credentials. This method intelligently removes\n        and reapplies authentication where possible to avoid credential loss.\n        \"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 302, "code": "    def rebuild_proxies(self, prepared_request, proxies):\n        headers = prepared_request.headers\n        scheme = urlparse(prepared_request.url).scheme\n        new_proxies = resolve_proxies(prepared_request, proxies, self.trust_env)\n        if \"Proxy-Authorization\" in headers:\n            del headers[\"Proxy-Authorization\"]\n        try:\n            username, password = get_auth_from_url(new_proxies[scheme])\n        except KeyError:\n            username, password = None, None\n        if not scheme.startswith(\"https\") and username and password:", "documentation": "        \"\"\"This method re-evaluates the proxy configuration by considering the\n        environment variables. If we are redirected to a URL covered by\n        NO_PROXY, we strip the proxy configuration. Otherwise, we set missing\n        proxy keys for this URL (in case they were stripped by a previous\n        redirect).\n\n        This method also replaces the Proxy-Authorization header where\n        necessary.\n\n        :rtype: dict\n        \"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 333, "code": "    def rebuild_method(self, prepared_request, response):\n        method = prepared_request.method\n        if response.status_code == codes.see_other and method != \"HEAD\":\n            method = \"GET\"\n        if response.status_code == codes.found and method != \"HEAD\":\n            method = \"GET\"\n        if response.status_code == codes.moved and method == \"POST\":\n            method = \"GET\"\n        prepared_request.method = method", "documentation": "        \"\"\"When being redirected we may want to change the method of the request\n        based on certain specs or browser behavior.\n        \"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 356, "code": "class Session(SessionRedirectMixin):\n    __attrs__ = [\n        \"headers\",\n        \"cookies\",\n        \"auth\",\n        \"proxies\",\n        \"hooks\",\n        \"params\",\n        \"verify\",\n        \"cert\",\n        \"adapters\",", "documentation": "    \"\"\"A Requests session.\n\n    Provides cookie persistence, connection-pooling, and configuration.\n\n    Basic Usage::\n\n      >>> import requests\n      >>> s = requests.Session()\n      >>> s.get('https://httpbin.org/get')\n      <Response [200]>\n\n    Or as a context manager::\n\n      >>> with requests.Session() as s:\n      ...     s.get('https://httpbin.org/get')\n      <Response [200]>\n    \"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 457, "code": "    def prepare_request(self, request):\n        cookies = request.cookies or {}\n        if not isinstance(cookies, cookielib.CookieJar):\n            cookies = cookiejar_from_dict(cookies)\n        merged_cookies = merge_cookies(\n            merge_cookies(RequestsCookieJar(), self.cookies), cookies\n        )\n        auth = request.auth\n        if self.trust_env and not auth and not self.auth:\n            auth = get_netrc_auth(request.url)\n        p = PreparedRequest()", "documentation": "        \"\"\"Constructs a :class:`PreparedRequest <PreparedRequest>` for\n        transmission and returns it. The :class:`PreparedRequest` has settings\n        merged from the :class:`Request <Request>` instance and those of the\n        :class:`Session`.\n\n        :param request: :class:`Request` instance to prepare with this\n            session's settings.\n        :rtype: requests.PreparedRequest\n        \"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 673, "code": "    def send(self, request, **kwargs):\n        kwargs.setdefault(\"stream\", self.stream)\n        kwargs.setdefault(\"verify\", self.verify)\n        kwargs.setdefault(\"cert\", self.cert)\n        if \"proxies\" not in kwargs:\n            kwargs[\"proxies\"] = resolve_proxies(request, self.proxies, self.trust_env)\n        if isinstance(request, Request):\n            raise ValueError(\"You can only send PreparedRequests.\")\n        allow_redirects = kwargs.pop(\"allow_redirects\", True)\n        stream = kwargs.get(\"stream\")\n        hooks = request.hooks", "documentation": "        \"\"\"Send a given PreparedRequest.\n\n        :rtype: requests.Response\n        \"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 750, "code": "    def merge_environment_settings(self, url, proxies, stream, verify, cert):\n        if self.trust_env:\n            no_proxy = proxies.get(\"no_proxy\") if proxies is not None else None\n            env_proxies = get_environ_proxies(url, no_proxy=no_proxy)\n            for k, v in env_proxies.items():\n                proxies.setdefault(k, v)\n            if verify is True or verify is None:\n                verify = (\n                    os.environ.get(\"REQUESTS_CA_BUNDLE\")\n                    or os.environ.get(\"CURL_CA_BUNDLE\")\n                    or verify", "documentation": "        \"\"\"\n        Check the environment and merge it with some settings.\n\n        :rtype: dict\n        \"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 781, "code": "    def get_adapter(self, url):\n        for prefix, adapter in self.adapters.items():\n            if url.lower().startswith(prefix.lower()):\n                return adapter\n        raise InvalidSchema(f\"No connection adapters were found for {url!r}\")", "documentation": "        \"\"\"\n        Returns the appropriate connection adapter for the given URL.\n\n        :rtype: requests.adapters.BaseAdapter\n        \"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 794, "code": "    def close(self):\n        for v in self.adapters.values():\n            v.close()", "documentation": "        \"\"\"Closes all adapters and as such the session\"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 799, "code": "    def mount(self, prefix, adapter):\n        self.adapters[prefix] = adapter\n        keys_to_move = [k for k in self.adapters if len(k) < len(prefix)]\n        for key in keys_to_move:\n            self.adapters[key] = self.adapters.pop(key)", "documentation": "        \"\"\"Registers a connection adapter to a prefix.\n\n        Adapters are sorted in descending order by prefix length.\n        \"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 819, "code": "def session():\n    return Session()", "documentation": "    \"\"\"\n    Returns a :class:`Session` for context-management.\n\n    .. deprecated:: 1.0.0\n\n        This method has been deprecated since version 1.0.0 and is only kept for\n        backwards compatibility. New code should use :class:`~requests.sessions.Session`\n        to create a session. This may be removed at a future date.\n\n    :rtype: Session\n    \"\"\""}], "after_segments": [{"filename": "src/requests/sessions.py", "start_line": 61, "code": "def merge_setting(request_setting, session_setting, dict_class=OrderedDict):\n    if session_setting is None:\n        return request_setting\n    if request_setting is None:\n        return session_setting\n    if not (\n        isinstance(session_setting, Mapping) and isinstance(request_setting, Mapping)\n    ):\n        return request_setting\n    merged_setting = dict_class(to_key_val_list(session_setting))\n    merged_setting.update(to_key_val_list(request_setting))", "documentation": "    \"\"\"Determines appropriate setting for a given request, taking into account\n    the explicit setting on that request, and the setting in the session. If a\n    setting is a dictionary, they will be merged together using `dict_class`\n    \"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 91, "code": "def merge_hooks(request_hooks, session_hooks, dict_class=OrderedDict):\n    if session_hooks is None or session_hooks.get(\"response\") == []:\n        return request_hooks\n    if request_hooks is None or request_hooks.get(\"response\") == []:\n        return session_hooks\n    return merge_setting(request_hooks, session_hooks, dict_class)", "documentation": "    \"\"\"Properly merges both requests and session hooks.\n\n    This is necessary because when request_hooks == {'response': []}, the\n    merge breaks Session hooks entirely.\n    \"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 107, "code": "    def get_redirect_target(self, resp):\n        if resp.is_redirect:\n            location = resp.headers[\"location\"]\n            location = location.encode(\"latin1\")\n            return to_native_string(location, \"utf8\")\n        return None", "documentation": "        \"\"\"Receives a Response. Returns a redirect URI or ``None``\"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 127, "code": "    def should_strip_auth(self, old_url, new_url):\n        old_parsed = urlparse(old_url)\n        new_parsed = urlparse(new_url)\n        if old_parsed.hostname != new_parsed.hostname:\n            return True\n        if (\n            old_parsed.scheme == \"http\"\n            and old_parsed.port in (80, None)\n            and new_parsed.scheme == \"https\"\n            and new_parsed.port in (443, None)\n        ):", "documentation": "        \"\"\"Decide whether Authorization header should be removed when redirecting\"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 282, "code": "    def rebuild_auth(self, prepared_request, response):\n        headers = prepared_request.headers\n        url = prepared_request.url\n        if \"Authorization\" in headers and self.should_strip_auth(\n            response.request.url, url\n        ):\n            del headers[\"Authorization\"]\n        new_auth = get_netrc_auth(url) if self.trust_env else None\n        if new_auth is not None:\n            prepared_request.prepare_auth(new_auth)", "documentation": "        \"\"\"When being redirected we may want to strip authentication from the\n        request to avoid leaking credentials. This method intelligently removes\n        and reapplies authentication where possible to avoid credential loss.\n        \"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 302, "code": "    def rebuild_proxies(self, prepared_request, proxies):\n        headers = prepared_request.headers\n        scheme = urlparse(prepared_request.url).scheme\n        new_proxies = resolve_proxies(prepared_request, proxies, self.trust_env)\n        if \"Proxy-Authorization\" in headers:\n            del headers[\"Proxy-Authorization\"]\n        try:\n            username, password = get_auth_from_url(new_proxies[scheme])\n        except KeyError:\n            username, password = None, None\n        if not scheme.startswith(\"https\") and username and password:", "documentation": "        \"\"\"This method re-evaluates the proxy configuration by considering the\n        environment variables. If we are redirected to a URL covered by\n        NO_PROXY, we strip the proxy configuration. Otherwise, we set missing\n        proxy keys for this URL (in case they were stripped by a previous\n        redirect).\n\n        This method also replaces the Proxy-Authorization header where\n        necessary.\n\n        :rtype: dict\n        \"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 333, "code": "    def rebuild_method(self, prepared_request, response):\n        method = prepared_request.method\n        if response.status_code == codes.see_other and method != \"HEAD\":\n            method = \"GET\"\n        if response.status_code == codes.found and method != \"HEAD\":\n            method = \"GET\"\n        if response.status_code == codes.moved and method == \"POST\":\n            method = \"GET\"\n        prepared_request.method = method", "documentation": "        \"\"\"When being redirected we may want to change the method of the request\n        based on certain specs or browser behavior.\n        \"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 356, "code": "class Session(SessionRedirectMixin):\n    __attrs__ = [\n        \"headers\",\n        \"cookies\",\n        \"auth\",\n        \"proxies\",\n        \"hooks\",\n        \"params\",\n        \"verify\",\n        \"cert\",\n        \"adapters\",", "documentation": "    \"\"\"A Requests session.\n\n    Provides cookie persistence, connection-pooling, and configuration.\n\n    Basic Usage::\n\n      >>> import requests\n      >>> s = requests.Session()\n      >>> s.get('https://httpbin.org/get')\n      <Response [200]>\n\n    Or as a context manager::\n\n      >>> with requests.Session() as s:\n      ...     s.get('https://httpbin.org/get')\n      <Response [200]>\n    \"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 459, "code": "    def prepare_request(self, request):\n        cookies = request.cookies or {}\n        if not isinstance(cookies, cookielib.CookieJar):\n            cookies = cookiejar_from_dict(cookies)\n        merged_cookies = merge_cookies(\n            merge_cookies(RequestsCookieJar(), self.cookies), cookies\n        )\n        auth = request.auth\n        if self.trust_env and not auth and not self.auth:\n            auth = get_netrc_auth(request.url)\n        p = PreparedRequest()", "documentation": "        \"\"\"Constructs a :class:`PreparedRequest <PreparedRequest>` for\n        transmission and returns it. The :class:`PreparedRequest` has settings\n        merged from the :class:`Request <Request>` instance and those of the\n        :class:`Session`.\n\n        :param request: :class:`Request` instance to prepare with this\n            session's settings.\n        :rtype: requests.PreparedRequest\n        \"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 675, "code": "    def send(self, request, **kwargs):\n        kwargs.setdefault(\"stream\", self.stream)\n        kwargs.setdefault(\"verify\", self.verify)\n        kwargs.setdefault(\"cert\", self.cert)\n        if \"proxies\" not in kwargs:\n            kwargs[\"proxies\"] = resolve_proxies(request, self.proxies, self.trust_env)\n        if isinstance(request, Request):\n            raise ValueError(\"You can only send PreparedRequests.\")\n        allow_redirects = kwargs.pop(\"allow_redirects\", True)\n        stream = kwargs.get(\"stream\")\n        hooks = request.hooks", "documentation": "        \"\"\"Send a given PreparedRequest.\n\n        :rtype: requests.Response\n        \"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 752, "code": "    def merge_environment_settings(self, url, proxies, stream, verify, cert):\n        if self.trust_env:\n            no_proxy = proxies.get(\"no_proxy\") if proxies is not None else None\n            env_proxies = get_environ_proxies(url, no_proxy=no_proxy)\n            for k, v in env_proxies.items():\n                proxies.setdefault(k, v)\n            if verify is True or verify is None:\n                verify = (\n                    os.environ.get(\"REQUESTS_CA_BUNDLE\")\n                    or os.environ.get(\"CURL_CA_BUNDLE\")\n                    or verify", "documentation": "        \"\"\"\n        Check the environment and merge it with some settings.\n\n        :rtype: dict\n        \"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 783, "code": "    def get_adapter(self, url):\n        for prefix, adapter in self.adapters.items():\n            if url.lower().startswith(prefix.lower()):\n                return adapter\n        raise InvalidSchema(f\"No connection adapters were found for {url!r}\")", "documentation": "        \"\"\"\n        Returns the appropriate connection adapter for the given URL.\n\n        :rtype: requests.adapters.BaseAdapter\n        \"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 796, "code": "    def close(self):\n        for v in self.adapters.values():\n            v.close()", "documentation": "        \"\"\"Closes all adapters and as such the session\"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 801, "code": "    def mount(self, prefix, adapter):\n        self.adapters[prefix] = adapter\n        keys_to_move = [k for k in self.adapters if len(k) < len(prefix)]\n        for key in keys_to_move:\n            self.adapters[key] = self.adapters.pop(key)", "documentation": "        \"\"\"Registers a connection adapter to a prefix.\n\n        Adapters are sorted in descending order by prefix length.\n        \"\"\""}, {"filename": "src/requests/sessions.py", "start_line": 821, "code": "def session():\n    return Session()", "documentation": "    \"\"\"\n    Returns a :class:`Session` for context-management.\n\n    .. deprecated:: 1.0.0\n\n        This method has been deprecated since version 1.0.0 and is only kept for\n        backwards compatibility. New code should use :class:`~requests.sessions.Session`\n        to create a session. This may be removed at a future date.\n\n    :rtype: Session\n    \"\"\""}]}
{"repository": "psf/requests", "commit_sha": "a7e1c745dc23c18e836febd672416ed0c5d8d8ae", "commit_message": "Update docs/conf.py\n\nCo-authored-by: Nate Prewitt <nate.prewitt@gmail.com>", "commit_date": "2025-05-31T16:59:51+00:00", "author": "Ian Stapleton Cordasco", "file": "docs/conf.py", "patch": "@@ -58,7 +58,7 @@\n \n # General information about the project.\n project = u\"Requests\"\n-copyright = u'MMXVIX. A <a href=\"https://kennethreitz.org/software/\">Kenneth Reitz</a> Project'\n+copyright = u'MMXVIX. A Kenneth Reitz Project'\n author = u\"Kenneth Reitz\"\n \n # The version info for the project you're documenting, acts as replacement for", "before_segments": [], "after_segments": []}
{"repository": "psf/requests", "commit_sha": "0b4d494192de489701d3a2e32acef8fb5d3f042e", "commit_message": "Merge pull request #6581 from EFord36/typo-fix\n\nfix docstring typo: a -> as", "commit_date": "2023-11-22T15:10:47+00:00", "author": "Ian Stapleton Cordasco", "file": "src/requests/utils.py", "patch": "@@ -859,7 +859,7 @@ def select_proxy(url, proxies):\n def resolve_proxies(request, proxies, trust_env=True):\n     \"\"\"This method takes proxy information from a request and configuration\n     input to resolve a mapping of target proxies. This will consider settings\n-    such a NO_PROXY to strip proxy configurations.\n+    such as NO_PROXY to strip proxy configurations.\n \n     :param request: Request or PreparedRequest\n     :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs", "before_segments": [{"filename": "src/requests/utils.py", "start_line": 111, "code": "    def proxy_bypass(host):  # noqa\n        if getproxies_environment():\n            return proxy_bypass_environment(host)\n        else:\n            return proxy_bypass_registry(host)", "documentation": "        \"\"\"Return True, if the host should be bypassed.\n\n        Checks proxy settings gathered from the environment, if specified,\n        or the registry.\n        \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 123, "code": "def dict_to_sequence(d):\n    if hasattr(d, \"items\"):\n        d = d.items()\n    return d", "documentation": "    \"\"\"Returns an internal sequence dictionary update.\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 198, "code": "def get_netrc_auth(url, raise_errors=False):\n    netrc_file = os.environ.get(\"NETRC\")\n    if netrc_file is not None:\n        netrc_locations = (netrc_file,)\n    else:\n        netrc_locations = (f\"~/{f}\" for f in NETRC_FILES)\n    try:\n        from netrc import NetrcParseError, netrc\n        netrc_path = None\n        for f in netrc_locations:\n            try:", "documentation": "    \"\"\"Returns the Requests tuple auth for a given url from netrc.\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 255, "code": "def guess_filename(obj):\n    name = getattr(obj, \"name\", None)\n    if name and isinstance(name, basestring) and name[0] != \"<\" and name[-1] != \">\":\n        return os.path.basename(name)", "documentation": "    \"\"\"Tries to guess the filename of the given object.\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 262, "code": "def extract_zipped_paths(path):\n    if os.path.exists(path):\n        return path\n    archive, member = os.path.split(path)\n    while archive and not os.path.exists(archive):\n        archive, prefix = os.path.split(archive)\n        if not prefix:\n            break\n        member = \"/\".join([prefix, member])\n    if not zipfile.is_zipfile(archive):\n        return path", "documentation": "    \"\"\"Replace nonexistent paths that look like they refer to a member of a zip\n    archive with the location of an extracted copy of the target, or else\n    just return the provided path unchanged.\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 300, "code": "def atomic_open(filename):\n    tmp_descriptor, tmp_name = tempfile.mkstemp(dir=os.path.dirname(filename))\n    try:\n        with os.fdopen(tmp_descriptor, \"wb\") as tmp_handler:\n            yield tmp_handler\n        os.replace(tmp_name, filename)\n    except BaseException:\n        os.remove(tmp_name)\n        raise", "documentation": "    \"\"\"Write a file to the disk in an atomic fashion\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 312, "code": "def from_key_val_list(value):\n    if value is None:\n        return None\n    if isinstance(value, (str, bytes, bool, int)):\n        raise ValueError(\"cannot encode objects that are not 2-tuples\")\n    return OrderedDict(value)", "documentation": "    \"\"\"Take an object and test to see if it can be represented as a\n    dictionary. Unless it can not be represented as such, return an\n    OrderedDict, e.g.,\n\n    ::\n\n        >>> from_key_val_list([('key', 'val')])\n        OrderedDict([('key', 'val')])\n        >>> from_key_val_list('string')\n        Traceback (most recent call last):\n        ...\n        ValueError: cannot encode objects that are not 2-tuples\n        >>> from_key_val_list({'key': 'val'})\n        OrderedDict([('key', 'val')])\n\n    :rtype: OrderedDict\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 339, "code": "def to_key_val_list(value):\n    if value is None:\n        return None\n    if isinstance(value, (str, bytes, bool, int)):\n        raise ValueError(\"cannot encode objects that are not 2-tuples\")\n    if isinstance(value, Mapping):\n        value = value.items()\n    return list(value)", "documentation": "    \"\"\"Take an object and test to see if it can be represented as a\n    dictionary. If it can be, return a list of tuples, e.g.,\n\n    ::\n\n        >>> to_key_val_list([('key', 'val')])\n        [('key', 'val')]\n        >>> to_key_val_list({'key': 'val'})\n        [('key', 'val')]\n        >>> to_key_val_list('string')\n        Traceback (most recent call last):\n        ...\n        ValueError: cannot encode objects that are not 2-tuples\n\n    :rtype: list\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 369, "code": "def parse_list_header(value):\n    result = []\n    for item in _parse_list_header(value):\n        if item[:1] == item[-1:] == '\"':\n            item = unquote_header_value(item[1:-1])\n        result.append(item)\n    return result", "documentation": "    \"\"\"Parse lists as described by RFC 2068 Section 2.\n\n    In particular, parse comma-separated lists where the elements of\n    the list may include quoted-strings.  A quoted-string could\n    contain a comma.  A non-quoted string could have quotes in the\n    middle.  Quotes are removed automatically after parsing.\n\n    It basically works like :func:`parse_set_header` just that items\n    may appear multiple times and case sensitivity is preserved.\n\n    The return value is a standard :class:`list`:\n\n    >>> parse_list_header('token, \"quoted value\"')\n    ['token', 'quoted value']\n\n    To create a header from the :class:`list` again, use the\n    :func:`dump_header` function.\n\n    :param value: a string with a list header.\n    :return: :class:`list`\n    :rtype: list\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 401, "code": "def parse_dict_header(value):\n    result = {}\n    for item in _parse_list_header(value):\n        if \"=\" not in item:\n            result[item] = None\n            continue\n        name, value = item.split(\"=\", 1)\n        if value[:1] == value[-1:] == '\"':\n            value = unquote_header_value(value[1:-1])\n        result[name] = value\n    return result", "documentation": "    \"\"\"Parse lists of key, value pairs as described by RFC 2068 Section 2 and\n    convert them into a python dict:\n\n    >>> d = parse_dict_header('foo=\"is a fish\", bar=\"as well\"')\n    >>> type(d) is dict\n    True\n    >>> sorted(d.items())\n    [('bar', 'as well'), ('foo', 'is a fish')]\n\n    If there is no value for a key it will be `None`:\n\n    >>> parse_dict_header('key_without_value')\n    {'key_without_value': None}\n\n    To create a header from the :class:`dict` again, use the\n    :func:`dump_header` function.\n\n    :param value: a string with a dict header.\n    :return: :class:`dict`\n    :rtype: dict\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 461, "code": "def dict_from_cookiejar(cj):\n    cookie_dict = {cookie.name: cookie.value for cookie in cj}\n    return cookie_dict", "documentation": "    \"\"\"Returns a key/value dictionary from a CookieJar.\n\n    :param cj: CookieJar object to extract cookies from.\n    :rtype: dict\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 472, "code": "def add_dict_to_cookiejar(cj, cookie_dict):\n    return cookiejar_from_dict(cookie_dict, cj)", "documentation": "    \"\"\"Returns a CookieJar from a key/value dictionary.\n\n    :param cj: CookieJar to insert cookies into.\n    :param cookie_dict: Dict of key/values to insert into CookieJar.\n    :rtype: CookieJar\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 483, "code": "def get_encodings_from_content(content):\n    warnings.warn(\n        (\n            \"In requests 3.0, get_encodings_from_content will be removed. For \"\n            \"more information, please see the discussion on issue #2266. (This\"\n            \" warning should only appear once.)\"\n        ),\n        DeprecationWarning,\n    )\n    charset_re = re.compile(r'<meta.*?charset=[\"\\']*(.+?)[\"\\'>]', flags=re.I)\n    pragma_re = re.compile(r'<meta.*?content=[\"\\']*;?charset=(.+?)[\"\\'>]', flags=re.I)", "documentation": "    \"\"\"Returns encodings from given content string.\n\n    :param content: bytestring to extract encodings from.\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 508, "code": "def _parse_content_type_header(header):\n    tokens = header.split(\";\")\n    content_type, params = tokens[0].strip(), tokens[1:]\n    params_dict = {}\n    items_to_strip = \"\\\"' \"\n    for param in params:\n        param = param.strip()\n        if param:\n            key, value = param, True\n            index_of_equals = param.find(\"=\")\n            if index_of_equals != -1:", "documentation": "    \"\"\"Returns content type and parameters from given header\n\n    :param header: string\n    :return: tuple containing content type and dictionary of\n         parameters\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 533, "code": "def get_encoding_from_headers(headers):\n    content_type = headers.get(\"content-type\")\n    if not content_type:\n        return None\n    content_type, params = _parse_content_type_header(content_type)\n    if \"charset\" in params:\n        return params[\"charset\"].strip(\"'\\\"\")\n    if \"text\" in content_type:\n        return \"ISO-8859-1\"\n    if \"application/json\" in content_type:\n        return \"utf-8\"", "documentation": "    \"\"\"Returns encodings from given HTTP Header Dict.\n\n    :param headers: dictionary to extract encoding from.\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 558, "code": "def stream_decode_response_unicode(iterator, r):\n    if r.encoding is None:\n        yield from iterator\n        return\n    decoder = codecs.getincrementaldecoder(r.encoding)(errors=\"replace\")\n    for chunk in iterator:\n        rv = decoder.decode(chunk)\n        if rv:\n            yield rv\n    rv = decoder.decode(b\"\", final=True)\n    if rv:", "documentation": "    \"\"\"Stream decodes an iterator.\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 575, "code": "def iter_slices(string, slice_length):\n    pos = 0\n    if slice_length is None or slice_length <= 0:\n        slice_length = len(string)\n    while pos < len(string):\n        yield string[pos : pos + slice_length]\n        pos += slice_length", "documentation": "    \"\"\"Iterate over slices of a string.\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 585, "code": "def get_unicode_from_response(r):\n    warnings.warn(\n        (\n            \"In requests 3.0, get_unicode_from_response will be removed. For \"\n            \"more information, please see the discussion on issue #2266. (This\"\n            \" warning should only appear once.)\"\n        ),\n        DeprecationWarning,\n    )\n    tried_encodings = []\n    encoding = get_encoding_from_headers(r.headers)", "documentation": "    \"\"\"Returns the requested content back in unicode.\n\n    :param r: Response object to get unicode content from.\n\n    Tried:\n\n    1. charset from content-type\n    2. fall back and replace all unicode characters\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 630, "code": "def unquote_unreserved(uri):\n    parts = uri.split(\"%\")\n    for i in range(1, len(parts)):\n        h = parts[i][0:2]\n        if len(h) == 2 and h.isalnum():\n            try:\n                c = chr(int(h, 16))\n            except ValueError:\n                raise InvalidURL(f\"Invalid percent-escape sequence: '{h}'\")\n            if c in UNRESERVED_SET:\n                parts[i] = c + parts[i][2:]", "documentation": "    \"\"\"Un-escape any percent-escape sequences in a URI that are unreserved\n    characters. This leaves all reserved, illegal and non-ASCII bytes encoded.\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 654, "code": "def requote_uri(uri):\n    safe_with_percent = \"!#$%&'()*+,/:;=?@[]~\"\n    safe_without_percent = \"!#$&'()*+,/:;=?@[]~\"\n    try:\n        return quote(unquote_unreserved(uri), safe=safe_with_percent)\n    except InvalidURL:\n        return quote(uri, safe=safe_without_percent)", "documentation": "    \"\"\"Re-quote the given URI.\n\n    This function passes the given URI through an unquote/quote cycle to\n    ensure that it is fully and consistently quoted.\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 676, "code": "def address_in_network(ip, net):\n    ipaddr = struct.unpack(\"=L\", socket.inet_aton(ip))[0]\n    netaddr, bits = net.split(\"/\")\n    netmask = struct.unpack(\"=L\", socket.inet_aton(dotted_netmask(int(bits))))[0]\n    network = struct.unpack(\"=L\", socket.inet_aton(netaddr))[0] & netmask\n    return (ipaddr & netmask) == (network & netmask)", "documentation": "    \"\"\"This function allows you to check if an IP belongs to a network subnet\n\n    Example: returns True if ip = 192.168.1.1 and net = 192.168.1.0/24\n             returns False if ip = 192.168.1.1 and net = 192.168.100.0/24\n\n    :rtype: bool\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 691, "code": "def dotted_netmask(mask):\n    bits = 0xFFFFFFFF ^ (1 << 32 - mask) - 1\n    return socket.inet_ntoa(struct.pack(\">I\", bits))", "documentation": "    \"\"\"Converts mask from /xx format to xxx.xxx.xxx.xxx\n\n    Example: if mask is 24 function returns 255.255.255.0\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 702, "code": "def is_ipv4_address(string_ip):\n    try:\n        socket.inet_aton(string_ip)\n    except OSError:\n        return False\n    return True", "documentation": "    \"\"\"\n    :rtype: bool\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 713, "code": "def is_valid_cidr(string_network):\n    if string_network.count(\"/\") == 1:\n        try:\n            mask = int(string_network.split(\"/\")[1])\n        except ValueError:\n            return False\n        if mask < 1 or mask > 32:\n            return False\n        try:\n            socket.inet_aton(string_network.split(\"/\")[0])\n        except OSError:", "documentation": "    \"\"\"\n    Very simple check of the cidr format in no_proxy variable.\n\n    :rtype: bool\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 738, "code": "def set_environ(env_name, value):\n    value_changed = value is not None\n    if value_changed:\n        old_value = os.environ.get(env_name)\n        os.environ[env_name] = value\n    try:\n        yield\n    finally:\n        if value_changed:\n            if old_value is None:\n                del os.environ[env_name]", "documentation": "    \"\"\"Set the environment variable 'env_name' to 'value'\n\n    Save previous value, yield, and then restore the previous value stored in\n    the environment variable 'env_name'.\n\n    If 'value' is None, do nothing\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 759, "code": "def should_bypass_proxies(url, no_proxy):", "documentation": "    \"\"\"\n    Returns whether we should bypass proxies or not.\n\n    :rtype: bool\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 820, "code": "def get_environ_proxies(url, no_proxy=None):\n    if should_bypass_proxies(url, no_proxy=no_proxy):\n        return {}\n    else:\n        return getproxies()", "documentation": "    \"\"\"\n    Return a dict of environment proxies.\n\n    :rtype: dict\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 832, "code": "def select_proxy(url, proxies):\n    proxies = proxies or {}\n    urlparts = urlparse(url)\n    if urlparts.hostname is None:\n        return proxies.get(urlparts.scheme, proxies.get(\"all\"))\n    proxy_keys = [\n        urlparts.scheme + \"://\" + urlparts.hostname,\n        urlparts.scheme,\n        \"all://\" + urlparts.hostname,\n        \"all\",\n    ]", "documentation": "    \"\"\"Select a proxy for the url, if applicable.\n\n    :param url: The url being for the request\n    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 858, "code": "def resolve_proxies(request, proxies, trust_env=True):\n    proxies = proxies if proxies is not None else {}\n    url = request.url\n    scheme = urlparse(url).scheme\n    no_proxy = proxies.get(\"no_proxy\")\n    new_proxies = proxies.copy()\n    if trust_env and not should_bypass_proxies(url, no_proxy=no_proxy):\n        environ_proxies = get_environ_proxies(url, no_proxy=no_proxy)\n        proxy = environ_proxies.get(scheme, environ_proxies.get(\"all\"))\n        if proxy:\n            new_proxies.setdefault(scheme, proxy)", "documentation": "    \"\"\"This method takes proxy information from a request and configuration\n    input to resolve a mapping of target proxies. This will consider settings\n    such a NO_PROXY to strip proxy configurations.\n\n    :param request: Request or PreparedRequest\n    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs\n    :param trust_env: Boolean declaring whether to trust environment configs\n\n    :rtype: dict\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 885, "code": "def default_user_agent(name=\"python-requests\"):\n    return f\"{name}/{__version__}\"", "documentation": "    \"\"\"\n    Return a string representing the default user agent.\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 894, "code": "def default_headers():\n    return CaseInsensitiveDict(\n        {\n            \"User-Agent\": default_user_agent(),\n            \"Accept-Encoding\": DEFAULT_ACCEPT_ENCODING,\n            \"Accept\": \"*/*\",\n            \"Connection\": \"keep-alive\",\n        }\n    )", "documentation": "    \"\"\"\n    :rtype: requests.structures.CaseInsensitiveDict\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 908, "code": "def parse_header_links(value):\n    links = []\n    replace_chars = \" '\\\"\"\n    value = value.strip(replace_chars)\n    if not value:\n        return links\n    for val in re.split(\", *<\", value):\n        try:\n            url, params = val.split(\";\", 1)\n        except ValueError:\n            url, params = val, \"\"", "documentation": "    \"\"\"Return a list of parsed link headers proxies.\n\n    i.e. Link: <http:/.../front.jpeg>; rel=front; type=\"image/jpeg\",<http://.../back.jpeg>; rel=back;type=\"image/jpeg\"\n\n    :rtype: list\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 951, "code": "def guess_json_utf(data):\n    sample = data[:4]\n    if sample in (codecs.BOM_UTF32_LE, codecs.BOM_UTF32_BE):\n        return \"utf-32\"  # BOM included\n    if sample[:3] == codecs.BOM_UTF8:\n        return \"utf-8-sig\"  # BOM included, MS style (discouraged)\n    if sample[:2] in (codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE):\n        return \"utf-16\"  # BOM included\n    nullcount = sample.count(_null)\n    if nullcount == 0:\n        return \"utf-8\"", "documentation": "    \"\"\"\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 983, "code": "def prepend_scheme_if_needed(url, new_scheme):\n    parsed = parse_url(url)\n    scheme, auth, host, port, path, query, fragment = parsed\n    netloc = parsed.netloc\n    if not netloc:\n        netloc, path = path, netloc\n    if auth:\n        netloc = \"@\".join([auth, netloc])\n    if scheme is None:\n        scheme = new_scheme\n    if path is None:", "documentation": "    \"\"\"Given a URL that may or may not have a scheme, prepend the given scheme.\n    Does not replace a present scheme with the one provided as an argument.\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 1012, "code": "def get_auth_from_url(url):\n    parsed = urlparse(url)\n    try:\n        auth = (unquote(parsed.username), unquote(parsed.password))\n    except (AttributeError, TypeError):\n        auth = (\"\", \"\")\n    return auth", "documentation": "    \"\"\"Given a url with authentication components, extract them into a tuple of\n    username,password.\n\n    :rtype: (str,str)\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 1028, "code": "def check_header_validity(header):\n    name, value = header\n    _validate_header_part(header, name, 0)\n    _validate_header_part(header, value, 1)", "documentation": "    \"\"\"Verifies that header parts don't contain leading whitespace\n    reserved characters, or return characters.\n\n    :param header: tuple, in the format (name, value).\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 1058, "code": "def urldefragauth(url):\n    scheme, netloc, path, params, query, fragment = urlparse(url)\n    if not netloc:\n        netloc, path = path, netloc\n    netloc = netloc.rsplit(\"@\", 1)[-1]\n    return urlunparse((scheme, netloc, path, params, query, \"\"))", "documentation": "    \"\"\"\n    Given a url remove the fragment and the authentication part.\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 1075, "code": "def rewind_body(prepared_request):\n    body_seek = getattr(prepared_request.body, \"seek\", None)\n    if body_seek is not None and isinstance(\n        prepared_request._body_position, integer_types\n    ):\n        try:\n            body_seek(prepared_request._body_position)\n        except OSError:\n            raise UnrewindableBodyError(\n                \"An error occurred when rewinding request body for redirect.\"\n            )", "documentation": "    \"\"\"Move file pointer back to its recorded starting position\n    so it can be read again on redirect.\n    \"\"\""}], "after_segments": [{"filename": "src/requests/utils.py", "start_line": 111, "code": "    def proxy_bypass(host):  # noqa\n        if getproxies_environment():\n            return proxy_bypass_environment(host)\n        else:\n            return proxy_bypass_registry(host)", "documentation": "        \"\"\"Return True, if the host should be bypassed.\n\n        Checks proxy settings gathered from the environment, if specified,\n        or the registry.\n        \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 123, "code": "def dict_to_sequence(d):\n    if hasattr(d, \"items\"):\n        d = d.items()\n    return d", "documentation": "    \"\"\"Returns an internal sequence dictionary update.\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 198, "code": "def get_netrc_auth(url, raise_errors=False):\n    netrc_file = os.environ.get(\"NETRC\")\n    if netrc_file is not None:\n        netrc_locations = (netrc_file,)\n    else:\n        netrc_locations = (f\"~/{f}\" for f in NETRC_FILES)\n    try:\n        from netrc import NetrcParseError, netrc\n        netrc_path = None\n        for f in netrc_locations:\n            try:", "documentation": "    \"\"\"Returns the Requests tuple auth for a given url from netrc.\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 255, "code": "def guess_filename(obj):\n    name = getattr(obj, \"name\", None)\n    if name and isinstance(name, basestring) and name[0] != \"<\" and name[-1] != \">\":\n        return os.path.basename(name)", "documentation": "    \"\"\"Tries to guess the filename of the given object.\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 262, "code": "def extract_zipped_paths(path):\n    if os.path.exists(path):\n        return path\n    archive, member = os.path.split(path)\n    while archive and not os.path.exists(archive):\n        archive, prefix = os.path.split(archive)\n        if not prefix:\n            break\n        member = \"/\".join([prefix, member])\n    if not zipfile.is_zipfile(archive):\n        return path", "documentation": "    \"\"\"Replace nonexistent paths that look like they refer to a member of a zip\n    archive with the location of an extracted copy of the target, or else\n    just return the provided path unchanged.\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 300, "code": "def atomic_open(filename):\n    tmp_descriptor, tmp_name = tempfile.mkstemp(dir=os.path.dirname(filename))\n    try:\n        with os.fdopen(tmp_descriptor, \"wb\") as tmp_handler:\n            yield tmp_handler\n        os.replace(tmp_name, filename)\n    except BaseException:\n        os.remove(tmp_name)\n        raise", "documentation": "    \"\"\"Write a file to the disk in an atomic fashion\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 312, "code": "def from_key_val_list(value):\n    if value is None:\n        return None\n    if isinstance(value, (str, bytes, bool, int)):\n        raise ValueError(\"cannot encode objects that are not 2-tuples\")\n    return OrderedDict(value)", "documentation": "    \"\"\"Take an object and test to see if it can be represented as a\n    dictionary. Unless it can not be represented as such, return an\n    OrderedDict, e.g.,\n\n    ::\n\n        >>> from_key_val_list([('key', 'val')])\n        OrderedDict([('key', 'val')])\n        >>> from_key_val_list('string')\n        Traceback (most recent call last):\n        ...\n        ValueError: cannot encode objects that are not 2-tuples\n        >>> from_key_val_list({'key': 'val'})\n        OrderedDict([('key', 'val')])\n\n    :rtype: OrderedDict\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 339, "code": "def to_key_val_list(value):\n    if value is None:\n        return None\n    if isinstance(value, (str, bytes, bool, int)):\n        raise ValueError(\"cannot encode objects that are not 2-tuples\")\n    if isinstance(value, Mapping):\n        value = value.items()\n    return list(value)", "documentation": "    \"\"\"Take an object and test to see if it can be represented as a\n    dictionary. If it can be, return a list of tuples, e.g.,\n\n    ::\n\n        >>> to_key_val_list([('key', 'val')])\n        [('key', 'val')]\n        >>> to_key_val_list({'key': 'val'})\n        [('key', 'val')]\n        >>> to_key_val_list('string')\n        Traceback (most recent call last):\n        ...\n        ValueError: cannot encode objects that are not 2-tuples\n\n    :rtype: list\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 369, "code": "def parse_list_header(value):\n    result = []\n    for item in _parse_list_header(value):\n        if item[:1] == item[-1:] == '\"':\n            item = unquote_header_value(item[1:-1])\n        result.append(item)\n    return result", "documentation": "    \"\"\"Parse lists as described by RFC 2068 Section 2.\n\n    In particular, parse comma-separated lists where the elements of\n    the list may include quoted-strings.  A quoted-string could\n    contain a comma.  A non-quoted string could have quotes in the\n    middle.  Quotes are removed automatically after parsing.\n\n    It basically works like :func:`parse_set_header` just that items\n    may appear multiple times and case sensitivity is preserved.\n\n    The return value is a standard :class:`list`:\n\n    >>> parse_list_header('token, \"quoted value\"')\n    ['token', 'quoted value']\n\n    To create a header from the :class:`list` again, use the\n    :func:`dump_header` function.\n\n    :param value: a string with a list header.\n    :return: :class:`list`\n    :rtype: list\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 401, "code": "def parse_dict_header(value):\n    result = {}\n    for item in _parse_list_header(value):\n        if \"=\" not in item:\n            result[item] = None\n            continue\n        name, value = item.split(\"=\", 1)\n        if value[:1] == value[-1:] == '\"':\n            value = unquote_header_value(value[1:-1])\n        result[name] = value\n    return result", "documentation": "    \"\"\"Parse lists of key, value pairs as described by RFC 2068 Section 2 and\n    convert them into a python dict:\n\n    >>> d = parse_dict_header('foo=\"is a fish\", bar=\"as well\"')\n    >>> type(d) is dict\n    True\n    >>> sorted(d.items())\n    [('bar', 'as well'), ('foo', 'is a fish')]\n\n    If there is no value for a key it will be `None`:\n\n    >>> parse_dict_header('key_without_value')\n    {'key_without_value': None}\n\n    To create a header from the :class:`dict` again, use the\n    :func:`dump_header` function.\n\n    :param value: a string with a dict header.\n    :return: :class:`dict`\n    :rtype: dict\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 461, "code": "def dict_from_cookiejar(cj):\n    cookie_dict = {cookie.name: cookie.value for cookie in cj}\n    return cookie_dict", "documentation": "    \"\"\"Returns a key/value dictionary from a CookieJar.\n\n    :param cj: CookieJar object to extract cookies from.\n    :rtype: dict\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 472, "code": "def add_dict_to_cookiejar(cj, cookie_dict):\n    return cookiejar_from_dict(cookie_dict, cj)", "documentation": "    \"\"\"Returns a CookieJar from a key/value dictionary.\n\n    :param cj: CookieJar to insert cookies into.\n    :param cookie_dict: Dict of key/values to insert into CookieJar.\n    :rtype: CookieJar\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 483, "code": "def get_encodings_from_content(content):\n    warnings.warn(\n        (\n            \"In requests 3.0, get_encodings_from_content will be removed. For \"\n            \"more information, please see the discussion on issue #2266. (This\"\n            \" warning should only appear once.)\"\n        ),\n        DeprecationWarning,\n    )\n    charset_re = re.compile(r'<meta.*?charset=[\"\\']*(.+?)[\"\\'>]', flags=re.I)\n    pragma_re = re.compile(r'<meta.*?content=[\"\\']*;?charset=(.+?)[\"\\'>]', flags=re.I)", "documentation": "    \"\"\"Returns encodings from given content string.\n\n    :param content: bytestring to extract encodings from.\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 508, "code": "def _parse_content_type_header(header):\n    tokens = header.split(\";\")\n    content_type, params = tokens[0].strip(), tokens[1:]\n    params_dict = {}\n    items_to_strip = \"\\\"' \"\n    for param in params:\n        param = param.strip()\n        if param:\n            key, value = param, True\n            index_of_equals = param.find(\"=\")\n            if index_of_equals != -1:", "documentation": "    \"\"\"Returns content type and parameters from given header\n\n    :param header: string\n    :return: tuple containing content type and dictionary of\n         parameters\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 533, "code": "def get_encoding_from_headers(headers):\n    content_type = headers.get(\"content-type\")\n    if not content_type:\n        return None\n    content_type, params = _parse_content_type_header(content_type)\n    if \"charset\" in params:\n        return params[\"charset\"].strip(\"'\\\"\")\n    if \"text\" in content_type:\n        return \"ISO-8859-1\"\n    if \"application/json\" in content_type:\n        return \"utf-8\"", "documentation": "    \"\"\"Returns encodings from given HTTP Header Dict.\n\n    :param headers: dictionary to extract encoding from.\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 558, "code": "def stream_decode_response_unicode(iterator, r):\n    if r.encoding is None:\n        yield from iterator\n        return\n    decoder = codecs.getincrementaldecoder(r.encoding)(errors=\"replace\")\n    for chunk in iterator:\n        rv = decoder.decode(chunk)\n        if rv:\n            yield rv\n    rv = decoder.decode(b\"\", final=True)\n    if rv:", "documentation": "    \"\"\"Stream decodes an iterator.\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 575, "code": "def iter_slices(string, slice_length):\n    pos = 0\n    if slice_length is None or slice_length <= 0:\n        slice_length = len(string)\n    while pos < len(string):\n        yield string[pos : pos + slice_length]\n        pos += slice_length", "documentation": "    \"\"\"Iterate over slices of a string.\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 585, "code": "def get_unicode_from_response(r):\n    warnings.warn(\n        (\n            \"In requests 3.0, get_unicode_from_response will be removed. For \"\n            \"more information, please see the discussion on issue #2266. (This\"\n            \" warning should only appear once.)\"\n        ),\n        DeprecationWarning,\n    )\n    tried_encodings = []\n    encoding = get_encoding_from_headers(r.headers)", "documentation": "    \"\"\"Returns the requested content back in unicode.\n\n    :param r: Response object to get unicode content from.\n\n    Tried:\n\n    1. charset from content-type\n    2. fall back and replace all unicode characters\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 630, "code": "def unquote_unreserved(uri):\n    parts = uri.split(\"%\")\n    for i in range(1, len(parts)):\n        h = parts[i][0:2]\n        if len(h) == 2 and h.isalnum():\n            try:\n                c = chr(int(h, 16))\n            except ValueError:\n                raise InvalidURL(f\"Invalid percent-escape sequence: '{h}'\")\n            if c in UNRESERVED_SET:\n                parts[i] = c + parts[i][2:]", "documentation": "    \"\"\"Un-escape any percent-escape sequences in a URI that are unreserved\n    characters. This leaves all reserved, illegal and non-ASCII bytes encoded.\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 654, "code": "def requote_uri(uri):\n    safe_with_percent = \"!#$%&'()*+,/:;=?@[]~\"\n    safe_without_percent = \"!#$&'()*+,/:;=?@[]~\"\n    try:\n        return quote(unquote_unreserved(uri), safe=safe_with_percent)\n    except InvalidURL:\n        return quote(uri, safe=safe_without_percent)", "documentation": "    \"\"\"Re-quote the given URI.\n\n    This function passes the given URI through an unquote/quote cycle to\n    ensure that it is fully and consistently quoted.\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 676, "code": "def address_in_network(ip, net):\n    ipaddr = struct.unpack(\"=L\", socket.inet_aton(ip))[0]\n    netaddr, bits = net.split(\"/\")\n    netmask = struct.unpack(\"=L\", socket.inet_aton(dotted_netmask(int(bits))))[0]\n    network = struct.unpack(\"=L\", socket.inet_aton(netaddr))[0] & netmask\n    return (ipaddr & netmask) == (network & netmask)", "documentation": "    \"\"\"This function allows you to check if an IP belongs to a network subnet\n\n    Example: returns True if ip = 192.168.1.1 and net = 192.168.1.0/24\n             returns False if ip = 192.168.1.1 and net = 192.168.100.0/24\n\n    :rtype: bool\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 691, "code": "def dotted_netmask(mask):\n    bits = 0xFFFFFFFF ^ (1 << 32 - mask) - 1\n    return socket.inet_ntoa(struct.pack(\">I\", bits))", "documentation": "    \"\"\"Converts mask from /xx format to xxx.xxx.xxx.xxx\n\n    Example: if mask is 24 function returns 255.255.255.0\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 702, "code": "def is_ipv4_address(string_ip):\n    try:\n        socket.inet_aton(string_ip)\n    except OSError:\n        return False\n    return True", "documentation": "    \"\"\"\n    :rtype: bool\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 713, "code": "def is_valid_cidr(string_network):\n    if string_network.count(\"/\") == 1:\n        try:\n            mask = int(string_network.split(\"/\")[1])\n        except ValueError:\n            return False\n        if mask < 1 or mask > 32:\n            return False\n        try:\n            socket.inet_aton(string_network.split(\"/\")[0])\n        except OSError:", "documentation": "    \"\"\"\n    Very simple check of the cidr format in no_proxy variable.\n\n    :rtype: bool\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 738, "code": "def set_environ(env_name, value):\n    value_changed = value is not None\n    if value_changed:\n        old_value = os.environ.get(env_name)\n        os.environ[env_name] = value\n    try:\n        yield\n    finally:\n        if value_changed:\n            if old_value is None:\n                del os.environ[env_name]", "documentation": "    \"\"\"Set the environment variable 'env_name' to 'value'\n\n    Save previous value, yield, and then restore the previous value stored in\n    the environment variable 'env_name'.\n\n    If 'value' is None, do nothing\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 759, "code": "def should_bypass_proxies(url, no_proxy):", "documentation": "    \"\"\"\n    Returns whether we should bypass proxies or not.\n\n    :rtype: bool\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 820, "code": "def get_environ_proxies(url, no_proxy=None):\n    if should_bypass_proxies(url, no_proxy=no_proxy):\n        return {}\n    else:\n        return getproxies()", "documentation": "    \"\"\"\n    Return a dict of environment proxies.\n\n    :rtype: dict\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 832, "code": "def select_proxy(url, proxies):\n    proxies = proxies or {}\n    urlparts = urlparse(url)\n    if urlparts.hostname is None:\n        return proxies.get(urlparts.scheme, proxies.get(\"all\"))\n    proxy_keys = [\n        urlparts.scheme + \"://\" + urlparts.hostname,\n        urlparts.scheme,\n        \"all://\" + urlparts.hostname,\n        \"all\",\n    ]", "documentation": "    \"\"\"Select a proxy for the url, if applicable.\n\n    :param url: The url being for the request\n    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 858, "code": "def resolve_proxies(request, proxies, trust_env=True):\n    proxies = proxies if proxies is not None else {}\n    url = request.url\n    scheme = urlparse(url).scheme\n    no_proxy = proxies.get(\"no_proxy\")\n    new_proxies = proxies.copy()\n    if trust_env and not should_bypass_proxies(url, no_proxy=no_proxy):\n        environ_proxies = get_environ_proxies(url, no_proxy=no_proxy)\n        proxy = environ_proxies.get(scheme, environ_proxies.get(\"all\"))\n        if proxy:\n            new_proxies.setdefault(scheme, proxy)", "documentation": "    \"\"\"This method takes proxy information from a request and configuration\n    input to resolve a mapping of target proxies. This will consider settings\n    such as NO_PROXY to strip proxy configurations.\n\n    :param request: Request or PreparedRequest\n    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs\n    :param trust_env: Boolean declaring whether to trust environment configs\n\n    :rtype: dict\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 885, "code": "def default_user_agent(name=\"python-requests\"):\n    return f\"{name}/{__version__}\"", "documentation": "    \"\"\"\n    Return a string representing the default user agent.\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 894, "code": "def default_headers():\n    return CaseInsensitiveDict(\n        {\n            \"User-Agent\": default_user_agent(),\n            \"Accept-Encoding\": DEFAULT_ACCEPT_ENCODING,\n            \"Accept\": \"*/*\",\n            \"Connection\": \"keep-alive\",\n        }\n    )", "documentation": "    \"\"\"\n    :rtype: requests.structures.CaseInsensitiveDict\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 908, "code": "def parse_header_links(value):\n    links = []\n    replace_chars = \" '\\\"\"\n    value = value.strip(replace_chars)\n    if not value:\n        return links\n    for val in re.split(\", *<\", value):\n        try:\n            url, params = val.split(\";\", 1)\n        except ValueError:\n            url, params = val, \"\"", "documentation": "    \"\"\"Return a list of parsed link headers proxies.\n\n    i.e. Link: <http:/.../front.jpeg>; rel=front; type=\"image/jpeg\",<http://.../back.jpeg>; rel=back;type=\"image/jpeg\"\n\n    :rtype: list\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 951, "code": "def guess_json_utf(data):\n    sample = data[:4]\n    if sample in (codecs.BOM_UTF32_LE, codecs.BOM_UTF32_BE):\n        return \"utf-32\"  # BOM included\n    if sample[:3] == codecs.BOM_UTF8:\n        return \"utf-8-sig\"  # BOM included, MS style (discouraged)\n    if sample[:2] in (codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE):\n        return \"utf-16\"  # BOM included\n    nullcount = sample.count(_null)\n    if nullcount == 0:\n        return \"utf-8\"", "documentation": "    \"\"\"\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 983, "code": "def prepend_scheme_if_needed(url, new_scheme):\n    parsed = parse_url(url)\n    scheme, auth, host, port, path, query, fragment = parsed\n    netloc = parsed.netloc\n    if not netloc:\n        netloc, path = path, netloc\n    if auth:\n        netloc = \"@\".join([auth, netloc])\n    if scheme is None:\n        scheme = new_scheme\n    if path is None:", "documentation": "    \"\"\"Given a URL that may or may not have a scheme, prepend the given scheme.\n    Does not replace a present scheme with the one provided as an argument.\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 1012, "code": "def get_auth_from_url(url):\n    parsed = urlparse(url)\n    try:\n        auth = (unquote(parsed.username), unquote(parsed.password))\n    except (AttributeError, TypeError):\n        auth = (\"\", \"\")\n    return auth", "documentation": "    \"\"\"Given a url with authentication components, extract them into a tuple of\n    username,password.\n\n    :rtype: (str,str)\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 1028, "code": "def check_header_validity(header):\n    name, value = header\n    _validate_header_part(header, name, 0)\n    _validate_header_part(header, value, 1)", "documentation": "    \"\"\"Verifies that header parts don't contain leading whitespace\n    reserved characters, or return characters.\n\n    :param header: tuple, in the format (name, value).\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 1058, "code": "def urldefragauth(url):\n    scheme, netloc, path, params, query, fragment = urlparse(url)\n    if not netloc:\n        netloc, path = path, netloc\n    netloc = netloc.rsplit(\"@\", 1)[-1]\n    return urlunparse((scheme, netloc, path, params, query, \"\"))", "documentation": "    \"\"\"\n    Given a url remove the fragment and the authentication part.\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 1075, "code": "def rewind_body(prepared_request):\n    body_seek = getattr(prepared_request.body, \"seek\", None)\n    if body_seek is not None and isinstance(\n        prepared_request._body_position, integer_types\n    ):\n        try:\n            body_seek(prepared_request._body_position)\n        except OSError:\n            raise UnrewindableBodyError(\n                \"An error occurred when rewinding request body for redirect.\"\n            )", "documentation": "    \"\"\"Move file pointer back to its recorded starting position\n    so it can be read again on redirect.\n    \"\"\""}]}
{"repository": "psf/requests", "commit_sha": "15849947ec4b8b7c3c136a47e950499edfd36921", "commit_message": "fix docstring typo: a -> as", "commit_date": "2023-11-22T11:31:53+00:00", "author": "Elliot Ford", "file": "src/requests/utils.py", "patch": "@@ -859,7 +859,7 @@ def select_proxy(url, proxies):\n def resolve_proxies(request, proxies, trust_env=True):\n     \"\"\"This method takes proxy information from a request and configuration\n     input to resolve a mapping of target proxies. This will consider settings\n-    such a NO_PROXY to strip proxy configurations.\n+    such as NO_PROXY to strip proxy configurations.\n \n     :param request: Request or PreparedRequest\n     :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs", "before_segments": [{"filename": "src/requests/utils.py", "start_line": 111, "code": "    def proxy_bypass(host):  # noqa\n        if getproxies_environment():\n            return proxy_bypass_environment(host)\n        else:\n            return proxy_bypass_registry(host)", "documentation": "        \"\"\"Return True, if the host should be bypassed.\n\n        Checks proxy settings gathered from the environment, if specified,\n        or the registry.\n        \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 123, "code": "def dict_to_sequence(d):\n    if hasattr(d, \"items\"):\n        d = d.items()\n    return d", "documentation": "    \"\"\"Returns an internal sequence dictionary update.\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 198, "code": "def get_netrc_auth(url, raise_errors=False):\n    netrc_file = os.environ.get(\"NETRC\")\n    if netrc_file is not None:\n        netrc_locations = (netrc_file,)\n    else:\n        netrc_locations = (f\"~/{f}\" for f in NETRC_FILES)\n    try:\n        from netrc import NetrcParseError, netrc\n        netrc_path = None\n        for f in netrc_locations:\n            try:", "documentation": "    \"\"\"Returns the Requests tuple auth for a given url from netrc.\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 255, "code": "def guess_filename(obj):\n    name = getattr(obj, \"name\", None)\n    if name and isinstance(name, basestring) and name[0] != \"<\" and name[-1] != \">\":\n        return os.path.basename(name)", "documentation": "    \"\"\"Tries to guess the filename of the given object.\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 262, "code": "def extract_zipped_paths(path):\n    if os.path.exists(path):\n        return path\n    archive, member = os.path.split(path)\n    while archive and not os.path.exists(archive):\n        archive, prefix = os.path.split(archive)\n        if not prefix:\n            break\n        member = \"/\".join([prefix, member])\n    if not zipfile.is_zipfile(archive):\n        return path", "documentation": "    \"\"\"Replace nonexistent paths that look like they refer to a member of a zip\n    archive with the location of an extracted copy of the target, or else\n    just return the provided path unchanged.\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 300, "code": "def atomic_open(filename):\n    tmp_descriptor, tmp_name = tempfile.mkstemp(dir=os.path.dirname(filename))\n    try:\n        with os.fdopen(tmp_descriptor, \"wb\") as tmp_handler:\n            yield tmp_handler\n        os.replace(tmp_name, filename)\n    except BaseException:\n        os.remove(tmp_name)\n        raise", "documentation": "    \"\"\"Write a file to the disk in an atomic fashion\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 312, "code": "def from_key_val_list(value):\n    if value is None:\n        return None\n    if isinstance(value, (str, bytes, bool, int)):\n        raise ValueError(\"cannot encode objects that are not 2-tuples\")\n    return OrderedDict(value)", "documentation": "    \"\"\"Take an object and test to see if it can be represented as a\n    dictionary. Unless it can not be represented as such, return an\n    OrderedDict, e.g.,\n\n    ::\n\n        >>> from_key_val_list([('key', 'val')])\n        OrderedDict([('key', 'val')])\n        >>> from_key_val_list('string')\n        Traceback (most recent call last):\n        ...\n        ValueError: cannot encode objects that are not 2-tuples\n        >>> from_key_val_list({'key': 'val'})\n        OrderedDict([('key', 'val')])\n\n    :rtype: OrderedDict\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 339, "code": "def to_key_val_list(value):\n    if value is None:\n        return None\n    if isinstance(value, (str, bytes, bool, int)):\n        raise ValueError(\"cannot encode objects that are not 2-tuples\")\n    if isinstance(value, Mapping):\n        value = value.items()\n    return list(value)", "documentation": "    \"\"\"Take an object and test to see if it can be represented as a\n    dictionary. If it can be, return a list of tuples, e.g.,\n\n    ::\n\n        >>> to_key_val_list([('key', 'val')])\n        [('key', 'val')]\n        >>> to_key_val_list({'key': 'val'})\n        [('key', 'val')]\n        >>> to_key_val_list('string')\n        Traceback (most recent call last):\n        ...\n        ValueError: cannot encode objects that are not 2-tuples\n\n    :rtype: list\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 369, "code": "def parse_list_header(value):\n    result = []\n    for item in _parse_list_header(value):\n        if item[:1] == item[-1:] == '\"':\n            item = unquote_header_value(item[1:-1])\n        result.append(item)\n    return result", "documentation": "    \"\"\"Parse lists as described by RFC 2068 Section 2.\n\n    In particular, parse comma-separated lists where the elements of\n    the list may include quoted-strings.  A quoted-string could\n    contain a comma.  A non-quoted string could have quotes in the\n    middle.  Quotes are removed automatically after parsing.\n\n    It basically works like :func:`parse_set_header` just that items\n    may appear multiple times and case sensitivity is preserved.\n\n    The return value is a standard :class:`list`:\n\n    >>> parse_list_header('token, \"quoted value\"')\n    ['token', 'quoted value']\n\n    To create a header from the :class:`list` again, use the\n    :func:`dump_header` function.\n\n    :param value: a string with a list header.\n    :return: :class:`list`\n    :rtype: list\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 401, "code": "def parse_dict_header(value):\n    result = {}\n    for item in _parse_list_header(value):\n        if \"=\" not in item:\n            result[item] = None\n            continue\n        name, value = item.split(\"=\", 1)\n        if value[:1] == value[-1:] == '\"':\n            value = unquote_header_value(value[1:-1])\n        result[name] = value\n    return result", "documentation": "    \"\"\"Parse lists of key, value pairs as described by RFC 2068 Section 2 and\n    convert them into a python dict:\n\n    >>> d = parse_dict_header('foo=\"is a fish\", bar=\"as well\"')\n    >>> type(d) is dict\n    True\n    >>> sorted(d.items())\n    [('bar', 'as well'), ('foo', 'is a fish')]\n\n    If there is no value for a key it will be `None`:\n\n    >>> parse_dict_header('key_without_value')\n    {'key_without_value': None}\n\n    To create a header from the :class:`dict` again, use the\n    :func:`dump_header` function.\n\n    :param value: a string with a dict header.\n    :return: :class:`dict`\n    :rtype: dict\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 461, "code": "def dict_from_cookiejar(cj):\n    cookie_dict = {cookie.name: cookie.value for cookie in cj}\n    return cookie_dict", "documentation": "    \"\"\"Returns a key/value dictionary from a CookieJar.\n\n    :param cj: CookieJar object to extract cookies from.\n    :rtype: dict\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 472, "code": "def add_dict_to_cookiejar(cj, cookie_dict):\n    return cookiejar_from_dict(cookie_dict, cj)", "documentation": "    \"\"\"Returns a CookieJar from a key/value dictionary.\n\n    :param cj: CookieJar to insert cookies into.\n    :param cookie_dict: Dict of key/values to insert into CookieJar.\n    :rtype: CookieJar\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 483, "code": "def get_encodings_from_content(content):\n    warnings.warn(\n        (\n            \"In requests 3.0, get_encodings_from_content will be removed. For \"\n            \"more information, please see the discussion on issue #2266. (This\"\n            \" warning should only appear once.)\"\n        ),\n        DeprecationWarning,\n    )\n    charset_re = re.compile(r'<meta.*?charset=[\"\\']*(.+?)[\"\\'>]', flags=re.I)\n    pragma_re = re.compile(r'<meta.*?content=[\"\\']*;?charset=(.+?)[\"\\'>]', flags=re.I)", "documentation": "    \"\"\"Returns encodings from given content string.\n\n    :param content: bytestring to extract encodings from.\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 508, "code": "def _parse_content_type_header(header):\n    tokens = header.split(\";\")\n    content_type, params = tokens[0].strip(), tokens[1:]\n    params_dict = {}\n    items_to_strip = \"\\\"' \"\n    for param in params:\n        param = param.strip()\n        if param:\n            key, value = param, True\n            index_of_equals = param.find(\"=\")\n            if index_of_equals != -1:", "documentation": "    \"\"\"Returns content type and parameters from given header\n\n    :param header: string\n    :return: tuple containing content type and dictionary of\n         parameters\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 533, "code": "def get_encoding_from_headers(headers):\n    content_type = headers.get(\"content-type\")\n    if not content_type:\n        return None\n    content_type, params = _parse_content_type_header(content_type)\n    if \"charset\" in params:\n        return params[\"charset\"].strip(\"'\\\"\")\n    if \"text\" in content_type:\n        return \"ISO-8859-1\"\n    if \"application/json\" in content_type:\n        return \"utf-8\"", "documentation": "    \"\"\"Returns encodings from given HTTP Header Dict.\n\n    :param headers: dictionary to extract encoding from.\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 558, "code": "def stream_decode_response_unicode(iterator, r):\n    if r.encoding is None:\n        yield from iterator\n        return\n    decoder = codecs.getincrementaldecoder(r.encoding)(errors=\"replace\")\n    for chunk in iterator:\n        rv = decoder.decode(chunk)\n        if rv:\n            yield rv\n    rv = decoder.decode(b\"\", final=True)\n    if rv:", "documentation": "    \"\"\"Stream decodes an iterator.\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 575, "code": "def iter_slices(string, slice_length):\n    pos = 0\n    if slice_length is None or slice_length <= 0:\n        slice_length = len(string)\n    while pos < len(string):\n        yield string[pos : pos + slice_length]\n        pos += slice_length", "documentation": "    \"\"\"Iterate over slices of a string.\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 585, "code": "def get_unicode_from_response(r):\n    warnings.warn(\n        (\n            \"In requests 3.0, get_unicode_from_response will be removed. For \"\n            \"more information, please see the discussion on issue #2266. (This\"\n            \" warning should only appear once.)\"\n        ),\n        DeprecationWarning,\n    )\n    tried_encodings = []\n    encoding = get_encoding_from_headers(r.headers)", "documentation": "    \"\"\"Returns the requested content back in unicode.\n\n    :param r: Response object to get unicode content from.\n\n    Tried:\n\n    1. charset from content-type\n    2. fall back and replace all unicode characters\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 630, "code": "def unquote_unreserved(uri):\n    parts = uri.split(\"%\")\n    for i in range(1, len(parts)):\n        h = parts[i][0:2]\n        if len(h) == 2 and h.isalnum():\n            try:\n                c = chr(int(h, 16))\n            except ValueError:\n                raise InvalidURL(f\"Invalid percent-escape sequence: '{h}'\")\n            if c in UNRESERVED_SET:\n                parts[i] = c + parts[i][2:]", "documentation": "    \"\"\"Un-escape any percent-escape sequences in a URI that are unreserved\n    characters. This leaves all reserved, illegal and non-ASCII bytes encoded.\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 654, "code": "def requote_uri(uri):\n    safe_with_percent = \"!#$%&'()*+,/:;=?@[]~\"\n    safe_without_percent = \"!#$&'()*+,/:;=?@[]~\"\n    try:\n        return quote(unquote_unreserved(uri), safe=safe_with_percent)\n    except InvalidURL:\n        return quote(uri, safe=safe_without_percent)", "documentation": "    \"\"\"Re-quote the given URI.\n\n    This function passes the given URI through an unquote/quote cycle to\n    ensure that it is fully and consistently quoted.\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 676, "code": "def address_in_network(ip, net):\n    ipaddr = struct.unpack(\"=L\", socket.inet_aton(ip))[0]\n    netaddr, bits = net.split(\"/\")\n    netmask = struct.unpack(\"=L\", socket.inet_aton(dotted_netmask(int(bits))))[0]\n    network = struct.unpack(\"=L\", socket.inet_aton(netaddr))[0] & netmask\n    return (ipaddr & netmask) == (network & netmask)", "documentation": "    \"\"\"This function allows you to check if an IP belongs to a network subnet\n\n    Example: returns True if ip = 192.168.1.1 and net = 192.168.1.0/24\n             returns False if ip = 192.168.1.1 and net = 192.168.100.0/24\n\n    :rtype: bool\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 691, "code": "def dotted_netmask(mask):\n    bits = 0xFFFFFFFF ^ (1 << 32 - mask) - 1\n    return socket.inet_ntoa(struct.pack(\">I\", bits))", "documentation": "    \"\"\"Converts mask from /xx format to xxx.xxx.xxx.xxx\n\n    Example: if mask is 24 function returns 255.255.255.0\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 702, "code": "def is_ipv4_address(string_ip):\n    try:\n        socket.inet_aton(string_ip)\n    except OSError:\n        return False\n    return True", "documentation": "    \"\"\"\n    :rtype: bool\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 713, "code": "def is_valid_cidr(string_network):\n    if string_network.count(\"/\") == 1:\n        try:\n            mask = int(string_network.split(\"/\")[1])\n        except ValueError:\n            return False\n        if mask < 1 or mask > 32:\n            return False\n        try:\n            socket.inet_aton(string_network.split(\"/\")[0])\n        except OSError:", "documentation": "    \"\"\"\n    Very simple check of the cidr format in no_proxy variable.\n\n    :rtype: bool\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 738, "code": "def set_environ(env_name, value):\n    value_changed = value is not None\n    if value_changed:\n        old_value = os.environ.get(env_name)\n        os.environ[env_name] = value\n    try:\n        yield\n    finally:\n        if value_changed:\n            if old_value is None:\n                del os.environ[env_name]", "documentation": "    \"\"\"Set the environment variable 'env_name' to 'value'\n\n    Save previous value, yield, and then restore the previous value stored in\n    the environment variable 'env_name'.\n\n    If 'value' is None, do nothing\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 759, "code": "def should_bypass_proxies(url, no_proxy):", "documentation": "    \"\"\"\n    Returns whether we should bypass proxies or not.\n\n    :rtype: bool\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 820, "code": "def get_environ_proxies(url, no_proxy=None):\n    if should_bypass_proxies(url, no_proxy=no_proxy):\n        return {}\n    else:\n        return getproxies()", "documentation": "    \"\"\"\n    Return a dict of environment proxies.\n\n    :rtype: dict\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 832, "code": "def select_proxy(url, proxies):\n    proxies = proxies or {}\n    urlparts = urlparse(url)\n    if urlparts.hostname is None:\n        return proxies.get(urlparts.scheme, proxies.get(\"all\"))\n    proxy_keys = [\n        urlparts.scheme + \"://\" + urlparts.hostname,\n        urlparts.scheme,\n        \"all://\" + urlparts.hostname,\n        \"all\",\n    ]", "documentation": "    \"\"\"Select a proxy for the url, if applicable.\n\n    :param url: The url being for the request\n    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 858, "code": "def resolve_proxies(request, proxies, trust_env=True):\n    proxies = proxies if proxies is not None else {}\n    url = request.url\n    scheme = urlparse(url).scheme\n    no_proxy = proxies.get(\"no_proxy\")\n    new_proxies = proxies.copy()\n    if trust_env and not should_bypass_proxies(url, no_proxy=no_proxy):\n        environ_proxies = get_environ_proxies(url, no_proxy=no_proxy)\n        proxy = environ_proxies.get(scheme, environ_proxies.get(\"all\"))\n        if proxy:\n            new_proxies.setdefault(scheme, proxy)", "documentation": "    \"\"\"This method takes proxy information from a request and configuration\n    input to resolve a mapping of target proxies. This will consider settings\n    such a NO_PROXY to strip proxy configurations.\n\n    :param request: Request or PreparedRequest\n    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs\n    :param trust_env: Boolean declaring whether to trust environment configs\n\n    :rtype: dict\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 885, "code": "def default_user_agent(name=\"python-requests\"):\n    return f\"{name}/{__version__}\"", "documentation": "    \"\"\"\n    Return a string representing the default user agent.\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 894, "code": "def default_headers():\n    return CaseInsensitiveDict(\n        {\n            \"User-Agent\": default_user_agent(),\n            \"Accept-Encoding\": DEFAULT_ACCEPT_ENCODING,\n            \"Accept\": \"*/*\",\n            \"Connection\": \"keep-alive\",\n        }\n    )", "documentation": "    \"\"\"\n    :rtype: requests.structures.CaseInsensitiveDict\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 908, "code": "def parse_header_links(value):\n    links = []\n    replace_chars = \" '\\\"\"\n    value = value.strip(replace_chars)\n    if not value:\n        return links\n    for val in re.split(\", *<\", value):\n        try:\n            url, params = val.split(\";\", 1)\n        except ValueError:\n            url, params = val, \"\"", "documentation": "    \"\"\"Return a list of parsed link headers proxies.\n\n    i.e. Link: <http:/.../front.jpeg>; rel=front; type=\"image/jpeg\",<http://.../back.jpeg>; rel=back;type=\"image/jpeg\"\n\n    :rtype: list\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 951, "code": "def guess_json_utf(data):\n    sample = data[:4]\n    if sample in (codecs.BOM_UTF32_LE, codecs.BOM_UTF32_BE):\n        return \"utf-32\"  # BOM included\n    if sample[:3] == codecs.BOM_UTF8:\n        return \"utf-8-sig\"  # BOM included, MS style (discouraged)\n    if sample[:2] in (codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE):\n        return \"utf-16\"  # BOM included\n    nullcount = sample.count(_null)\n    if nullcount == 0:\n        return \"utf-8\"", "documentation": "    \"\"\"\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 983, "code": "def prepend_scheme_if_needed(url, new_scheme):\n    parsed = parse_url(url)\n    scheme, auth, host, port, path, query, fragment = parsed\n    netloc = parsed.netloc\n    if not netloc:\n        netloc, path = path, netloc\n    if auth:\n        netloc = \"@\".join([auth, netloc])\n    if scheme is None:\n        scheme = new_scheme\n    if path is None:", "documentation": "    \"\"\"Given a URL that may or may not have a scheme, prepend the given scheme.\n    Does not replace a present scheme with the one provided as an argument.\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 1012, "code": "def get_auth_from_url(url):\n    parsed = urlparse(url)\n    try:\n        auth = (unquote(parsed.username), unquote(parsed.password))\n    except (AttributeError, TypeError):\n        auth = (\"\", \"\")\n    return auth", "documentation": "    \"\"\"Given a url with authentication components, extract them into a tuple of\n    username,password.\n\n    :rtype: (str,str)\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 1028, "code": "def check_header_validity(header):\n    name, value = header\n    _validate_header_part(header, name, 0)\n    _validate_header_part(header, value, 1)", "documentation": "    \"\"\"Verifies that header parts don't contain leading whitespace\n    reserved characters, or return characters.\n\n    :param header: tuple, in the format (name, value).\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 1058, "code": "def urldefragauth(url):\n    scheme, netloc, path, params, query, fragment = urlparse(url)\n    if not netloc:\n        netloc, path = path, netloc\n    netloc = netloc.rsplit(\"@\", 1)[-1]\n    return urlunparse((scheme, netloc, path, params, query, \"\"))", "documentation": "    \"\"\"\n    Given a url remove the fragment and the authentication part.\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 1075, "code": "def rewind_body(prepared_request):\n    body_seek = getattr(prepared_request.body, \"seek\", None)\n    if body_seek is not None and isinstance(\n        prepared_request._body_position, integer_types\n    ):\n        try:\n            body_seek(prepared_request._body_position)\n        except OSError:\n            raise UnrewindableBodyError(\n                \"An error occurred when rewinding request body for redirect.\"\n            )", "documentation": "    \"\"\"Move file pointer back to its recorded starting position\n    so it can be read again on redirect.\n    \"\"\""}], "after_segments": [{"filename": "src/requests/utils.py", "start_line": 111, "code": "    def proxy_bypass(host):  # noqa\n        if getproxies_environment():\n            return proxy_bypass_environment(host)\n        else:\n            return proxy_bypass_registry(host)", "documentation": "        \"\"\"Return True, if the host should be bypassed.\n\n        Checks proxy settings gathered from the environment, if specified,\n        or the registry.\n        \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 123, "code": "def dict_to_sequence(d):\n    if hasattr(d, \"items\"):\n        d = d.items()\n    return d", "documentation": "    \"\"\"Returns an internal sequence dictionary update.\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 198, "code": "def get_netrc_auth(url, raise_errors=False):\n    netrc_file = os.environ.get(\"NETRC\")\n    if netrc_file is not None:\n        netrc_locations = (netrc_file,)\n    else:\n        netrc_locations = (f\"~/{f}\" for f in NETRC_FILES)\n    try:\n        from netrc import NetrcParseError, netrc\n        netrc_path = None\n        for f in netrc_locations:\n            try:", "documentation": "    \"\"\"Returns the Requests tuple auth for a given url from netrc.\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 255, "code": "def guess_filename(obj):\n    name = getattr(obj, \"name\", None)\n    if name and isinstance(name, basestring) and name[0] != \"<\" and name[-1] != \">\":\n        return os.path.basename(name)", "documentation": "    \"\"\"Tries to guess the filename of the given object.\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 262, "code": "def extract_zipped_paths(path):\n    if os.path.exists(path):\n        return path\n    archive, member = os.path.split(path)\n    while archive and not os.path.exists(archive):\n        archive, prefix = os.path.split(archive)\n        if not prefix:\n            break\n        member = \"/\".join([prefix, member])\n    if not zipfile.is_zipfile(archive):\n        return path", "documentation": "    \"\"\"Replace nonexistent paths that look like they refer to a member of a zip\n    archive with the location of an extracted copy of the target, or else\n    just return the provided path unchanged.\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 300, "code": "def atomic_open(filename):\n    tmp_descriptor, tmp_name = tempfile.mkstemp(dir=os.path.dirname(filename))\n    try:\n        with os.fdopen(tmp_descriptor, \"wb\") as tmp_handler:\n            yield tmp_handler\n        os.replace(tmp_name, filename)\n    except BaseException:\n        os.remove(tmp_name)\n        raise", "documentation": "    \"\"\"Write a file to the disk in an atomic fashion\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 312, "code": "def from_key_val_list(value):\n    if value is None:\n        return None\n    if isinstance(value, (str, bytes, bool, int)):\n        raise ValueError(\"cannot encode objects that are not 2-tuples\")\n    return OrderedDict(value)", "documentation": "    \"\"\"Take an object and test to see if it can be represented as a\n    dictionary. Unless it can not be represented as such, return an\n    OrderedDict, e.g.,\n\n    ::\n\n        >>> from_key_val_list([('key', 'val')])\n        OrderedDict([('key', 'val')])\n        >>> from_key_val_list('string')\n        Traceback (most recent call last):\n        ...\n        ValueError: cannot encode objects that are not 2-tuples\n        >>> from_key_val_list({'key': 'val'})\n        OrderedDict([('key', 'val')])\n\n    :rtype: OrderedDict\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 339, "code": "def to_key_val_list(value):\n    if value is None:\n        return None\n    if isinstance(value, (str, bytes, bool, int)):\n        raise ValueError(\"cannot encode objects that are not 2-tuples\")\n    if isinstance(value, Mapping):\n        value = value.items()\n    return list(value)", "documentation": "    \"\"\"Take an object and test to see if it can be represented as a\n    dictionary. If it can be, return a list of tuples, e.g.,\n\n    ::\n\n        >>> to_key_val_list([('key', 'val')])\n        [('key', 'val')]\n        >>> to_key_val_list({'key': 'val'})\n        [('key', 'val')]\n        >>> to_key_val_list('string')\n        Traceback (most recent call last):\n        ...\n        ValueError: cannot encode objects that are not 2-tuples\n\n    :rtype: list\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 369, "code": "def parse_list_header(value):\n    result = []\n    for item in _parse_list_header(value):\n        if item[:1] == item[-1:] == '\"':\n            item = unquote_header_value(item[1:-1])\n        result.append(item)\n    return result", "documentation": "    \"\"\"Parse lists as described by RFC 2068 Section 2.\n\n    In particular, parse comma-separated lists where the elements of\n    the list may include quoted-strings.  A quoted-string could\n    contain a comma.  A non-quoted string could have quotes in the\n    middle.  Quotes are removed automatically after parsing.\n\n    It basically works like :func:`parse_set_header` just that items\n    may appear multiple times and case sensitivity is preserved.\n\n    The return value is a standard :class:`list`:\n\n    >>> parse_list_header('token, \"quoted value\"')\n    ['token', 'quoted value']\n\n    To create a header from the :class:`list` again, use the\n    :func:`dump_header` function.\n\n    :param value: a string with a list header.\n    :return: :class:`list`\n    :rtype: list\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 401, "code": "def parse_dict_header(value):\n    result = {}\n    for item in _parse_list_header(value):\n        if \"=\" not in item:\n            result[item] = None\n            continue\n        name, value = item.split(\"=\", 1)\n        if value[:1] == value[-1:] == '\"':\n            value = unquote_header_value(value[1:-1])\n        result[name] = value\n    return result", "documentation": "    \"\"\"Parse lists of key, value pairs as described by RFC 2068 Section 2 and\n    convert them into a python dict:\n\n    >>> d = parse_dict_header('foo=\"is a fish\", bar=\"as well\"')\n    >>> type(d) is dict\n    True\n    >>> sorted(d.items())\n    [('bar', 'as well'), ('foo', 'is a fish')]\n\n    If there is no value for a key it will be `None`:\n\n    >>> parse_dict_header('key_without_value')\n    {'key_without_value': None}\n\n    To create a header from the :class:`dict` again, use the\n    :func:`dump_header` function.\n\n    :param value: a string with a dict header.\n    :return: :class:`dict`\n    :rtype: dict\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 461, "code": "def dict_from_cookiejar(cj):\n    cookie_dict = {cookie.name: cookie.value for cookie in cj}\n    return cookie_dict", "documentation": "    \"\"\"Returns a key/value dictionary from a CookieJar.\n\n    :param cj: CookieJar object to extract cookies from.\n    :rtype: dict\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 472, "code": "def add_dict_to_cookiejar(cj, cookie_dict):\n    return cookiejar_from_dict(cookie_dict, cj)", "documentation": "    \"\"\"Returns a CookieJar from a key/value dictionary.\n\n    :param cj: CookieJar to insert cookies into.\n    :param cookie_dict: Dict of key/values to insert into CookieJar.\n    :rtype: CookieJar\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 483, "code": "def get_encodings_from_content(content):\n    warnings.warn(\n        (\n            \"In requests 3.0, get_encodings_from_content will be removed. For \"\n            \"more information, please see the discussion on issue #2266. (This\"\n            \" warning should only appear once.)\"\n        ),\n        DeprecationWarning,\n    )\n    charset_re = re.compile(r'<meta.*?charset=[\"\\']*(.+?)[\"\\'>]', flags=re.I)\n    pragma_re = re.compile(r'<meta.*?content=[\"\\']*;?charset=(.+?)[\"\\'>]', flags=re.I)", "documentation": "    \"\"\"Returns encodings from given content string.\n\n    :param content: bytestring to extract encodings from.\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 508, "code": "def _parse_content_type_header(header):\n    tokens = header.split(\";\")\n    content_type, params = tokens[0].strip(), tokens[1:]\n    params_dict = {}\n    items_to_strip = \"\\\"' \"\n    for param in params:\n        param = param.strip()\n        if param:\n            key, value = param, True\n            index_of_equals = param.find(\"=\")\n            if index_of_equals != -1:", "documentation": "    \"\"\"Returns content type and parameters from given header\n\n    :param header: string\n    :return: tuple containing content type and dictionary of\n         parameters\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 533, "code": "def get_encoding_from_headers(headers):\n    content_type = headers.get(\"content-type\")\n    if not content_type:\n        return None\n    content_type, params = _parse_content_type_header(content_type)\n    if \"charset\" in params:\n        return params[\"charset\"].strip(\"'\\\"\")\n    if \"text\" in content_type:\n        return \"ISO-8859-1\"\n    if \"application/json\" in content_type:\n        return \"utf-8\"", "documentation": "    \"\"\"Returns encodings from given HTTP Header Dict.\n\n    :param headers: dictionary to extract encoding from.\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 558, "code": "def stream_decode_response_unicode(iterator, r):\n    if r.encoding is None:\n        yield from iterator\n        return\n    decoder = codecs.getincrementaldecoder(r.encoding)(errors=\"replace\")\n    for chunk in iterator:\n        rv = decoder.decode(chunk)\n        if rv:\n            yield rv\n    rv = decoder.decode(b\"\", final=True)\n    if rv:", "documentation": "    \"\"\"Stream decodes an iterator.\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 575, "code": "def iter_slices(string, slice_length):\n    pos = 0\n    if slice_length is None or slice_length <= 0:\n        slice_length = len(string)\n    while pos < len(string):\n        yield string[pos : pos + slice_length]\n        pos += slice_length", "documentation": "    \"\"\"Iterate over slices of a string.\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 585, "code": "def get_unicode_from_response(r):\n    warnings.warn(\n        (\n            \"In requests 3.0, get_unicode_from_response will be removed. For \"\n            \"more information, please see the discussion on issue #2266. (This\"\n            \" warning should only appear once.)\"\n        ),\n        DeprecationWarning,\n    )\n    tried_encodings = []\n    encoding = get_encoding_from_headers(r.headers)", "documentation": "    \"\"\"Returns the requested content back in unicode.\n\n    :param r: Response object to get unicode content from.\n\n    Tried:\n\n    1. charset from content-type\n    2. fall back and replace all unicode characters\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 630, "code": "def unquote_unreserved(uri):\n    parts = uri.split(\"%\")\n    for i in range(1, len(parts)):\n        h = parts[i][0:2]\n        if len(h) == 2 and h.isalnum():\n            try:\n                c = chr(int(h, 16))\n            except ValueError:\n                raise InvalidURL(f\"Invalid percent-escape sequence: '{h}'\")\n            if c in UNRESERVED_SET:\n                parts[i] = c + parts[i][2:]", "documentation": "    \"\"\"Un-escape any percent-escape sequences in a URI that are unreserved\n    characters. This leaves all reserved, illegal and non-ASCII bytes encoded.\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 654, "code": "def requote_uri(uri):\n    safe_with_percent = \"!#$%&'()*+,/:;=?@[]~\"\n    safe_without_percent = \"!#$&'()*+,/:;=?@[]~\"\n    try:\n        return quote(unquote_unreserved(uri), safe=safe_with_percent)\n    except InvalidURL:\n        return quote(uri, safe=safe_without_percent)", "documentation": "    \"\"\"Re-quote the given URI.\n\n    This function passes the given URI through an unquote/quote cycle to\n    ensure that it is fully and consistently quoted.\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 676, "code": "def address_in_network(ip, net):\n    ipaddr = struct.unpack(\"=L\", socket.inet_aton(ip))[0]\n    netaddr, bits = net.split(\"/\")\n    netmask = struct.unpack(\"=L\", socket.inet_aton(dotted_netmask(int(bits))))[0]\n    network = struct.unpack(\"=L\", socket.inet_aton(netaddr))[0] & netmask\n    return (ipaddr & netmask) == (network & netmask)", "documentation": "    \"\"\"This function allows you to check if an IP belongs to a network subnet\n\n    Example: returns True if ip = 192.168.1.1 and net = 192.168.1.0/24\n             returns False if ip = 192.168.1.1 and net = 192.168.100.0/24\n\n    :rtype: bool\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 691, "code": "def dotted_netmask(mask):\n    bits = 0xFFFFFFFF ^ (1 << 32 - mask) - 1\n    return socket.inet_ntoa(struct.pack(\">I\", bits))", "documentation": "    \"\"\"Converts mask from /xx format to xxx.xxx.xxx.xxx\n\n    Example: if mask is 24 function returns 255.255.255.0\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 702, "code": "def is_ipv4_address(string_ip):\n    try:\n        socket.inet_aton(string_ip)\n    except OSError:\n        return False\n    return True", "documentation": "    \"\"\"\n    :rtype: bool\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 713, "code": "def is_valid_cidr(string_network):\n    if string_network.count(\"/\") == 1:\n        try:\n            mask = int(string_network.split(\"/\")[1])\n        except ValueError:\n            return False\n        if mask < 1 or mask > 32:\n            return False\n        try:\n            socket.inet_aton(string_network.split(\"/\")[0])\n        except OSError:", "documentation": "    \"\"\"\n    Very simple check of the cidr format in no_proxy variable.\n\n    :rtype: bool\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 738, "code": "def set_environ(env_name, value):\n    value_changed = value is not None\n    if value_changed:\n        old_value = os.environ.get(env_name)\n        os.environ[env_name] = value\n    try:\n        yield\n    finally:\n        if value_changed:\n            if old_value is None:\n                del os.environ[env_name]", "documentation": "    \"\"\"Set the environment variable 'env_name' to 'value'\n\n    Save previous value, yield, and then restore the previous value stored in\n    the environment variable 'env_name'.\n\n    If 'value' is None, do nothing\"\"\""}, {"filename": "src/requests/utils.py", "start_line": 759, "code": "def should_bypass_proxies(url, no_proxy):", "documentation": "    \"\"\"\n    Returns whether we should bypass proxies or not.\n\n    :rtype: bool\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 820, "code": "def get_environ_proxies(url, no_proxy=None):\n    if should_bypass_proxies(url, no_proxy=no_proxy):\n        return {}\n    else:\n        return getproxies()", "documentation": "    \"\"\"\n    Return a dict of environment proxies.\n\n    :rtype: dict\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 832, "code": "def select_proxy(url, proxies):\n    proxies = proxies or {}\n    urlparts = urlparse(url)\n    if urlparts.hostname is None:\n        return proxies.get(urlparts.scheme, proxies.get(\"all\"))\n    proxy_keys = [\n        urlparts.scheme + \"://\" + urlparts.hostname,\n        urlparts.scheme,\n        \"all://\" + urlparts.hostname,\n        \"all\",\n    ]", "documentation": "    \"\"\"Select a proxy for the url, if applicable.\n\n    :param url: The url being for the request\n    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 858, "code": "def resolve_proxies(request, proxies, trust_env=True):\n    proxies = proxies if proxies is not None else {}\n    url = request.url\n    scheme = urlparse(url).scheme\n    no_proxy = proxies.get(\"no_proxy\")\n    new_proxies = proxies.copy()\n    if trust_env and not should_bypass_proxies(url, no_proxy=no_proxy):\n        environ_proxies = get_environ_proxies(url, no_proxy=no_proxy)\n        proxy = environ_proxies.get(scheme, environ_proxies.get(\"all\"))\n        if proxy:\n            new_proxies.setdefault(scheme, proxy)", "documentation": "    \"\"\"This method takes proxy information from a request and configuration\n    input to resolve a mapping of target proxies. This will consider settings\n    such as NO_PROXY to strip proxy configurations.\n\n    :param request: Request or PreparedRequest\n    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs\n    :param trust_env: Boolean declaring whether to trust environment configs\n\n    :rtype: dict\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 885, "code": "def default_user_agent(name=\"python-requests\"):\n    return f\"{name}/{__version__}\"", "documentation": "    \"\"\"\n    Return a string representing the default user agent.\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 894, "code": "def default_headers():\n    return CaseInsensitiveDict(\n        {\n            \"User-Agent\": default_user_agent(),\n            \"Accept-Encoding\": DEFAULT_ACCEPT_ENCODING,\n            \"Accept\": \"*/*\",\n            \"Connection\": \"keep-alive\",\n        }\n    )", "documentation": "    \"\"\"\n    :rtype: requests.structures.CaseInsensitiveDict\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 908, "code": "def parse_header_links(value):\n    links = []\n    replace_chars = \" '\\\"\"\n    value = value.strip(replace_chars)\n    if not value:\n        return links\n    for val in re.split(\", *<\", value):\n        try:\n            url, params = val.split(\";\", 1)\n        except ValueError:\n            url, params = val, \"\"", "documentation": "    \"\"\"Return a list of parsed link headers proxies.\n\n    i.e. Link: <http:/.../front.jpeg>; rel=front; type=\"image/jpeg\",<http://.../back.jpeg>; rel=back;type=\"image/jpeg\"\n\n    :rtype: list\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 951, "code": "def guess_json_utf(data):\n    sample = data[:4]\n    if sample in (codecs.BOM_UTF32_LE, codecs.BOM_UTF32_BE):\n        return \"utf-32\"  # BOM included\n    if sample[:3] == codecs.BOM_UTF8:\n        return \"utf-8-sig\"  # BOM included, MS style (discouraged)\n    if sample[:2] in (codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE):\n        return \"utf-16\"  # BOM included\n    nullcount = sample.count(_null)\n    if nullcount == 0:\n        return \"utf-8\"", "documentation": "    \"\"\"\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 983, "code": "def prepend_scheme_if_needed(url, new_scheme):\n    parsed = parse_url(url)\n    scheme, auth, host, port, path, query, fragment = parsed\n    netloc = parsed.netloc\n    if not netloc:\n        netloc, path = path, netloc\n    if auth:\n        netloc = \"@\".join([auth, netloc])\n    if scheme is None:\n        scheme = new_scheme\n    if path is None:", "documentation": "    \"\"\"Given a URL that may or may not have a scheme, prepend the given scheme.\n    Does not replace a present scheme with the one provided as an argument.\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 1012, "code": "def get_auth_from_url(url):\n    parsed = urlparse(url)\n    try:\n        auth = (unquote(parsed.username), unquote(parsed.password))\n    except (AttributeError, TypeError):\n        auth = (\"\", \"\")\n    return auth", "documentation": "    \"\"\"Given a url with authentication components, extract them into a tuple of\n    username,password.\n\n    :rtype: (str,str)\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 1028, "code": "def check_header_validity(header):\n    name, value = header\n    _validate_header_part(header, name, 0)\n    _validate_header_part(header, value, 1)", "documentation": "    \"\"\"Verifies that header parts don't contain leading whitespace\n    reserved characters, or return characters.\n\n    :param header: tuple, in the format (name, value).\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 1058, "code": "def urldefragauth(url):\n    scheme, netloc, path, params, query, fragment = urlparse(url)\n    if not netloc:\n        netloc, path = path, netloc\n    netloc = netloc.rsplit(\"@\", 1)[-1]\n    return urlunparse((scheme, netloc, path, params, query, \"\"))", "documentation": "    \"\"\"\n    Given a url remove the fragment and the authentication part.\n\n    :rtype: str\n    \"\"\""}, {"filename": "src/requests/utils.py", "start_line": 1075, "code": "def rewind_body(prepared_request):\n    body_seek = getattr(prepared_request.body, \"seek\", None)\n    if body_seek is not None and isinstance(\n        prepared_request._body_position, integer_types\n    ):\n        try:\n            body_seek(prepared_request._body_position)\n        except OSError:\n            raise UnrewindableBodyError(\n                \"An error occurred when rewinding request body for redirect.\"\n            )", "documentation": "    \"\"\"Move file pointer back to its recorded starting position\n    so it can be read again on redirect.\n    \"\"\""}]}
{"repository": "pallets/flask", "commit_sha": "0d8c8ba71bc6362e6ea9af08146dc97e1a0a8abc", "commit_message": "Fix docstring of test_request_context (#4821)\n\nAdd missing `app.`.", "commit_date": "2022-09-18T11:53:47+00:00", "author": "Yang Yang", "file": "src/flask/app.py", "patch": "@@ -2448,7 +2448,7 @@ def test_request_context(self, *args: t.Any, **kwargs: t.Any) -> RequestContext:\n         :data:`request` point at the request for the created\n         environment. ::\n \n-            with test_request_context(...):\n+            with app.test_request_context(...):\n                 generate_report()\n \n         When using the shell, it may be easier to push and pop the", "before_segments": [{"filename": "src/flask/app.py", "start_line": 109, "code": "class Flask(Scaffold):\n    request_class = Request\n    response_class = Response\n    aborter_class = Aborter\n    jinja_environment = Environment\n    app_ctx_globals_class = _AppCtxGlobals\n    config_class = Config\n    testing = ConfigAttribute(\"TESTING\")\n    secret_key = ConfigAttribute(\"SECRET_KEY\")\n    @property", "documentation": "    \"\"\"The flask object implements a WSGI application and acts as the central\n    object.  It is passed the name of the module or package of the\n    application.  Once it is created it will act as a central registry for\n    the view functions, the URL rules, template configuration and much more.\n\n    The name of the package is used to resolve resources from inside the\n    package or the folder the module is contained in depending on if the\n    package parameter resolves to an actual python package (a folder with\n    an :file:`__init__.py` file inside) or a standard module (just a ``.py`` file).\n\n    For more information about resource loading, see :func:`open_resource`.\n\n    Usually you create a :class:`Flask` instance in your main module or\n    in the :file:`__init__.py` file of your package like this::\n\n        from flask import Flask\n        app = Flask(__name__)\n\n    .. admonition:: About the First Parameter\n\n        The idea of the first parameter is to give Flask an idea of what\n        belongs to your application.  This name is used to find resources\n        on the filesystem, can be used by extensions to improve debugging\n        information and a lot more.\n\n        So it's important what you provide there.  If you are using a single\n        module, `__name__` is always the correct value.  If you however are\n        using a package, it's usually recommended to hardcode the name of\n        your package there.\n\n        For example if your application is defined in :file:`yourapplication/app.py`\n        you should create it with one of the two versions below::\n\n            app = Flask('yourapplication')\n            app = Flask(__name__.split('.')[0])\n\n        Why is that?  The application will work even with `__name__`, thanks\n        to how resources are looked up.  However it will make debugging more\n        painful.  Certain extensions can make assumptions based on the\n        import name of your application.  For example the Flask-SQLAlchemy\n        extension will look for the code in your application that triggered\n        an SQL query in debug mode.  If the import name is not properly set\n        up, that debugging information is lost.  (For example it would only\n        pick up SQL queries in `yourapplication.app` and not\n        `yourapplication.views.frontend`)\n\n    .. versionadded:: 0.7\n       The `static_url_path`, `static_folder`, and `template_folder`\n       parameters were added.\n\n    .. versionadded:: 0.8\n       The `instance_path` and `instance_relative_config` parameters were\n       added.\n\n    .. versionadded:: 0.11\n       The `root_path` parameter was added.\n\n    .. versionadded:: 1.0\n       The ``host_matching`` and ``static_host`` parameters were added.\n\n    .. versionadded:: 1.0\n       The ``subdomain_matching`` parameter was added. Subdomain\n       matching needs to be enabled manually now. Setting\n       :data:`SERVER_NAME` does not implicitly enable it.\n\n    :param import_name: the name of the application package\n    :param static_url_path: can be used to specify a different path for the\n                            static files on the web.  Defaults to the name\n                            of the `static_folder` folder.\n    :param static_folder: The folder with static files that is served at\n        ``static_url_path``. Relative to the application ``root_path``\n        or an absolute path. Defaults to ``'static'``.\n    :param static_host: the host to use when adding the static route.\n        Defaults to None. Required when using ``host_matching=True``\n        with a ``static_folder`` configured.\n    :param host_matching: set ``url_map.host_matching`` attribute.\n        Defaults to False.\n    :param subdomain_matching: consider the subdomain relative to\n        :data:`SERVER_NAME` when matching routes. Defaults to False.\n    :param template_folder: the folder that contains the templates that should\n                            be used by the application.  Defaults to\n                            ``'templates'`` folder in the root path of the\n                            application.\n    :param instance_path: An alternative instance path for the application.\n                          By default the folder ``'instance'`` next to the\n                          package or module is assumed to be the instance\n                          path.\n    :param instance_relative_config: if set to ``True`` relative filenames\n                                     for loading the config are assumed to\n                                     be relative to the instance path instead\n                                     of the application root.\n    :param root_path: The path to the root of the application files.\n        This should only be set manually when it can't be detected\n        automatically, such as for namespace packages.\n    \"\"\""}, {"filename": "src/flask/app.py", "start_line": 277, "code": "    def session_cookie_name(self) -> str:\n        import warnings\n        warnings.warn(\n            \"'session_cookie_name' is deprecated and will be removed in Flask 2.3. Use\"\n            \" 'SESSION_COOKIE_NAME' in 'app.config' instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self.config[\"SESSION_COOKIE_NAME\"]\n    @session_cookie_name.setter", "documentation": "        \"\"\"The name of the cookie set by the session interface.\n\n        .. deprecated:: 2.2\n            Will be removed in Flask 2.3. Use ``app.config[\"SESSION_COOKIE_NAME\"]``\n            instead.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 318, "code": "    def send_file_max_age_default(self) -> t.Optional[timedelta]:\n        import warnings\n        warnings.warn(\n            \"'send_file_max_age_default' is deprecated and will be removed in Flask\"\n            \" 2.3. Use 'SEND_FILE_MAX_AGE_DEFAULT' in 'app.config' instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return _make_timedelta(self.config[\"SEND_FILE_MAX_AGE_DEFAULT\"])\n    @send_file_max_age_default.setter", "documentation": "        \"\"\"The default value for ``max_age`` for :func:`~flask.send_file`. The default\n        is ``None``, which tells the browser to use conditional requests instead of a\n        timed cache.\n\n        .. deprecated:: 2.2\n            Will be removed in Flask 2.3. Use\n            ``app.config[\"SEND_FILE_MAX_AGE_DEFAULT\"]`` instead.\n\n        .. versionchanged:: 2.0\n            Defaults to ``None`` instead of 12 hours.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 353, "code": "    def use_x_sendfile(self) -> bool:\n        import warnings\n        warnings.warn(\n            \"'use_x_sendfile' is deprecated and will be removed in Flask 2.3. Use\"\n            \" 'USE_X_SENDFILE' in 'app.config' instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self.config[\"USE_X_SENDFILE\"]\n    @use_x_sendfile.setter", "documentation": "        \"\"\"Enable this to use the ``X-Sendfile`` feature, assuming the server supports\n        it, from :func:`~flask.send_file`.\n\n        .. deprecated:: 2.2\n            Will be removed in Flask 2.3. Use ``app.config[\"USE_X_SENDFILE\"]`` instead.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 386, "code": "    def json_encoder(self) -> t.Type[json.JSONEncoder]:  # type: ignore[override]\n        import warnings\n        warnings.warn(\n            \"'app.json_encoder' is deprecated and will be removed in Flask 2.3.\"\n            \" Customize 'app.json_provider_class' or 'app.json' instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        if self._json_encoder is None:\n            from . import json\n            return json.JSONEncoder", "documentation": "        \"\"\"The JSON encoder class to use. Defaults to\n        :class:`~flask.json.JSONEncoder`.\n\n        .. deprecated:: 2.2\n             Will be removed in Flask 2.3. Customize\n             :attr:`json_provider_class` instead.\n\n        .. versionadded:: 0.10\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 425, "code": "    def json_decoder(self) -> t.Type[json.JSONDecoder]:  # type: ignore[override]\n        import warnings\n        warnings.warn(\n            \"'app.json_decoder' is deprecated and will be removed in Flask 2.3.\"\n            \" Customize 'app.json_provider_class' or 'app.json' instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        if self._json_decoder is None:\n            from . import json\n            return json.JSONDecoder", "documentation": "        \"\"\"The JSON decoder class to use. Defaults to\n        :class:`~flask.json.JSONDecoder`.\n\n        .. deprecated:: 2.2\n             Will be removed in Flask 2.3. Customize\n             :attr:`json_provider_class` instead.\n\n        .. versionadded:: 0.10\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 732, "code": "    def name(self) -> str:  # type: ignore\n        if self.import_name == \"__main__\":\n            fn = getattr(sys.modules[\"__main__\"], \"__file__\", None)\n            if fn is None:\n                return \"__main__\"\n            return os.path.splitext(os.path.basename(fn))[0]\n        return self.import_name\n    @property", "documentation": "        \"\"\"The name of the application.  This is usually the import name\n        with the difference that it's guessed from the run file if the\n        import name is main.  This name is used as a display name when\n        Flask needs the name of the application.  It can be set and overridden\n        to change the value.\n\n        .. versionadded:: 0.8\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 749, "code": "    def propagate_exceptions(self) -> bool:\n        import warnings\n        warnings.warn(\n            \"'propagate_exceptions' is deprecated and will be removed in Flask 2.3.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        rv = self.config[\"PROPAGATE_EXCEPTIONS\"]\n        if rv is not None:\n            return rv\n        return self.testing or self.debug", "documentation": "        \"\"\"Returns the value of the ``PROPAGATE_EXCEPTIONS`` configuration\n        value in case it's set, otherwise a sensible default is returned.\n\n        .. deprecated:: 2.2\n            Will be removed in Flask 2.3.\n\n        .. versionadded:: 0.7\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 771, "code": "    def logger(self) -> logging.Logger:\n        return create_logger(self)\n    @locked_cached_property", "documentation": "        \"\"\"A standard Python :class:`~logging.Logger` for the app, with\n        the same name as :attr:`name`.\n\n        In debug mode, the logger's :attr:`~logging.Logger.level` will\n        be set to :data:`~logging.DEBUG`.\n\n        If there are no handlers configured, a default handler will be\n        added. See :doc:`/logging` for more information.\n\n        .. versionchanged:: 1.1.0\n            The logger takes the same name as :attr:`name` rather than\n            hard-coding ``\"flask.app\"``.\n\n        .. versionchanged:: 1.0.0\n            Behavior was simplified. The logger is always named\n            ``\"flask.app\"``. The level is only set during configuration,\n            it doesn't check ``app.debug`` each time. Only one format is\n            used, not different ones depending on ``app.debug``. No\n            handlers are removed, and a handler is only added if no\n            handlers are already configured.\n\n        .. versionadded:: 0.3\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 798, "code": "    def jinja_env(self) -> Environment:\n        return self.create_jinja_environment()\n    @property", "documentation": "        \"\"\"The Jinja environment used to load templates.\n\n        The environment is created the first time this property is\n        accessed. Changing :attr:`jinja_options` after that will have no\n        effect.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 808, "code": "    def got_first_request(self) -> bool:\n        return self._got_first_request", "documentation": "        \"\"\"This attribute is set to ``True`` if the application started\n        handling the first request.\n\n        .. versionadded:: 0.8\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 816, "code": "    def make_config(self, instance_relative: bool = False) -> Config:\n        root_path = self.root_path\n        if instance_relative:\n            root_path = self.instance_path\n        defaults = dict(self.default_config)\n        defaults[\"ENV\"] = os.environ.get(\"FLASK_ENV\") or \"production\"\n        defaults[\"DEBUG\"] = get_debug_flag()\n        return self.config_class(root_path, defaults)", "documentation": "        \"\"\"Used to create the config attribute by the Flask constructor.\n        The `instance_relative` parameter is passed in from the constructor\n        of Flask (there named `instance_relative_config`) and indicates if\n        the config should be relative to the instance path or the root path\n        of the application.\n\n        .. versionadded:: 0.8\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 833, "code": "    def make_aborter(self) -> Aborter:\n        return self.aborter_class()", "documentation": "        \"\"\"Create the object to assign to :attr:`aborter`. That object\n        is called by :func:`flask.abort` to raise HTTP errors, and can\n        be called directly as well.\n\n        By default, this creates an instance of :attr:`aborter_class`,\n        which defaults to :class:`werkzeug.exceptions.Aborter`.\n\n        .. versionadded:: 2.2\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 845, "code": "    def auto_find_instance_path(self) -> str:\n        prefix, package_path = find_package(self.import_name)\n        if prefix is None:\n            return os.path.join(package_path, \"instance\")\n        return os.path.join(prefix, \"var\", f\"{self.name}-instance\")", "documentation": "        \"\"\"Tries to locate the instance path if it was not provided to the\n        constructor of the application class.  It will basically calculate\n        the path to a folder named ``instance`` next to your main file or\n        the package.\n\n        .. versionadded:: 0.8\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 858, "code": "    def open_instance_resource(self, resource: str, mode: str = \"rb\") -> t.IO[t.AnyStr]:\n        return open(os.path.join(self.instance_path, resource), mode)\n    @property", "documentation": "        \"\"\"Opens a resource from the application's instance folder\n        (:attr:`instance_path`).  Otherwise works like\n        :meth:`open_resource`.  Instance resources can also be opened for\n        writing.\n\n        :param resource: the name of the resource.  To access resources within\n                         subfolders use forward slashes as separator.\n        :param mode: resource file opening mode, default is 'rb'.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 871, "code": "    def templates_auto_reload(self) -> bool:\n        import warnings\n        warnings.warn(\n            \"'templates_auto_reload' is deprecated and will be removed in Flask 2.3.\"\n            \" Use 'TEMPLATES_AUTO_RELOAD' in 'app.config' instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        rv = self.config[\"TEMPLATES_AUTO_RELOAD\"]\n        return rv if rv is not None else self.debug\n    @templates_auto_reload.setter", "documentation": "        \"\"\"Reload templates when they are changed. Used by\n        :meth:`create_jinja_environment`. It is enabled by default in debug mode.\n\n        .. deprecated:: 2.2\n            Will be removed in Flask 2.3. Use ``app.config[\"TEMPLATES_AUTO_RELOAD\"]``\n            instead.\n\n        .. versionadded:: 1.0\n            This property was added but the underlying config and behavior\n            already existed.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 906, "code": "    def create_jinja_environment(self) -> Environment:\n        options = dict(self.jinja_options)\n        if \"autoescape\" not in options:\n            options[\"autoescape\"] = self.select_jinja_autoescape\n        if \"auto_reload\" not in options:\n            auto_reload = self.config[\"TEMPLATES_AUTO_RELOAD\"]\n            if auto_reload is None:\n                auto_reload = self.debug\n            options[\"auto_reload\"] = auto_reload\n        rv = self.jinja_environment(self, **options)\n        rv.globals.update(", "documentation": "        \"\"\"Create the Jinja environment based on :attr:`jinja_options`\n        and the various Jinja-related methods of the app. Changing\n        :attr:`jinja_options` after this will have no effect. Also adds\n        Flask-related globals and filters to the environment.\n\n        .. versionchanged:: 0.11\n           ``Environment.auto_reload`` set in accordance with\n           ``TEMPLATES_AUTO_RELOAD`` configuration option.\n\n        .. versionadded:: 0.5\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 946, "code": "    def create_global_jinja_loader(self) -> DispatchingJinjaLoader:\n        return DispatchingJinjaLoader(self)", "documentation": "        \"\"\"Creates the loader for the Jinja2 environment.  Can be used to\n        override just the loader and keeping the rest unchanged.  It's\n        discouraged to override this function.  Instead one should override\n        the :meth:`jinja_loader` function instead.\n\n        The global loader dispatches between the loaders of the application\n        and the individual blueprints.\n\n        .. versionadded:: 0.7\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 959, "code": "    def select_jinja_autoescape(self, filename: str) -> bool:\n        if filename is None:\n            return True\n        return filename.endswith((\".html\", \".htm\", \".xml\", \".xhtml\"))", "documentation": "        \"\"\"Returns ``True`` if autoescaping should be active for the given\n        template name. If no template name is given, returns `True`.\n\n        .. versionadded:: 0.5\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 969, "code": "    def update_template_context(self, context: dict) -> None:\n        names: t.Iterable[t.Optional[str]] = (None,)\n        if request:\n            names = chain(names, reversed(request.blueprints))\n        orig_ctx = context.copy()\n        for name in names:\n            if name in self.template_context_processors:\n                for func in self.template_context_processors[name]:\n                    context.update(func())\n        context.update(orig_ctx)", "documentation": "        \"\"\"Update the template context with some commonly used variables.\n        This injects request, session, config and g into the template\n        context as well as everything template context processors want\n        to inject.  Note that the as of Flask 0.6, the original values\n        in the context will not be overridden if a context processor\n        decides to return a value with the same key.\n\n        :param context: the context as a dictionary that is updated in place\n                        to add extra variables.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 997, "code": "    def make_shell_context(self) -> dict:\n        rv = {\"app\": self, \"g\": g}\n        for processor in self.shell_context_processors:\n            rv.update(processor())\n        return rv\n    @property", "documentation": "        \"\"\"Returns the shell context for an interactive shell for this\n        application.  This runs all the registered shell context\n        processors.\n\n        .. versionadded:: 0.11\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1010, "code": "    def env(self) -> str:\n        import warnings\n        warnings.warn(\n            \"'app.env' is deprecated and will be removed in Flask 2.3.\"\n            \" Use 'app.debug' instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self.config[\"ENV\"]\n    @env.setter", "documentation": "        \"\"\"What environment the app is running in. This maps to the :data:`ENV` config\n        key.\n\n        **Do not enable development when deploying in production.**\n\n        Default: ``'production'``\n\n        .. deprecated:: 2.2\n            Will be removed in Flask 2.3.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1044, "code": "    def debug(self) -> bool:\n        return self.config[\"DEBUG\"]\n    @debug.setter", "documentation": "        \"\"\"Whether debug mode is enabled. When using ``flask run`` to start the\n        development server, an interactive debugger will be shown for unhandled\n        exceptions, and the server will be reloaded when code changes. This maps to the\n        :data:`DEBUG` config key. It may not behave as expected if set late.\n\n        **Do not enable debug mode when deploying in production.**\n\n        Default: ``False``\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1194, "code": "    def test_client(self, use_cookies: bool = True, **kwargs: t.Any) -> \"FlaskClient\":\n        cls = self.test_client_class\n        if cls is None:\n            from .testing import FlaskClient as cls  # type: ignore\n        return cls(  # type: ignore\n            self, self.response_class, use_cookies=use_cookies, **kwargs\n        )", "documentation": "        \"\"\"Creates a test client for this application.  For information\n        about unit testing head over to :doc:`/testing`.\n\n        Note that if you are testing for assertions or exceptions in your\n        application code, you must set ``app.testing = True`` in order for the\n        exceptions to propagate to the test client.  Otherwise, the exception\n        will be handled by the application (not visible to the test client) and\n        the only indication of an AssertionError or other exception will be a\n        500 status code response to the test client.  See the :attr:`testing`\n        attribute.  For example::\n\n            app.testing = True\n            client = app.test_client()\n\n        The test client can be used in a ``with`` block to defer the closing down\n        of the context until the end of the ``with`` block.  This is useful if\n        you want to access the context locals for testing::\n\n            with app.test_client() as c:\n                rv = c.get('/?vodka=42')\n                assert request.args['vodka'] == '42'\n\n        Additionally, you may pass optional keyword arguments that will then\n        be passed to the application's :attr:`test_client_class` constructor.\n        For example::\n\n            from flask.testing import FlaskClient\n\n            class CustomClient(FlaskClient):\n                def __init__(self, *args, **kwargs):\n                    self._authentication = kwargs.pop(\"authentication\")\n                    super(CustomClient,self).__init__( *args, **kwargs)\n\n            app.test_client_class = CustomClient\n            client = app.test_client(authentication='Basic ....')\n\n        See :class:`~flask.testing.FlaskClient` for more information.\n\n        .. versionchanged:: 0.4\n           added support for ``with`` block usage for the client.\n\n        .. versionadded:: 0.7\n           The `use_cookies` parameter was added as well as the ability\n           to override the client to be used by setting the\n           :attr:`test_client_class` attribute.\n\n        .. versionchanged:: 0.11\n           Added `**kwargs` to support passing additional keyword arguments to\n           the constructor of :attr:`test_client_class`.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1252, "code": "    def test_cli_runner(self, **kwargs: t.Any) -> \"FlaskCliRunner\":\n        cls = self.test_cli_runner_class\n        if cls is None:\n            from .testing import FlaskCliRunner as cls  # type: ignore\n        return cls(self, **kwargs)  # type: ignore\n    @setupmethod", "documentation": "        \"\"\"Create a CLI runner for testing CLI commands.\n        See :ref:`testing-cli`.\n\n        Returns an instance of :attr:`test_cli_runner_class`, by default\n        :class:`~flask.testing.FlaskCliRunner`. The Flask app object is\n        passed as the first argument.\n\n        .. versionadded:: 1.0\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1270, "code": "    def register_blueprint(self, blueprint: \"Blueprint\", **options: t.Any) -> None:\n        blueprint.register(self, options)", "documentation": "        \"\"\"Register a :class:`~flask.Blueprint` on the application. Keyword\n        arguments passed to this method will override the defaults set on the\n        blueprint.\n\n        Calls the blueprint's :meth:`~flask.Blueprint.register` method after\n        recording the blueprint in the application's :attr:`blueprints`.\n\n        :param blueprint: The blueprint to register.\n        :param url_prefix: Blueprint routes will be prefixed with this.\n        :param subdomain: Blueprint routes will match on this subdomain.\n        :param url_defaults: Blueprint routes will use these default values for\n            view arguments.\n        :param options: Additional keyword arguments are passed to\n            :class:`~flask.blueprints.BlueprintSetupState`. They can be\n            accessed in :meth:`~flask.Blueprint.record` callbacks.\n\n        .. versionchanged:: 2.0.1\n            The ``name`` option can be used to change the (pre-dotted)\n            name the blueprint is registered with. This allows the same\n            blueprint to be registered multiple times with unique names\n            for ``url_for``.\n\n        .. versionadded:: 0.7\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1297, "code": "    def iter_blueprints(self) -> t.ValuesView[\"Blueprint\"]:\n        return self.blueprints.values()\n    @setupmethod", "documentation": "        \"\"\"Iterates over all blueprints by the order they were registered.\n\n        .. versionadded:: 0.11\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1479, "code": "    def before_first_request(self, f: T_before_first_request) -> T_before_first_request:\n        import warnings\n        warnings.warn(\n            \"'before_first_request' is deprecated and will be removed\"\n            \" in Flask 2.3. Run setup code while creating the\"\n            \" application instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        self.before_first_request_funcs.append(f)\n        return f", "documentation": "        \"\"\"Registers a function to be run before the first request to this\n        instance of the application.\n\n        The function will be called without any arguments and its return\n        value is ignored.\n\n        .. deprecated:: 2.2\n            Will be removed in Flask 2.3. Run setup code when creating\n            the application instead.\n\n        .. versionadded:: 0.8\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1505, "code": "    def teardown_appcontext(self, f: T_teardown) -> T_teardown:\n        self.teardown_appcontext_funcs.append(f)\n        return f\n    @setupmethod", "documentation": "        \"\"\"Registers a function to be called when the application\n        context is popped. The application context is typically popped\n        after the request context for each request, at the end of CLI\n        commands, or after a manually pushed context ends.\n\n        .. code-block:: python\n\n            with app.app_context():\n                ...\n\n        When the ``with`` block exits (or ``ctx.pop()`` is called), the\n        teardown functions are called just before the app context is\n        made inactive. Since a request context typically also manages an\n        application context it would also be called when you pop a\n        request context.\n\n        When a teardown function was called because of an unhandled\n        exception it will be passed an error object. If an\n        :meth:`errorhandler` is registered, it will handle the exception\n        and the teardown will not receive it.\n\n        Teardown functions must avoid raising exceptions. If they\n        execute code that might fail they must surround that code with a\n        ``try``/``except`` block and log any errors.\n\n        The return values of teardown functions are ignored.\n\n        .. versionadded:: 0.9\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1549, "code": "    def _find_error_handler(self, e: Exception) -> t.Optional[ft.ErrorHandlerCallable]:\n        exc_class, code = self._get_exc_class_and_code(type(e))\n        names = (*request.blueprints, None)\n        for c in (code, None) if code is not None else (None,):\n            for name in names:\n                handler_map = self.error_handler_spec[name][c]\n                if not handler_map:\n                    continue\n                for cls in exc_class.__mro__:\n                    handler = handler_map.get(cls)\n                    if handler is not None:", "documentation": "        \"\"\"Return a registered error handler for an exception in this order:\n        blueprint handler for a specific code, app handler for a specific code,\n        blueprint handler for an exception class, app handler for an exception\n        class, or ``None`` if a suitable handler is not found.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1607, "code": "    def trap_http_exception(self, e: Exception) -> bool:\n        if self.config[\"TRAP_HTTP_EXCEPTIONS\"]:\n            return True\n        trap_bad_request = self.config[\"TRAP_BAD_REQUEST_ERRORS\"]\n        if (\n            trap_bad_request is None\n            and self.debug\n            and isinstance(e, BadRequestKeyError)\n        ):\n            return True\n        if trap_bad_request:", "documentation": "        \"\"\"Checks if an HTTP exception should be trapped or not.  By default\n        this will return ``False`` for all exceptions except for a bad request\n        key error if ``TRAP_BAD_REQUEST_ERRORS`` is set to ``True``.  It\n        also returns ``True`` if ``TRAP_HTTP_EXCEPTIONS`` is set to ``True``.\n\n        This is called for all HTTP exceptions raised by a view function.\n        If it returns ``True`` for any exception the error handler for this\n        exception is not called and it shows up as regular exception in the\n        traceback.  This is helpful for debugging implicitly raised HTTP\n        exceptions.\n\n        .. versionchanged:: 1.0\n            Bad request errors are not trapped by default in debug mode.\n\n        .. versionadded:: 0.8\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1674, "code": "    def handle_exception(self, e: Exception) -> Response:\n        exc_info = sys.exc_info()\n        got_request_exception.send(self, exception=e)\n        propagate = self.config[\"PROPAGATE_EXCEPTIONS\"]\n        if propagate is None:\n            propagate = self.testing or self.debug\n        if propagate:\n            if exc_info[1] is e:\n                raise\n            raise e\n        self.log_exception(exc_info)", "documentation": "        \"\"\"Handle an exception that did not have an error handler\n        associated with it, or that was raised from an error handler.\n        This always causes a 500 ``InternalServerError``.\n\n        Always sends the :data:`got_request_exception` signal.\n\n        If :attr:`propagate_exceptions` is ``True``, such as in debug\n        mode, the error will be re-raised so that the debugger can\n        display it. Otherwise, the original exception is logged, and\n        an :exc:`~werkzeug.exceptions.InternalServerError` is returned.\n\n        If an error handler is registered for ``InternalServerError`` or\n        ``500``, it will be used. For consistency, the handler will\n        always receive the ``InternalServerError``. The original\n        unhandled exception is available as ``e.original_exception``.\n\n        .. versionchanged:: 1.1.0\n            Always passes the ``InternalServerError`` instance to the\n            handler, setting ``original_exception`` to the unhandled\n            error.\n\n        .. versionchanged:: 1.1.0\n            ``after_request`` functions and other finalization is done\n            even for the default 500 response when there is no handler.\n\n        .. versionadded:: 0.3\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1744, "code": "    def raise_routing_exception(self, request: Request) -> \"te.NoReturn\":\n        if (\n            not self.debug\n            or not isinstance(request.routing_exception, RequestRedirect)\n            or request.routing_exception.code in {307, 308}\n            or request.method in {\"GET\", \"HEAD\", \"OPTIONS\"}\n        ):\n            raise request.routing_exception  # type: ignore\n        from .debughelpers import FormDataRoutingRedirect\n        raise FormDataRoutingRedirect(request)", "documentation": "        \"\"\"Intercept routing exceptions and possibly do something else.\n\n        In debug mode, intercept a routing redirect and replace it with\n        an error if the body will be discarded.\n\n        With modern Werkzeug this shouldn't occur, since it now uses a\n        308 status which tells the browser to resend the method and\n        body.\n\n        .. versionchanged:: 2.1\n            Don't intercept 307 and 308 redirects.\n\n        :meta private:\n        :internal:\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1772, "code": "    def dispatch_request(self) -> ft.ResponseReturnValue:\n        req = request_ctx.request\n        if req.routing_exception is not None:\n            self.raise_routing_exception(req)\n        rule: Rule = req.url_rule  # type: ignore[assignment]\n        if (\n            getattr(rule, \"provide_automatic_options\", False)\n            and req.method == \"OPTIONS\"\n        ):\n            return self.make_default_options_response()\n        view_args: t.Dict[str, t.Any] = req.view_args  # type: ignore[assignment]", "documentation": "        \"\"\"Does the request dispatching.  Matches the URL and returns the\n        return value of the view or error handler.  This does not have to\n        be a response object.  In order to convert the return value to a\n        proper response object, call :func:`make_response`.\n\n        .. versionchanged:: 0.7\n           This no longer does the exception handling, this code was\n           moved to the new :meth:`full_dispatch_request`.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1797, "code": "    def full_dispatch_request(self) -> Response:\n        if not self._got_first_request:\n            with self._before_request_lock:\n                if not self._got_first_request:\n                    for func in self.before_first_request_funcs:\n                        self.ensure_sync(func)()\n                    self._got_first_request = True\n        try:\n            request_started.send(self)\n            rv = self.preprocess_request()\n            if rv is None:", "documentation": "        \"\"\"Dispatches the request and on top of that performs request\n        pre and postprocessing as well as HTTP exception catching and\n        error handling.\n\n        .. versionadded:: 0.7\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1853, "code": "    def make_default_options_response(self) -> Response:\n        adapter = request_ctx.url_adapter\n        methods = adapter.allowed_methods()  # type: ignore[union-attr]\n        rv = self.response_class()\n        rv.allow.update(methods)\n        return rv", "documentation": "        \"\"\"This method is called to create the default ``OPTIONS`` response.\n        This can be changed through subclassing to change the default\n        behavior of ``OPTIONS`` responses.\n\n        .. versionadded:: 0.7\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1866, "code": "    def should_ignore_error(self, error: t.Optional[BaseException]) -> bool:\n        return False", "documentation": "        \"\"\"This is called to figure out if an error should be ignored\n        or not as far as the teardown system is concerned.  If this\n        function returns ``True`` then the teardown handlers will not be\n        passed the error.\n\n        .. versionadded:: 0.10\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1876, "code": "    def ensure_sync(self, func: t.Callable) -> t.Callable:\n        if iscoroutinefunction(func):\n            return self.async_to_sync(func)\n        return func", "documentation": "        \"\"\"Ensure that the function is synchronous for WSGI workers.\n        Plain ``def`` functions are returned as-is. ``async def``\n        functions are wrapped to run and wait for the response.\n\n        Override this method to change how the app runs async views.\n\n        .. versionadded:: 2.0\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2037, "code": "    def redirect(self, location: str, code: int = 302) -> BaseResponse:\n        return _wz_redirect(location, code=code, Response=self.response_class)", "documentation": "        \"\"\"Create a redirect response object.\n\n        This is called by :func:`flask.redirect`, and can be called\n        directly as well.\n\n        :param location: The URL to redirect to.\n        :param code: The status code for the redirect.\n\n        .. versionadded:: 2.2\n            Moved from ``flask.redirect``, which calls this method.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2051, "code": "    def make_response(self, rv: ft.ResponseReturnValue) -> Response:\n        status = headers = None\n        if isinstance(rv, tuple):\n            len_rv = len(rv)\n            if len_rv == 3:\n                rv, status, headers = rv  # type: ignore[misc]\n            elif len_rv == 2:\n                if isinstance(rv[1], (Headers, dict, tuple, list)):\n                    rv, headers = rv\n                else:\n                    rv, status = rv  # type: ignore[assignment,misc]", "documentation": "        \"\"\"Convert the return value from a view function to an instance of\n        :attr:`response_class`.\n\n        :param rv: the return value from the view function. The view function\n            must return a response. Returning ``None``, or the view ending\n            without returning, is not allowed. The following types are allowed\n            for ``view_rv``:\n\n            ``str``\n                A response object is created with the string encoded to UTF-8\n                as the body.\n\n            ``bytes``\n                A response object is created with the bytes as the body.\n\n            ``dict``\n                A dictionary that will be jsonify'd before being returned.\n\n            ``list``\n                A list that will be jsonify'd before being returned.\n\n            ``generator`` or ``iterator``\n                A generator that returns ``str`` or ``bytes`` to be\n                streamed as the response.\n\n            ``tuple``\n                Either ``(body, status, headers)``, ``(body, status)``, or\n                ``(body, headers)``, where ``body`` is any of the other types\n                allowed here, ``status`` is a string or an integer, and\n                ``headers`` is a dictionary or a list of ``(key, value)``\n                tuples. If ``body`` is a :attr:`response_class` instance,\n                ``status`` overwrites the exiting value and ``headers`` are\n                extended.\n\n            :attr:`response_class`\n                The object is returned unchanged.\n\n            other :class:`~werkzeug.wrappers.Response` class\n                The object is coerced to :attr:`response_class`.\n\n            :func:`callable`\n                The function is called as a WSGI application. The result is\n                used to create a response object.\n\n        .. versionchanged:: 2.2\n            A generator will be converted to a streaming response.\n            A list will be converted to a JSON response.\n\n        .. versionchanged:: 1.1\n            A dict will be converted to a JSON response.\n\n        .. versionchanged:: 0.9\n           Previously a tuple was interpreted as the arguments for the\n           response object.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2233, "code": "    def inject_url_defaults(self, endpoint: str, values: dict) -> None:\n        names: t.Iterable[t.Optional[str]] = (None,)\n        if \".\" in endpoint:\n            names = chain(\n                names, reversed(_split_blueprint_path(endpoint.rpartition(\".\")[0]))\n            )\n        for name in names:\n            if name in self.url_default_functions:\n                for func in self.url_default_functions[name]:\n                    func(endpoint, values)", "documentation": "        \"\"\"Injects the URL defaults for the given endpoint directly into\n        the values dictionary passed.  This is used internally and\n        automatically called on URL building.\n\n        .. versionadded:: 0.7\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2288, "code": "    def preprocess_request(self) -> t.Optional[ft.ResponseReturnValue]:\n        names = (None, *reversed(request.blueprints))\n        for name in names:\n            if name in self.url_value_preprocessors:\n                for url_func in self.url_value_preprocessors[name]:\n                    url_func(request.endpoint, request.view_args)\n        for name in names:\n            if name in self.before_request_funcs:\n                for before_func in self.before_request_funcs[name]:\n                    rv = self.ensure_sync(before_func)()\n                    if rv is not None:", "documentation": "        \"\"\"Called before the request is dispatched. Calls\n        :attr:`url_value_preprocessors` registered with the app and the\n        current blueprint (if any). Then calls :attr:`before_request_funcs`\n        registered with the app and the blueprint.\n\n        If any :meth:`before_request` handler returns a non-None value, the\n        value is handled as if it was the return value from the view, and\n        further request handling is stopped.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2315, "code": "    def process_response(self, response: Response) -> Response:\n        ctx = request_ctx._get_current_object()  # type: ignore[attr-defined]\n        for func in ctx._after_request_functions:\n            response = self.ensure_sync(func)(response)\n        for name in chain(request.blueprints, (None,)):\n            if name in self.after_request_funcs:\n                for func in reversed(self.after_request_funcs[name]):\n                    response = self.ensure_sync(func)(response)\n        if not self.session_interface.is_null_session(ctx.session):\n            self.session_interface.save_session(self, ctx.session, response)\n        return response", "documentation": "        \"\"\"Can be overridden in order to modify the response object\n        before it's sent to the WSGI server.  By default this will\n        call all the :meth:`after_request` decorated functions.\n\n        .. versionchanged:: 0.5\n           As of Flask 0.5 the functions registered for after request\n           execution are called in reverse order of registration.\n\n        :param response: a :attr:`response_class` object.\n        :return: a new response object or the same, has to be an\n                 instance of :attr:`response_class`.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2401, "code": "    def app_context(self) -> AppContext:\n        return AppContext(self)", "documentation": "        \"\"\"Create an :class:`~flask.ctx.AppContext`. Use as a ``with``\n        block to push the context, which will make :data:`current_app`\n        point at this application.\n\n        An application context is automatically pushed by\n        :meth:`RequestContext.push() <flask.ctx.RequestContext.push>`\n        when handling a request, and when running a CLI command. Use\n        this to manually create a context outside of these situations.\n\n        ::\n\n            with app.app_context():\n                init_db()\n\n        See :doc:`/appcontext`.\n\n        .. versionadded:: 0.9\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2422, "code": "    def request_context(self, environ: dict) -> RequestContext:\n        return RequestContext(self, environ)", "documentation": "        \"\"\"Create a :class:`~flask.ctx.RequestContext` representing a\n        WSGI environment. Use a ``with`` block to push the context,\n        which will make :data:`request` point at this request.\n\n        See :doc:`/reqcontext`.\n\n        Typically you should not call this from your own code. A request\n        context is automatically pushed by the :meth:`wsgi_app` when\n        handling a request. Use :meth:`test_request_context` to create\n        an environment and context instead of this method.\n\n        :param environ: a WSGI environment\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2438, "code": "    def test_request_context(self, *args: t.Any, **kwargs: t.Any) -> RequestContext:\n        from .testing import EnvironBuilder\n        builder = EnvironBuilder(self, *args, **kwargs)\n        try:\n            return self.request_context(builder.get_environ())\n        finally:\n            builder.close()", "documentation": "        \"\"\"Create a :class:`~flask.ctx.RequestContext` for a WSGI\n        environment created from the given values. This is mostly useful\n        during testing, where you may want to run a function that uses\n        request data without dispatching a full request.\n\n        See :doc:`/reqcontext`.\n\n        Use a ``with`` block to push the context, which will make\n        :data:`request` point at the request for the created\n        environment. ::\n\n            with test_request_context(...):\n                generate_report()\n\n        When using the shell, it may be easier to push and pop the\n        context manually to avoid indentation. ::\n\n            ctx = app.test_request_context(...)\n            ctx.push()\n            ...\n            ctx.pop()\n\n        Takes the same arguments as Werkzeug's\n        :class:`~werkzeug.test.EnvironBuilder`, with some defaults from\n        the application. See the linked Werkzeug docs for most of the\n        available arguments. Flask-specific behavior is listed here.\n\n        :param path: URL path being requested.\n        :param base_url: Base URL where the app is being served, which\n            ``path`` is relative to. If not given, built from\n            :data:`PREFERRED_URL_SCHEME`, ``subdomain``,\n            :data:`SERVER_NAME`, and :data:`APPLICATION_ROOT`.\n        :param subdomain: Subdomain name to append to\n            :data:`SERVER_NAME`.\n        :param url_scheme: Scheme to use instead of\n            :data:`PREFERRED_URL_SCHEME`.\n        :param data: The request body, either as a string or a dict of\n            form keys and values.\n        :param json: If given, this is serialized as JSON and passed as\n            ``data``. Also defaults ``content_type`` to\n            ``application/json``.\n        :param args: other positional arguments passed to\n            :class:`~werkzeug.test.EnvironBuilder`.\n        :param kwargs: other keyword arguments passed to\n            :class:`~werkzeug.test.EnvironBuilder`.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2494, "code": "    def wsgi_app(self, environ: dict, start_response: t.Callable) -> t.Any:\n        ctx = self.request_context(environ)\n        error: t.Optional[BaseException] = None\n        try:\n            try:\n                ctx.push()\n                response = self.full_dispatch_request()\n            except Exception as e:\n                error = e\n                response = self.handle_exception(e)\n            except:  # noqa: B001", "documentation": "        \"\"\"The actual WSGI application. This is not implemented in\n        :meth:`__call__` so that middlewares can be applied without\n        losing a reference to the app object. Instead of doing this::\n\n            app = MyMiddleware(app)\n\n        It's a better idea to do this instead::\n\n            app.wsgi_app = MyMiddleware(app.wsgi_app)\n\n        Then you still have the original application object around and\n        can continue to call methods on it.\n\n        .. versionchanged:: 0.7\n            Teardown events for the request and app contexts are called\n            even if an unhandled error occurs. Other events may not be\n            called depending on when an error occurs during dispatch.\n            See :ref:`callbacks-and-errors`.\n\n        :param environ: A WSGI environment.\n        :param start_response: A callable accepting a status code,\n            a list of headers, and an optional exception context to\n            start the response.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2542, "code": "    def __call__(self, environ: dict, start_response: t.Callable) -> t.Any:\n        return self.wsgi_app(environ, start_response)", "documentation": "        \"\"\"The WSGI server calls the Flask application object as the\n        WSGI application. This calls :meth:`wsgi_app`, which can be\n        wrapped to apply middleware.\n        \"\"\""}], "after_segments": [{"filename": "src/flask/app.py", "start_line": 109, "code": "class Flask(Scaffold):\n    request_class = Request\n    response_class = Response\n    aborter_class = Aborter\n    jinja_environment = Environment\n    app_ctx_globals_class = _AppCtxGlobals\n    config_class = Config\n    testing = ConfigAttribute(\"TESTING\")\n    secret_key = ConfigAttribute(\"SECRET_KEY\")\n    @property", "documentation": "    \"\"\"The flask object implements a WSGI application and acts as the central\n    object.  It is passed the name of the module or package of the\n    application.  Once it is created it will act as a central registry for\n    the view functions, the URL rules, template configuration and much more.\n\n    The name of the package is used to resolve resources from inside the\n    package or the folder the module is contained in depending on if the\n    package parameter resolves to an actual python package (a folder with\n    an :file:`__init__.py` file inside) or a standard module (just a ``.py`` file).\n\n    For more information about resource loading, see :func:`open_resource`.\n\n    Usually you create a :class:`Flask` instance in your main module or\n    in the :file:`__init__.py` file of your package like this::\n\n        from flask import Flask\n        app = Flask(__name__)\n\n    .. admonition:: About the First Parameter\n\n        The idea of the first parameter is to give Flask an idea of what\n        belongs to your application.  This name is used to find resources\n        on the filesystem, can be used by extensions to improve debugging\n        information and a lot more.\n\n        So it's important what you provide there.  If you are using a single\n        module, `__name__` is always the correct value.  If you however are\n        using a package, it's usually recommended to hardcode the name of\n        your package there.\n\n        For example if your application is defined in :file:`yourapplication/app.py`\n        you should create it with one of the two versions below::\n\n            app = Flask('yourapplication')\n            app = Flask(__name__.split('.')[0])\n\n        Why is that?  The application will work even with `__name__`, thanks\n        to how resources are looked up.  However it will make debugging more\n        painful.  Certain extensions can make assumptions based on the\n        import name of your application.  For example the Flask-SQLAlchemy\n        extension will look for the code in your application that triggered\n        an SQL query in debug mode.  If the import name is not properly set\n        up, that debugging information is lost.  (For example it would only\n        pick up SQL queries in `yourapplication.app` and not\n        `yourapplication.views.frontend`)\n\n    .. versionadded:: 0.7\n       The `static_url_path`, `static_folder`, and `template_folder`\n       parameters were added.\n\n    .. versionadded:: 0.8\n       The `instance_path` and `instance_relative_config` parameters were\n       added.\n\n    .. versionadded:: 0.11\n       The `root_path` parameter was added.\n\n    .. versionadded:: 1.0\n       The ``host_matching`` and ``static_host`` parameters were added.\n\n    .. versionadded:: 1.0\n       The ``subdomain_matching`` parameter was added. Subdomain\n       matching needs to be enabled manually now. Setting\n       :data:`SERVER_NAME` does not implicitly enable it.\n\n    :param import_name: the name of the application package\n    :param static_url_path: can be used to specify a different path for the\n                            static files on the web.  Defaults to the name\n                            of the `static_folder` folder.\n    :param static_folder: The folder with static files that is served at\n        ``static_url_path``. Relative to the application ``root_path``\n        or an absolute path. Defaults to ``'static'``.\n    :param static_host: the host to use when adding the static route.\n        Defaults to None. Required when using ``host_matching=True``\n        with a ``static_folder`` configured.\n    :param host_matching: set ``url_map.host_matching`` attribute.\n        Defaults to False.\n    :param subdomain_matching: consider the subdomain relative to\n        :data:`SERVER_NAME` when matching routes. Defaults to False.\n    :param template_folder: the folder that contains the templates that should\n                            be used by the application.  Defaults to\n                            ``'templates'`` folder in the root path of the\n                            application.\n    :param instance_path: An alternative instance path for the application.\n                          By default the folder ``'instance'`` next to the\n                          package or module is assumed to be the instance\n                          path.\n    :param instance_relative_config: if set to ``True`` relative filenames\n                                     for loading the config are assumed to\n                                     be relative to the instance path instead\n                                     of the application root.\n    :param root_path: The path to the root of the application files.\n        This should only be set manually when it can't be detected\n        automatically, such as for namespace packages.\n    \"\"\""}, {"filename": "src/flask/app.py", "start_line": 277, "code": "    def session_cookie_name(self) -> str:\n        import warnings\n        warnings.warn(\n            \"'session_cookie_name' is deprecated and will be removed in Flask 2.3. Use\"\n            \" 'SESSION_COOKIE_NAME' in 'app.config' instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self.config[\"SESSION_COOKIE_NAME\"]\n    @session_cookie_name.setter", "documentation": "        \"\"\"The name of the cookie set by the session interface.\n\n        .. deprecated:: 2.2\n            Will be removed in Flask 2.3. Use ``app.config[\"SESSION_COOKIE_NAME\"]``\n            instead.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 318, "code": "    def send_file_max_age_default(self) -> t.Optional[timedelta]:\n        import warnings\n        warnings.warn(\n            \"'send_file_max_age_default' is deprecated and will be removed in Flask\"\n            \" 2.3. Use 'SEND_FILE_MAX_AGE_DEFAULT' in 'app.config' instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return _make_timedelta(self.config[\"SEND_FILE_MAX_AGE_DEFAULT\"])\n    @send_file_max_age_default.setter", "documentation": "        \"\"\"The default value for ``max_age`` for :func:`~flask.send_file`. The default\n        is ``None``, which tells the browser to use conditional requests instead of a\n        timed cache.\n\n        .. deprecated:: 2.2\n            Will be removed in Flask 2.3. Use\n            ``app.config[\"SEND_FILE_MAX_AGE_DEFAULT\"]`` instead.\n\n        .. versionchanged:: 2.0\n            Defaults to ``None`` instead of 12 hours.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 353, "code": "    def use_x_sendfile(self) -> bool:\n        import warnings\n        warnings.warn(\n            \"'use_x_sendfile' is deprecated and will be removed in Flask 2.3. Use\"\n            \" 'USE_X_SENDFILE' in 'app.config' instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self.config[\"USE_X_SENDFILE\"]\n    @use_x_sendfile.setter", "documentation": "        \"\"\"Enable this to use the ``X-Sendfile`` feature, assuming the server supports\n        it, from :func:`~flask.send_file`.\n\n        .. deprecated:: 2.2\n            Will be removed in Flask 2.3. Use ``app.config[\"USE_X_SENDFILE\"]`` instead.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 386, "code": "    def json_encoder(self) -> t.Type[json.JSONEncoder]:  # type: ignore[override]\n        import warnings\n        warnings.warn(\n            \"'app.json_encoder' is deprecated and will be removed in Flask 2.3.\"\n            \" Customize 'app.json_provider_class' or 'app.json' instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        if self._json_encoder is None:\n            from . import json\n            return json.JSONEncoder", "documentation": "        \"\"\"The JSON encoder class to use. Defaults to\n        :class:`~flask.json.JSONEncoder`.\n\n        .. deprecated:: 2.2\n             Will be removed in Flask 2.3. Customize\n             :attr:`json_provider_class` instead.\n\n        .. versionadded:: 0.10\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 425, "code": "    def json_decoder(self) -> t.Type[json.JSONDecoder]:  # type: ignore[override]\n        import warnings\n        warnings.warn(\n            \"'app.json_decoder' is deprecated and will be removed in Flask 2.3.\"\n            \" Customize 'app.json_provider_class' or 'app.json' instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        if self._json_decoder is None:\n            from . import json\n            return json.JSONDecoder", "documentation": "        \"\"\"The JSON decoder class to use. Defaults to\n        :class:`~flask.json.JSONDecoder`.\n\n        .. deprecated:: 2.2\n             Will be removed in Flask 2.3. Customize\n             :attr:`json_provider_class` instead.\n\n        .. versionadded:: 0.10\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 732, "code": "    def name(self) -> str:  # type: ignore\n        if self.import_name == \"__main__\":\n            fn = getattr(sys.modules[\"__main__\"], \"__file__\", None)\n            if fn is None:\n                return \"__main__\"\n            return os.path.splitext(os.path.basename(fn))[0]\n        return self.import_name\n    @property", "documentation": "        \"\"\"The name of the application.  This is usually the import name\n        with the difference that it's guessed from the run file if the\n        import name is main.  This name is used as a display name when\n        Flask needs the name of the application.  It can be set and overridden\n        to change the value.\n\n        .. versionadded:: 0.8\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 749, "code": "    def propagate_exceptions(self) -> bool:\n        import warnings\n        warnings.warn(\n            \"'propagate_exceptions' is deprecated and will be removed in Flask 2.3.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        rv = self.config[\"PROPAGATE_EXCEPTIONS\"]\n        if rv is not None:\n            return rv\n        return self.testing or self.debug", "documentation": "        \"\"\"Returns the value of the ``PROPAGATE_EXCEPTIONS`` configuration\n        value in case it's set, otherwise a sensible default is returned.\n\n        .. deprecated:: 2.2\n            Will be removed in Flask 2.3.\n\n        .. versionadded:: 0.7\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 771, "code": "    def logger(self) -> logging.Logger:\n        return create_logger(self)\n    @locked_cached_property", "documentation": "        \"\"\"A standard Python :class:`~logging.Logger` for the app, with\n        the same name as :attr:`name`.\n\n        In debug mode, the logger's :attr:`~logging.Logger.level` will\n        be set to :data:`~logging.DEBUG`.\n\n        If there are no handlers configured, a default handler will be\n        added. See :doc:`/logging` for more information.\n\n        .. versionchanged:: 1.1.0\n            The logger takes the same name as :attr:`name` rather than\n            hard-coding ``\"flask.app\"``.\n\n        .. versionchanged:: 1.0.0\n            Behavior was simplified. The logger is always named\n            ``\"flask.app\"``. The level is only set during configuration,\n            it doesn't check ``app.debug`` each time. Only one format is\n            used, not different ones depending on ``app.debug``. No\n            handlers are removed, and a handler is only added if no\n            handlers are already configured.\n\n        .. versionadded:: 0.3\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 798, "code": "    def jinja_env(self) -> Environment:\n        return self.create_jinja_environment()\n    @property", "documentation": "        \"\"\"The Jinja environment used to load templates.\n\n        The environment is created the first time this property is\n        accessed. Changing :attr:`jinja_options` after that will have no\n        effect.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 808, "code": "    def got_first_request(self) -> bool:\n        return self._got_first_request", "documentation": "        \"\"\"This attribute is set to ``True`` if the application started\n        handling the first request.\n\n        .. versionadded:: 0.8\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 816, "code": "    def make_config(self, instance_relative: bool = False) -> Config:\n        root_path = self.root_path\n        if instance_relative:\n            root_path = self.instance_path\n        defaults = dict(self.default_config)\n        defaults[\"ENV\"] = os.environ.get(\"FLASK_ENV\") or \"production\"\n        defaults[\"DEBUG\"] = get_debug_flag()\n        return self.config_class(root_path, defaults)", "documentation": "        \"\"\"Used to create the config attribute by the Flask constructor.\n        The `instance_relative` parameter is passed in from the constructor\n        of Flask (there named `instance_relative_config`) and indicates if\n        the config should be relative to the instance path or the root path\n        of the application.\n\n        .. versionadded:: 0.8\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 833, "code": "    def make_aborter(self) -> Aborter:\n        return self.aborter_class()", "documentation": "        \"\"\"Create the object to assign to :attr:`aborter`. That object\n        is called by :func:`flask.abort` to raise HTTP errors, and can\n        be called directly as well.\n\n        By default, this creates an instance of :attr:`aborter_class`,\n        which defaults to :class:`werkzeug.exceptions.Aborter`.\n\n        .. versionadded:: 2.2\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 845, "code": "    def auto_find_instance_path(self) -> str:\n        prefix, package_path = find_package(self.import_name)\n        if prefix is None:\n            return os.path.join(package_path, \"instance\")\n        return os.path.join(prefix, \"var\", f\"{self.name}-instance\")", "documentation": "        \"\"\"Tries to locate the instance path if it was not provided to the\n        constructor of the application class.  It will basically calculate\n        the path to a folder named ``instance`` next to your main file or\n        the package.\n\n        .. versionadded:: 0.8\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 858, "code": "    def open_instance_resource(self, resource: str, mode: str = \"rb\") -> t.IO[t.AnyStr]:\n        return open(os.path.join(self.instance_path, resource), mode)\n    @property", "documentation": "        \"\"\"Opens a resource from the application's instance folder\n        (:attr:`instance_path`).  Otherwise works like\n        :meth:`open_resource`.  Instance resources can also be opened for\n        writing.\n\n        :param resource: the name of the resource.  To access resources within\n                         subfolders use forward slashes as separator.\n        :param mode: resource file opening mode, default is 'rb'.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 871, "code": "    def templates_auto_reload(self) -> bool:\n        import warnings\n        warnings.warn(\n            \"'templates_auto_reload' is deprecated and will be removed in Flask 2.3.\"\n            \" Use 'TEMPLATES_AUTO_RELOAD' in 'app.config' instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        rv = self.config[\"TEMPLATES_AUTO_RELOAD\"]\n        return rv if rv is not None else self.debug\n    @templates_auto_reload.setter", "documentation": "        \"\"\"Reload templates when they are changed. Used by\n        :meth:`create_jinja_environment`. It is enabled by default in debug mode.\n\n        .. deprecated:: 2.2\n            Will be removed in Flask 2.3. Use ``app.config[\"TEMPLATES_AUTO_RELOAD\"]``\n            instead.\n\n        .. versionadded:: 1.0\n            This property was added but the underlying config and behavior\n            already existed.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 906, "code": "    def create_jinja_environment(self) -> Environment:\n        options = dict(self.jinja_options)\n        if \"autoescape\" not in options:\n            options[\"autoescape\"] = self.select_jinja_autoescape\n        if \"auto_reload\" not in options:\n            auto_reload = self.config[\"TEMPLATES_AUTO_RELOAD\"]\n            if auto_reload is None:\n                auto_reload = self.debug\n            options[\"auto_reload\"] = auto_reload\n        rv = self.jinja_environment(self, **options)\n        rv.globals.update(", "documentation": "        \"\"\"Create the Jinja environment based on :attr:`jinja_options`\n        and the various Jinja-related methods of the app. Changing\n        :attr:`jinja_options` after this will have no effect. Also adds\n        Flask-related globals and filters to the environment.\n\n        .. versionchanged:: 0.11\n           ``Environment.auto_reload`` set in accordance with\n           ``TEMPLATES_AUTO_RELOAD`` configuration option.\n\n        .. versionadded:: 0.5\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 946, "code": "    def create_global_jinja_loader(self) -> DispatchingJinjaLoader:\n        return DispatchingJinjaLoader(self)", "documentation": "        \"\"\"Creates the loader for the Jinja2 environment.  Can be used to\n        override just the loader and keeping the rest unchanged.  It's\n        discouraged to override this function.  Instead one should override\n        the :meth:`jinja_loader` function instead.\n\n        The global loader dispatches between the loaders of the application\n        and the individual blueprints.\n\n        .. versionadded:: 0.7\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 959, "code": "    def select_jinja_autoescape(self, filename: str) -> bool:\n        if filename is None:\n            return True\n        return filename.endswith((\".html\", \".htm\", \".xml\", \".xhtml\"))", "documentation": "        \"\"\"Returns ``True`` if autoescaping should be active for the given\n        template name. If no template name is given, returns `True`.\n\n        .. versionadded:: 0.5\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 969, "code": "    def update_template_context(self, context: dict) -> None:\n        names: t.Iterable[t.Optional[str]] = (None,)\n        if request:\n            names = chain(names, reversed(request.blueprints))\n        orig_ctx = context.copy()\n        for name in names:\n            if name in self.template_context_processors:\n                for func in self.template_context_processors[name]:\n                    context.update(func())\n        context.update(orig_ctx)", "documentation": "        \"\"\"Update the template context with some commonly used variables.\n        This injects request, session, config and g into the template\n        context as well as everything template context processors want\n        to inject.  Note that the as of Flask 0.6, the original values\n        in the context will not be overridden if a context processor\n        decides to return a value with the same key.\n\n        :param context: the context as a dictionary that is updated in place\n                        to add extra variables.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 997, "code": "    def make_shell_context(self) -> dict:\n        rv = {\"app\": self, \"g\": g}\n        for processor in self.shell_context_processors:\n            rv.update(processor())\n        return rv\n    @property", "documentation": "        \"\"\"Returns the shell context for an interactive shell for this\n        application.  This runs all the registered shell context\n        processors.\n\n        .. versionadded:: 0.11\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1010, "code": "    def env(self) -> str:\n        import warnings\n        warnings.warn(\n            \"'app.env' is deprecated and will be removed in Flask 2.3.\"\n            \" Use 'app.debug' instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self.config[\"ENV\"]\n    @env.setter", "documentation": "        \"\"\"What environment the app is running in. This maps to the :data:`ENV` config\n        key.\n\n        **Do not enable development when deploying in production.**\n\n        Default: ``'production'``\n\n        .. deprecated:: 2.2\n            Will be removed in Flask 2.3.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1044, "code": "    def debug(self) -> bool:\n        return self.config[\"DEBUG\"]\n    @debug.setter", "documentation": "        \"\"\"Whether debug mode is enabled. When using ``flask run`` to start the\n        development server, an interactive debugger will be shown for unhandled\n        exceptions, and the server will be reloaded when code changes. This maps to the\n        :data:`DEBUG` config key. It may not behave as expected if set late.\n\n        **Do not enable debug mode when deploying in production.**\n\n        Default: ``False``\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1194, "code": "    def test_client(self, use_cookies: bool = True, **kwargs: t.Any) -> \"FlaskClient\":\n        cls = self.test_client_class\n        if cls is None:\n            from .testing import FlaskClient as cls  # type: ignore\n        return cls(  # type: ignore\n            self, self.response_class, use_cookies=use_cookies, **kwargs\n        )", "documentation": "        \"\"\"Creates a test client for this application.  For information\n        about unit testing head over to :doc:`/testing`.\n\n        Note that if you are testing for assertions or exceptions in your\n        application code, you must set ``app.testing = True`` in order for the\n        exceptions to propagate to the test client.  Otherwise, the exception\n        will be handled by the application (not visible to the test client) and\n        the only indication of an AssertionError or other exception will be a\n        500 status code response to the test client.  See the :attr:`testing`\n        attribute.  For example::\n\n            app.testing = True\n            client = app.test_client()\n\n        The test client can be used in a ``with`` block to defer the closing down\n        of the context until the end of the ``with`` block.  This is useful if\n        you want to access the context locals for testing::\n\n            with app.test_client() as c:\n                rv = c.get('/?vodka=42')\n                assert request.args['vodka'] == '42'\n\n        Additionally, you may pass optional keyword arguments that will then\n        be passed to the application's :attr:`test_client_class` constructor.\n        For example::\n\n            from flask.testing import FlaskClient\n\n            class CustomClient(FlaskClient):\n                def __init__(self, *args, **kwargs):\n                    self._authentication = kwargs.pop(\"authentication\")\n                    super(CustomClient,self).__init__( *args, **kwargs)\n\n            app.test_client_class = CustomClient\n            client = app.test_client(authentication='Basic ....')\n\n        See :class:`~flask.testing.FlaskClient` for more information.\n\n        .. versionchanged:: 0.4\n           added support for ``with`` block usage for the client.\n\n        .. versionadded:: 0.7\n           The `use_cookies` parameter was added as well as the ability\n           to override the client to be used by setting the\n           :attr:`test_client_class` attribute.\n\n        .. versionchanged:: 0.11\n           Added `**kwargs` to support passing additional keyword arguments to\n           the constructor of :attr:`test_client_class`.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1252, "code": "    def test_cli_runner(self, **kwargs: t.Any) -> \"FlaskCliRunner\":\n        cls = self.test_cli_runner_class\n        if cls is None:\n            from .testing import FlaskCliRunner as cls  # type: ignore\n        return cls(self, **kwargs)  # type: ignore\n    @setupmethod", "documentation": "        \"\"\"Create a CLI runner for testing CLI commands.\n        See :ref:`testing-cli`.\n\n        Returns an instance of :attr:`test_cli_runner_class`, by default\n        :class:`~flask.testing.FlaskCliRunner`. The Flask app object is\n        passed as the first argument.\n\n        .. versionadded:: 1.0\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1270, "code": "    def register_blueprint(self, blueprint: \"Blueprint\", **options: t.Any) -> None:\n        blueprint.register(self, options)", "documentation": "        \"\"\"Register a :class:`~flask.Blueprint` on the application. Keyword\n        arguments passed to this method will override the defaults set on the\n        blueprint.\n\n        Calls the blueprint's :meth:`~flask.Blueprint.register` method after\n        recording the blueprint in the application's :attr:`blueprints`.\n\n        :param blueprint: The blueprint to register.\n        :param url_prefix: Blueprint routes will be prefixed with this.\n        :param subdomain: Blueprint routes will match on this subdomain.\n        :param url_defaults: Blueprint routes will use these default values for\n            view arguments.\n        :param options: Additional keyword arguments are passed to\n            :class:`~flask.blueprints.BlueprintSetupState`. They can be\n            accessed in :meth:`~flask.Blueprint.record` callbacks.\n\n        .. versionchanged:: 2.0.1\n            The ``name`` option can be used to change the (pre-dotted)\n            name the blueprint is registered with. This allows the same\n            blueprint to be registered multiple times with unique names\n            for ``url_for``.\n\n        .. versionadded:: 0.7\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1297, "code": "    def iter_blueprints(self) -> t.ValuesView[\"Blueprint\"]:\n        return self.blueprints.values()\n    @setupmethod", "documentation": "        \"\"\"Iterates over all blueprints by the order they were registered.\n\n        .. versionadded:: 0.11\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1479, "code": "    def before_first_request(self, f: T_before_first_request) -> T_before_first_request:\n        import warnings\n        warnings.warn(\n            \"'before_first_request' is deprecated and will be removed\"\n            \" in Flask 2.3. Run setup code while creating the\"\n            \" application instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        self.before_first_request_funcs.append(f)\n        return f", "documentation": "        \"\"\"Registers a function to be run before the first request to this\n        instance of the application.\n\n        The function will be called without any arguments and its return\n        value is ignored.\n\n        .. deprecated:: 2.2\n            Will be removed in Flask 2.3. Run setup code when creating\n            the application instead.\n\n        .. versionadded:: 0.8\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1505, "code": "    def teardown_appcontext(self, f: T_teardown) -> T_teardown:\n        self.teardown_appcontext_funcs.append(f)\n        return f\n    @setupmethod", "documentation": "        \"\"\"Registers a function to be called when the application\n        context is popped. The application context is typically popped\n        after the request context for each request, at the end of CLI\n        commands, or after a manually pushed context ends.\n\n        .. code-block:: python\n\n            with app.app_context():\n                ...\n\n        When the ``with`` block exits (or ``ctx.pop()`` is called), the\n        teardown functions are called just before the app context is\n        made inactive. Since a request context typically also manages an\n        application context it would also be called when you pop a\n        request context.\n\n        When a teardown function was called because of an unhandled\n        exception it will be passed an error object. If an\n        :meth:`errorhandler` is registered, it will handle the exception\n        and the teardown will not receive it.\n\n        Teardown functions must avoid raising exceptions. If they\n        execute code that might fail they must surround that code with a\n        ``try``/``except`` block and log any errors.\n\n        The return values of teardown functions are ignored.\n\n        .. versionadded:: 0.9\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1549, "code": "    def _find_error_handler(self, e: Exception) -> t.Optional[ft.ErrorHandlerCallable]:\n        exc_class, code = self._get_exc_class_and_code(type(e))\n        names = (*request.blueprints, None)\n        for c in (code, None) if code is not None else (None,):\n            for name in names:\n                handler_map = self.error_handler_spec[name][c]\n                if not handler_map:\n                    continue\n                for cls in exc_class.__mro__:\n                    handler = handler_map.get(cls)\n                    if handler is not None:", "documentation": "        \"\"\"Return a registered error handler for an exception in this order:\n        blueprint handler for a specific code, app handler for a specific code,\n        blueprint handler for an exception class, app handler for an exception\n        class, or ``None`` if a suitable handler is not found.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1607, "code": "    def trap_http_exception(self, e: Exception) -> bool:\n        if self.config[\"TRAP_HTTP_EXCEPTIONS\"]:\n            return True\n        trap_bad_request = self.config[\"TRAP_BAD_REQUEST_ERRORS\"]\n        if (\n            trap_bad_request is None\n            and self.debug\n            and isinstance(e, BadRequestKeyError)\n        ):\n            return True\n        if trap_bad_request:", "documentation": "        \"\"\"Checks if an HTTP exception should be trapped or not.  By default\n        this will return ``False`` for all exceptions except for a bad request\n        key error if ``TRAP_BAD_REQUEST_ERRORS`` is set to ``True``.  It\n        also returns ``True`` if ``TRAP_HTTP_EXCEPTIONS`` is set to ``True``.\n\n        This is called for all HTTP exceptions raised by a view function.\n        If it returns ``True`` for any exception the error handler for this\n        exception is not called and it shows up as regular exception in the\n        traceback.  This is helpful for debugging implicitly raised HTTP\n        exceptions.\n\n        .. versionchanged:: 1.0\n            Bad request errors are not trapped by default in debug mode.\n\n        .. versionadded:: 0.8\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1674, "code": "    def handle_exception(self, e: Exception) -> Response:\n        exc_info = sys.exc_info()\n        got_request_exception.send(self, exception=e)\n        propagate = self.config[\"PROPAGATE_EXCEPTIONS\"]\n        if propagate is None:\n            propagate = self.testing or self.debug\n        if propagate:\n            if exc_info[1] is e:\n                raise\n            raise e\n        self.log_exception(exc_info)", "documentation": "        \"\"\"Handle an exception that did not have an error handler\n        associated with it, or that was raised from an error handler.\n        This always causes a 500 ``InternalServerError``.\n\n        Always sends the :data:`got_request_exception` signal.\n\n        If :attr:`propagate_exceptions` is ``True``, such as in debug\n        mode, the error will be re-raised so that the debugger can\n        display it. Otherwise, the original exception is logged, and\n        an :exc:`~werkzeug.exceptions.InternalServerError` is returned.\n\n        If an error handler is registered for ``InternalServerError`` or\n        ``500``, it will be used. For consistency, the handler will\n        always receive the ``InternalServerError``. The original\n        unhandled exception is available as ``e.original_exception``.\n\n        .. versionchanged:: 1.1.0\n            Always passes the ``InternalServerError`` instance to the\n            handler, setting ``original_exception`` to the unhandled\n            error.\n\n        .. versionchanged:: 1.1.0\n            ``after_request`` functions and other finalization is done\n            even for the default 500 response when there is no handler.\n\n        .. versionadded:: 0.3\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1744, "code": "    def raise_routing_exception(self, request: Request) -> \"te.NoReturn\":\n        if (\n            not self.debug\n            or not isinstance(request.routing_exception, RequestRedirect)\n            or request.routing_exception.code in {307, 308}\n            or request.method in {\"GET\", \"HEAD\", \"OPTIONS\"}\n        ):\n            raise request.routing_exception  # type: ignore\n        from .debughelpers import FormDataRoutingRedirect\n        raise FormDataRoutingRedirect(request)", "documentation": "        \"\"\"Intercept routing exceptions and possibly do something else.\n\n        In debug mode, intercept a routing redirect and replace it with\n        an error if the body will be discarded.\n\n        With modern Werkzeug this shouldn't occur, since it now uses a\n        308 status which tells the browser to resend the method and\n        body.\n\n        .. versionchanged:: 2.1\n            Don't intercept 307 and 308 redirects.\n\n        :meta private:\n        :internal:\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1772, "code": "    def dispatch_request(self) -> ft.ResponseReturnValue:\n        req = request_ctx.request\n        if req.routing_exception is not None:\n            self.raise_routing_exception(req)\n        rule: Rule = req.url_rule  # type: ignore[assignment]\n        if (\n            getattr(rule, \"provide_automatic_options\", False)\n            and req.method == \"OPTIONS\"\n        ):\n            return self.make_default_options_response()\n        view_args: t.Dict[str, t.Any] = req.view_args  # type: ignore[assignment]", "documentation": "        \"\"\"Does the request dispatching.  Matches the URL and returns the\n        return value of the view or error handler.  This does not have to\n        be a response object.  In order to convert the return value to a\n        proper response object, call :func:`make_response`.\n\n        .. versionchanged:: 0.7\n           This no longer does the exception handling, this code was\n           moved to the new :meth:`full_dispatch_request`.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1797, "code": "    def full_dispatch_request(self) -> Response:\n        if not self._got_first_request:\n            with self._before_request_lock:\n                if not self._got_first_request:\n                    for func in self.before_first_request_funcs:\n                        self.ensure_sync(func)()\n                    self._got_first_request = True\n        try:\n            request_started.send(self)\n            rv = self.preprocess_request()\n            if rv is None:", "documentation": "        \"\"\"Dispatches the request and on top of that performs request\n        pre and postprocessing as well as HTTP exception catching and\n        error handling.\n\n        .. versionadded:: 0.7\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1853, "code": "    def make_default_options_response(self) -> Response:\n        adapter = request_ctx.url_adapter\n        methods = adapter.allowed_methods()  # type: ignore[union-attr]\n        rv = self.response_class()\n        rv.allow.update(methods)\n        return rv", "documentation": "        \"\"\"This method is called to create the default ``OPTIONS`` response.\n        This can be changed through subclassing to change the default\n        behavior of ``OPTIONS`` responses.\n\n        .. versionadded:: 0.7\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1866, "code": "    def should_ignore_error(self, error: t.Optional[BaseException]) -> bool:\n        return False", "documentation": "        \"\"\"This is called to figure out if an error should be ignored\n        or not as far as the teardown system is concerned.  If this\n        function returns ``True`` then the teardown handlers will not be\n        passed the error.\n\n        .. versionadded:: 0.10\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1876, "code": "    def ensure_sync(self, func: t.Callable) -> t.Callable:\n        if iscoroutinefunction(func):\n            return self.async_to_sync(func)\n        return func", "documentation": "        \"\"\"Ensure that the function is synchronous for WSGI workers.\n        Plain ``def`` functions are returned as-is. ``async def``\n        functions are wrapped to run and wait for the response.\n\n        Override this method to change how the app runs async views.\n\n        .. versionadded:: 2.0\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2037, "code": "    def redirect(self, location: str, code: int = 302) -> BaseResponse:\n        return _wz_redirect(location, code=code, Response=self.response_class)", "documentation": "        \"\"\"Create a redirect response object.\n\n        This is called by :func:`flask.redirect`, and can be called\n        directly as well.\n\n        :param location: The URL to redirect to.\n        :param code: The status code for the redirect.\n\n        .. versionadded:: 2.2\n            Moved from ``flask.redirect``, which calls this method.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2051, "code": "    def make_response(self, rv: ft.ResponseReturnValue) -> Response:\n        status = headers = None\n        if isinstance(rv, tuple):\n            len_rv = len(rv)\n            if len_rv == 3:\n                rv, status, headers = rv  # type: ignore[misc]\n            elif len_rv == 2:\n                if isinstance(rv[1], (Headers, dict, tuple, list)):\n                    rv, headers = rv\n                else:\n                    rv, status = rv  # type: ignore[assignment,misc]", "documentation": "        \"\"\"Convert the return value from a view function to an instance of\n        :attr:`response_class`.\n\n        :param rv: the return value from the view function. The view function\n            must return a response. Returning ``None``, or the view ending\n            without returning, is not allowed. The following types are allowed\n            for ``view_rv``:\n\n            ``str``\n                A response object is created with the string encoded to UTF-8\n                as the body.\n\n            ``bytes``\n                A response object is created with the bytes as the body.\n\n            ``dict``\n                A dictionary that will be jsonify'd before being returned.\n\n            ``list``\n                A list that will be jsonify'd before being returned.\n\n            ``generator`` or ``iterator``\n                A generator that returns ``str`` or ``bytes`` to be\n                streamed as the response.\n\n            ``tuple``\n                Either ``(body, status, headers)``, ``(body, status)``, or\n                ``(body, headers)``, where ``body`` is any of the other types\n                allowed here, ``status`` is a string or an integer, and\n                ``headers`` is a dictionary or a list of ``(key, value)``\n                tuples. If ``body`` is a :attr:`response_class` instance,\n                ``status`` overwrites the exiting value and ``headers`` are\n                extended.\n\n            :attr:`response_class`\n                The object is returned unchanged.\n\n            other :class:`~werkzeug.wrappers.Response` class\n                The object is coerced to :attr:`response_class`.\n\n            :func:`callable`\n                The function is called as a WSGI application. The result is\n                used to create a response object.\n\n        .. versionchanged:: 2.2\n            A generator will be converted to a streaming response.\n            A list will be converted to a JSON response.\n\n        .. versionchanged:: 1.1\n            A dict will be converted to a JSON response.\n\n        .. versionchanged:: 0.9\n           Previously a tuple was interpreted as the arguments for the\n           response object.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2233, "code": "    def inject_url_defaults(self, endpoint: str, values: dict) -> None:\n        names: t.Iterable[t.Optional[str]] = (None,)\n        if \".\" in endpoint:\n            names = chain(\n                names, reversed(_split_blueprint_path(endpoint.rpartition(\".\")[0]))\n            )\n        for name in names:\n            if name in self.url_default_functions:\n                for func in self.url_default_functions[name]:\n                    func(endpoint, values)", "documentation": "        \"\"\"Injects the URL defaults for the given endpoint directly into\n        the values dictionary passed.  This is used internally and\n        automatically called on URL building.\n\n        .. versionadded:: 0.7\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2288, "code": "    def preprocess_request(self) -> t.Optional[ft.ResponseReturnValue]:\n        names = (None, *reversed(request.blueprints))\n        for name in names:\n            if name in self.url_value_preprocessors:\n                for url_func in self.url_value_preprocessors[name]:\n                    url_func(request.endpoint, request.view_args)\n        for name in names:\n            if name in self.before_request_funcs:\n                for before_func in self.before_request_funcs[name]:\n                    rv = self.ensure_sync(before_func)()\n                    if rv is not None:", "documentation": "        \"\"\"Called before the request is dispatched. Calls\n        :attr:`url_value_preprocessors` registered with the app and the\n        current blueprint (if any). Then calls :attr:`before_request_funcs`\n        registered with the app and the blueprint.\n\n        If any :meth:`before_request` handler returns a non-None value, the\n        value is handled as if it was the return value from the view, and\n        further request handling is stopped.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2315, "code": "    def process_response(self, response: Response) -> Response:\n        ctx = request_ctx._get_current_object()  # type: ignore[attr-defined]\n        for func in ctx._after_request_functions:\n            response = self.ensure_sync(func)(response)\n        for name in chain(request.blueprints, (None,)):\n            if name in self.after_request_funcs:\n                for func in reversed(self.after_request_funcs[name]):\n                    response = self.ensure_sync(func)(response)\n        if not self.session_interface.is_null_session(ctx.session):\n            self.session_interface.save_session(self, ctx.session, response)\n        return response", "documentation": "        \"\"\"Can be overridden in order to modify the response object\n        before it's sent to the WSGI server.  By default this will\n        call all the :meth:`after_request` decorated functions.\n\n        .. versionchanged:: 0.5\n           As of Flask 0.5 the functions registered for after request\n           execution are called in reverse order of registration.\n\n        :param response: a :attr:`response_class` object.\n        :return: a new response object or the same, has to be an\n                 instance of :attr:`response_class`.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2401, "code": "    def app_context(self) -> AppContext:\n        return AppContext(self)", "documentation": "        \"\"\"Create an :class:`~flask.ctx.AppContext`. Use as a ``with``\n        block to push the context, which will make :data:`current_app`\n        point at this application.\n\n        An application context is automatically pushed by\n        :meth:`RequestContext.push() <flask.ctx.RequestContext.push>`\n        when handling a request, and when running a CLI command. Use\n        this to manually create a context outside of these situations.\n\n        ::\n\n            with app.app_context():\n                init_db()\n\n        See :doc:`/appcontext`.\n\n        .. versionadded:: 0.9\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2422, "code": "    def request_context(self, environ: dict) -> RequestContext:\n        return RequestContext(self, environ)", "documentation": "        \"\"\"Create a :class:`~flask.ctx.RequestContext` representing a\n        WSGI environment. Use a ``with`` block to push the context,\n        which will make :data:`request` point at this request.\n\n        See :doc:`/reqcontext`.\n\n        Typically you should not call this from your own code. A request\n        context is automatically pushed by the :meth:`wsgi_app` when\n        handling a request. Use :meth:`test_request_context` to create\n        an environment and context instead of this method.\n\n        :param environ: a WSGI environment\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2438, "code": "    def test_request_context(self, *args: t.Any, **kwargs: t.Any) -> RequestContext:\n        from .testing import EnvironBuilder\n        builder = EnvironBuilder(self, *args, **kwargs)\n        try:\n            return self.request_context(builder.get_environ())\n        finally:\n            builder.close()", "documentation": "        \"\"\"Create a :class:`~flask.ctx.RequestContext` for a WSGI\n        environment created from the given values. This is mostly useful\n        during testing, where you may want to run a function that uses\n        request data without dispatching a full request.\n\n        See :doc:`/reqcontext`.\n\n        Use a ``with`` block to push the context, which will make\n        :data:`request` point at the request for the created\n        environment. ::\n\n            with app.test_request_context(...):\n                generate_report()\n\n        When using the shell, it may be easier to push and pop the\n        context manually to avoid indentation. ::\n\n            ctx = app.test_request_context(...)\n            ctx.push()\n            ...\n            ctx.pop()\n\n        Takes the same arguments as Werkzeug's\n        :class:`~werkzeug.test.EnvironBuilder`, with some defaults from\n        the application. See the linked Werkzeug docs for most of the\n        available arguments. Flask-specific behavior is listed here.\n\n        :param path: URL path being requested.\n        :param base_url: Base URL where the app is being served, which\n            ``path`` is relative to. If not given, built from\n            :data:`PREFERRED_URL_SCHEME`, ``subdomain``,\n            :data:`SERVER_NAME`, and :data:`APPLICATION_ROOT`.\n        :param subdomain: Subdomain name to append to\n            :data:`SERVER_NAME`.\n        :param url_scheme: Scheme to use instead of\n            :data:`PREFERRED_URL_SCHEME`.\n        :param data: The request body, either as a string or a dict of\n            form keys and values.\n        :param json: If given, this is serialized as JSON and passed as\n            ``data``. Also defaults ``content_type`` to\n            ``application/json``.\n        :param args: other positional arguments passed to\n            :class:`~werkzeug.test.EnvironBuilder`.\n        :param kwargs: other keyword arguments passed to\n            :class:`~werkzeug.test.EnvironBuilder`.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2494, "code": "    def wsgi_app(self, environ: dict, start_response: t.Callable) -> t.Any:\n        ctx = self.request_context(environ)\n        error: t.Optional[BaseException] = None\n        try:\n            try:\n                ctx.push()\n                response = self.full_dispatch_request()\n            except Exception as e:\n                error = e\n                response = self.handle_exception(e)\n            except:  # noqa: B001", "documentation": "        \"\"\"The actual WSGI application. This is not implemented in\n        :meth:`__call__` so that middlewares can be applied without\n        losing a reference to the app object. Instead of doing this::\n\n            app = MyMiddleware(app)\n\n        It's a better idea to do this instead::\n\n            app.wsgi_app = MyMiddleware(app.wsgi_app)\n\n        Then you still have the original application object around and\n        can continue to call methods on it.\n\n        .. versionchanged:: 0.7\n            Teardown events for the request and app contexts are called\n            even if an unhandled error occurs. Other events may not be\n            called depending on when an error occurs during dispatch.\n            See :ref:`callbacks-and-errors`.\n\n        :param environ: A WSGI environment.\n        :param start_response: A callable accepting a status code,\n            a list of headers, and an optional exception context to\n            start the response.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2542, "code": "    def __call__(self, environ: dict, start_response: t.Callable) -> t.Any:\n        return self.wsgi_app(environ, start_response)", "documentation": "        \"\"\"The WSGI server calls the Flask application object as the\n        WSGI application. This calls :meth:`wsgi_app`, which can be\n        wrapped to apply middleware.\n        \"\"\""}]}
{"repository": "pallets/flask", "commit_sha": "e0dad454810dd081947d3ca2ff376c5096185698", "commit_message": "update docs about contexts", "commit_date": "2022-07-08T14:08:54+00:00", "author": "David Lord", "file": "src/flask/app.py", "patch": "@@ -1284,29 +1284,30 @@ def before_first_request(self, f: T_before_first_request) -> T_before_first_requ\n \n     @setupmethod\n     def teardown_appcontext(self, f: T_teardown) -> T_teardown:\n-        \"\"\"Registers a function to be called when the application context\n-        ends.  These functions are typically also called when the request\n-        context is popped.\n+        \"\"\"Registers a function to be called when the application\n+        context is popped. The application context is typically popped\n+        after the request context for each request, at the end of CLI\n+        commands, or after a manually pushed context ends.\n \n-        Example::\n-\n-            ctx = app.app_context()\n-            ctx.push()\n-            ...\n-            ctx.pop()\n-\n-        When ``ctx.pop()`` is executed in the above example, the teardown\n-        functions are called just before the app context moves from the\n-        stack of active contexts.  This becomes relevant if you are using\n-        such constructs in tests.\n-\n-        Since a request context typically also manages an application\n-        context it would also be called when you pop a request context.\n+        .. code-block:: python\n \n-        When a teardown function was called because of an unhandled exception\n-        it will be passed an error object. If an :meth:`errorhandler` is\n-        registered, it will handle the exception and the teardown will not\n-        receive it.\n+            with app.app_context():\n+                ...\n+\n+        When the ``with`` block exits (or ``ctx.pop()`` is called), the\n+        teardown functions are called just before the app context is\n+        made inactive. Since a request context typically also manages an\n+        application context it would also be called when you pop a\n+        request context.\n+\n+        When a teardown function was called because of an unhandled\n+        exception it will be passed an error object. If an\n+        :meth:`errorhandler` is registered, it will handle the exception\n+        and the teardown will not receive it.\n+\n+        Teardown functions must avoid raising exceptions. If they\n+        execute code that might fail they must surround that code with a\n+        ``try``/``except`` block and log any errors.\n \n         The return values of teardown functions are ignored.\n ", "before_segments": [{"filename": "src/flask/app.py", "start_line": 109, "code": "class Flask(Scaffold):\n    request_class = Request\n    response_class = Response\n    aborter_class = Aborter\n    jinja_environment = Environment\n    app_ctx_globals_class = _AppCtxGlobals\n    config_class = Config\n    testing = ConfigAttribute(\"TESTING\")\n    secret_key = ConfigAttribute(\"SECRET_KEY\")\n    session_cookie_name = ConfigAttribute(\"SESSION_COOKIE_NAME\")\n    permanent_session_lifetime = ConfigAttribute(", "documentation": "    \"\"\"The flask object implements a WSGI application and acts as the central\n    object.  It is passed the name of the module or package of the\n    application.  Once it is created it will act as a central registry for\n    the view functions, the URL rules, template configuration and much more.\n\n    The name of the package is used to resolve resources from inside the\n    package or the folder the module is contained in depending on if the\n    package parameter resolves to an actual python package (a folder with\n    an :file:`__init__.py` file inside) or a standard module (just a ``.py`` file).\n\n    For more information about resource loading, see :func:`open_resource`.\n\n    Usually you create a :class:`Flask` instance in your main module or\n    in the :file:`__init__.py` file of your package like this::\n\n        from flask import Flask\n        app = Flask(__name__)\n\n    .. admonition:: About the First Parameter\n\n        The idea of the first parameter is to give Flask an idea of what\n        belongs to your application.  This name is used to find resources\n        on the filesystem, can be used by extensions to improve debugging\n        information and a lot more.\n\n        So it's important what you provide there.  If you are using a single\n        module, `__name__` is always the correct value.  If you however are\n        using a package, it's usually recommended to hardcode the name of\n        your package there.\n\n        For example if your application is defined in :file:`yourapplication/app.py`\n        you should create it with one of the two versions below::\n\n            app = Flask('yourapplication')\n            app = Flask(__name__.split('.')[0])\n\n        Why is that?  The application will work even with `__name__`, thanks\n        to how resources are looked up.  However it will make debugging more\n        painful.  Certain extensions can make assumptions based on the\n        import name of your application.  For example the Flask-SQLAlchemy\n        extension will look for the code in your application that triggered\n        an SQL query in debug mode.  If the import name is not properly set\n        up, that debugging information is lost.  (For example it would only\n        pick up SQL queries in `yourapplication.app` and not\n        `yourapplication.views.frontend`)\n\n    .. versionadded:: 0.7\n       The `static_url_path`, `static_folder`, and `template_folder`\n       parameters were added.\n\n    .. versionadded:: 0.8\n       The `instance_path` and `instance_relative_config` parameters were\n       added.\n\n    .. versionadded:: 0.11\n       The `root_path` parameter was added.\n\n    .. versionadded:: 1.0\n       The ``host_matching`` and ``static_host`` parameters were added.\n\n    .. versionadded:: 1.0\n       The ``subdomain_matching`` parameter was added. Subdomain\n       matching needs to be enabled manually now. Setting\n       :data:`SERVER_NAME` does not implicitly enable it.\n\n    :param import_name: the name of the application package\n    :param static_url_path: can be used to specify a different path for the\n                            static files on the web.  Defaults to the name\n                            of the `static_folder` folder.\n    :param static_folder: The folder with static files that is served at\n        ``static_url_path``. Relative to the application ``root_path``\n        or an absolute path. Defaults to ``'static'``.\n    :param static_host: the host to use when adding the static route.\n        Defaults to None. Required when using ``host_matching=True``\n        with a ``static_folder`` configured.\n    :param host_matching: set ``url_map.host_matching`` attribute.\n        Defaults to False.\n    :param subdomain_matching: consider the subdomain relative to\n        :data:`SERVER_NAME` when matching routes. Defaults to False.\n    :param template_folder: the folder that contains the templates that should\n                            be used by the application.  Defaults to\n                            ``'templates'`` folder in the root path of the\n                            application.\n    :param instance_path: An alternative instance path for the application.\n                          By default the folder ``'instance'`` next to the\n                          package or module is assumed to be the instance\n                          path.\n    :param instance_relative_config: if set to ``True`` relative filenames\n                                     for loading the config are assumed to\n                                     be relative to the instance path instead\n                                     of the application root.\n    :param root_path: The path to the root of the application files.\n        This should only be set manually when it can't be detected\n        automatically, such as for namespace packages.\n    \"\"\""}, {"filename": "src/flask/app.py", "start_line": 568, "code": "    def name(self) -> str:  # type: ignore\n        if self.import_name == \"__main__\":\n            fn = getattr(sys.modules[\"__main__\"], \"__file__\", None)\n            if fn is None:\n                return \"__main__\"\n            return os.path.splitext(os.path.basename(fn))[0]\n        return self.import_name\n    @property", "documentation": "        \"\"\"The name of the application.  This is usually the import name\n        with the difference that it's guessed from the run file if the\n        import name is main.  This name is used as a display name when\n        Flask needs the name of the application.  It can be set and overridden\n        to change the value.\n\n        .. versionadded:: 0.8\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 585, "code": "    def propagate_exceptions(self) -> bool:\n        rv = self.config[\"PROPAGATE_EXCEPTIONS\"]\n        if rv is not None:\n            return rv\n        return self.testing or self.debug\n    @locked_cached_property", "documentation": "        \"\"\"Returns the value of the ``PROPAGATE_EXCEPTIONS`` configuration\n        value in case it's set, otherwise a sensible default is returned.\n\n        .. versionadded:: 0.7\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 597, "code": "    def logger(self) -> logging.Logger:\n        return create_logger(self)\n    @locked_cached_property", "documentation": "        \"\"\"A standard Python :class:`~logging.Logger` for the app, with\n        the same name as :attr:`name`.\n\n        In debug mode, the logger's :attr:`~logging.Logger.level` will\n        be set to :data:`~logging.DEBUG`.\n\n        If there are no handlers configured, a default handler will be\n        added. See :doc:`/logging` for more information.\n\n        .. versionchanged:: 1.1.0\n            The logger takes the same name as :attr:`name` rather than\n            hard-coding ``\"flask.app\"``.\n\n        .. versionchanged:: 1.0.0\n            Behavior was simplified. The logger is always named\n            ``\"flask.app\"``. The level is only set during configuration,\n            it doesn't check ``app.debug`` each time. Only one format is\n            used, not different ones depending on ``app.debug``. No\n            handlers are removed, and a handler is only added if no\n            handlers are already configured.\n\n        .. versionadded:: 0.3\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 624, "code": "    def jinja_env(self) -> Environment:\n        return self.create_jinja_environment()\n    @property", "documentation": "        \"\"\"The Jinja environment used to load templates.\n\n        The environment is created the first time this property is\n        accessed. Changing :attr:`jinja_options` after that will have no\n        effect.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 634, "code": "    def got_first_request(self) -> bool:\n        return self._got_first_request", "documentation": "        \"\"\"This attribute is set to ``True`` if the application started\n        handling the first request.\n\n        .. versionadded:: 0.8\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 642, "code": "    def make_config(self, instance_relative: bool = False) -> Config:\n        root_path = self.root_path\n        if instance_relative:\n            root_path = self.instance_path\n        defaults = dict(self.default_config)\n        defaults[\"ENV\"] = get_env()\n        defaults[\"DEBUG\"] = get_debug_flag()\n        return self.config_class(root_path, defaults)", "documentation": "        \"\"\"Used to create the config attribute by the Flask constructor.\n        The `instance_relative` parameter is passed in from the constructor\n        of Flask (there named `instance_relative_config`) and indicates if\n        the config should be relative to the instance path or the root path\n        of the application.\n\n        .. versionadded:: 0.8\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 659, "code": "    def make_aborter(self) -> Aborter:\n        return self.aborter_class()", "documentation": "        \"\"\"Create the object to assign to :attr:`aborter`. That object\n        is called by :func:`flask.abort` to raise HTTP errors, and can\n        be called directly as well.\n\n        By default, this creates an instance of :attr:`aborter_class`,\n        which defaults to :class:`werkzeug.exceptions.Aborter`.\n\n        .. versionadded:: 2.2\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 671, "code": "    def auto_find_instance_path(self) -> str:\n        prefix, package_path = find_package(self.import_name)\n        if prefix is None:\n            return os.path.join(package_path, \"instance\")\n        return os.path.join(prefix, \"var\", f\"{self.name}-instance\")", "documentation": "        \"\"\"Tries to locate the instance path if it was not provided to the\n        constructor of the application class.  It will basically calculate\n        the path to a folder named ``instance`` next to your main file or\n        the package.\n\n        .. versionadded:: 0.8\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 684, "code": "    def open_instance_resource(self, resource: str, mode: str = \"rb\") -> t.IO[t.AnyStr]:\n        return open(os.path.join(self.instance_path, resource), mode)\n    @property", "documentation": "        \"\"\"Opens a resource from the application's instance folder\n        (:attr:`instance_path`).  Otherwise works like\n        :meth:`open_resource`.  Instance resources can also be opened for\n        writing.\n\n        :param resource: the name of the resource.  To access resources within\n                         subfolders use forward slashes as separator.\n        :param mode: resource file opening mode, default is 'rb'.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 697, "code": "    def templates_auto_reload(self) -> bool:\n        rv = self.config[\"TEMPLATES_AUTO_RELOAD\"]\n        return rv if rv is not None else self.debug\n    @templates_auto_reload.setter", "documentation": "        \"\"\"Reload templates when they are changed. Used by\n        :meth:`create_jinja_environment`.\n\n        This attribute can be configured with :data:`TEMPLATES_AUTO_RELOAD`. If\n        not set, it will be enabled in debug mode.\n\n        .. versionadded:: 1.0\n            This property was added but the underlying config and behavior\n            already existed.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 715, "code": "    def create_jinja_environment(self) -> Environment:\n        options = dict(self.jinja_options)\n        if \"autoescape\" not in options:\n            options[\"autoescape\"] = self.select_jinja_autoescape\n        if \"auto_reload\" not in options:\n            options[\"auto_reload\"] = self.templates_auto_reload\n        rv = self.jinja_environment(self, **options)\n        rv.globals.update(\n            url_for=self.url_for,\n            get_flashed_messages=get_flashed_messages,\n            config=self.config,", "documentation": "        \"\"\"Create the Jinja environment based on :attr:`jinja_options`\n        and the various Jinja-related methods of the app. Changing\n        :attr:`jinja_options` after this will have no effect. Also adds\n        Flask-related globals and filters to the environment.\n\n        .. versionchanged:: 0.11\n           ``Environment.auto_reload`` set in accordance with\n           ``TEMPLATES_AUTO_RELOAD`` configuration option.\n\n        .. versionadded:: 0.5\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 750, "code": "    def create_global_jinja_loader(self) -> DispatchingJinjaLoader:\n        return DispatchingJinjaLoader(self)", "documentation": "        \"\"\"Creates the loader for the Jinja2 environment.  Can be used to\n        override just the loader and keeping the rest unchanged.  It's\n        discouraged to override this function.  Instead one should override\n        the :meth:`jinja_loader` function instead.\n\n        The global loader dispatches between the loaders of the application\n        and the individual blueprints.\n\n        .. versionadded:: 0.7\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 763, "code": "    def select_jinja_autoescape(self, filename: str) -> bool:\n        if filename is None:\n            return True\n        return filename.endswith((\".html\", \".htm\", \".xml\", \".xhtml\"))", "documentation": "        \"\"\"Returns ``True`` if autoescaping should be active for the given\n        template name. If no template name is given, returns `True`.\n\n        .. versionadded:: 0.5\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 773, "code": "    def update_template_context(self, context: dict) -> None:\n        names: t.Iterable[t.Optional[str]] = (None,)\n        if request:\n            names = chain(names, reversed(request.blueprints))\n        orig_ctx = context.copy()\n        for name in names:\n            if name in self.template_context_processors:\n                for func in self.template_context_processors[name]:\n                    context.update(func())\n        context.update(orig_ctx)", "documentation": "        \"\"\"Update the template context with some commonly used variables.\n        This injects request, session, config and g into the template\n        context as well as everything template context processors want\n        to inject.  Note that the as of Flask 0.6, the original values\n        in the context will not be overridden if a context processor\n        decides to return a value with the same key.\n\n        :param context: the context as a dictionary that is updated in place\n                        to add extra variables.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 801, "code": "    def make_shell_context(self) -> dict:\n        rv = {\"app\": self, \"g\": g}\n        for processor in self.shell_context_processors:\n            rv.update(processor())\n        return rv\n    env = ConfigAttribute(\"ENV\")\n    @property", "documentation": "        \"\"\"Returns the shell context for an interactive shell for this\n        application.  This runs all the registered shell context\n        processors.\n\n        .. versionadded:: 0.11\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 825, "code": "    def debug(self) -> bool:\n        return self.config[\"DEBUG\"]\n    @debug.setter", "documentation": "        \"\"\"Whether debug mode is enabled. When using ``flask run`` to start\n        the development server, an interactive debugger will be shown for\n        unhandled exceptions, and the server will be reloaded when code\n        changes. This maps to the :data:`DEBUG` config key. This is\n        enabled when :attr:`env` is ``'development'`` and is overridden\n        by the ``FLASK_DEBUG`` environment variable. It may not behave as\n        expected if set in code.\n\n        **Do not enable debug mode when deploying in production.**\n\n        Default: ``True`` if :attr:`env` is ``'development'``, or\n        ``False`` otherwise.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 974, "code": "    def test_client(self, use_cookies: bool = True, **kwargs: t.Any) -> \"FlaskClient\":\n        cls = self.test_client_class\n        if cls is None:\n            from .testing import FlaskClient as cls  # type: ignore\n        return cls(  # type: ignore\n            self, self.response_class, use_cookies=use_cookies, **kwargs\n        )", "documentation": "        \"\"\"Creates a test client for this application.  For information\n        about unit testing head over to :doc:`/testing`.\n\n        Note that if you are testing for assertions or exceptions in your\n        application code, you must set ``app.testing = True`` in order for the\n        exceptions to propagate to the test client.  Otherwise, the exception\n        will be handled by the application (not visible to the test client) and\n        the only indication of an AssertionError or other exception will be a\n        500 status code response to the test client.  See the :attr:`testing`\n        attribute.  For example::\n\n            app.testing = True\n            client = app.test_client()\n\n        The test client can be used in a ``with`` block to defer the closing down\n        of the context until the end of the ``with`` block.  This is useful if\n        you want to access the context locals for testing::\n\n            with app.test_client() as c:\n                rv = c.get('/?vodka=42')\n                assert request.args['vodka'] == '42'\n\n        Additionally, you may pass optional keyword arguments that will then\n        be passed to the application's :attr:`test_client_class` constructor.\n        For example::\n\n            from flask.testing import FlaskClient\n\n            class CustomClient(FlaskClient):\n                def __init__(self, *args, **kwargs):\n                    self._authentication = kwargs.pop(\"authentication\")\n                    super(CustomClient,self).__init__( *args, **kwargs)\n\n            app.test_client_class = CustomClient\n            client = app.test_client(authentication='Basic ....')\n\n        See :class:`~flask.testing.FlaskClient` for more information.\n\n        .. versionchanged:: 0.4\n           added support for ``with`` block usage for the client.\n\n        .. versionadded:: 0.7\n           The `use_cookies` parameter was added as well as the ability\n           to override the client to be used by setting the\n           :attr:`test_client_class` attribute.\n\n        .. versionchanged:: 0.11\n           Added `**kwargs` to support passing additional keyword arguments to\n           the constructor of :attr:`test_client_class`.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1032, "code": "    def test_cli_runner(self, **kwargs: t.Any) -> \"FlaskCliRunner\":\n        cls = self.test_cli_runner_class\n        if cls is None:\n            from .testing import FlaskCliRunner as cls  # type: ignore\n        return cls(self, **kwargs)  # type: ignore\n    @setupmethod", "documentation": "        \"\"\"Create a CLI runner for testing CLI commands.\n        See :ref:`testing-cli`.\n\n        Returns an instance of :attr:`test_cli_runner_class`, by default\n        :class:`~flask.testing.FlaskCliRunner`. The Flask app object is\n        passed as the first argument.\n\n        .. versionadded:: 1.0\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1050, "code": "    def register_blueprint(self, blueprint: \"Blueprint\", **options: t.Any) -> None:\n        blueprint.register(self, options)", "documentation": "        \"\"\"Register a :class:`~flask.Blueprint` on the application. Keyword\n        arguments passed to this method will override the defaults set on the\n        blueprint.\n\n        Calls the blueprint's :meth:`~flask.Blueprint.register` method after\n        recording the blueprint in the application's :attr:`blueprints`.\n\n        :param blueprint: The blueprint to register.\n        :param url_prefix: Blueprint routes will be prefixed with this.\n        :param subdomain: Blueprint routes will match on this subdomain.\n        :param url_defaults: Blueprint routes will use these default values for\n            view arguments.\n        :param options: Additional keyword arguments are passed to\n            :class:`~flask.blueprints.BlueprintSetupState`. They can be\n            accessed in :meth:`~flask.Blueprint.record` callbacks.\n\n        .. versionchanged:: 2.0.1\n            The ``name`` option can be used to change the (pre-dotted)\n            name the blueprint is registered with. This allows the same\n            blueprint to be registered multiple times with unique names\n            for ``url_for``.\n\n        .. versionadded:: 0.7\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1077, "code": "    def iter_blueprints(self) -> t.ValuesView[\"Blueprint\"]:\n        return self.blueprints.values()\n    @setupmethod", "documentation": "        \"\"\"Iterates over all blueprints by the order they were registered.\n\n        .. versionadded:: 0.11\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1259, "code": "    def before_first_request(self, f: T_before_first_request) -> T_before_first_request:\n        import warnings\n        warnings.warn(\n            \"'before_first_request' is deprecated and will be removed\"\n            \" in Flask 2.3. Run setup code while creating the\"\n            \" application instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        self.before_first_request_funcs.append(f)\n        return f", "documentation": "        \"\"\"Registers a function to be run before the first request to this\n        instance of the application.\n\n        The function will be called without any arguments and its return\n        value is ignored.\n\n        .. deprecated:: 2.2\n            Will be removed in Flask 2.3. Run setup code when creating\n            the application instead.\n\n        .. versionadded:: 0.8\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1285, "code": "    def teardown_appcontext(self, f: T_teardown) -> T_teardown:\n        self.teardown_appcontext_funcs.append(f)\n        return f\n    @setupmethod", "documentation": "        \"\"\"Registers a function to be called when the application context\n        ends.  These functions are typically also called when the request\n        context is popped.\n\n        Example::\n\n            ctx = app.app_context()\n            ctx.push()\n            ...\n            ctx.pop()\n\n        When ``ctx.pop()`` is executed in the above example, the teardown\n        functions are called just before the app context moves from the\n        stack of active contexts.  This becomes relevant if you are using\n        such constructs in tests.\n\n        Since a request context typically also manages an application\n        context it would also be called when you pop a request context.\n\n        When a teardown function was called because of an unhandled exception\n        it will be passed an error object. If an :meth:`errorhandler` is\n        registered, it will handle the exception and the teardown will not\n        receive it.\n\n        The return values of teardown functions are ignored.\n\n        .. versionadded:: 0.9\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1328, "code": "    def _find_error_handler(self, e: Exception) -> t.Optional[ft.ErrorHandlerCallable]:\n        exc_class, code = self._get_exc_class_and_code(type(e))\n        names = (*request.blueprints, None)\n        for c in (code, None) if code is not None else (None,):\n            for name in names:\n                handler_map = self.error_handler_spec[name][c]\n                if not handler_map:\n                    continue\n                for cls in exc_class.__mro__:\n                    handler = handler_map.get(cls)\n                    if handler is not None:", "documentation": "        \"\"\"Return a registered error handler for an exception in this order:\n        blueprint handler for a specific code, app handler for a specific code,\n        blueprint handler for an exception class, app handler for an exception\n        class, or ``None`` if a suitable handler is not found.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1386, "code": "    def trap_http_exception(self, e: Exception) -> bool:\n        if self.config[\"TRAP_HTTP_EXCEPTIONS\"]:\n            return True\n        trap_bad_request = self.config[\"TRAP_BAD_REQUEST_ERRORS\"]\n        if (\n            trap_bad_request is None\n            and self.debug\n            and isinstance(e, BadRequestKeyError)\n        ):\n            return True\n        if trap_bad_request:", "documentation": "        \"\"\"Checks if an HTTP exception should be trapped or not.  By default\n        this will return ``False`` for all exceptions except for a bad request\n        key error if ``TRAP_BAD_REQUEST_ERRORS`` is set to ``True``.  It\n        also returns ``True`` if ``TRAP_HTTP_EXCEPTIONS`` is set to ``True``.\n\n        This is called for all HTTP exceptions raised by a view function.\n        If it returns ``True`` for any exception the error handler for this\n        exception is not called and it shows up as regular exception in the\n        traceback.  This is helpful for debugging implicitly raised HTTP\n        exceptions.\n\n        .. versionchanged:: 1.0\n            Bad request errors are not trapped by default in debug mode.\n\n        .. versionadded:: 0.8\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1453, "code": "    def handle_exception(self, e: Exception) -> Response:\n        exc_info = sys.exc_info()\n        got_request_exception.send(self, exception=e)\n        if self.propagate_exceptions:\n            if exc_info[1] is e:\n                raise\n            raise e\n        self.log_exception(exc_info)\n        server_error: t.Union[InternalServerError, ft.ResponseReturnValue]\n        server_error = InternalServerError(original_exception=e)\n        handler = self._find_error_handler(server_error)", "documentation": "        \"\"\"Handle an exception that did not have an error handler\n        associated with it, or that was raised from an error handler.\n        This always causes a 500 ``InternalServerError``.\n\n        Always sends the :data:`got_request_exception` signal.\n\n        If :attr:`propagate_exceptions` is ``True``, such as in debug\n        mode, the error will be re-raised so that the debugger can\n        display it. Otherwise, the original exception is logged, and\n        an :exc:`~werkzeug.exceptions.InternalServerError` is returned.\n\n        If an error handler is registered for ``InternalServerError`` or\n        ``500``, it will be used. For consistency, the handler will\n        always receive the ``InternalServerError``. The original\n        unhandled exception is available as ``e.original_exception``.\n\n        .. versionchanged:: 1.1.0\n            Always passes the ``InternalServerError`` instance to the\n            handler, setting ``original_exception`` to the unhandled\n            error.\n\n        .. versionchanged:: 1.1.0\n            ``after_request`` functions and other finalization is done\n            even for the default 500 response when there is no handler.\n\n        .. versionadded:: 0.3\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1519, "code": "    def raise_routing_exception(self, request: Request) -> \"te.NoReturn\":\n        if (\n            not self.debug\n            or not isinstance(request.routing_exception, RequestRedirect)\n            or request.routing_exception.code in {307, 308}\n            or request.method in {\"GET\", \"HEAD\", \"OPTIONS\"}\n        ):\n            raise request.routing_exception  # type: ignore\n        from .debughelpers import FormDataRoutingRedirect\n        raise FormDataRoutingRedirect(request)", "documentation": "        \"\"\"Intercept routing exceptions and possibly do something else.\n\n        In debug mode, intercept a routing redirect and replace it with\n        an error if the body will be discarded.\n\n        With modern Werkzeug this shouldn't occur, since it now uses a\n        308 status which tells the browser to resend the method and\n        body.\n\n        .. versionchanged:: 2.1\n            Don't intercept 307 and 308 redirects.\n\n        :meta private:\n        :internal:\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1547, "code": "    def dispatch_request(self) -> ft.ResponseReturnValue:\n        req = request_ctx.request\n        if req.routing_exception is not None:\n            self.raise_routing_exception(req)\n        rule: Rule = req.url_rule  # type: ignore[assignment]\n        if (\n            getattr(rule, \"provide_automatic_options\", False)\n            and req.method == \"OPTIONS\"\n        ):\n            return self.make_default_options_response()\n        view_args: t.Dict[str, t.Any] = req.view_args  # type: ignore[assignment]", "documentation": "        \"\"\"Does the request dispatching.  Matches the URL and returns the\n        return value of the view or error handler.  This does not have to\n        be a response object.  In order to convert the return value to a\n        proper response object, call :func:`make_response`.\n\n        .. versionchanged:: 0.7\n           This no longer does the exception handling, this code was\n           moved to the new :meth:`full_dispatch_request`.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1572, "code": "    def full_dispatch_request(self) -> Response:\n        if not self._got_first_request:\n            with self._before_request_lock:\n                if not self._got_first_request:\n                    for func in self.before_first_request_funcs:\n                        self.ensure_sync(func)()\n                    self._got_first_request = True\n        try:\n            request_started.send(self)\n            rv = self.preprocess_request()\n            if rv is None:", "documentation": "        \"\"\"Dispatches the request and on top of that performs request\n        pre and postprocessing as well as HTTP exception catching and\n        error handling.\n\n        .. versionadded:: 0.7\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1628, "code": "    def make_default_options_response(self) -> Response:\n        adapter = request_ctx.url_adapter\n        methods = adapter.allowed_methods()  # type: ignore[union-attr]\n        rv = self.response_class()\n        rv.allow.update(methods)\n        return rv", "documentation": "        \"\"\"This method is called to create the default ``OPTIONS`` response.\n        This can be changed through subclassing to change the default\n        behavior of ``OPTIONS`` responses.\n\n        .. versionadded:: 0.7\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1641, "code": "    def should_ignore_error(self, error: t.Optional[BaseException]) -> bool:\n        return False", "documentation": "        \"\"\"This is called to figure out if an error should be ignored\n        or not as far as the teardown system is concerned.  If this\n        function returns ``True`` then the teardown handlers will not be\n        passed the error.\n\n        .. versionadded:: 0.10\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1651, "code": "    def ensure_sync(self, func: t.Callable) -> t.Callable:\n        if iscoroutinefunction(func):\n            return self.async_to_sync(func)\n        return func", "documentation": "        \"\"\"Ensure that the function is synchronous for WSGI workers.\n        Plain ``def`` functions are returned as-is. ``async def``\n        functions are wrapped to run and wait for the response.\n\n        Override this method to change how the app runs async views.\n\n        .. versionadded:: 2.0\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1812, "code": "    def redirect(self, location: str, code: int = 302) -> BaseResponse:\n        return _wz_redirect(location, code=code, Response=self.response_class)", "documentation": "        \"\"\"Create a redirect response object.\n\n        This is called by :func:`flask.redirect`, and can be called\n        directly as well.\n\n        :param location: The URL to redirect to.\n        :param code: The status code for the redirect.\n\n        .. versionadded:: 2.2\n            Moved from ``flask.redirect``, which calls this method.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1826, "code": "    def make_response(self, rv: ft.ResponseReturnValue) -> Response:\n        status = headers = None\n        if isinstance(rv, tuple):\n            len_rv = len(rv)\n            if len_rv == 3:\n                rv, status, headers = rv  # type: ignore[misc]\n            elif len_rv == 2:\n                if isinstance(rv[1], (Headers, dict, tuple, list)):\n                    rv, headers = rv\n                else:\n                    rv, status = rv  # type: ignore[assignment,misc]", "documentation": "        \"\"\"Convert the return value from a view function to an instance of\n        :attr:`response_class`.\n\n        :param rv: the return value from the view function. The view function\n            must return a response. Returning ``None``, or the view ending\n            without returning, is not allowed. The following types are allowed\n            for ``view_rv``:\n\n            ``str``\n                A response object is created with the string encoded to UTF-8\n                as the body.\n\n            ``bytes``\n                A response object is created with the bytes as the body.\n\n            ``dict``\n                A dictionary that will be jsonify'd before being returned.\n\n            ``list``\n                A list that will be jsonify'd before being returned.\n\n            ``generator`` or ``iterator``\n                A generator that returns ``str`` or ``bytes`` to be\n                streamed as the response.\n\n            ``tuple``\n                Either ``(body, status, headers)``, ``(body, status)``, or\n                ``(body, headers)``, where ``body`` is any of the other types\n                allowed here, ``status`` is a string or an integer, and\n                ``headers`` is a dictionary or a list of ``(key, value)``\n                tuples. If ``body`` is a :attr:`response_class` instance,\n                ``status`` overwrites the exiting value and ``headers`` are\n                extended.\n\n            :attr:`response_class`\n                The object is returned unchanged.\n\n            other :class:`~werkzeug.wrappers.Response` class\n                The object is coerced to :attr:`response_class`.\n\n            :func:`callable`\n                The function is called as a WSGI application. The result is\n                used to create a response object.\n\n        .. versionchanged:: 2.2\n            A generator will be converted to a streaming response.\n            A list will be converted to a JSON response.\n\n        .. versionchanged:: 1.1\n            A dict will be converted to a JSON response.\n\n        .. versionchanged:: 0.9\n           Previously a tuple was interpreted as the arguments for the\n           response object.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2008, "code": "    def inject_url_defaults(self, endpoint: str, values: dict) -> None:\n        names: t.Iterable[t.Optional[str]] = (None,)\n        if \".\" in endpoint:\n            names = chain(\n                names, reversed(_split_blueprint_path(endpoint.rpartition(\".\")[0]))\n            )\n        for name in names:\n            if name in self.url_default_functions:\n                for func in self.url_default_functions[name]:\n                    func(endpoint, values)", "documentation": "        \"\"\"Injects the URL defaults for the given endpoint directly into\n        the values dictionary passed.  This is used internally and\n        automatically called on URL building.\n\n        .. versionadded:: 0.7\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2063, "code": "    def preprocess_request(self) -> t.Optional[ft.ResponseReturnValue]:\n        names = (None, *reversed(request.blueprints))\n        for name in names:\n            if name in self.url_value_preprocessors:\n                for url_func in self.url_value_preprocessors[name]:\n                    url_func(request.endpoint, request.view_args)\n        for name in names:\n            if name in self.before_request_funcs:\n                for before_func in self.before_request_funcs[name]:\n                    rv = self.ensure_sync(before_func)()\n                    if rv is not None:", "documentation": "        \"\"\"Called before the request is dispatched. Calls\n        :attr:`url_value_preprocessors` registered with the app and the\n        current blueprint (if any). Then calls :attr:`before_request_funcs`\n        registered with the app and the blueprint.\n\n        If any :meth:`before_request` handler returns a non-None value, the\n        value is handled as if it was the return value from the view, and\n        further request handling is stopped.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2090, "code": "    def process_response(self, response: Response) -> Response:\n        ctx = request_ctx._get_current_object()  # type: ignore[attr-defined]\n        for func in ctx._after_request_functions:\n            response = self.ensure_sync(func)(response)\n        for name in chain(request.blueprints, (None,)):\n            if name in self.after_request_funcs:\n                for func in reversed(self.after_request_funcs[name]):\n                    response = self.ensure_sync(func)(response)\n        if not self.session_interface.is_null_session(ctx.session):\n            self.session_interface.save_session(self, ctx.session, response)\n        return response", "documentation": "        \"\"\"Can be overridden in order to modify the response object\n        before it's sent to the WSGI server.  By default this will\n        call all the :meth:`after_request` decorated functions.\n\n        .. versionchanged:: 0.5\n           As of Flask 0.5 the functions registered for after request\n           execution are called in reverse order of registration.\n\n        :param response: a :attr:`response_class` object.\n        :return: a new response object or the same, has to be an\n                 instance of :attr:`response_class`.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2176, "code": "    def app_context(self) -> AppContext:\n        return AppContext(self)", "documentation": "        \"\"\"Create an :class:`~flask.ctx.AppContext`. Use as a ``with``\n        block to push the context, which will make :data:`current_app`\n        point at this application.\n\n        An application context is automatically pushed by\n        :meth:`RequestContext.push() <flask.ctx.RequestContext.push>`\n        when handling a request, and when running a CLI command. Use\n        this to manually create a context outside of these situations.\n\n        ::\n\n            with app.app_context():\n                init_db()\n\n        See :doc:`/appcontext`.\n\n        .. versionadded:: 0.9\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2197, "code": "    def request_context(self, environ: dict) -> RequestContext:\n        return RequestContext(self, environ)", "documentation": "        \"\"\"Create a :class:`~flask.ctx.RequestContext` representing a\n        WSGI environment. Use a ``with`` block to push the context,\n        which will make :data:`request` point at this request.\n\n        See :doc:`/reqcontext`.\n\n        Typically you should not call this from your own code. A request\n        context is automatically pushed by the :meth:`wsgi_app` when\n        handling a request. Use :meth:`test_request_context` to create\n        an environment and context instead of this method.\n\n        :param environ: a WSGI environment\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2213, "code": "    def test_request_context(self, *args: t.Any, **kwargs: t.Any) -> RequestContext:\n        from .testing import EnvironBuilder\n        builder = EnvironBuilder(self, *args, **kwargs)\n        try:\n            return self.request_context(builder.get_environ())\n        finally:\n            builder.close()", "documentation": "        \"\"\"Create a :class:`~flask.ctx.RequestContext` for a WSGI\n        environment created from the given values. This is mostly useful\n        during testing, where you may want to run a function that uses\n        request data without dispatching a full request.\n\n        See :doc:`/reqcontext`.\n\n        Use a ``with`` block to push the context, which will make\n        :data:`request` point at the request for the created\n        environment. ::\n\n            with test_request_context(...):\n                generate_report()\n\n        When using the shell, it may be easier to push and pop the\n        context manually to avoid indentation. ::\n\n            ctx = app.test_request_context(...)\n            ctx.push()\n            ...\n            ctx.pop()\n\n        Takes the same arguments as Werkzeug's\n        :class:`~werkzeug.test.EnvironBuilder`, with some defaults from\n        the application. See the linked Werkzeug docs for most of the\n        available arguments. Flask-specific behavior is listed here.\n\n        :param path: URL path being requested.\n        :param base_url: Base URL where the app is being served, which\n            ``path`` is relative to. If not given, built from\n            :data:`PREFERRED_URL_SCHEME`, ``subdomain``,\n            :data:`SERVER_NAME`, and :data:`APPLICATION_ROOT`.\n        :param subdomain: Subdomain name to append to\n            :data:`SERVER_NAME`.\n        :param url_scheme: Scheme to use instead of\n            :data:`PREFERRED_URL_SCHEME`.\n        :param data: The request body, either as a string or a dict of\n            form keys and values.\n        :param json: If given, this is serialized as JSON and passed as\n            ``data``. Also defaults ``content_type`` to\n            ``application/json``.\n        :param args: other positional arguments passed to\n            :class:`~werkzeug.test.EnvironBuilder`.\n        :param kwargs: other keyword arguments passed to\n            :class:`~werkzeug.test.EnvironBuilder`.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2269, "code": "    def wsgi_app(self, environ: dict, start_response: t.Callable) -> t.Any:\n        ctx = self.request_context(environ)\n        error: t.Optional[BaseException] = None\n        try:\n            try:\n                ctx.push()\n                response = self.full_dispatch_request()\n            except Exception as e:\n                error = e\n                response = self.handle_exception(e)\n            except:  # noqa: B001", "documentation": "        \"\"\"The actual WSGI application. This is not implemented in\n        :meth:`__call__` so that middlewares can be applied without\n        losing a reference to the app object. Instead of doing this::\n\n            app = MyMiddleware(app)\n\n        It's a better idea to do this instead::\n\n            app.wsgi_app = MyMiddleware(app.wsgi_app)\n\n        Then you still have the original application object around and\n        can continue to call methods on it.\n\n        .. versionchanged:: 0.7\n            Teardown events for the request and app contexts are called\n            even if an unhandled error occurs. Other events may not be\n            called depending on when an error occurs during dispatch.\n            See :ref:`callbacks-and-errors`.\n\n        :param environ: A WSGI environment.\n        :param start_response: A callable accepting a status code,\n            a list of headers, and an optional exception context to\n            start the response.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2317, "code": "    def __call__(self, environ: dict, start_response: t.Callable) -> t.Any:\n        return self.wsgi_app(environ, start_response)", "documentation": "        \"\"\"The WSGI server calls the Flask application object as the\n        WSGI application. This calls :meth:`wsgi_app`, which can be\n        wrapped to apply middleware.\n        \"\"\""}], "after_segments": [{"filename": "src/flask/app.py", "start_line": 109, "code": "class Flask(Scaffold):\n    request_class = Request\n    response_class = Response\n    aborter_class = Aborter\n    jinja_environment = Environment\n    app_ctx_globals_class = _AppCtxGlobals\n    config_class = Config\n    testing = ConfigAttribute(\"TESTING\")\n    secret_key = ConfigAttribute(\"SECRET_KEY\")\n    session_cookie_name = ConfigAttribute(\"SESSION_COOKIE_NAME\")\n    permanent_session_lifetime = ConfigAttribute(", "documentation": "    \"\"\"The flask object implements a WSGI application and acts as the central\n    object.  It is passed the name of the module or package of the\n    application.  Once it is created it will act as a central registry for\n    the view functions, the URL rules, template configuration and much more.\n\n    The name of the package is used to resolve resources from inside the\n    package or the folder the module is contained in depending on if the\n    package parameter resolves to an actual python package (a folder with\n    an :file:`__init__.py` file inside) or a standard module (just a ``.py`` file).\n\n    For more information about resource loading, see :func:`open_resource`.\n\n    Usually you create a :class:`Flask` instance in your main module or\n    in the :file:`__init__.py` file of your package like this::\n\n        from flask import Flask\n        app = Flask(__name__)\n\n    .. admonition:: About the First Parameter\n\n        The idea of the first parameter is to give Flask an idea of what\n        belongs to your application.  This name is used to find resources\n        on the filesystem, can be used by extensions to improve debugging\n        information and a lot more.\n\n        So it's important what you provide there.  If you are using a single\n        module, `__name__` is always the correct value.  If you however are\n        using a package, it's usually recommended to hardcode the name of\n        your package there.\n\n        For example if your application is defined in :file:`yourapplication/app.py`\n        you should create it with one of the two versions below::\n\n            app = Flask('yourapplication')\n            app = Flask(__name__.split('.')[0])\n\n        Why is that?  The application will work even with `__name__`, thanks\n        to how resources are looked up.  However it will make debugging more\n        painful.  Certain extensions can make assumptions based on the\n        import name of your application.  For example the Flask-SQLAlchemy\n        extension will look for the code in your application that triggered\n        an SQL query in debug mode.  If the import name is not properly set\n        up, that debugging information is lost.  (For example it would only\n        pick up SQL queries in `yourapplication.app` and not\n        `yourapplication.views.frontend`)\n\n    .. versionadded:: 0.7\n       The `static_url_path`, `static_folder`, and `template_folder`\n       parameters were added.\n\n    .. versionadded:: 0.8\n       The `instance_path` and `instance_relative_config` parameters were\n       added.\n\n    .. versionadded:: 0.11\n       The `root_path` parameter was added.\n\n    .. versionadded:: 1.0\n       The ``host_matching`` and ``static_host`` parameters were added.\n\n    .. versionadded:: 1.0\n       The ``subdomain_matching`` parameter was added. Subdomain\n       matching needs to be enabled manually now. Setting\n       :data:`SERVER_NAME` does not implicitly enable it.\n\n    :param import_name: the name of the application package\n    :param static_url_path: can be used to specify a different path for the\n                            static files on the web.  Defaults to the name\n                            of the `static_folder` folder.\n    :param static_folder: The folder with static files that is served at\n        ``static_url_path``. Relative to the application ``root_path``\n        or an absolute path. Defaults to ``'static'``.\n    :param static_host: the host to use when adding the static route.\n        Defaults to None. Required when using ``host_matching=True``\n        with a ``static_folder`` configured.\n    :param host_matching: set ``url_map.host_matching`` attribute.\n        Defaults to False.\n    :param subdomain_matching: consider the subdomain relative to\n        :data:`SERVER_NAME` when matching routes. Defaults to False.\n    :param template_folder: the folder that contains the templates that should\n                            be used by the application.  Defaults to\n                            ``'templates'`` folder in the root path of the\n                            application.\n    :param instance_path: An alternative instance path for the application.\n                          By default the folder ``'instance'`` next to the\n                          package or module is assumed to be the instance\n                          path.\n    :param instance_relative_config: if set to ``True`` relative filenames\n                                     for loading the config are assumed to\n                                     be relative to the instance path instead\n                                     of the application root.\n    :param root_path: The path to the root of the application files.\n        This should only be set manually when it can't be detected\n        automatically, such as for namespace packages.\n    \"\"\""}, {"filename": "src/flask/app.py", "start_line": 568, "code": "    def name(self) -> str:  # type: ignore\n        if self.import_name == \"__main__\":\n            fn = getattr(sys.modules[\"__main__\"], \"__file__\", None)\n            if fn is None:\n                return \"__main__\"\n            return os.path.splitext(os.path.basename(fn))[0]\n        return self.import_name\n    @property", "documentation": "        \"\"\"The name of the application.  This is usually the import name\n        with the difference that it's guessed from the run file if the\n        import name is main.  This name is used as a display name when\n        Flask needs the name of the application.  It can be set and overridden\n        to change the value.\n\n        .. versionadded:: 0.8\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 585, "code": "    def propagate_exceptions(self) -> bool:\n        rv = self.config[\"PROPAGATE_EXCEPTIONS\"]\n        if rv is not None:\n            return rv\n        return self.testing or self.debug\n    @locked_cached_property", "documentation": "        \"\"\"Returns the value of the ``PROPAGATE_EXCEPTIONS`` configuration\n        value in case it's set, otherwise a sensible default is returned.\n\n        .. versionadded:: 0.7\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 597, "code": "    def logger(self) -> logging.Logger:\n        return create_logger(self)\n    @locked_cached_property", "documentation": "        \"\"\"A standard Python :class:`~logging.Logger` for the app, with\n        the same name as :attr:`name`.\n\n        In debug mode, the logger's :attr:`~logging.Logger.level` will\n        be set to :data:`~logging.DEBUG`.\n\n        If there are no handlers configured, a default handler will be\n        added. See :doc:`/logging` for more information.\n\n        .. versionchanged:: 1.1.0\n            The logger takes the same name as :attr:`name` rather than\n            hard-coding ``\"flask.app\"``.\n\n        .. versionchanged:: 1.0.0\n            Behavior was simplified. The logger is always named\n            ``\"flask.app\"``. The level is only set during configuration,\n            it doesn't check ``app.debug`` each time. Only one format is\n            used, not different ones depending on ``app.debug``. No\n            handlers are removed, and a handler is only added if no\n            handlers are already configured.\n\n        .. versionadded:: 0.3\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 624, "code": "    def jinja_env(self) -> Environment:\n        return self.create_jinja_environment()\n    @property", "documentation": "        \"\"\"The Jinja environment used to load templates.\n\n        The environment is created the first time this property is\n        accessed. Changing :attr:`jinja_options` after that will have no\n        effect.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 634, "code": "    def got_first_request(self) -> bool:\n        return self._got_first_request", "documentation": "        \"\"\"This attribute is set to ``True`` if the application started\n        handling the first request.\n\n        .. versionadded:: 0.8\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 642, "code": "    def make_config(self, instance_relative: bool = False) -> Config:\n        root_path = self.root_path\n        if instance_relative:\n            root_path = self.instance_path\n        defaults = dict(self.default_config)\n        defaults[\"ENV\"] = get_env()\n        defaults[\"DEBUG\"] = get_debug_flag()\n        return self.config_class(root_path, defaults)", "documentation": "        \"\"\"Used to create the config attribute by the Flask constructor.\n        The `instance_relative` parameter is passed in from the constructor\n        of Flask (there named `instance_relative_config`) and indicates if\n        the config should be relative to the instance path or the root path\n        of the application.\n\n        .. versionadded:: 0.8\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 659, "code": "    def make_aborter(self) -> Aborter:\n        return self.aborter_class()", "documentation": "        \"\"\"Create the object to assign to :attr:`aborter`. That object\n        is called by :func:`flask.abort` to raise HTTP errors, and can\n        be called directly as well.\n\n        By default, this creates an instance of :attr:`aborter_class`,\n        which defaults to :class:`werkzeug.exceptions.Aborter`.\n\n        .. versionadded:: 2.2\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 671, "code": "    def auto_find_instance_path(self) -> str:\n        prefix, package_path = find_package(self.import_name)\n        if prefix is None:\n            return os.path.join(package_path, \"instance\")\n        return os.path.join(prefix, \"var\", f\"{self.name}-instance\")", "documentation": "        \"\"\"Tries to locate the instance path if it was not provided to the\n        constructor of the application class.  It will basically calculate\n        the path to a folder named ``instance`` next to your main file or\n        the package.\n\n        .. versionadded:: 0.8\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 684, "code": "    def open_instance_resource(self, resource: str, mode: str = \"rb\") -> t.IO[t.AnyStr]:\n        return open(os.path.join(self.instance_path, resource), mode)\n    @property", "documentation": "        \"\"\"Opens a resource from the application's instance folder\n        (:attr:`instance_path`).  Otherwise works like\n        :meth:`open_resource`.  Instance resources can also be opened for\n        writing.\n\n        :param resource: the name of the resource.  To access resources within\n                         subfolders use forward slashes as separator.\n        :param mode: resource file opening mode, default is 'rb'.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 697, "code": "    def templates_auto_reload(self) -> bool:\n        rv = self.config[\"TEMPLATES_AUTO_RELOAD\"]\n        return rv if rv is not None else self.debug\n    @templates_auto_reload.setter", "documentation": "        \"\"\"Reload templates when they are changed. Used by\n        :meth:`create_jinja_environment`.\n\n        This attribute can be configured with :data:`TEMPLATES_AUTO_RELOAD`. If\n        not set, it will be enabled in debug mode.\n\n        .. versionadded:: 1.0\n            This property was added but the underlying config and behavior\n            already existed.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 715, "code": "    def create_jinja_environment(self) -> Environment:\n        options = dict(self.jinja_options)\n        if \"autoescape\" not in options:\n            options[\"autoescape\"] = self.select_jinja_autoescape\n        if \"auto_reload\" not in options:\n            options[\"auto_reload\"] = self.templates_auto_reload\n        rv = self.jinja_environment(self, **options)\n        rv.globals.update(\n            url_for=self.url_for,\n            get_flashed_messages=get_flashed_messages,\n            config=self.config,", "documentation": "        \"\"\"Create the Jinja environment based on :attr:`jinja_options`\n        and the various Jinja-related methods of the app. Changing\n        :attr:`jinja_options` after this will have no effect. Also adds\n        Flask-related globals and filters to the environment.\n\n        .. versionchanged:: 0.11\n           ``Environment.auto_reload`` set in accordance with\n           ``TEMPLATES_AUTO_RELOAD`` configuration option.\n\n        .. versionadded:: 0.5\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 750, "code": "    def create_global_jinja_loader(self) -> DispatchingJinjaLoader:\n        return DispatchingJinjaLoader(self)", "documentation": "        \"\"\"Creates the loader for the Jinja2 environment.  Can be used to\n        override just the loader and keeping the rest unchanged.  It's\n        discouraged to override this function.  Instead one should override\n        the :meth:`jinja_loader` function instead.\n\n        The global loader dispatches between the loaders of the application\n        and the individual blueprints.\n\n        .. versionadded:: 0.7\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 763, "code": "    def select_jinja_autoescape(self, filename: str) -> bool:\n        if filename is None:\n            return True\n        return filename.endswith((\".html\", \".htm\", \".xml\", \".xhtml\"))", "documentation": "        \"\"\"Returns ``True`` if autoescaping should be active for the given\n        template name. If no template name is given, returns `True`.\n\n        .. versionadded:: 0.5\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 773, "code": "    def update_template_context(self, context: dict) -> None:\n        names: t.Iterable[t.Optional[str]] = (None,)\n        if request:\n            names = chain(names, reversed(request.blueprints))\n        orig_ctx = context.copy()\n        for name in names:\n            if name in self.template_context_processors:\n                for func in self.template_context_processors[name]:\n                    context.update(func())\n        context.update(orig_ctx)", "documentation": "        \"\"\"Update the template context with some commonly used variables.\n        This injects request, session, config and g into the template\n        context as well as everything template context processors want\n        to inject.  Note that the as of Flask 0.6, the original values\n        in the context will not be overridden if a context processor\n        decides to return a value with the same key.\n\n        :param context: the context as a dictionary that is updated in place\n                        to add extra variables.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 801, "code": "    def make_shell_context(self) -> dict:\n        rv = {\"app\": self, \"g\": g}\n        for processor in self.shell_context_processors:\n            rv.update(processor())\n        return rv\n    env = ConfigAttribute(\"ENV\")\n    @property", "documentation": "        \"\"\"Returns the shell context for an interactive shell for this\n        application.  This runs all the registered shell context\n        processors.\n\n        .. versionadded:: 0.11\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 825, "code": "    def debug(self) -> bool:\n        return self.config[\"DEBUG\"]\n    @debug.setter", "documentation": "        \"\"\"Whether debug mode is enabled. When using ``flask run`` to start\n        the development server, an interactive debugger will be shown for\n        unhandled exceptions, and the server will be reloaded when code\n        changes. This maps to the :data:`DEBUG` config key. This is\n        enabled when :attr:`env` is ``'development'`` and is overridden\n        by the ``FLASK_DEBUG`` environment variable. It may not behave as\n        expected if set in code.\n\n        **Do not enable debug mode when deploying in production.**\n\n        Default: ``True`` if :attr:`env` is ``'development'``, or\n        ``False`` otherwise.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 974, "code": "    def test_client(self, use_cookies: bool = True, **kwargs: t.Any) -> \"FlaskClient\":\n        cls = self.test_client_class\n        if cls is None:\n            from .testing import FlaskClient as cls  # type: ignore\n        return cls(  # type: ignore\n            self, self.response_class, use_cookies=use_cookies, **kwargs\n        )", "documentation": "        \"\"\"Creates a test client for this application.  For information\n        about unit testing head over to :doc:`/testing`.\n\n        Note that if you are testing for assertions or exceptions in your\n        application code, you must set ``app.testing = True`` in order for the\n        exceptions to propagate to the test client.  Otherwise, the exception\n        will be handled by the application (not visible to the test client) and\n        the only indication of an AssertionError or other exception will be a\n        500 status code response to the test client.  See the :attr:`testing`\n        attribute.  For example::\n\n            app.testing = True\n            client = app.test_client()\n\n        The test client can be used in a ``with`` block to defer the closing down\n        of the context until the end of the ``with`` block.  This is useful if\n        you want to access the context locals for testing::\n\n            with app.test_client() as c:\n                rv = c.get('/?vodka=42')\n                assert request.args['vodka'] == '42'\n\n        Additionally, you may pass optional keyword arguments that will then\n        be passed to the application's :attr:`test_client_class` constructor.\n        For example::\n\n            from flask.testing import FlaskClient\n\n            class CustomClient(FlaskClient):\n                def __init__(self, *args, **kwargs):\n                    self._authentication = kwargs.pop(\"authentication\")\n                    super(CustomClient,self).__init__( *args, **kwargs)\n\n            app.test_client_class = CustomClient\n            client = app.test_client(authentication='Basic ....')\n\n        See :class:`~flask.testing.FlaskClient` for more information.\n\n        .. versionchanged:: 0.4\n           added support for ``with`` block usage for the client.\n\n        .. versionadded:: 0.7\n           The `use_cookies` parameter was added as well as the ability\n           to override the client to be used by setting the\n           :attr:`test_client_class` attribute.\n\n        .. versionchanged:: 0.11\n           Added `**kwargs` to support passing additional keyword arguments to\n           the constructor of :attr:`test_client_class`.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1032, "code": "    def test_cli_runner(self, **kwargs: t.Any) -> \"FlaskCliRunner\":\n        cls = self.test_cli_runner_class\n        if cls is None:\n            from .testing import FlaskCliRunner as cls  # type: ignore\n        return cls(self, **kwargs)  # type: ignore\n    @setupmethod", "documentation": "        \"\"\"Create a CLI runner for testing CLI commands.\n        See :ref:`testing-cli`.\n\n        Returns an instance of :attr:`test_cli_runner_class`, by default\n        :class:`~flask.testing.FlaskCliRunner`. The Flask app object is\n        passed as the first argument.\n\n        .. versionadded:: 1.0\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1050, "code": "    def register_blueprint(self, blueprint: \"Blueprint\", **options: t.Any) -> None:\n        blueprint.register(self, options)", "documentation": "        \"\"\"Register a :class:`~flask.Blueprint` on the application. Keyword\n        arguments passed to this method will override the defaults set on the\n        blueprint.\n\n        Calls the blueprint's :meth:`~flask.Blueprint.register` method after\n        recording the blueprint in the application's :attr:`blueprints`.\n\n        :param blueprint: The blueprint to register.\n        :param url_prefix: Blueprint routes will be prefixed with this.\n        :param subdomain: Blueprint routes will match on this subdomain.\n        :param url_defaults: Blueprint routes will use these default values for\n            view arguments.\n        :param options: Additional keyword arguments are passed to\n            :class:`~flask.blueprints.BlueprintSetupState`. They can be\n            accessed in :meth:`~flask.Blueprint.record` callbacks.\n\n        .. versionchanged:: 2.0.1\n            The ``name`` option can be used to change the (pre-dotted)\n            name the blueprint is registered with. This allows the same\n            blueprint to be registered multiple times with unique names\n            for ``url_for``.\n\n        .. versionadded:: 0.7\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1077, "code": "    def iter_blueprints(self) -> t.ValuesView[\"Blueprint\"]:\n        return self.blueprints.values()\n    @setupmethod", "documentation": "        \"\"\"Iterates over all blueprints by the order they were registered.\n\n        .. versionadded:: 0.11\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1259, "code": "    def before_first_request(self, f: T_before_first_request) -> T_before_first_request:\n        import warnings\n        warnings.warn(\n            \"'before_first_request' is deprecated and will be removed\"\n            \" in Flask 2.3. Run setup code while creating the\"\n            \" application instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        self.before_first_request_funcs.append(f)\n        return f", "documentation": "        \"\"\"Registers a function to be run before the first request to this\n        instance of the application.\n\n        The function will be called without any arguments and its return\n        value is ignored.\n\n        .. deprecated:: 2.2\n            Will be removed in Flask 2.3. Run setup code when creating\n            the application instead.\n\n        .. versionadded:: 0.8\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1285, "code": "    def teardown_appcontext(self, f: T_teardown) -> T_teardown:\n        self.teardown_appcontext_funcs.append(f)\n        return f\n    @setupmethod", "documentation": "        \"\"\"Registers a function to be called when the application\n        context is popped. The application context is typically popped\n        after the request context for each request, at the end of CLI\n        commands, or after a manually pushed context ends.\n\n        .. code-block:: python\n\n            with app.app_context():\n                ...\n\n        When the ``with`` block exits (or ``ctx.pop()`` is called), the\n        teardown functions are called just before the app context is\n        made inactive. Since a request context typically also manages an\n        application context it would also be called when you pop a\n        request context.\n\n        When a teardown function was called because of an unhandled\n        exception it will be passed an error object. If an\n        :meth:`errorhandler` is registered, it will handle the exception\n        and the teardown will not receive it.\n\n        Teardown functions must avoid raising exceptions. If they\n        execute code that might fail they must surround that code with a\n        ``try``/``except`` block and log any errors.\n\n        The return values of teardown functions are ignored.\n\n        .. versionadded:: 0.9\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1329, "code": "    def _find_error_handler(self, e: Exception) -> t.Optional[ft.ErrorHandlerCallable]:\n        exc_class, code = self._get_exc_class_and_code(type(e))\n        names = (*request.blueprints, None)\n        for c in (code, None) if code is not None else (None,):\n            for name in names:\n                handler_map = self.error_handler_spec[name][c]\n                if not handler_map:\n                    continue\n                for cls in exc_class.__mro__:\n                    handler = handler_map.get(cls)\n                    if handler is not None:", "documentation": "        \"\"\"Return a registered error handler for an exception in this order:\n        blueprint handler for a specific code, app handler for a specific code,\n        blueprint handler for an exception class, app handler for an exception\n        class, or ``None`` if a suitable handler is not found.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1387, "code": "    def trap_http_exception(self, e: Exception) -> bool:\n        if self.config[\"TRAP_HTTP_EXCEPTIONS\"]:\n            return True\n        trap_bad_request = self.config[\"TRAP_BAD_REQUEST_ERRORS\"]\n        if (\n            trap_bad_request is None\n            and self.debug\n            and isinstance(e, BadRequestKeyError)\n        ):\n            return True\n        if trap_bad_request:", "documentation": "        \"\"\"Checks if an HTTP exception should be trapped or not.  By default\n        this will return ``False`` for all exceptions except for a bad request\n        key error if ``TRAP_BAD_REQUEST_ERRORS`` is set to ``True``.  It\n        also returns ``True`` if ``TRAP_HTTP_EXCEPTIONS`` is set to ``True``.\n\n        This is called for all HTTP exceptions raised by a view function.\n        If it returns ``True`` for any exception the error handler for this\n        exception is not called and it shows up as regular exception in the\n        traceback.  This is helpful for debugging implicitly raised HTTP\n        exceptions.\n\n        .. versionchanged:: 1.0\n            Bad request errors are not trapped by default in debug mode.\n\n        .. versionadded:: 0.8\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1454, "code": "    def handle_exception(self, e: Exception) -> Response:\n        exc_info = sys.exc_info()\n        got_request_exception.send(self, exception=e)\n        if self.propagate_exceptions:\n            if exc_info[1] is e:\n                raise\n            raise e\n        self.log_exception(exc_info)\n        server_error: t.Union[InternalServerError, ft.ResponseReturnValue]\n        server_error = InternalServerError(original_exception=e)\n        handler = self._find_error_handler(server_error)", "documentation": "        \"\"\"Handle an exception that did not have an error handler\n        associated with it, or that was raised from an error handler.\n        This always causes a 500 ``InternalServerError``.\n\n        Always sends the :data:`got_request_exception` signal.\n\n        If :attr:`propagate_exceptions` is ``True``, such as in debug\n        mode, the error will be re-raised so that the debugger can\n        display it. Otherwise, the original exception is logged, and\n        an :exc:`~werkzeug.exceptions.InternalServerError` is returned.\n\n        If an error handler is registered for ``InternalServerError`` or\n        ``500``, it will be used. For consistency, the handler will\n        always receive the ``InternalServerError``. The original\n        unhandled exception is available as ``e.original_exception``.\n\n        .. versionchanged:: 1.1.0\n            Always passes the ``InternalServerError`` instance to the\n            handler, setting ``original_exception`` to the unhandled\n            error.\n\n        .. versionchanged:: 1.1.0\n            ``after_request`` functions and other finalization is done\n            even for the default 500 response when there is no handler.\n\n        .. versionadded:: 0.3\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1520, "code": "    def raise_routing_exception(self, request: Request) -> \"te.NoReturn\":\n        if (\n            not self.debug\n            or not isinstance(request.routing_exception, RequestRedirect)\n            or request.routing_exception.code in {307, 308}\n            or request.method in {\"GET\", \"HEAD\", \"OPTIONS\"}\n        ):\n            raise request.routing_exception  # type: ignore\n        from .debughelpers import FormDataRoutingRedirect\n        raise FormDataRoutingRedirect(request)", "documentation": "        \"\"\"Intercept routing exceptions and possibly do something else.\n\n        In debug mode, intercept a routing redirect and replace it with\n        an error if the body will be discarded.\n\n        With modern Werkzeug this shouldn't occur, since it now uses a\n        308 status which tells the browser to resend the method and\n        body.\n\n        .. versionchanged:: 2.1\n            Don't intercept 307 and 308 redirects.\n\n        :meta private:\n        :internal:\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1548, "code": "    def dispatch_request(self) -> ft.ResponseReturnValue:\n        req = request_ctx.request\n        if req.routing_exception is not None:\n            self.raise_routing_exception(req)\n        rule: Rule = req.url_rule  # type: ignore[assignment]\n        if (\n            getattr(rule, \"provide_automatic_options\", False)\n            and req.method == \"OPTIONS\"\n        ):\n            return self.make_default_options_response()\n        view_args: t.Dict[str, t.Any] = req.view_args  # type: ignore[assignment]", "documentation": "        \"\"\"Does the request dispatching.  Matches the URL and returns the\n        return value of the view or error handler.  This does not have to\n        be a response object.  In order to convert the return value to a\n        proper response object, call :func:`make_response`.\n\n        .. versionchanged:: 0.7\n           This no longer does the exception handling, this code was\n           moved to the new :meth:`full_dispatch_request`.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1573, "code": "    def full_dispatch_request(self) -> Response:\n        if not self._got_first_request:\n            with self._before_request_lock:\n                if not self._got_first_request:\n                    for func in self.before_first_request_funcs:\n                        self.ensure_sync(func)()\n                    self._got_first_request = True\n        try:\n            request_started.send(self)\n            rv = self.preprocess_request()\n            if rv is None:", "documentation": "        \"\"\"Dispatches the request and on top of that performs request\n        pre and postprocessing as well as HTTP exception catching and\n        error handling.\n\n        .. versionadded:: 0.7\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1629, "code": "    def make_default_options_response(self) -> Response:\n        adapter = request_ctx.url_adapter\n        methods = adapter.allowed_methods()  # type: ignore[union-attr]\n        rv = self.response_class()\n        rv.allow.update(methods)\n        return rv", "documentation": "        \"\"\"This method is called to create the default ``OPTIONS`` response.\n        This can be changed through subclassing to change the default\n        behavior of ``OPTIONS`` responses.\n\n        .. versionadded:: 0.7\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1642, "code": "    def should_ignore_error(self, error: t.Optional[BaseException]) -> bool:\n        return False", "documentation": "        \"\"\"This is called to figure out if an error should be ignored\n        or not as far as the teardown system is concerned.  If this\n        function returns ``True`` then the teardown handlers will not be\n        passed the error.\n\n        .. versionadded:: 0.10\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1652, "code": "    def ensure_sync(self, func: t.Callable) -> t.Callable:\n        if iscoroutinefunction(func):\n            return self.async_to_sync(func)\n        return func", "documentation": "        \"\"\"Ensure that the function is synchronous for WSGI workers.\n        Plain ``def`` functions are returned as-is. ``async def``\n        functions are wrapped to run and wait for the response.\n\n        Override this method to change how the app runs async views.\n\n        .. versionadded:: 2.0\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1813, "code": "    def redirect(self, location: str, code: int = 302) -> BaseResponse:\n        return _wz_redirect(location, code=code, Response=self.response_class)", "documentation": "        \"\"\"Create a redirect response object.\n\n        This is called by :func:`flask.redirect`, and can be called\n        directly as well.\n\n        :param location: The URL to redirect to.\n        :param code: The status code for the redirect.\n\n        .. versionadded:: 2.2\n            Moved from ``flask.redirect``, which calls this method.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 1827, "code": "    def make_response(self, rv: ft.ResponseReturnValue) -> Response:\n        status = headers = None\n        if isinstance(rv, tuple):\n            len_rv = len(rv)\n            if len_rv == 3:\n                rv, status, headers = rv  # type: ignore[misc]\n            elif len_rv == 2:\n                if isinstance(rv[1], (Headers, dict, tuple, list)):\n                    rv, headers = rv\n                else:\n                    rv, status = rv  # type: ignore[assignment,misc]", "documentation": "        \"\"\"Convert the return value from a view function to an instance of\n        :attr:`response_class`.\n\n        :param rv: the return value from the view function. The view function\n            must return a response. Returning ``None``, or the view ending\n            without returning, is not allowed. The following types are allowed\n            for ``view_rv``:\n\n            ``str``\n                A response object is created with the string encoded to UTF-8\n                as the body.\n\n            ``bytes``\n                A response object is created with the bytes as the body.\n\n            ``dict``\n                A dictionary that will be jsonify'd before being returned.\n\n            ``list``\n                A list that will be jsonify'd before being returned.\n\n            ``generator`` or ``iterator``\n                A generator that returns ``str`` or ``bytes`` to be\n                streamed as the response.\n\n            ``tuple``\n                Either ``(body, status, headers)``, ``(body, status)``, or\n                ``(body, headers)``, where ``body`` is any of the other types\n                allowed here, ``status`` is a string or an integer, and\n                ``headers`` is a dictionary or a list of ``(key, value)``\n                tuples. If ``body`` is a :attr:`response_class` instance,\n                ``status`` overwrites the exiting value and ``headers`` are\n                extended.\n\n            :attr:`response_class`\n                The object is returned unchanged.\n\n            other :class:`~werkzeug.wrappers.Response` class\n                The object is coerced to :attr:`response_class`.\n\n            :func:`callable`\n                The function is called as a WSGI application. The result is\n                used to create a response object.\n\n        .. versionchanged:: 2.2\n            A generator will be converted to a streaming response.\n            A list will be converted to a JSON response.\n\n        .. versionchanged:: 1.1\n            A dict will be converted to a JSON response.\n\n        .. versionchanged:: 0.9\n           Previously a tuple was interpreted as the arguments for the\n           response object.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2009, "code": "    def inject_url_defaults(self, endpoint: str, values: dict) -> None:\n        names: t.Iterable[t.Optional[str]] = (None,)\n        if \".\" in endpoint:\n            names = chain(\n                names, reversed(_split_blueprint_path(endpoint.rpartition(\".\")[0]))\n            )\n        for name in names:\n            if name in self.url_default_functions:\n                for func in self.url_default_functions[name]:\n                    func(endpoint, values)", "documentation": "        \"\"\"Injects the URL defaults for the given endpoint directly into\n        the values dictionary passed.  This is used internally and\n        automatically called on URL building.\n\n        .. versionadded:: 0.7\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2064, "code": "    def preprocess_request(self) -> t.Optional[ft.ResponseReturnValue]:\n        names = (None, *reversed(request.blueprints))\n        for name in names:\n            if name in self.url_value_preprocessors:\n                for url_func in self.url_value_preprocessors[name]:\n                    url_func(request.endpoint, request.view_args)\n        for name in names:\n            if name in self.before_request_funcs:\n                for before_func in self.before_request_funcs[name]:\n                    rv = self.ensure_sync(before_func)()\n                    if rv is not None:", "documentation": "        \"\"\"Called before the request is dispatched. Calls\n        :attr:`url_value_preprocessors` registered with the app and the\n        current blueprint (if any). Then calls :attr:`before_request_funcs`\n        registered with the app and the blueprint.\n\n        If any :meth:`before_request` handler returns a non-None value, the\n        value is handled as if it was the return value from the view, and\n        further request handling is stopped.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2091, "code": "    def process_response(self, response: Response) -> Response:\n        ctx = request_ctx._get_current_object()  # type: ignore[attr-defined]\n        for func in ctx._after_request_functions:\n            response = self.ensure_sync(func)(response)\n        for name in chain(request.blueprints, (None,)):\n            if name in self.after_request_funcs:\n                for func in reversed(self.after_request_funcs[name]):\n                    response = self.ensure_sync(func)(response)\n        if not self.session_interface.is_null_session(ctx.session):\n            self.session_interface.save_session(self, ctx.session, response)\n        return response", "documentation": "        \"\"\"Can be overridden in order to modify the response object\n        before it's sent to the WSGI server.  By default this will\n        call all the :meth:`after_request` decorated functions.\n\n        .. versionchanged:: 0.5\n           As of Flask 0.5 the functions registered for after request\n           execution are called in reverse order of registration.\n\n        :param response: a :attr:`response_class` object.\n        :return: a new response object or the same, has to be an\n                 instance of :attr:`response_class`.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2177, "code": "    def app_context(self) -> AppContext:\n        return AppContext(self)", "documentation": "        \"\"\"Create an :class:`~flask.ctx.AppContext`. Use as a ``with``\n        block to push the context, which will make :data:`current_app`\n        point at this application.\n\n        An application context is automatically pushed by\n        :meth:`RequestContext.push() <flask.ctx.RequestContext.push>`\n        when handling a request, and when running a CLI command. Use\n        this to manually create a context outside of these situations.\n\n        ::\n\n            with app.app_context():\n                init_db()\n\n        See :doc:`/appcontext`.\n\n        .. versionadded:: 0.9\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2198, "code": "    def request_context(self, environ: dict) -> RequestContext:\n        return RequestContext(self, environ)", "documentation": "        \"\"\"Create a :class:`~flask.ctx.RequestContext` representing a\n        WSGI environment. Use a ``with`` block to push the context,\n        which will make :data:`request` point at this request.\n\n        See :doc:`/reqcontext`.\n\n        Typically you should not call this from your own code. A request\n        context is automatically pushed by the :meth:`wsgi_app` when\n        handling a request. Use :meth:`test_request_context` to create\n        an environment and context instead of this method.\n\n        :param environ: a WSGI environment\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2214, "code": "    def test_request_context(self, *args: t.Any, **kwargs: t.Any) -> RequestContext:\n        from .testing import EnvironBuilder\n        builder = EnvironBuilder(self, *args, **kwargs)\n        try:\n            return self.request_context(builder.get_environ())\n        finally:\n            builder.close()", "documentation": "        \"\"\"Create a :class:`~flask.ctx.RequestContext` for a WSGI\n        environment created from the given values. This is mostly useful\n        during testing, where you may want to run a function that uses\n        request data without dispatching a full request.\n\n        See :doc:`/reqcontext`.\n\n        Use a ``with`` block to push the context, which will make\n        :data:`request` point at the request for the created\n        environment. ::\n\n            with test_request_context(...):\n                generate_report()\n\n        When using the shell, it may be easier to push and pop the\n        context manually to avoid indentation. ::\n\n            ctx = app.test_request_context(...)\n            ctx.push()\n            ...\n            ctx.pop()\n\n        Takes the same arguments as Werkzeug's\n        :class:`~werkzeug.test.EnvironBuilder`, with some defaults from\n        the application. See the linked Werkzeug docs for most of the\n        available arguments. Flask-specific behavior is listed here.\n\n        :param path: URL path being requested.\n        :param base_url: Base URL where the app is being served, which\n            ``path`` is relative to. If not given, built from\n            :data:`PREFERRED_URL_SCHEME`, ``subdomain``,\n            :data:`SERVER_NAME`, and :data:`APPLICATION_ROOT`.\n        :param subdomain: Subdomain name to append to\n            :data:`SERVER_NAME`.\n        :param url_scheme: Scheme to use instead of\n            :data:`PREFERRED_URL_SCHEME`.\n        :param data: The request body, either as a string or a dict of\n            form keys and values.\n        :param json: If given, this is serialized as JSON and passed as\n            ``data``. Also defaults ``content_type`` to\n            ``application/json``.\n        :param args: other positional arguments passed to\n            :class:`~werkzeug.test.EnvironBuilder`.\n        :param kwargs: other keyword arguments passed to\n            :class:`~werkzeug.test.EnvironBuilder`.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2270, "code": "    def wsgi_app(self, environ: dict, start_response: t.Callable) -> t.Any:\n        ctx = self.request_context(environ)\n        error: t.Optional[BaseException] = None\n        try:\n            try:\n                ctx.push()\n                response = self.full_dispatch_request()\n            except Exception as e:\n                error = e\n                response = self.handle_exception(e)\n            except:  # noqa: B001", "documentation": "        \"\"\"The actual WSGI application. This is not implemented in\n        :meth:`__call__` so that middlewares can be applied without\n        losing a reference to the app object. Instead of doing this::\n\n            app = MyMiddleware(app)\n\n        It's a better idea to do this instead::\n\n            app.wsgi_app = MyMiddleware(app.wsgi_app)\n\n        Then you still have the original application object around and\n        can continue to call methods on it.\n\n        .. versionchanged:: 0.7\n            Teardown events for the request and app contexts are called\n            even if an unhandled error occurs. Other events may not be\n            called depending on when an error occurs during dispatch.\n            See :ref:`callbacks-and-errors`.\n\n        :param environ: A WSGI environment.\n        :param start_response: A callable accepting a status code,\n            a list of headers, and an optional exception context to\n            start the response.\n        \"\"\""}, {"filename": "src/flask/app.py", "start_line": 2318, "code": "    def __call__(self, environ: dict, start_response: t.Callable) -> t.Any:\n        return self.wsgi_app(environ, start_response)", "documentation": "        \"\"\"The WSGI server calls the Flask application object as the\n        WSGI application. This calls :meth:`wsgi_app`, which can be\n        wrapped to apply middleware.\n        \"\"\""}]}
{"repository": "pallets/flask", "commit_sha": "e0dad454810dd081947d3ca2ff376c5096185698", "commit_message": "update docs about contexts", "commit_date": "2022-07-08T14:08:54+00:00", "author": "David Lord", "file": "src/flask/ctx.py", "patch": "@@ -227,12 +227,10 @@ def has_app_context() -> bool:\n \n \n class AppContext:\n-    \"\"\"The application context binds an application object implicitly\n-    to the current thread or greenlet, similar to how the\n-    :class:`RequestContext` binds request information.  The application\n-    context is also implicitly created if a request context is created\n-    but the application is not on top of the individual application\n-    context.\n+    \"\"\"The app context contains application-specific information. An app\n+    context is created and pushed at the beginning of each request if\n+    one is not already active. An app context is also pushed when\n+    running CLI commands.\n     \"\"\"\n \n     def __init__(self, app: \"Flask\") -> None:\n@@ -278,10 +276,10 @@ def __exit__(\n \n \n class RequestContext:\n-    \"\"\"The request context contains all request relevant information.  It is\n-    created at the beginning of the request and pushed to the\n-    `_request_ctx_stack` and removed at the end of it.  It will create the\n-    URL adapter and request object for the WSGI environment provided.\n+    \"\"\"The request context contains per-request information. The Flask\n+    app creates and pushes it at the beginning of the request, then pops\n+    it at the end of the request. It will create the URL adapter and\n+    request object for the WSGI environment provided.\n \n     Do not attempt to use this class directly, instead use\n     :meth:`~flask.Flask.test_request_context` and", "before_segments": [{"filename": "src/flask/ctx.py", "start_line": 24, "code": "class _AppCtxGlobals:", "documentation": "    \"\"\"A plain object. Used as a namespace for storing data during an\n    application context.\n\n    Creating an app context automatically creates this object, which is\n    made available as the :data:`g` proxy.\n\n    .. describe:: 'key' in g\n\n        Check whether an attribute is present.\n\n        .. versionadded:: 0.10\n\n    .. describe:: iter(g)\n\n        Return an iterator over the attribute names.\n\n        .. versionadded:: 0.10\n    \"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 62, "code": "    def get(self, name: str, default: t.Optional[t.Any] = None) -> t.Any:\n        return self.__dict__.get(name, default)", "documentation": "        \"\"\"Get an attribute by name, or a default value. Like\n        :meth:`dict.get`.\n\n        :param name: Name of attribute to get.\n        :param default: Value to return if the attribute is not present.\n\n        .. versionadded:: 0.10\n        \"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 73, "code": "    def pop(self, name: str, default: t.Any = _sentinel) -> t.Any:\n        if default is _sentinel:\n            return self.__dict__.pop(name)\n        else:\n            return self.__dict__.pop(name, default)", "documentation": "        \"\"\"Get and remove an attribute by name. Like :meth:`dict.pop`.\n\n        :param name: Name of attribute to pop.\n        :param default: Value to return if the attribute is not present,\n            instead of raising a ``KeyError``.\n\n        .. versionadded:: 0.11\n        \"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 87, "code": "    def setdefault(self, name: str, default: t.Any = None) -> t.Any:\n        return self.__dict__.setdefault(name, default)", "documentation": "        \"\"\"Get the value of an attribute if it is present, otherwise\n        set and return a default value. Like :meth:`dict.setdefault`.\n\n        :param name: Name of attribute to get.\n        :param default: Value to set and return if the attribute is not\n            present.\n\n        .. versionadded:: 0.11\n        \"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 112, "code": "def after_this_request(f: ft.AfterRequestCallable) -> ft.AfterRequestCallable:\n    ctx = _cv_req.get(None)\n    if ctx is None:\n        raise RuntimeError(\n            \"'after_this_request' can only be used when a request\"\n            \" context is active, such as in a view function.\"\n        )\n    ctx._after_request_functions.append(f)\n    return f", "documentation": "    \"\"\"Executes a function after this request.  This is useful to modify\n    response objects.  The function is passed the response object and has\n    to return the same or a new one.\n\n    Example::\n\n        @app.route('/')\n        def index():\n            @after_this_request\n            def add_header(response):\n                response.headers['X-Foo'] = 'Parachute'\n                return response\n            return 'Hello World!'\n\n    This is more useful if a function other than the view function wants to\n    modify a response.  For instance think of a decorator that wants to add\n    some headers without converting the return value into a response object.\n\n    .. versionadded:: 0.9\n    \"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 145, "code": "def copy_current_request_context(f: t.Callable) -> t.Callable:\n    ctx = _cv_req.get(None)\n    if ctx is None:\n        raise RuntimeError(\n            \"'copy_current_request_context' can only be used when a\"\n            \" request context is active, such as in a view function.\"\n        )\n    ctx = ctx.copy()", "documentation": "    \"\"\"A helper function that decorates a function to retain the current\n    request context.  This is useful when working with greenlets.  The moment\n    the function is decorated a copy of the request context is created and\n    then pushed when the function is called.  The current session is also\n    included in the copied request context.\n\n    Example::\n\n        import gevent\n        from flask import copy_current_request_context\n\n        @app.route('/')\n        def index():\n            @copy_current_request_context\n            def do_some_work():\n                # do some work here, it can access flask.request or\n                # flask.session like you would otherwise in the view function.\n                ...\n            gevent.spawn(do_some_work)\n            return 'Regular response'\n\n    .. versionadded:: 0.10\n    \"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 186, "code": "def has_request_context() -> bool:\n    return _cv_app.get(None) is not None", "documentation": "    \"\"\"If you have code that wants to test if a request context is there or\n    not this function can be used.  For instance, you may want to take advantage\n    of request information if the request object is available, but fail\n    silently if it is unavailable.\n\n    ::\n\n        class User(db.Model):\n\n            def __init__(self, username, remote_addr=None):\n                self.username = username\n                if remote_addr is None and has_request_context():\n                    remote_addr = request.remote_addr\n                self.remote_addr = remote_addr\n\n    Alternatively you can also just test any of the context bound objects\n    (such as :class:`request` or :class:`g`) for truthness::\n\n        class User(db.Model):\n\n            def __init__(self, username, remote_addr=None):\n                self.username = username\n                if remote_addr is None and request:\n                    remote_addr = request.remote_addr\n                self.remote_addr = remote_addr\n\n    .. versionadded:: 0.7\n    \"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 218, "code": "def has_app_context() -> bool:\n    return _cv_req.get(None) is not None", "documentation": "    \"\"\"Works like :func:`has_request_context` but for the application\n    context.  You can also just do a boolean check on the\n    :data:`current_app` object instead.\n\n    .. versionadded:: 0.9\n    \"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 228, "code": "class AppContext:", "documentation": "    \"\"\"The application context binds an application object implicitly\n    to the current thread or greenlet, similar to how the\n    :class:`RequestContext` binds request information.  The application\n    context is also implicitly created if a request context is created\n    but the application is not on top of the individual application\n    context.\n    \"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 243, "code": "    def push(self) -> None:\n        self._cv_tokens.append(_cv_app.set(self))\n        appcontext_pushed.send(self.app)", "documentation": "        \"\"\"Binds the app context to the current context.\"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 248, "code": "    def pop(self, exc: t.Optional[BaseException] = _sentinel) -> None:  # type: ignore\n        try:\n            if len(self._cv_tokens) == 1:\n                if exc is _sentinel:\n                    exc = sys.exc_info()[1]\n                self.app.do_teardown_appcontext(exc)\n        finally:\n            ctx = _cv_app.get()\n            _cv_app.reset(self._cv_tokens.pop())\n        if ctx is not self:\n            raise AssertionError(", "documentation": "        \"\"\"Pops the app context.\"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 279, "code": "class RequestContext:", "documentation": "    \"\"\"The request context contains all request relevant information.  It is\n    created at the beginning of the request and pushed to the\n    `_request_ctx_stack` and removed at the end of it.  It will create the\n    URL adapter and request object for the WSGI environment provided.\n\n    Do not attempt to use this class directly, instead use\n    :meth:`~flask.Flask.test_request_context` and\n    :meth:`~flask.Flask.request_context` to create this object.\n\n    When the request context is popped, it will evaluate all the\n    functions registered on the application for teardown execution\n    (:meth:`~flask.Flask.teardown_request`).\n\n    The request context is automatically popped at the end of the\n    request. When using the interactive debugger, the context will be\n    restored so ``request`` is still accessible. Similarly, the test\n    client can preserve the context after the request ends. However,\n    teardown functions may already have closed some resources such as\n    database connections.\n    \"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 326, "code": "    def copy(self) -> \"RequestContext\":\n        return self.__class__(\n            self.app,\n            environ=self.request.environ,\n            request=self.request,\n            session=self.session,\n        )", "documentation": "        \"\"\"Creates a copy of this request context with the same request object.\n        This can be used to move a request context to a different greenlet.\n        Because the actual request object is the same this cannot be used to\n        move a request context to a different thread unless access to the\n        request object is locked.\n\n        .. versionadded:: 0.10\n\n        .. versionchanged:: 1.1\n           The current session object is used instead of reloading the original\n           data. This prevents `flask.session` pointing to an out-of-date object.\n        \"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 346, "code": "    def match_request(self) -> None:\n        try:\n            result = self.url_adapter.match(return_rule=True)  # type: ignore\n            self.request.url_rule, self.request.view_args = result  # type: ignore\n        except HTTPException as e:\n            self.request.routing_exception = e", "documentation": "        \"\"\"Can be overridden by a subclass to hook into the matching\n        of the request.\n        \"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 385, "code": "    def pop(self, exc: t.Optional[BaseException] = _sentinel) -> None:  # type: ignore\n        clear_request = len(self._cv_tokens) == 1\n        try:\n            if clear_request:\n                if exc is _sentinel:\n                    exc = sys.exc_info()[1]\n                self.app.do_teardown_request(exc)\n                request_close = getattr(self.request, \"close\", None)\n                if request_close is not None:\n                    request_close()\n        finally:", "documentation": "        \"\"\"Pops the request context and unbinds it by doing that.  This will\n        also trigger the execution of functions registered by the\n        :meth:`~flask.Flask.teardown_request` decorator.\n\n        .. versionchanged:: 0.9\n           Added the `exc` argument.\n        \"\"\""}], "after_segments": [{"filename": "src/flask/ctx.py", "start_line": 24, "code": "class _AppCtxGlobals:", "documentation": "    \"\"\"A plain object. Used as a namespace for storing data during an\n    application context.\n\n    Creating an app context automatically creates this object, which is\n    made available as the :data:`g` proxy.\n\n    .. describe:: 'key' in g\n\n        Check whether an attribute is present.\n\n        .. versionadded:: 0.10\n\n    .. describe:: iter(g)\n\n        Return an iterator over the attribute names.\n\n        .. versionadded:: 0.10\n    \"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 62, "code": "    def get(self, name: str, default: t.Optional[t.Any] = None) -> t.Any:\n        return self.__dict__.get(name, default)", "documentation": "        \"\"\"Get an attribute by name, or a default value. Like\n        :meth:`dict.get`.\n\n        :param name: Name of attribute to get.\n        :param default: Value to return if the attribute is not present.\n\n        .. versionadded:: 0.10\n        \"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 73, "code": "    def pop(self, name: str, default: t.Any = _sentinel) -> t.Any:\n        if default is _sentinel:\n            return self.__dict__.pop(name)\n        else:\n            return self.__dict__.pop(name, default)", "documentation": "        \"\"\"Get and remove an attribute by name. Like :meth:`dict.pop`.\n\n        :param name: Name of attribute to pop.\n        :param default: Value to return if the attribute is not present,\n            instead of raising a ``KeyError``.\n\n        .. versionadded:: 0.11\n        \"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 87, "code": "    def setdefault(self, name: str, default: t.Any = None) -> t.Any:\n        return self.__dict__.setdefault(name, default)", "documentation": "        \"\"\"Get the value of an attribute if it is present, otherwise\n        set and return a default value. Like :meth:`dict.setdefault`.\n\n        :param name: Name of attribute to get.\n        :param default: Value to set and return if the attribute is not\n            present.\n\n        .. versionadded:: 0.11\n        \"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 112, "code": "def after_this_request(f: ft.AfterRequestCallable) -> ft.AfterRequestCallable:\n    ctx = _cv_req.get(None)\n    if ctx is None:\n        raise RuntimeError(\n            \"'after_this_request' can only be used when a request\"\n            \" context is active, such as in a view function.\"\n        )\n    ctx._after_request_functions.append(f)\n    return f", "documentation": "    \"\"\"Executes a function after this request.  This is useful to modify\n    response objects.  The function is passed the response object and has\n    to return the same or a new one.\n\n    Example::\n\n        @app.route('/')\n        def index():\n            @after_this_request\n            def add_header(response):\n                response.headers['X-Foo'] = 'Parachute'\n                return response\n            return 'Hello World!'\n\n    This is more useful if a function other than the view function wants to\n    modify a response.  For instance think of a decorator that wants to add\n    some headers without converting the return value into a response object.\n\n    .. versionadded:: 0.9\n    \"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 145, "code": "def copy_current_request_context(f: t.Callable) -> t.Callable:\n    ctx = _cv_req.get(None)\n    if ctx is None:\n        raise RuntimeError(\n            \"'copy_current_request_context' can only be used when a\"\n            \" request context is active, such as in a view function.\"\n        )\n    ctx = ctx.copy()", "documentation": "    \"\"\"A helper function that decorates a function to retain the current\n    request context.  This is useful when working with greenlets.  The moment\n    the function is decorated a copy of the request context is created and\n    then pushed when the function is called.  The current session is also\n    included in the copied request context.\n\n    Example::\n\n        import gevent\n        from flask import copy_current_request_context\n\n        @app.route('/')\n        def index():\n            @copy_current_request_context\n            def do_some_work():\n                # do some work here, it can access flask.request or\n                # flask.session like you would otherwise in the view function.\n                ...\n            gevent.spawn(do_some_work)\n            return 'Regular response'\n\n    .. versionadded:: 0.10\n    \"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 186, "code": "def has_request_context() -> bool:\n    return _cv_app.get(None) is not None", "documentation": "    \"\"\"If you have code that wants to test if a request context is there or\n    not this function can be used.  For instance, you may want to take advantage\n    of request information if the request object is available, but fail\n    silently if it is unavailable.\n\n    ::\n\n        class User(db.Model):\n\n            def __init__(self, username, remote_addr=None):\n                self.username = username\n                if remote_addr is None and has_request_context():\n                    remote_addr = request.remote_addr\n                self.remote_addr = remote_addr\n\n    Alternatively you can also just test any of the context bound objects\n    (such as :class:`request` or :class:`g`) for truthness::\n\n        class User(db.Model):\n\n            def __init__(self, username, remote_addr=None):\n                self.username = username\n                if remote_addr is None and request:\n                    remote_addr = request.remote_addr\n                self.remote_addr = remote_addr\n\n    .. versionadded:: 0.7\n    \"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 218, "code": "def has_app_context() -> bool:\n    return _cv_req.get(None) is not None", "documentation": "    \"\"\"Works like :func:`has_request_context` but for the application\n    context.  You can also just do a boolean check on the\n    :data:`current_app` object instead.\n\n    .. versionadded:: 0.9\n    \"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 228, "code": "class AppContext:", "documentation": "    \"\"\"The app context contains application-specific information. An app\n    context is created and pushed at the beginning of each request if\n    one is not already active. An app context is also pushed when\n    running CLI commands.\n    \"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 241, "code": "    def push(self) -> None:\n        self._cv_tokens.append(_cv_app.set(self))\n        appcontext_pushed.send(self.app)", "documentation": "        \"\"\"Binds the app context to the current context.\"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 246, "code": "    def pop(self, exc: t.Optional[BaseException] = _sentinel) -> None:  # type: ignore\n        try:\n            if len(self._cv_tokens) == 1:\n                if exc is _sentinel:\n                    exc = sys.exc_info()[1]\n                self.app.do_teardown_appcontext(exc)\n        finally:\n            ctx = _cv_app.get()\n            _cv_app.reset(self._cv_tokens.pop())\n        if ctx is not self:\n            raise AssertionError(", "documentation": "        \"\"\"Pops the app context.\"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 277, "code": "class RequestContext:", "documentation": "    \"\"\"The request context contains per-request information. The Flask\n    app creates and pushes it at the beginning of the request, then pops\n    it at the end of the request. It will create the URL adapter and\n    request object for the WSGI environment provided.\n\n    Do not attempt to use this class directly, instead use\n    :meth:`~flask.Flask.test_request_context` and\n    :meth:`~flask.Flask.request_context` to create this object.\n\n    When the request context is popped, it will evaluate all the\n    functions registered on the application for teardown execution\n    (:meth:`~flask.Flask.teardown_request`).\n\n    The request context is automatically popped at the end of the\n    request. When using the interactive debugger, the context will be\n    restored so ``request`` is still accessible. Similarly, the test\n    client can preserve the context after the request ends. However,\n    teardown functions may already have closed some resources such as\n    database connections.\n    \"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 324, "code": "    def copy(self) -> \"RequestContext\":\n        return self.__class__(\n            self.app,\n            environ=self.request.environ,\n            request=self.request,\n            session=self.session,\n        )", "documentation": "        \"\"\"Creates a copy of this request context with the same request object.\n        This can be used to move a request context to a different greenlet.\n        Because the actual request object is the same this cannot be used to\n        move a request context to a different thread unless access to the\n        request object is locked.\n\n        .. versionadded:: 0.10\n\n        .. versionchanged:: 1.1\n           The current session object is used instead of reloading the original\n           data. This prevents `flask.session` pointing to an out-of-date object.\n        \"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 344, "code": "    def match_request(self) -> None:\n        try:\n            result = self.url_adapter.match(return_rule=True)  # type: ignore\n            self.request.url_rule, self.request.view_args = result  # type: ignore\n        except HTTPException as e:\n            self.request.routing_exception = e", "documentation": "        \"\"\"Can be overridden by a subclass to hook into the matching\n        of the request.\n        \"\"\""}, {"filename": "src/flask/ctx.py", "start_line": 383, "code": "    def pop(self, exc: t.Optional[BaseException] = _sentinel) -> None:  # type: ignore\n        clear_request = len(self._cv_tokens) == 1\n        try:\n            if clear_request:\n                if exc is _sentinel:\n                    exc = sys.exc_info()[1]\n                self.app.do_teardown_request(exc)\n                request_close = getattr(self.request, \"close\", None)\n                if request_close is not None:\n                    request_close()\n        finally:", "documentation": "        \"\"\"Pops the request context and unbinds it by doing that.  This will\n        also trigger the execution of functions registered by the\n        :meth:`~flask.Flask.teardown_request` decorator.\n\n        .. versionchanged:: 0.9\n           Added the `exc` argument.\n        \"\"\""}]}
{"repository": "pallets/flask", "commit_sha": "e0dad454810dd081947d3ca2ff376c5096185698", "commit_message": "update docs about contexts", "commit_date": "2022-07-08T14:08:54+00:00", "author": "David Lord", "file": "src/flask/scaffold.py", "patch": "@@ -574,30 +574,27 @@ def after_request(self, f: T_after_request) -> T_after_request:\n \n     @setupmethod\n     def teardown_request(self, f: T_teardown) -> T_teardown:\n-        \"\"\"Register a function to be run at the end of each request,\n-        regardless of whether there was an exception or not.  These functions\n-        are executed when the request context is popped, even if not an\n-        actual request was performed.\n-\n-        Example::\n-\n-            ctx = app.test_request_context()\n-            ctx.push()\n-            ...\n-            ctx.pop()\n-\n-        When ``ctx.pop()`` is executed in the above example, the teardown\n-        functions are called just before the request context moves from the\n-        stack of active contexts.  This becomes relevant if you are using\n-        such constructs in tests.\n-\n-        Teardown functions must avoid raising exceptions. If\n-        they execute code that might fail they\n-        will have to surround the execution of that code with try/except\n-        statements and log any errors.\n-\n-        When a teardown function was called because of an exception it will\n-        be passed an error object.\n+        \"\"\"Register a function to be called when the request context is\n+        popped. Typically this happens at the end of each request, but\n+        contexts may be pushed manually as well during testing.\n+\n+        .. code-block:: python\n+\n+            with app.test_request_context():\n+                ...\n+\n+        When the ``with`` block exits (or ``ctx.pop()`` is called), the\n+        teardown functions are called just before the request context is\n+        made inactive.\n+\n+        When a teardown function was called because of an unhandled\n+        exception it will be passed an error object. If an\n+        :meth:`errorhandler` is registered, it will handle the exception\n+        and the teardown will not receive it.\n+\n+        Teardown functions must avoid raising exceptions. If they\n+        execute code that might fail they must surround that code with a\n+        ``try``/``except`` block and log any errors.\n \n         The return values of teardown functions are ignored.\n         \"\"\"", "before_segments": [{"filename": "src/flask/scaffold.py", "start_line": 54, "code": "class Scaffold:\n    name: str\n    _static_folder: t.Optional[str] = None\n    _static_url_path: t.Optional[str] = None\n    json_encoder: t.Optional[t.Type[JSONEncoder]] = None\n    json_decoder: t.Optional[t.Type[JSONDecoder]] = None", "documentation": "    \"\"\"Common behavior shared between :class:`~flask.Flask` and\n    :class:`~flask.blueprints.Blueprint`.\n\n    :param import_name: The import name of the module where this object\n        is defined. Usually :attr:`__name__` should be used.\n    :param static_folder: Path to a folder of static files to serve.\n        If this is set, a static route will be added.\n    :param static_url_path: URL prefix for the static route.\n    :param template_folder: Path to a folder containing template files.\n        for rendering. If this is set, a Jinja loader will be added.\n    :param root_path: The path that static, template, and resource files\n        are relative to. Typically not set, it is discovered based on\n        the ``import_name``.\n\n    .. versionadded:: 2.0\n    \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 239, "code": "    def static_folder(self) -> t.Optional[str]:\n        if self._static_folder is not None:\n            return os.path.join(self.root_path, self._static_folder)\n        else:\n            return None\n    @static_folder.setter", "documentation": "        \"\"\"The absolute path to the configured static folder. ``None``\n        if no static folder is set.\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 256, "code": "    def has_static_folder(self) -> bool:\n        return self.static_folder is not None\n    @property", "documentation": "        \"\"\"``True`` if :attr:`static_folder` is set.\n\n        .. versionadded:: 0.5\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 264, "code": "    def static_url_path(self) -> t.Optional[str]:\n        if self._static_url_path is not None:\n            return self._static_url_path\n        if self.static_folder is not None:\n            basename = os.path.basename(self.static_folder)\n            return f\"/{basename}\".rstrip(\"/\")\n        return None\n    @static_url_path.setter", "documentation": "        \"\"\"The URL prefix that the static route will be accessible from.\n\n        If it was not configured during init, it is derived from\n        :attr:`static_folder`.\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 286, "code": "    def get_send_file_max_age(self, filename: t.Optional[str]) -> t.Optional[int]:\n        value = current_app.send_file_max_age_default\n        if value is None:\n            return None\n        return int(value.total_seconds())", "documentation": "        \"\"\"Used by :func:`send_file` to determine the ``max_age`` cache\n        value for a given file path if it wasn't passed.\n\n        By default, this returns :data:`SEND_FILE_MAX_AGE_DEFAULT` from\n        the configuration of :data:`~flask.current_app`. This defaults\n        to ``None``, which tells the browser to use conditional requests\n        instead of a timed cache, which is usually preferable.\n\n        .. versionchanged:: 2.0\n            The default configuration is ``None`` instead of 12 hours.\n\n        .. versionadded:: 0.9\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 307, "code": "    def send_static_file(self, filename: str) -> \"Response\":\n        if not self.has_static_folder:\n            raise RuntimeError(\"'static_folder' must be set to serve static_files.\")\n        max_age = self.get_send_file_max_age(filename)\n        return send_from_directory(\n            t.cast(str, self.static_folder), filename, max_age=max_age\n        )\n    @locked_cached_property", "documentation": "        \"\"\"The view function used to serve files from\n        :attr:`static_folder`. A route is automatically registered for\n        this view at :attr:`static_url_path` if :attr:`static_folder` is\n        set.\n\n        .. versionadded:: 0.5\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 326, "code": "    def jinja_loader(self) -> t.Optional[FileSystemLoader]:\n        if self.template_folder is not None:\n            return FileSystemLoader(os.path.join(self.root_path, self.template_folder))\n        else:\n            return None", "documentation": "        \"\"\"The Jinja loader for this object's templates. By default this\n        is a class :class:`jinja2.loaders.FileSystemLoader` to\n        :attr:`template_folder` if it is set.\n\n        .. versionadded:: 0.5\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 338, "code": "    def open_resource(self, resource: str, mode: str = \"rb\") -> t.IO[t.AnyStr]:\n        if mode not in {\"r\", \"rt\", \"rb\"}:\n            raise ValueError(\"Resources can only be opened for reading.\")\n        return open(os.path.join(self.root_path, resource), mode)", "documentation": "        \"\"\"Open a resource file relative to :attr:`root_path` for\n        reading.\n\n        For example, if the file ``schema.sql`` is next to the file\n        ``app.py`` where the ``Flask`` app is defined, it can be opened\n        with:\n\n        .. code-block:: python\n\n            with app.open_resource(\"schema.sql\") as f:\n                conn.executescript(f.read())\n\n        :param resource: Path to the resource relative to\n            :attr:`root_path`.\n        :param mode: Open the file in this mode. Only reading is\n            supported, valid values are \"r\" (or \"rt\") and \"rb\".\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 373, "code": "    def get(self, rule: str, **options: t.Any) -> t.Callable[[T_route], T_route]:\n        return self._method_route(\"GET\", rule, options)\n    @setupmethod", "documentation": "        \"\"\"Shortcut for :meth:`route` with ``methods=[\"GET\"]``.\n\n        .. versionadded:: 2.0\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 381, "code": "    def post(self, rule: str, **options: t.Any) -> t.Callable[[T_route], T_route]:\n        return self._method_route(\"POST\", rule, options)\n    @setupmethod", "documentation": "        \"\"\"Shortcut for :meth:`route` with ``methods=[\"POST\"]``.\n\n        .. versionadded:: 2.0\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 389, "code": "    def put(self, rule: str, **options: t.Any) -> t.Callable[[T_route], T_route]:\n        return self._method_route(\"PUT\", rule, options)\n    @setupmethod", "documentation": "        \"\"\"Shortcut for :meth:`route` with ``methods=[\"PUT\"]``.\n\n        .. versionadded:: 2.0\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 397, "code": "    def delete(self, rule: str, **options: t.Any) -> t.Callable[[T_route], T_route]:\n        return self._method_route(\"DELETE\", rule, options)\n    @setupmethod", "documentation": "        \"\"\"Shortcut for :meth:`route` with ``methods=[\"DELETE\"]``.\n\n        .. versionadded:: 2.0\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 405, "code": "    def patch(self, rule: str, **options: t.Any) -> t.Callable[[T_route], T_route]:\n        return self._method_route(\"PATCH\", rule, options)\n    @setupmethod", "documentation": "        \"\"\"Shortcut for :meth:`route` with ``methods=[\"PATCH\"]``.\n\n        .. versionadded:: 2.0\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 413, "code": "    def route(self, rule: str, **options: t.Any) -> t.Callable[[T_route], T_route]:", "documentation": "        \"\"\"Decorate a view function to register it with the given URL\n        rule and options. Calls :meth:`add_url_rule`, which has more\n        details about the implementation.\n\n        .. code-block:: python\n\n            @app.route(\"/\")\n            def index():\n                return \"Hello, World!\"\n\n        See :ref:`url-route-registrations`.\n\n        The endpoint name for the route defaults to the name of the view\n        function if the ``endpoint`` parameter isn't passed.\n\n        The ``methods`` parameter defaults to ``[\"GET\"]``. ``HEAD`` and\n        ``OPTIONS`` are added automatically.\n\n        :param rule: The URL rule string.\n        :param options: Extra options passed to the\n            :class:`~werkzeug.routing.Rule` object.\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 513, "code": "    def endpoint(self, endpoint: str) -> t.Callable[[F], F]:", "documentation": "        \"\"\"Decorate a view function to register it for the given\n        endpoint. Used if a rule is added without a ``view_func`` with\n        :meth:`add_url_rule`.\n\n        .. code-block:: python\n\n            app.add_url_rule(\"/ex\", endpoint=\"example\")\n\n            @app.endpoint(\"example\")\n            def example():\n                ...\n\n        :param endpoint: The endpoint name to associate with the view\n            function.\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 537, "code": "    def before_request(self, f: T_before_request) -> T_before_request:\n        self.before_request_funcs.setdefault(None, []).append(f)\n        return f\n    @setupmethod", "documentation": "        \"\"\"Register a function to run before each request.\n\n        For example, this can be used to open a database connection, or\n        to load the logged in user from the session.\n\n        .. code-block:: python\n\n            @app.before_request\n            def load_user():\n                if \"user_id\" in session:\n                    g.user = db.session.get(session[\"user_id\"])\n\n        The function will be called without any arguments. If it returns\n        a non-``None`` value, the value is handled as if it was the\n        return value from the view, and further request handling is\n        stopped.\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 559, "code": "    def after_request(self, f: T_after_request) -> T_after_request:\n        self.after_request_funcs.setdefault(None, []).append(f)\n        return f\n    @setupmethod", "documentation": "        \"\"\"Register a function to run after each request to this object.\n\n        The function is called with the response object, and must return\n        a response object. This allows the functions to modify or\n        replace the response before it is sent.\n\n        If a function raises an exception, any remaining\n        ``after_request`` functions will not be called. Therefore, this\n        should not be used for actions that must execute, such as to\n        close resources. Use :meth:`teardown_request` for that.\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 575, "code": "    def teardown_request(self, f: T_teardown) -> T_teardown:\n        self.teardown_request_funcs.setdefault(None, []).append(f)\n        return f\n    @setupmethod", "documentation": "        \"\"\"Register a function to be run at the end of each request,\n        regardless of whether there was an exception or not.  These functions\n        are executed when the request context is popped, even if not an\n        actual request was performed.\n\n        Example::\n\n            ctx = app.test_request_context()\n            ctx.push()\n            ...\n            ctx.pop()\n\n        When ``ctx.pop()`` is executed in the above example, the teardown\n        functions are called just before the request context moves from the\n        stack of active contexts.  This becomes relevant if you are using\n        such constructs in tests.\n\n        Teardown functions must avoid raising exceptions. If\n        they execute code that might fail they\n        will have to surround the execution of that code with try/except\n        statements and log any errors.\n\n        When a teardown function was called because of an exception it will\n        be passed an error object.\n\n        The return values of teardown functions are ignored.\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 636, "code": "    def url_defaults(self, f: T_url_defaults) -> T_url_defaults:\n        self.url_default_functions[None].append(f)\n        return f\n    @setupmethod", "documentation": "        \"\"\"Callback function for URL defaults for all view functions of the\n        application.  It's called with the endpoint and values and should\n        update the values passed in place.\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 743, "code": "def _endpoint_from_view_func(view_func: t.Callable) -> str:\n    assert view_func is not None, \"expected view func if endpoint is not provided.\"\n    return view_func.__name__", "documentation": "    \"\"\"Internal helper that returns the default endpoint for a given\n    function.  This always is the function name.\n    \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 751, "code": "def _matching_loader_thinks_module_is_package(loader, mod_name):\n    if hasattr(loader, \"is_package\"):\n        return loader.is_package(mod_name)\n    cls = type(loader)\n    if cls.__module__ == \"_frozen_importlib\" and cls.__name__ == \"NamespaceLoader\":\n        return True\n    raise AttributeError(\n        f\"'{cls.__name__}.is_package()' must be implemented for PEP 302\"\n        f\" import hooks.\"\n    )", "documentation": "    \"\"\"Attempt to figure out if the given name is a package or a module.\n\n    :param: loader: The loader that handled the name.\n    :param mod_name: The name of the package or module.\n    \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 785, "code": "def _find_package_path(import_name):\n    root_mod_name, _, _ = import_name.partition(\".\")\n    try:\n        root_spec = importlib.util.find_spec(root_mod_name)\n        if root_spec is None:\n            raise ValueError(\"not found\")\n    except (ImportError, ValueError):\n        pass  # handled below\n    else:\n        if root_spec.origin in {\"namespace\", None}:\n            package_spec = importlib.util.find_spec(import_name)", "documentation": "    \"\"\"Find the path that contains the package or module.\"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 853, "code": "def find_package(import_name: str):\n    package_path = _find_package_path(import_name)\n    py_prefix = os.path.abspath(sys.prefix)\n    if _path_is_relative_to(pathlib.PurePath(package_path), py_prefix):\n        return py_prefix, package_path\n    site_parent, site_folder = os.path.split(package_path)\n    if site_folder.lower() == \"site-packages\":\n        parent, folder = os.path.split(site_parent)\n        if folder.lower() == \"lib\":\n            return parent, package_path\n        if os.path.basename(parent).lower() == \"lib\":", "documentation": "    \"\"\"Find the prefix that a package is installed under, and the path\n    that it would be imported from.\n\n    The prefix is the directory containing the standard directory\n    hierarchy (lib, bin, etc.). If the package is not installed to the\n    system (:attr:`sys.prefix`) or a virtualenv (``site-packages``),\n    ``None`` is returned.\n\n    The path is the entry in :attr:`sys.path` that contains the package\n    for import. If the package is not installed, it's assumed that the\n    package was imported from the current working directory.\n    \"\"\""}], "after_segments": [{"filename": "src/flask/scaffold.py", "start_line": 54, "code": "class Scaffold:\n    name: str\n    _static_folder: t.Optional[str] = None\n    _static_url_path: t.Optional[str] = None\n    json_encoder: t.Optional[t.Type[JSONEncoder]] = None\n    json_decoder: t.Optional[t.Type[JSONDecoder]] = None", "documentation": "    \"\"\"Common behavior shared between :class:`~flask.Flask` and\n    :class:`~flask.blueprints.Blueprint`.\n\n    :param import_name: The import name of the module where this object\n        is defined. Usually :attr:`__name__` should be used.\n    :param static_folder: Path to a folder of static files to serve.\n        If this is set, a static route will be added.\n    :param static_url_path: URL prefix for the static route.\n    :param template_folder: Path to a folder containing template files.\n        for rendering. If this is set, a Jinja loader will be added.\n    :param root_path: The path that static, template, and resource files\n        are relative to. Typically not set, it is discovered based on\n        the ``import_name``.\n\n    .. versionadded:: 2.0\n    \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 239, "code": "    def static_folder(self) -> t.Optional[str]:\n        if self._static_folder is not None:\n            return os.path.join(self.root_path, self._static_folder)\n        else:\n            return None\n    @static_folder.setter", "documentation": "        \"\"\"The absolute path to the configured static folder. ``None``\n        if no static folder is set.\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 256, "code": "    def has_static_folder(self) -> bool:\n        return self.static_folder is not None\n    @property", "documentation": "        \"\"\"``True`` if :attr:`static_folder` is set.\n\n        .. versionadded:: 0.5\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 264, "code": "    def static_url_path(self) -> t.Optional[str]:\n        if self._static_url_path is not None:\n            return self._static_url_path\n        if self.static_folder is not None:\n            basename = os.path.basename(self.static_folder)\n            return f\"/{basename}\".rstrip(\"/\")\n        return None\n    @static_url_path.setter", "documentation": "        \"\"\"The URL prefix that the static route will be accessible from.\n\n        If it was not configured during init, it is derived from\n        :attr:`static_folder`.\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 286, "code": "    def get_send_file_max_age(self, filename: t.Optional[str]) -> t.Optional[int]:\n        value = current_app.send_file_max_age_default\n        if value is None:\n            return None\n        return int(value.total_seconds())", "documentation": "        \"\"\"Used by :func:`send_file` to determine the ``max_age`` cache\n        value for a given file path if it wasn't passed.\n\n        By default, this returns :data:`SEND_FILE_MAX_AGE_DEFAULT` from\n        the configuration of :data:`~flask.current_app`. This defaults\n        to ``None``, which tells the browser to use conditional requests\n        instead of a timed cache, which is usually preferable.\n\n        .. versionchanged:: 2.0\n            The default configuration is ``None`` instead of 12 hours.\n\n        .. versionadded:: 0.9\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 307, "code": "    def send_static_file(self, filename: str) -> \"Response\":\n        if not self.has_static_folder:\n            raise RuntimeError(\"'static_folder' must be set to serve static_files.\")\n        max_age = self.get_send_file_max_age(filename)\n        return send_from_directory(\n            t.cast(str, self.static_folder), filename, max_age=max_age\n        )\n    @locked_cached_property", "documentation": "        \"\"\"The view function used to serve files from\n        :attr:`static_folder`. A route is automatically registered for\n        this view at :attr:`static_url_path` if :attr:`static_folder` is\n        set.\n\n        .. versionadded:: 0.5\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 326, "code": "    def jinja_loader(self) -> t.Optional[FileSystemLoader]:\n        if self.template_folder is not None:\n            return FileSystemLoader(os.path.join(self.root_path, self.template_folder))\n        else:\n            return None", "documentation": "        \"\"\"The Jinja loader for this object's templates. By default this\n        is a class :class:`jinja2.loaders.FileSystemLoader` to\n        :attr:`template_folder` if it is set.\n\n        .. versionadded:: 0.5\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 338, "code": "    def open_resource(self, resource: str, mode: str = \"rb\") -> t.IO[t.AnyStr]:\n        if mode not in {\"r\", \"rt\", \"rb\"}:\n            raise ValueError(\"Resources can only be opened for reading.\")\n        return open(os.path.join(self.root_path, resource), mode)", "documentation": "        \"\"\"Open a resource file relative to :attr:`root_path` for\n        reading.\n\n        For example, if the file ``schema.sql`` is next to the file\n        ``app.py`` where the ``Flask`` app is defined, it can be opened\n        with:\n\n        .. code-block:: python\n\n            with app.open_resource(\"schema.sql\") as f:\n                conn.executescript(f.read())\n\n        :param resource: Path to the resource relative to\n            :attr:`root_path`.\n        :param mode: Open the file in this mode. Only reading is\n            supported, valid values are \"r\" (or \"rt\") and \"rb\".\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 373, "code": "    def get(self, rule: str, **options: t.Any) -> t.Callable[[T_route], T_route]:\n        return self._method_route(\"GET\", rule, options)\n    @setupmethod", "documentation": "        \"\"\"Shortcut for :meth:`route` with ``methods=[\"GET\"]``.\n\n        .. versionadded:: 2.0\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 381, "code": "    def post(self, rule: str, **options: t.Any) -> t.Callable[[T_route], T_route]:\n        return self._method_route(\"POST\", rule, options)\n    @setupmethod", "documentation": "        \"\"\"Shortcut for :meth:`route` with ``methods=[\"POST\"]``.\n\n        .. versionadded:: 2.0\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 389, "code": "    def put(self, rule: str, **options: t.Any) -> t.Callable[[T_route], T_route]:\n        return self._method_route(\"PUT\", rule, options)\n    @setupmethod", "documentation": "        \"\"\"Shortcut for :meth:`route` with ``methods=[\"PUT\"]``.\n\n        .. versionadded:: 2.0\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 397, "code": "    def delete(self, rule: str, **options: t.Any) -> t.Callable[[T_route], T_route]:\n        return self._method_route(\"DELETE\", rule, options)\n    @setupmethod", "documentation": "        \"\"\"Shortcut for :meth:`route` with ``methods=[\"DELETE\"]``.\n\n        .. versionadded:: 2.0\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 405, "code": "    def patch(self, rule: str, **options: t.Any) -> t.Callable[[T_route], T_route]:\n        return self._method_route(\"PATCH\", rule, options)\n    @setupmethod", "documentation": "        \"\"\"Shortcut for :meth:`route` with ``methods=[\"PATCH\"]``.\n\n        .. versionadded:: 2.0\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 413, "code": "    def route(self, rule: str, **options: t.Any) -> t.Callable[[T_route], T_route]:", "documentation": "        \"\"\"Decorate a view function to register it with the given URL\n        rule and options. Calls :meth:`add_url_rule`, which has more\n        details about the implementation.\n\n        .. code-block:: python\n\n            @app.route(\"/\")\n            def index():\n                return \"Hello, World!\"\n\n        See :ref:`url-route-registrations`.\n\n        The endpoint name for the route defaults to the name of the view\n        function if the ``endpoint`` parameter isn't passed.\n\n        The ``methods`` parameter defaults to ``[\"GET\"]``. ``HEAD`` and\n        ``OPTIONS`` are added automatically.\n\n        :param rule: The URL rule string.\n        :param options: Extra options passed to the\n            :class:`~werkzeug.routing.Rule` object.\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 513, "code": "    def endpoint(self, endpoint: str) -> t.Callable[[F], F]:", "documentation": "        \"\"\"Decorate a view function to register it for the given\n        endpoint. Used if a rule is added without a ``view_func`` with\n        :meth:`add_url_rule`.\n\n        .. code-block:: python\n\n            app.add_url_rule(\"/ex\", endpoint=\"example\")\n\n            @app.endpoint(\"example\")\n            def example():\n                ...\n\n        :param endpoint: The endpoint name to associate with the view\n            function.\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 537, "code": "    def before_request(self, f: T_before_request) -> T_before_request:\n        self.before_request_funcs.setdefault(None, []).append(f)\n        return f\n    @setupmethod", "documentation": "        \"\"\"Register a function to run before each request.\n\n        For example, this can be used to open a database connection, or\n        to load the logged in user from the session.\n\n        .. code-block:: python\n\n            @app.before_request\n            def load_user():\n                if \"user_id\" in session:\n                    g.user = db.session.get(session[\"user_id\"])\n\n        The function will be called without any arguments. If it returns\n        a non-``None`` value, the value is handled as if it was the\n        return value from the view, and further request handling is\n        stopped.\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 559, "code": "    def after_request(self, f: T_after_request) -> T_after_request:\n        self.after_request_funcs.setdefault(None, []).append(f)\n        return f\n    @setupmethod", "documentation": "        \"\"\"Register a function to run after each request to this object.\n\n        The function is called with the response object, and must return\n        a response object. This allows the functions to modify or\n        replace the response before it is sent.\n\n        If a function raises an exception, any remaining\n        ``after_request`` functions will not be called. Therefore, this\n        should not be used for actions that must execute, such as to\n        close resources. Use :meth:`teardown_request` for that.\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 575, "code": "    def teardown_request(self, f: T_teardown) -> T_teardown:\n        self.teardown_request_funcs.setdefault(None, []).append(f)\n        return f\n    @setupmethod", "documentation": "        \"\"\"Register a function to be called when the request context is\n        popped. Typically this happens at the end of each request, but\n        contexts may be pushed manually as well during testing.\n\n        .. code-block:: python\n\n            with app.test_request_context():\n                ...\n\n        When the ``with`` block exits (or ``ctx.pop()`` is called), the\n        teardown functions are called just before the request context is\n        made inactive.\n\n        When a teardown function was called because of an unhandled\n        exception it will be passed an error object. If an\n        :meth:`errorhandler` is registered, it will handle the exception\n        and the teardown will not receive it.\n\n        Teardown functions must avoid raising exceptions. If they\n        execute code that might fail they must surround that code with a\n        ``try``/``except`` block and log any errors.\n\n        The return values of teardown functions are ignored.\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 633, "code": "    def url_defaults(self, f: T_url_defaults) -> T_url_defaults:\n        self.url_default_functions[None].append(f)\n        return f\n    @setupmethod", "documentation": "        \"\"\"Callback function for URL defaults for all view functions of the\n        application.  It's called with the endpoint and values and should\n        update the values passed in place.\n        \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 740, "code": "def _endpoint_from_view_func(view_func: t.Callable) -> str:\n    assert view_func is not None, \"expected view func if endpoint is not provided.\"\n    return view_func.__name__", "documentation": "    \"\"\"Internal helper that returns the default endpoint for a given\n    function.  This always is the function name.\n    \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 748, "code": "def _matching_loader_thinks_module_is_package(loader, mod_name):\n    if hasattr(loader, \"is_package\"):\n        return loader.is_package(mod_name)\n    cls = type(loader)\n    if cls.__module__ == \"_frozen_importlib\" and cls.__name__ == \"NamespaceLoader\":\n        return True\n    raise AttributeError(\n        f\"'{cls.__name__}.is_package()' must be implemented for PEP 302\"\n        f\" import hooks.\"\n    )", "documentation": "    \"\"\"Attempt to figure out if the given name is a package or a module.\n\n    :param: loader: The loader that handled the name.\n    :param mod_name: The name of the package or module.\n    \"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 782, "code": "def _find_package_path(import_name):\n    root_mod_name, _, _ = import_name.partition(\".\")\n    try:\n        root_spec = importlib.util.find_spec(root_mod_name)\n        if root_spec is None:\n            raise ValueError(\"not found\")\n    except (ImportError, ValueError):\n        pass  # handled below\n    else:\n        if root_spec.origin in {\"namespace\", None}:\n            package_spec = importlib.util.find_spec(import_name)", "documentation": "    \"\"\"Find the path that contains the package or module.\"\"\""}, {"filename": "src/flask/scaffold.py", "start_line": 850, "code": "def find_package(import_name: str):\n    package_path = _find_package_path(import_name)\n    py_prefix = os.path.abspath(sys.prefix)\n    if _path_is_relative_to(pathlib.PurePath(package_path), py_prefix):\n        return py_prefix, package_path\n    site_parent, site_folder = os.path.split(package_path)\n    if site_folder.lower() == \"site-packages\":\n        parent, folder = os.path.split(site_parent)\n        if folder.lower() == \"lib\":\n            return parent, package_path\n        if os.path.basename(parent).lower() == \"lib\":", "documentation": "    \"\"\"Find the prefix that a package is installed under, and the path\n    that it would be imported from.\n\n    The prefix is the directory containing the standard directory\n    hierarchy (lib, bin, etc.). If the package is not installed to the\n    system (:attr:`sys.prefix`) or a virtualenv (``site-packages``),\n    ``None`` is returned.\n\n    The path is the entry in :attr:`sys.path` that contains the package\n    for import. If the package is not installed, it's assumed that the\n    package was imported from the current working directory.\n    \"\"\""}]}
{"repository": "pallets/flask", "commit_sha": "e0dad454810dd081947d3ca2ff376c5096185698", "commit_message": "update docs about contexts", "commit_date": "2022-07-08T14:08:54+00:00", "author": "David Lord", "file": "src/flask/testing.py", "patch": "@@ -94,11 +94,10 @@ def json_dumps(self, obj: t.Any, **kwargs: t.Any) -> str:  # type: ignore\n \n \n class FlaskClient(Client):\n-    \"\"\"Works like a regular Werkzeug test client but has some knowledge about\n-    how Flask works to defer the cleanup of the request context stack to the\n-    end of a ``with`` body when used in a ``with`` statement.  For general\n-    information about how to use this class refer to\n-    :class:`werkzeug.test.Client`.\n+    \"\"\"Works like a regular Werkzeug test client but has knowledge about\n+    Flask's contexts to defer the cleanup of the request context until\n+    the end of a ``with`` block. For general information about how to\n+    use this class refer to :class:`werkzeug.test.Client`.\n \n     .. versionchanged:: 0.12\n        `app.test_client()` includes preset default environment, which can be", "before_segments": [{"filename": "src/flask/testing.py", "start_line": 23, "code": "class EnvironBuilder(werkzeug.test.EnvironBuilder):", "documentation": "    \"\"\"An :class:`~werkzeug.test.EnvironBuilder`, that takes defaults from the\n    application.\n\n    :param app: The Flask application to configure the environment from.\n    :param path: URL path being requested.\n    :param base_url: Base URL where the app is being served, which\n        ``path`` is relative to. If not given, built from\n        :data:`PREFERRED_URL_SCHEME`, ``subdomain``,\n        :data:`SERVER_NAME`, and :data:`APPLICATION_ROOT`.\n    :param subdomain: Subdomain name to append to :data:`SERVER_NAME`.\n    :param url_scheme: Scheme to use instead of\n        :data:`PREFERRED_URL_SCHEME`.\n    :param json: If given, this is serialized as JSON and passed as\n        ``data``. Also defaults ``content_type`` to\n        ``application/json``.\n    :param args: other positional arguments passed to\n        :class:`~werkzeug.test.EnvironBuilder`.\n    :param kwargs: other keyword arguments passed to\n        :class:`~werkzeug.test.EnvironBuilder`.\n    \"\"\""}, {"filename": "src/flask/testing.py", "start_line": 85, "code": "    def json_dumps(self, obj: t.Any, **kwargs: t.Any) -> str:  # type: ignore\n        kwargs.setdefault(\"app\", self.app)\n        return json_dumps(obj, **kwargs)", "documentation": "        \"\"\"Serialize ``obj`` to a JSON-formatted string.\n\n        The serialization will be configured according to the config associated\n        with this EnvironBuilder's ``app``.\n        \"\"\""}, {"filename": "src/flask/testing.py", "start_line": 95, "code": "class FlaskClient(Client):\n    application: \"Flask\"", "documentation": "    \"\"\"Works like a regular Werkzeug test client but has some knowledge about\n    how Flask works to defer the cleanup of the request context stack to the\n    end of a ``with`` body when used in a ``with`` statement.  For general\n    information about how to use this class refer to\n    :class:`werkzeug.test.Client`.\n\n    .. versionchanged:: 0.12\n       `app.test_client()` includes preset default environment, which can be\n       set after instantiation of the `app.test_client()` object in\n       `client.environ_base`.\n\n    Basic usage is outlined in the :doc:`/testing` chapter.\n    \"\"\""}, {"filename": "src/flask/testing.py", "start_line": 254, "code": "class FlaskCliRunner(CliRunner):", "documentation": "    \"\"\"A :class:`~click.testing.CliRunner` for testing a Flask app's\n    CLI commands. Typically created using\n    :meth:`~flask.Flask.test_cli_runner`. See :ref:`testing-cli`.\n    \"\"\""}], "after_segments": [{"filename": "src/flask/testing.py", "start_line": 23, "code": "class EnvironBuilder(werkzeug.test.EnvironBuilder):", "documentation": "    \"\"\"An :class:`~werkzeug.test.EnvironBuilder`, that takes defaults from the\n    application.\n\n    :param app: The Flask application to configure the environment from.\n    :param path: URL path being requested.\n    :param base_url: Base URL where the app is being served, which\n        ``path`` is relative to. If not given, built from\n        :data:`PREFERRED_URL_SCHEME`, ``subdomain``,\n        :data:`SERVER_NAME`, and :data:`APPLICATION_ROOT`.\n    :param subdomain: Subdomain name to append to :data:`SERVER_NAME`.\n    :param url_scheme: Scheme to use instead of\n        :data:`PREFERRED_URL_SCHEME`.\n    :param json: If given, this is serialized as JSON and passed as\n        ``data``. Also defaults ``content_type`` to\n        ``application/json``.\n    :param args: other positional arguments passed to\n        :class:`~werkzeug.test.EnvironBuilder`.\n    :param kwargs: other keyword arguments passed to\n        :class:`~werkzeug.test.EnvironBuilder`.\n    \"\"\""}, {"filename": "src/flask/testing.py", "start_line": 85, "code": "    def json_dumps(self, obj: t.Any, **kwargs: t.Any) -> str:  # type: ignore\n        kwargs.setdefault(\"app\", self.app)\n        return json_dumps(obj, **kwargs)", "documentation": "        \"\"\"Serialize ``obj`` to a JSON-formatted string.\n\n        The serialization will be configured according to the config associated\n        with this EnvironBuilder's ``app``.\n        \"\"\""}, {"filename": "src/flask/testing.py", "start_line": 95, "code": "class FlaskClient(Client):\n    application: \"Flask\"", "documentation": "    \"\"\"Works like a regular Werkzeug test client but has knowledge about\n    Flask's contexts to defer the cleanup of the request context until\n    the end of a ``with`` block. For general information about how to\n    use this class refer to :class:`werkzeug.test.Client`.\n\n    .. versionchanged:: 0.12\n       `app.test_client()` includes preset default environment, which can be\n       set after instantiation of the `app.test_client()` object in\n       `client.environ_base`.\n\n    Basic usage is outlined in the :doc:`/testing` chapter.\n    \"\"\""}, {"filename": "src/flask/testing.py", "start_line": 253, "code": "class FlaskCliRunner(CliRunner):", "documentation": "    \"\"\"A :class:`~click.testing.CliRunner` for testing a Flask app's\n    CLI commands. Typically created using\n    :meth:`~flask.Flask.test_cli_runner`. See :ref:`testing-cli`.\n    \"\"\""}]}
{"repository": "pydantic/pydantic", "commit_sha": "b5885e720ba5148ecbba9787b86a49ed63948d68", "commit_message": "Fix docstring of `core_schema.invalid_schema()` (#12588)", "commit_date": "2025-11-28T15:46:14+00:00", "author": "Victorien", "file": "pydantic-core/python/pydantic_core/core_schema.py", "patch": "@@ -485,8 +485,6 @@ def invalid_schema(ref: str | None = None, metadata: dict[str, Any] | None = Non\n     \"\"\"\n     Returns an invalid schema, used to indicate that a schema is invalid.\n \n-        Returns a schema that matches any value, e.g.:\n-\n     Args:\n         ref: optional unique identifier of the schema, used to reference the schema in other places\n         metadata: Any other information you want to include with the schema, not used by pydantic-core", "before_segments": [{"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 42, "code": "class CoreConfig(TypedDict, total=False):\n    title: str\n    strict: bool\n    extra_fields_behavior: ExtraBehavior\n    typed_dict_total: bool  # default: True\n    from_attributes: bool\n    loc_by_alias: bool\n    revalidate_instances: Literal['always', 'never', 'subclass-instances']\n    validate_default: bool\n    str_max_length: int\n    str_min_length: int", "documentation": "    \"\"\"\n    Base class for schema configuration options.\n\n    Attributes:\n        title: The name of the configuration.\n        strict: Whether the configuration should strictly adhere to specified rules.\n        extra_fields_behavior: The behavior for handling extra fields.\n        typed_dict_total: Whether the TypedDict should be considered total. Default is `True`.\n        from_attributes: Whether to use attributes for models, dataclasses, and tagged union keys.\n        loc_by_alias: Whether to use the used alias (or first alias for \"field required\" errors) instead of\n            `field_names` to construct error `loc`s. Default is `True`.\n        revalidate_instances: Whether instances of models and dataclasses should re-validate. Default is 'never'.\n        validate_default: Whether to validate default values during validation. Default is `False`.\n        str_max_length: The maximum length for string fields.\n        str_min_length: The minimum length for string fields.\n        str_strip_whitespace: Whether to strip whitespace from string fields.\n        str_to_lower: Whether to convert string fields to lowercase.\n        str_to_upper: Whether to convert string fields to uppercase.\n        allow_inf_nan: Whether to allow infinity and NaN values for float fields. Default is `True`.\n        ser_json_timedelta: The serialization option for `timedelta` values. Default is 'iso8601'.\n            Note that if ser_json_temporal is set, then this param will be ignored.\n        ser_json_temporal: The serialization option for datetime like values. Default is 'iso8601'.\n            The types this covers are datetime, date, time and timedelta.\n            If this is set, it will take precedence over ser_json_timedelta\n        ser_json_bytes: The serialization option for `bytes` values. Default is 'utf8'.\n        ser_json_inf_nan: The serialization option for infinity and NaN values\n            in float fields. Default is 'null'.\n        val_json_bytes: The validation option for `bytes` values, complementing ser_json_bytes. Default is 'utf8'.\n        hide_input_in_errors: Whether to hide input data from `ValidationError` representation.\n        validation_error_cause: Whether to add user-python excs to the __cause__ of a ValidationError.\n            Requires exceptiongroup backport pre Python 3.11.\n        coerce_numbers_to_str: Whether to enable coercion of any `Number` type to `str` (not applicable in `strict` mode).\n        regex_engine: The regex engine to use for regex pattern validation. Default is 'rust-regex'. See `StringSchema`.\n        cache_strings: Whether to cache strings. Default is `True`, `True` or `'all'` is required to cache strings\n            during general validation since validators don't know if they're in a key or a value.\n        validate_by_alias: Whether to use the field's alias when validating against the provided input data. Default is `True`.\n        validate_by_name: Whether to use the field's name when validating against the provided input data. Default is `False`. Replacement for `populate_by_name`.\n        serialize_by_alias: Whether to serialize by alias. Default is `False`, expected to change to `True` in V3.\n        url_preserve_empty_path: Whether to preserve empty URL paths when validating values for a URL type. Defaults to `False`.\n    \"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 130, "code": "class SerializationInfo(Protocol[ContextT]):\n    @property", "documentation": "    \"\"\"Extra data used during serialization.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 134, "code": "    def include(self) -> IncExCall:\n        ...\n    @property", "documentation": "        \"\"\"The `include` argument set during serialization.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 139, "code": "    def exclude(self) -> IncExCall:\n        ...\n    @property", "documentation": "        \"\"\"The `exclude` argument set during serialization.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 144, "code": "    def context(self) -> ContextT:\n        ...\n    @property", "documentation": "        \"\"\"The current serialization context.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 149, "code": "    def mode(self) -> Literal['python', 'json'] | str:\n        ...\n    @property", "documentation": "        \"\"\"The serialization mode set during serialization.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 154, "code": "    def by_alias(self) -> bool:\n        ...\n    @property", "documentation": "        \"\"\"The `by_alias` argument set during serialization.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 159, "code": "    def exclude_unset(self) -> bool:\n        ...\n    @property", "documentation": "        \"\"\"The `exclude_unset` argument set during serialization.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 164, "code": "    def exclude_defaults(self) -> bool:\n        ...\n    @property", "documentation": "        \"\"\"The `exclude_defaults` argument set during serialization.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 169, "code": "    def exclude_none(self) -> bool:\n        ...\n    @property", "documentation": "        \"\"\"The `exclude_none` argument set during serialization.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 174, "code": "    def exclude_computed_fields(self) -> bool:\n        ...\n    @property", "documentation": "        \"\"\"The `exclude_computed_fields` argument set during serialization.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 179, "code": "    def serialize_as_any(self) -> bool:\n        ...\n    @property", "documentation": "        \"\"\"The `serialize_as_any` argument set during serialization.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 184, "code": "    def round_trip(self) -> bool:\n        ...", "documentation": "        \"\"\"The `round_trip` argument set during serialization.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 195, "code": "class FieldSerializationInfo(SerializationInfo[ContextT], Protocol):\n    @property", "documentation": "    \"\"\"Extra data used during field serialization.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 199, "code": "    def field_name(self) -> str:\n        ...", "documentation": "        \"\"\"The name of the current field being serialized.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 204, "code": "class ValidationInfo(Protocol[ContextT]):\n    @property", "documentation": "    \"\"\"Extra data used during validation.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 208, "code": "    def context(self) -> ContextT:\n        ...\n    @property", "documentation": "        \"\"\"The current validation context.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 213, "code": "    def config(self) -> CoreConfig | None:\n        ...\n    @property", "documentation": "        \"\"\"The CoreConfig that applies to this validation.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 218, "code": "    def mode(self) -> Literal['python', 'json']:\n        ...\n    @property", "documentation": "        \"\"\"The type of input data we are currently validating.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 223, "code": "    def data(self) -> dict[str, Any]:\n        ...\n    @property", "documentation": "        \"\"\"The data being validated for this model.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 228, "code": "    def field_name(self) -> str | None:\n        ...\nExpectedSerializationTypes = Literal[\n    'none',\n    'int',\n    'bool',\n    'float',\n    'str',\n    'bytes',\n    'bytearray',\n    'list',", "documentation": "        \"\"\"\n        The name of the current field being validated if this validator is\n        attached to a model field.\n        \"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 266, "code": "def simple_ser_schema(type: ExpectedSerializationTypes) -> SimpleSerSchema:\n    return SimpleSerSchema(type=type)\nGeneralPlainNoInfoSerializerFunction = Callable[[Any], Any]\nGeneralPlainInfoSerializerFunction = Callable[[Any, SerializationInfo[Any]], Any]\nFieldPlainNoInfoSerializerFunction = Callable[[Any, Any], Any]\nFieldPlainInfoSerializerFunction = Callable[[Any, Any, FieldSerializationInfo[Any]], Any]\nSerializerFunction = Union[\n    GeneralPlainNoInfoSerializerFunction,\n    GeneralPlainInfoSerializerFunction,\n    FieldPlainNoInfoSerializerFunction,\n    FieldPlainInfoSerializerFunction,", "documentation": "    \"\"\"\n    Returns a schema for serialization with a custom type.\n\n    Args:\n        type: The type to use for serialization\n    \"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 414, "code": "def format_ser_schema(formatting_string: str, *, when_used: WhenUsed = 'json-unless-none') -> FormatSerSchema:\n    if when_used == 'json-unless-none':\n        when_used = None  # type: ignore\n    return _dict_not_none(type='format', formatting_string=formatting_string, when_used=when_used)", "documentation": "    \"\"\"\n    Returns a schema for serialization using python's `format` method.\n\n    Args:\n        formatting_string: String defining the format to use\n        when_used: Same meaning as for [general_function_plain_ser_schema], but with a different default\n    \"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 433, "code": "def to_string_ser_schema(*, when_used: WhenUsed = 'json-unless-none') -> ToStringSerSchema:\n    s = dict(type='to-string')\n    if when_used != 'json-unless-none':\n        s['when_used'] = when_used\n    return s  # type: ignore", "documentation": "    \"\"\"\n    Returns a schema for serialization using python's `str()` / `__str__` method.\n\n    Args:\n        when_used: Same meaning as for [general_function_plain_ser_schema], but with a different default\n    \"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 453, "code": "def model_ser_schema(cls: type[Any], schema: CoreSchema) -> ModelSerSchema:\n    return ModelSerSchema(type='model', cls=cls, schema=schema)\nSerSchema = Union[\n    SimpleSerSchema,\n    PlainSerializerFunctionSerSchema,\n    WrapSerializerFunctionSerSchema,\n    FormatSerSchema,\n    ToStringSerSchema,\n    ModelSerSchema,\n]", "documentation": "    \"\"\"\n    Returns a schema for serialization using a model.\n\n    Args:\n        cls: The expected class type, used to generate warnings if the wrong type is passed\n        schema: Internal schema to use to serialize the model dict\n    \"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 483, "code": "def invalid_schema(ref: str | None = None, metadata: dict[str, Any] | None = None) -> InvalidSchema:\n    return _dict_not_none(type='invalid', ref=ref, metadata=metadata)", "documentation": "    \"\"\"\n    Returns an invalid schema, used to indicate that a schema is invalid.\n\n        Returns a schema that matches any value, e.g.:\n\n    Args:\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n    \"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 4036, "code": "def definitions_schema(schema: CoreSchema, definitions: list[CoreSchema]) -> DefinitionsSchema:\n    return DefinitionsSchema(type='definitions', schema=schema, definitions=definitions)", "documentation": "    \"\"\"\n    Build a schema that contains both an inner schema and a list of definitions which can be used\n    within the inner schema.\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.definitions_schema(\n        core_schema.list_schema(core_schema.definition_reference_schema('foobar')),\n        [core_schema.int_schema(ref='foobar')],\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_python([1, 2, '3']) == [1, 2, 3]\n    ```\n\n    Args:\n        schema: The inner schema\n        definitions: List of definitions which can be referenced within inner schema\n    \"\"\""}], "after_segments": [{"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 42, "code": "class CoreConfig(TypedDict, total=False):\n    title: str\n    strict: bool\n    extra_fields_behavior: ExtraBehavior\n    typed_dict_total: bool  # default: True\n    from_attributes: bool\n    loc_by_alias: bool\n    revalidate_instances: Literal['always', 'never', 'subclass-instances']\n    validate_default: bool\n    str_max_length: int\n    str_min_length: int", "documentation": "    \"\"\"\n    Base class for schema configuration options.\n\n    Attributes:\n        title: The name of the configuration.\n        strict: Whether the configuration should strictly adhere to specified rules.\n        extra_fields_behavior: The behavior for handling extra fields.\n        typed_dict_total: Whether the TypedDict should be considered total. Default is `True`.\n        from_attributes: Whether to use attributes for models, dataclasses, and tagged union keys.\n        loc_by_alias: Whether to use the used alias (or first alias for \"field required\" errors) instead of\n            `field_names` to construct error `loc`s. Default is `True`.\n        revalidate_instances: Whether instances of models and dataclasses should re-validate. Default is 'never'.\n        validate_default: Whether to validate default values during validation. Default is `False`.\n        str_max_length: The maximum length for string fields.\n        str_min_length: The minimum length for string fields.\n        str_strip_whitespace: Whether to strip whitespace from string fields.\n        str_to_lower: Whether to convert string fields to lowercase.\n        str_to_upper: Whether to convert string fields to uppercase.\n        allow_inf_nan: Whether to allow infinity and NaN values for float fields. Default is `True`.\n        ser_json_timedelta: The serialization option for `timedelta` values. Default is 'iso8601'.\n            Note that if ser_json_temporal is set, then this param will be ignored.\n        ser_json_temporal: The serialization option for datetime like values. Default is 'iso8601'.\n            The types this covers are datetime, date, time and timedelta.\n            If this is set, it will take precedence over ser_json_timedelta\n        ser_json_bytes: The serialization option for `bytes` values. Default is 'utf8'.\n        ser_json_inf_nan: The serialization option for infinity and NaN values\n            in float fields. Default is 'null'.\n        val_json_bytes: The validation option for `bytes` values, complementing ser_json_bytes. Default is 'utf8'.\n        hide_input_in_errors: Whether to hide input data from `ValidationError` representation.\n        validation_error_cause: Whether to add user-python excs to the __cause__ of a ValidationError.\n            Requires exceptiongroup backport pre Python 3.11.\n        coerce_numbers_to_str: Whether to enable coercion of any `Number` type to `str` (not applicable in `strict` mode).\n        regex_engine: The regex engine to use for regex pattern validation. Default is 'rust-regex'. See `StringSchema`.\n        cache_strings: Whether to cache strings. Default is `True`, `True` or `'all'` is required to cache strings\n            during general validation since validators don't know if they're in a key or a value.\n        validate_by_alias: Whether to use the field's alias when validating against the provided input data. Default is `True`.\n        validate_by_name: Whether to use the field's name when validating against the provided input data. Default is `False`. Replacement for `populate_by_name`.\n        serialize_by_alias: Whether to serialize by alias. Default is `False`, expected to change to `True` in V3.\n        url_preserve_empty_path: Whether to preserve empty URL paths when validating values for a URL type. Defaults to `False`.\n    \"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 130, "code": "class SerializationInfo(Protocol[ContextT]):\n    @property", "documentation": "    \"\"\"Extra data used during serialization.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 134, "code": "    def include(self) -> IncExCall:\n        ...\n    @property", "documentation": "        \"\"\"The `include` argument set during serialization.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 139, "code": "    def exclude(self) -> IncExCall:\n        ...\n    @property", "documentation": "        \"\"\"The `exclude` argument set during serialization.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 144, "code": "    def context(self) -> ContextT:\n        ...\n    @property", "documentation": "        \"\"\"The current serialization context.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 149, "code": "    def mode(self) -> Literal['python', 'json'] | str:\n        ...\n    @property", "documentation": "        \"\"\"The serialization mode set during serialization.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 154, "code": "    def by_alias(self) -> bool:\n        ...\n    @property", "documentation": "        \"\"\"The `by_alias` argument set during serialization.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 159, "code": "    def exclude_unset(self) -> bool:\n        ...\n    @property", "documentation": "        \"\"\"The `exclude_unset` argument set during serialization.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 164, "code": "    def exclude_defaults(self) -> bool:\n        ...\n    @property", "documentation": "        \"\"\"The `exclude_defaults` argument set during serialization.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 169, "code": "    def exclude_none(self) -> bool:\n        ...\n    @property", "documentation": "        \"\"\"The `exclude_none` argument set during serialization.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 174, "code": "    def exclude_computed_fields(self) -> bool:\n        ...\n    @property", "documentation": "        \"\"\"The `exclude_computed_fields` argument set during serialization.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 179, "code": "    def serialize_as_any(self) -> bool:\n        ...\n    @property", "documentation": "        \"\"\"The `serialize_as_any` argument set during serialization.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 184, "code": "    def round_trip(self) -> bool:\n        ...", "documentation": "        \"\"\"The `round_trip` argument set during serialization.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 195, "code": "class FieldSerializationInfo(SerializationInfo[ContextT], Protocol):\n    @property", "documentation": "    \"\"\"Extra data used during field serialization.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 199, "code": "    def field_name(self) -> str:\n        ...", "documentation": "        \"\"\"The name of the current field being serialized.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 204, "code": "class ValidationInfo(Protocol[ContextT]):\n    @property", "documentation": "    \"\"\"Extra data used during validation.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 208, "code": "    def context(self) -> ContextT:\n        ...\n    @property", "documentation": "        \"\"\"The current validation context.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 213, "code": "    def config(self) -> CoreConfig | None:\n        ...\n    @property", "documentation": "        \"\"\"The CoreConfig that applies to this validation.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 218, "code": "    def mode(self) -> Literal['python', 'json']:\n        ...\n    @property", "documentation": "        \"\"\"The type of input data we are currently validating.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 223, "code": "    def data(self) -> dict[str, Any]:\n        ...\n    @property", "documentation": "        \"\"\"The data being validated for this model.\"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 228, "code": "    def field_name(self) -> str | None:\n        ...\nExpectedSerializationTypes = Literal[\n    'none',\n    'int',\n    'bool',\n    'float',\n    'str',\n    'bytes',\n    'bytearray',\n    'list',", "documentation": "        \"\"\"\n        The name of the current field being validated if this validator is\n        attached to a model field.\n        \"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 266, "code": "def simple_ser_schema(type: ExpectedSerializationTypes) -> SimpleSerSchema:\n    return SimpleSerSchema(type=type)\nGeneralPlainNoInfoSerializerFunction = Callable[[Any], Any]\nGeneralPlainInfoSerializerFunction = Callable[[Any, SerializationInfo[Any]], Any]\nFieldPlainNoInfoSerializerFunction = Callable[[Any, Any], Any]\nFieldPlainInfoSerializerFunction = Callable[[Any, Any, FieldSerializationInfo[Any]], Any]\nSerializerFunction = Union[\n    GeneralPlainNoInfoSerializerFunction,\n    GeneralPlainInfoSerializerFunction,\n    FieldPlainNoInfoSerializerFunction,\n    FieldPlainInfoSerializerFunction,", "documentation": "    \"\"\"\n    Returns a schema for serialization with a custom type.\n\n    Args:\n        type: The type to use for serialization\n    \"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 414, "code": "def format_ser_schema(formatting_string: str, *, when_used: WhenUsed = 'json-unless-none') -> FormatSerSchema:\n    if when_used == 'json-unless-none':\n        when_used = None  # type: ignore\n    return _dict_not_none(type='format', formatting_string=formatting_string, when_used=when_used)", "documentation": "    \"\"\"\n    Returns a schema for serialization using python's `format` method.\n\n    Args:\n        formatting_string: String defining the format to use\n        when_used: Same meaning as for [general_function_plain_ser_schema], but with a different default\n    \"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 433, "code": "def to_string_ser_schema(*, when_used: WhenUsed = 'json-unless-none') -> ToStringSerSchema:\n    s = dict(type='to-string')\n    if when_used != 'json-unless-none':\n        s['when_used'] = when_used\n    return s  # type: ignore", "documentation": "    \"\"\"\n    Returns a schema for serialization using python's `str()` / `__str__` method.\n\n    Args:\n        when_used: Same meaning as for [general_function_plain_ser_schema], but with a different default\n    \"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 453, "code": "def model_ser_schema(cls: type[Any], schema: CoreSchema) -> ModelSerSchema:\n    return ModelSerSchema(type='model', cls=cls, schema=schema)\nSerSchema = Union[\n    SimpleSerSchema,\n    PlainSerializerFunctionSerSchema,\n    WrapSerializerFunctionSerSchema,\n    FormatSerSchema,\n    ToStringSerSchema,\n    ModelSerSchema,\n]", "documentation": "    \"\"\"\n    Returns a schema for serialization using a model.\n\n    Args:\n        cls: The expected class type, used to generate warnings if the wrong type is passed\n        schema: Internal schema to use to serialize the model dict\n    \"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 483, "code": "def invalid_schema(ref: str | None = None, metadata: dict[str, Any] | None = None) -> InvalidSchema:\n    return _dict_not_none(type='invalid', ref=ref, metadata=metadata)", "documentation": "    \"\"\"\n    Returns an invalid schema, used to indicate that a schema is invalid.\n\n    Args:\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n    \"\"\""}, {"filename": "pydantic-core/python/pydantic_core/core_schema.py", "start_line": 4034, "code": "def definitions_schema(schema: CoreSchema, definitions: list[CoreSchema]) -> DefinitionsSchema:\n    return DefinitionsSchema(type='definitions', schema=schema, definitions=definitions)", "documentation": "    \"\"\"\n    Build a schema that contains both an inner schema and a list of definitions which can be used\n    within the inner schema.\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.definitions_schema(\n        core_schema.list_schema(core_schema.definition_reference_schema('foobar')),\n        [core_schema.int_schema(ref='foobar')],\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_python([1, 2, '3']) == [1, 2, 3]\n    ```\n\n    Args:\n        schema: The inner schema\n        definitions: List of definitions which can be referenced within inner schema\n    \"\"\""}]}
{"repository": "pydantic/pydantic", "commit_sha": "495b03f92dc9aedd18af454b69f2605ac402a580", "commit_message": "Add `preverse_empty_path` URL options (#12336)\n\nUpgrade `pydantic-core` to v2.41.1.\nAlso sync documentation about added error code after `pydantic-core` upgrade.", "commit_date": "2025-10-07T14:05:44+00:00", "author": "Victorien", "file": "pydantic/_internal/_config.py", "patch": "@@ -87,6 +87,7 @@ class ConfigWrapper:\n     validate_by_alias: bool\n     validate_by_name: bool\n     serialize_by_alias: bool\n+    url_preserve_empty_path: bool\n \n     def __init__(self, config: ConfigDict | dict[str, Any] | type[Any] | None, *, check: bool = True):\n         if check:\n@@ -227,6 +228,7 @@ def core_config(self, title: str | None) -> core_schema.CoreConfig:\n                     ('validate_by_alias', config.get('validate_by_alias')),\n                     ('validate_by_name', config.get('validate_by_name')),\n                     ('serialize_by_alias', config.get('serialize_by_alias')),\n+                    ('url_preserve_empty_path', config.get('url_preserve_empty_path')),\n                 )\n                 if v is not None\n             }\n@@ -311,6 +313,7 @@ def push(self, config_wrapper: ConfigWrapper | ConfigDict | None):\n     validate_by_alias=True,\n     validate_by_name=False,\n     serialize_by_alias=False,\n+    url_preserve_empty_path=False,\n )\n \n ", "before_segments": [{"filename": "pydantic/_internal/_config.py", "start_line": 28, "code": "class ConfigWrapper:\n    __slots__ = ('config_dict',)\n    config_dict: ConfigDict\n    title: str | None\n    str_to_lower: bool\n    str_to_upper: bool\n    str_strip_whitespace: bool\n    str_min_length: int\n    str_max_length: int | None\n    extra: ExtraValues | None\n    frozen: bool", "documentation": "    \"\"\"Internal wrapper for Config which exposes ConfigDict items as attributes.\"\"\""}, {"filename": "pydantic/_internal/_config.py", "start_line": 160, "code": "    def core_config(self, title: str | None) -> core_schema.CoreConfig:\n        config = self.config_dict\n        if config.get('schema_generator') is not None:\n            warnings.warn(\n                'The `schema_generator` setting has been deprecated since v2.10. This setting no longer has any effect.',\n                PydanticDeprecatedSince210,\n                stacklevel=2,\n            )\n        if (populate_by_name := config.get('populate_by_name')) is not None:\n            if config.get('validate_by_name') is None:\n                config['validate_by_alias'] = True", "documentation": "        \"\"\"Create a pydantic-core config.\n\n        We don't use getattr here since we don't want to populate with defaults.\n\n        Args:\n            title: The title to use if not set in config.\n\n        Returns:\n            A `CoreConfig` object created from config.\n        \"\"\""}, {"filename": "pydantic/_internal/_config.py", "start_line": 239, "code": "class ConfigWrapperStack:", "documentation": "    \"\"\"A stack of `ConfigWrapper` instances.\"\"\""}, {"filename": "pydantic/_internal/_config.py", "start_line": 316, "code": "def prepare_config(config: ConfigDict | dict[str, Any] | type[Any] | None) -> ConfigDict:\n    if config is None:\n        return ConfigDict()\n    if not isinstance(config, dict):\n        warnings.warn(DEPRECATION_MESSAGE, PydanticDeprecatedSince20, stacklevel=4)\n        config = {k: getattr(config, k) for k in dir(config) if not k.startswith('__')}\n    config_dict = cast(ConfigDict, config)\n    check_deprecated(config_dict)\n    return config_dict\nconfig_keys = set(ConfigDict.__annotations__.keys())\nV2_REMOVED_KEYS = {", "documentation": "    \"\"\"Create a `ConfigDict` instance from an existing dict, a class (e.g. old class-based config) or None.\n\n    Args:\n        config: The input config.\n\n    Returns:\n        A ConfigDict object created from config.\n    \"\"\""}, {"filename": "pydantic/_internal/_config.py", "start_line": 366, "code": "def check_deprecated(config_dict: ConfigDict) -> None:\n    deprecated_removed_keys = V2_REMOVED_KEYS & config_dict.keys()\n    deprecated_renamed_keys = V2_RENAMED_KEYS.keys() & config_dict.keys()\n    if deprecated_removed_keys or deprecated_renamed_keys:\n        renamings = {k: V2_RENAMED_KEYS[k] for k in sorted(deprecated_renamed_keys)}\n        renamed_bullets = [f'* {k!r} has been renamed to {v!r}' for k, v in renamings.items()]\n        removed_bullets = [f'* {k!r} has been removed' for k in sorted(deprecated_removed_keys)]\n        message = '\\n'.join(['Valid config keys have changed in V2:'] + renamed_bullets + removed_bullets)\n        warnings.warn(message, UserWarning)", "documentation": "    \"\"\"Check for deprecated config keys and warn the user.\n\n    Args:\n        config_dict: The input config.\n    \"\"\""}], "after_segments": [{"filename": "pydantic/_internal/_config.py", "start_line": 28, "code": "class ConfigWrapper:\n    __slots__ = ('config_dict',)\n    config_dict: ConfigDict\n    title: str | None\n    str_to_lower: bool\n    str_to_upper: bool\n    str_strip_whitespace: bool\n    str_min_length: int\n    str_max_length: int | None\n    extra: ExtraValues | None\n    frozen: bool", "documentation": "    \"\"\"Internal wrapper for Config which exposes ConfigDict items as attributes.\"\"\""}, {"filename": "pydantic/_internal/_config.py", "start_line": 161, "code": "    def core_config(self, title: str | None) -> core_schema.CoreConfig:\n        config = self.config_dict\n        if config.get('schema_generator') is not None:\n            warnings.warn(\n                'The `schema_generator` setting has been deprecated since v2.10. This setting no longer has any effect.',\n                PydanticDeprecatedSince210,\n                stacklevel=2,\n            )\n        if (populate_by_name := config.get('populate_by_name')) is not None:\n            if config.get('validate_by_name') is None:\n                config['validate_by_alias'] = True", "documentation": "        \"\"\"Create a pydantic-core config.\n\n        We don't use getattr here since we don't want to populate with defaults.\n\n        Args:\n            title: The title to use if not set in config.\n\n        Returns:\n            A `CoreConfig` object created from config.\n        \"\"\""}, {"filename": "pydantic/_internal/_config.py", "start_line": 241, "code": "class ConfigWrapperStack:", "documentation": "    \"\"\"A stack of `ConfigWrapper` instances.\"\"\""}, {"filename": "pydantic/_internal/_config.py", "start_line": 319, "code": "def prepare_config(config: ConfigDict | dict[str, Any] | type[Any] | None) -> ConfigDict:\n    if config is None:\n        return ConfigDict()\n    if not isinstance(config, dict):\n        warnings.warn(DEPRECATION_MESSAGE, PydanticDeprecatedSince20, stacklevel=4)\n        config = {k: getattr(config, k) for k in dir(config) if not k.startswith('__')}\n    config_dict = cast(ConfigDict, config)\n    check_deprecated(config_dict)\n    return config_dict\nconfig_keys = set(ConfigDict.__annotations__.keys())\nV2_REMOVED_KEYS = {", "documentation": "    \"\"\"Create a `ConfigDict` instance from an existing dict, a class (e.g. old class-based config) or None.\n\n    Args:\n        config: The input config.\n\n    Returns:\n        A ConfigDict object created from config.\n    \"\"\""}, {"filename": "pydantic/_internal/_config.py", "start_line": 369, "code": "def check_deprecated(config_dict: ConfigDict) -> None:\n    deprecated_removed_keys = V2_REMOVED_KEYS & config_dict.keys()\n    deprecated_renamed_keys = V2_RENAMED_KEYS.keys() & config_dict.keys()\n    if deprecated_removed_keys or deprecated_renamed_keys:\n        renamings = {k: V2_RENAMED_KEYS[k] for k in sorted(deprecated_renamed_keys)}\n        renamed_bullets = [f'* {k!r} has been renamed to {v!r}' for k, v in renamings.items()]\n        removed_bullets = [f'* {k!r} has been removed' for k in sorted(deprecated_removed_keys)]\n        message = '\\n'.join(['Valid config keys have changed in V2:'] + renamed_bullets + removed_bullets)\n        warnings.warn(message, UserWarning)", "documentation": "    \"\"\"Check for deprecated config keys and warn the user.\n\n    Args:\n        config_dict: The input config.\n    \"\"\""}]}
{"repository": "pydantic/pydantic", "commit_sha": "495b03f92dc9aedd18af454b69f2605ac402a580", "commit_message": "Add `preverse_empty_path` URL options (#12336)\n\nUpgrade `pydantic-core` to v2.41.1.\nAlso sync documentation about added error code after `pydantic-core` upgrade.", "commit_date": "2025-10-07T14:05:44+00:00", "author": "Victorien", "file": "pydantic/config.py", "patch": "@@ -1201,6 +1201,24 @@ class Model(BaseModel):\n     2. The model is serialized using the alias `'my_alias'` for the `'my_field'` attribute.\n     \"\"\"\n \n+    url_preserve_empty_path: bool\n+    \"\"\"\n+    Whether to preserve empty URL paths when validating values for a URL type. Defaults to `False`.\n+\n+    ```python\n+    from pydantic import AnyUrl, BaseModel, ConfigDict\n+\n+    class Model(BaseModel):\n+        model_config = ConfigDict(url_preserve_empty_path=True)\n+\n+        url: AnyUrl\n+\n+    m = Model(url='http://example.com')\n+    print(m.url)\n+    #> http://example.com\n+    ```\n+    \"\"\"\n+\n \n _TypeT = TypeVar('_TypeT', bound=type)\n ", "before_segments": [{"filename": "pydantic/config.py", "start_line": 35, "code": "class ConfigDict(TypedDict, total=False):\n    title: str | None\n    \"\"\"The title for the generated JSON schema, defaults to the model's name\"\"\"\n    model_title_generator: Callable[[type], str] | None\n    \"\"\"A callable that takes a model class and returns the title for it. Defaults to `None`.\"\"\"\n    field_title_generator: Callable[[str, FieldInfo | ComputedFieldInfo], str] | None\n    \"\"\"A callable that takes a field's name and info and returns title for it. Defaults to `None`.\"\"\"\n    str_to_lower: bool\n    \"\"\"Whether to convert all characters to lowercase for str types. Defaults to `False`.\"\"\"\n    str_to_upper: bool\n    \"\"\"Whether to convert all characters to uppercase for str types. Defaults to `False`.\"\"\"", "documentation": "    \"\"\"A TypedDict for configuring Pydantic behaviour.\"\"\""}, {"filename": "pydantic/config.py", "start_line": 1220, "code": "def with_config(config: ConfigDict | None = None, /, **kwargs: Any) -> Callable[[_TypeT], _TypeT]:\n    if config is not None and kwargs:\n        raise ValueError('Cannot specify both `config` and keyword arguments')\n    if len(kwargs) == 1 and (kwargs_conf := kwargs.get('config')) is not None:\n        warnings.warn(\n            'Passing `config` as a keyword argument is deprecated. Pass `config` as a positional argument instead',\n            category=PydanticDeprecatedSince211,\n            stacklevel=2,\n        )\n        final_config = cast(ConfigDict, kwargs_conf)\n    else:", "documentation": "    \"\"\"!!! abstract \"Usage Documentation\"\n        [Configuration with other types](../concepts/config.md#configuration-on-other-supported-types)\n\n    A convenience decorator to set a [Pydantic configuration](config.md) on a `TypedDict` or a `dataclass` from the standard library.\n\n    Although the configuration can be set using the `__pydantic_config__` attribute, it does not play well with type checkers,\n    especially with `TypedDict`.\n\n    !!! example \"Usage\"\n\n        ```python\n        from typing_extensions import TypedDict\n\n        from pydantic import ConfigDict, TypeAdapter, with_config\n\n        @with_config(ConfigDict(str_to_lower=True))\n        class TD(TypedDict):\n            x: str\n\n        ta = TypeAdapter(TD)\n\n        print(ta.validate_python({'x': 'ABC'}))\n        #> {'x': 'abc'}\n        ```\n    \"\"\""}], "after_segments": [{"filename": "pydantic/config.py", "start_line": 35, "code": "class ConfigDict(TypedDict, total=False):\n    title: str | None\n    \"\"\"The title for the generated JSON schema, defaults to the model's name\"\"\"\n    model_title_generator: Callable[[type], str] | None\n    \"\"\"A callable that takes a model class and returns the title for it. Defaults to `None`.\"\"\"\n    field_title_generator: Callable[[str, FieldInfo | ComputedFieldInfo], str] | None\n    \"\"\"A callable that takes a field's name and info and returns title for it. Defaults to `None`.\"\"\"\n    str_to_lower: bool\n    \"\"\"Whether to convert all characters to lowercase for str types. Defaults to `False`.\"\"\"\n    str_to_upper: bool\n    \"\"\"Whether to convert all characters to uppercase for str types. Defaults to `False`.\"\"\"", "documentation": "    \"\"\"A TypedDict for configuring Pydantic behaviour.\"\"\""}, {"filename": "pydantic/config.py", "start_line": 1238, "code": "def with_config(config: ConfigDict | None = None, /, **kwargs: Any) -> Callable[[_TypeT], _TypeT]:\n    if config is not None and kwargs:\n        raise ValueError('Cannot specify both `config` and keyword arguments')\n    if len(kwargs) == 1 and (kwargs_conf := kwargs.get('config')) is not None:\n        warnings.warn(\n            'Passing `config` as a keyword argument is deprecated. Pass `config` as a positional argument instead',\n            category=PydanticDeprecatedSince211,\n            stacklevel=2,\n        )\n        final_config = cast(ConfigDict, kwargs_conf)\n    else:", "documentation": "    \"\"\"!!! abstract \"Usage Documentation\"\n        [Configuration with other types](../concepts/config.md#configuration-on-other-supported-types)\n\n    A convenience decorator to set a [Pydantic configuration](config.md) on a `TypedDict` or a `dataclass` from the standard library.\n\n    Although the configuration can be set using the `__pydantic_config__` attribute, it does not play well with type checkers,\n    especially with `TypedDict`.\n\n    !!! example \"Usage\"\n\n        ```python\n        from typing_extensions import TypedDict\n\n        from pydantic import ConfigDict, TypeAdapter, with_config\n\n        @with_config(ConfigDict(str_to_lower=True))\n        class TD(TypedDict):\n            x: str\n\n        ta = TypeAdapter(TD)\n\n        print(ta.validate_python({'x': 'ABC'}))\n        #> {'x': 'abc'}\n        ```\n    \"\"\""}]}
{"repository": "pydantic/pydantic", "commit_sha": "495b03f92dc9aedd18af454b69f2605ac402a580", "commit_message": "Add `preverse_empty_path` URL options (#12336)\n\nUpgrade `pydantic-core` to v2.41.1.\nAlso sync documentation about added error code after `pydantic-core` upgrade.", "commit_date": "2025-10-07T14:05:44+00:00", "author": "Victorien", "file": "pydantic/networks.py", "patch": "@@ -78,6 +78,7 @@ class UrlConstraints:\n         default_host: The default host. Defaults to `None`.\n         default_port: The default port. Defaults to `None`.\n         default_path: The default path. Defaults to `None`.\n+        preserve_empty_path: Whether to preserve empty URL paths. Defaults to `None`.\n     \"\"\"\n \n     max_length: int | None = None\n@@ -86,6 +87,7 @@ class UrlConstraints:\n     default_host: str | None = None\n     default_port: int | None = None\n     default_path: str | None = None\n+    preserve_empty_path: bool | None = None\n \n     def __hash__(self) -> int:\n         return hash(\n@@ -96,6 +98,7 @@ def __hash__(self) -> int:\n                 self.default_host,\n                 self.default_port,\n                 self.default_path,\n+                self.preserve_empty_path,\n             )\n         )\n ", "before_segments": [{"filename": "pydantic/networks.py", "start_line": 70, "code": "class UrlConstraints:\n    max_length: int | None = None\n    allowed_schemes: list[str] | None = None\n    host_required: bool | None = None\n    default_host: str | None = None\n    default_port: int | None = None\n    default_path: str | None = None", "documentation": "    \"\"\"Url constraints.\n\n    Attributes:\n        max_length: The maximum length of the url. Defaults to `None`.\n        allowed_schemes: The allowed schemes. Defaults to `None`.\n        host_required: Whether the host is required. Defaults to `None`.\n        default_host: The default host. Defaults to `None`.\n        default_port: The default port. Defaults to `None`.\n        default_path: The default path. Defaults to `None`.\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 102, "code": "    def defined_constraints(self) -> dict[str, Any]:\n        return {field.name: value for field in fields(self) if (value := getattr(self, field.name)) is not None}", "documentation": "        \"\"\"Fetch a key / value mapping of constraints to values that are not None. Used for core schema updates.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 130, "code": "    def scheme(self) -> str:\n        return self._url.scheme\n    @property", "documentation": "        \"\"\"The scheme part of the URL.\n\n        e.g. `https` in `https://user:pass@host:port/path?query#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 138, "code": "    def username(self) -> str | None:\n        return self._url.username\n    @property", "documentation": "        \"\"\"The username part of the URL, or `None`.\n\n        e.g. `user` in `https://user:pass@host:port/path?query#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 146, "code": "    def password(self) -> str | None:\n        return self._url.password\n    @property", "documentation": "        \"\"\"The password part of the URL, or `None`.\n\n        e.g. `pass` in `https://user:pass@host:port/path?query#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 154, "code": "    def host(self) -> str | None:\n        return self._url.host", "documentation": "        \"\"\"The host part of the URL, or `None`.\n\n        If the URL must be punycode encoded, this is the encoded host, e.g if the input URL is `https://\u00a3\u00a3\u00a3.com`,\n        `host` will be `xn--9aaa.com`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 162, "code": "    def unicode_host(self) -> str | None:\n        return self._url.unicode_host()\n    @property", "documentation": "        \"\"\"The host part of the URL as a unicode string, or `None`.\n\n        e.g. `host` in `https://user:pass@host:port/path?query#fragment`\n\n        If the URL must be punycode encoded, this is the decoded host, e.g if the input URL is `https://\u00a3\u00a3\u00a3.com`,\n        `unicode_host()` will be `\u00a3\u00a3\u00a3.com`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 173, "code": "    def port(self) -> int | None:\n        return self._url.port\n    @property", "documentation": "        \"\"\"The port part of the URL, or `None`.\n\n        e.g. `port` in `https://user:pass@host:port/path?query#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 181, "code": "    def path(self) -> str | None:\n        return self._url.path\n    @property", "documentation": "        \"\"\"The path part of the URL, or `None`.\n\n        e.g. `/path` in `https://user:pass@host:port/path?query#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 189, "code": "    def query(self) -> str | None:\n        return self._url.query", "documentation": "        \"\"\"The query part of the URL, or `None`.\n\n        e.g. `query` in `https://user:pass@host:port/path?query#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 196, "code": "    def query_params(self) -> list[tuple[str, str]]:\n        return self._url.query_params()\n    @property", "documentation": "        \"\"\"The query part of the URL as a list of key-value pairs.\n\n        e.g. `[('foo', 'bar')]` in `https://user:pass@host:port/path?foo=bar#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 204, "code": "    def fragment(self) -> str | None:\n        return self._url.fragment", "documentation": "        \"\"\"The fragment part of the URL, or `None`.\n\n        e.g. `fragment` in `https://user:pass@host:port/path?query#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 211, "code": "    def unicode_string(self) -> str:\n        return self._url.unicode_string()", "documentation": "        \"\"\"The URL as a unicode string, unlike `__str__()` this will not punycode encode the host.\n\n        If the URL must be punycode encoded, this is the decoded string, e.g if the input URL is `https://\u00a3\u00a3\u00a3.com`,\n        `unicode_string()` will be `https://\u00a3\u00a3\u00a3.com`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 219, "code": "    def encoded_string(self) -> str:\n        return str(self)", "documentation": "        \"\"\"The URL's encoded string representation via __str__().\n\n        This returns the punycode-encoded host version of the URL as a string.\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 226, "code": "    def __str__(self) -> str:\n        return str(self._url)", "documentation": "        \"\"\"The URL as a string, this will punycode encode the host if required.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 350, "code": "    def scheme(self) -> str:\n        return self._url.scheme\n    @property", "documentation": "        \"\"\"The scheme part of the URL.\n\n        e.g. `https` in `https://foo.com,bar.com/path?query#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 358, "code": "    def path(self) -> str | None:\n        return self._url.path\n    @property", "documentation": "        \"\"\"The path part of the URL, or `None`.\n\n        e.g. `/path` in `https://foo.com,bar.com/path?query#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 366, "code": "    def query(self) -> str | None:\n        return self._url.query", "documentation": "        \"\"\"The query part of the URL, or `None`.\n\n        e.g. `query` in `https://foo.com,bar.com/path?query#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 373, "code": "    def query_params(self) -> list[tuple[str, str]]:\n        return self._url.query_params()\n    @property", "documentation": "        \"\"\"The query part of the URL as a list of key-value pairs.\n\n        e.g. `[('foo', 'bar')]` in `https://foo.com,bar.com/path?foo=bar#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 381, "code": "    def fragment(self) -> str | None:\n        return self._url.fragment", "documentation": "        \"\"\"The fragment part of the URL, or `None`.\n\n        e.g. `fragment` in `https://foo.com,bar.com/path?query#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 388, "code": "    def hosts(self) -> list[MultiHostHost]:\n        return self._url.hosts()", "documentation": "        '''The hosts of the `MultiHostUrl` as [`MultiHostHost`][pydantic_core.MultiHostHost] typed dicts.\n\n        ```python\n        from pydantic_core import MultiHostUrl\n\n        mhu = MultiHostUrl('https://foo.com:123,foo:bar@bar.com/path')\n        print(mhu.hosts())\n        \"\"\"\n        [\n            {'username': None, 'password': None, 'host': 'foo.com', 'port': 123},\n            {'username': 'foo', 'password': 'bar', 'host': 'bar.com', 'port': 443}\n        ]\n        ```\n        Returns:\n            A list of dicts, each representing a host.\n        '''"}, {"filename": "pydantic/networks.py", "start_line": 407, "code": "    def encoded_string(self) -> str:\n        return str(self)", "documentation": "        \"\"\"The URL's encoded string representation via __str__().\n\n        This returns the punycode-encoded host version of the URL as a string.\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 414, "code": "    def unicode_string(self) -> str:\n        return self._url.unicode_string()", "documentation": "        \"\"\"The URL as a unicode string, unlike `__str__()` this will not punycode encode the hosts.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 418, "code": "    def __str__(self) -> str:\n        return str(self._url)", "documentation": "        \"\"\"The URL as a string, this will punycode encode the host if required.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 533, "code": "class AnyUrl(_BaseUrl):", "documentation": "    \"\"\"Base type for all URLs.\n\n    * Any scheme allowed\n    * Top-level domain (TLD) not required\n    * Host not required\n\n    Assuming an input URL of `http://samuel:pass@example.com:8000/the/path/?query=here#fragment=is;this=bit`,\n    the types export the following properties:\n\n    - `scheme`: the URL scheme (`http`), always set.\n    - `host`: the URL host (`example.com`).\n    - `username`: optional username if included (`samuel`).\n    - `password`: optional password if included (`pass`).\n    - `port`: optional port (`8000`).\n    - `path`: optional path (`/the/path/`).\n    - `query`: optional URL query (for example, `GET` arguments or \"search string\", such as `query=here`).\n    - `fragment`: optional fragment (`fragment=is;this=bit`).\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 558, "code": "class AnyHttpUrl(AnyUrl):\n    _constraints = UrlConstraints(allowed_schemes=['http', 'https'])", "documentation": "    \"\"\"A type that will accept any http or https URL.\n\n    * TLD not required\n    * Host not required\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 568, "code": "class HttpUrl(AnyUrl):\n    _constraints = UrlConstraints(max_length=2083, allowed_schemes=['http', 'https'])", "documentation": "    \"\"\"A type that will accept any http or https URL.\n\n    * TLD not required\n    * Host not required\n    * Max length 2083\n\n    ```python\n    from pydantic import BaseModel, HttpUrl, ValidationError\n\n    class MyModel(BaseModel):\n        url: HttpUrl\n\n    m = MyModel(url='http://www.example.com')  # (1)!\n    print(m.url)\n    #> http://www.example.com/\n\n    try:\n        MyModel(url='ftp://invalid.url')\n    except ValidationError as e:\n        print(e)\n        '''\n        1 validation error for MyModel\n        url\n          URL scheme should be 'http' or 'https' [type=url_scheme, input_value='ftp://invalid.url', input_type=str]\n        '''\n\n    try:\n        MyModel(url='not a url')\n    except ValidationError as e:\n        print(e)\n        '''\n        1 validation error for MyModel\n        url\n          Input should be a valid URL, relative URL without a base [type=url_parsing, input_value='not a url', input_type=str]\n        '''\n    ```\n\n    1. Note: mypy would prefer `m = MyModel(url=HttpUrl('http://www.example.com'))`, but Pydantic will convert the string to an HttpUrl instance anyway.\n\n    \"International domains\" (e.g. a URL where the host or TLD includes non-ascii characters) will be encoded via\n    [punycode](https://en.wikipedia.org/wiki/Punycode) (see\n    [this article](https://www.xudongz.com/blog/2017/idn-phishing/) for a good description of why this is important):\n\n    ```python\n    from pydantic import BaseModel, HttpUrl\n\n    class MyModel(BaseModel):\n        url: HttpUrl\n\n    m1 = MyModel(url='http://puny\u00a3code.com')\n    print(m1.url)\n    #> http://xn--punycode-eja.com/\n    m2 = MyModel(url='https://www.\u0430\u0440\u0440\u04cf\u0435.com/')\n    print(m2.url)\n    #> https://www.xn--80ak6aa92e.com/\n    m3 = MyModel(url='https://www.example.\u73e0\u5b9d/')\n    print(m3.url)\n    #> https://www.example.xn--pbt977c/\n    ```\n\n\n    !!! warning \"Underscores in Hostnames\"\n        In Pydantic, underscores are allowed in all parts of a domain except the TLD.\n        Technically this might be wrong - in theory the hostname cannot have underscores, but subdomains can.\n\n        To explain this; consider the following two cases:\n\n        - `exam_ple.co.uk`: the hostname is `exam_ple`, which should not be allowed since it contains an underscore.\n        - `foo_bar.example.com` the hostname is `example`, which should be allowed since the underscore is in the subdomain.\n\n        Without having an exhaustive list of TLDs, it would be impossible to differentiate between these two. Therefore\n        underscores are allowed, but you can always do further validation in a validator if desired.\n\n        Also, Chrome, Firefox, and Safari all currently accept `http://exam_ple.com` as a URL, so we're in good\n        (or at least big) company.\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 649, "code": "class AnyWebsocketUrl(AnyUrl):\n    _constraints = UrlConstraints(allowed_schemes=['ws', 'wss'])", "documentation": "    \"\"\"A type that will accept any ws or wss URL.\n\n    * TLD not required\n    * Host not required\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 659, "code": "class WebsocketUrl(AnyUrl):\n    _constraints = UrlConstraints(max_length=2083, allowed_schemes=['ws', 'wss'])", "documentation": "    \"\"\"A type that will accept any ws or wss URL.\n\n    * TLD not required\n    * Host not required\n    * Max length 2083\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 670, "code": "class FileUrl(AnyUrl):\n    _constraints = UrlConstraints(allowed_schemes=['file'])", "documentation": "    \"\"\"A type that will accept any file URL.\n\n    * Host not required\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 679, "code": "class FtpUrl(AnyUrl):\n    _constraints = UrlConstraints(allowed_schemes=['ftp'])", "documentation": "    \"\"\"A type that will accept ftp URL.\n\n    * TLD not required\n    * Host not required\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 689, "code": "class PostgresDsn(_BaseMultiHostUrl):\n    _constraints = UrlConstraints(\n        host_required=True,\n        allowed_schemes=[\n            'postgres',\n            'postgresql',\n            'postgresql+asyncpg',\n            'postgresql+pg8000',\n            'postgresql+psycopg',\n            'postgresql+psycopg2',\n            'postgresql+psycopg2cffi',", "documentation": "    \"\"\"A type that will accept any Postgres DSN.\n\n    * User info required\n    * TLD not required\n    * Host required\n    * Supports multiple hosts\n\n    If further validation is required, these properties can be used by validators to enforce specific behaviour:\n\n    ```python\n    from pydantic import (\n        BaseModel,\n        HttpUrl,\n        PostgresDsn,\n        ValidationError,\n        field_validator,\n    )\n\n    class MyModel(BaseModel):\n        url: HttpUrl\n\n    m = MyModel(url='http://www.example.com')\n\n    # the repr() method for a url will display all properties of the url\n    print(repr(m.url))\n    #> HttpUrl('http://www.example.com/')\n    print(m.url.scheme)\n    #> http\n    print(m.url.host)\n    #> www.example.com\n    print(m.url.port)\n    #> 80\n\n    class MyDatabaseModel(BaseModel):\n        db: PostgresDsn\n\n        @field_validator('db')\n        def check_db_name(cls, v):\n            assert v.path and len(v.path) > 1, 'database must be provided'\n            return v\n\n    m = MyDatabaseModel(db='postgres://user:pass@localhost:5432/foobar')\n    print(m.db)\n    #> postgres://user:pass@localhost:5432/foobar\n\n    try:\n        MyDatabaseModel(db='postgres://user:pass@localhost:5432')\n    except ValidationError as e:\n        print(e)\n        '''\n        1 validation error for MyDatabaseModel\n        db\n          Assertion failed, database must be provided\n        assert (None)\n         +  where None = PostgresDsn('postgres://user:pass@localhost:5432').path [type=assertion_error, input_value='postgres://user:pass@localhost:5432', input_type=str]\n        '''\n    ```\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 765, "code": "    def host(self) -> str:\n        return self._url.host  # pyright: ignore[reportAttributeAccessIssue]", "documentation": "        \"\"\"The required URL host.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 770, "code": "class CockroachDsn(AnyUrl):\n    _constraints = UrlConstraints(\n        host_required=True,\n        allowed_schemes=[\n            'cockroachdb',\n            'cockroachdb+psycopg2',\n            'cockroachdb+asyncpg',\n        ],\n    )\n    @property", "documentation": "    \"\"\"A type that will accept any Cockroach DSN.\n\n    * User info required\n    * TLD not required\n    * Host required\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 788, "code": "    def host(self) -> str:\n        return self._url.host  # pyright: ignore[reportReturnType]", "documentation": "        \"\"\"The required URL host.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 793, "code": "class AmqpDsn(AnyUrl):\n    _constraints = UrlConstraints(allowed_schemes=['amqp', 'amqps'])", "documentation": "    \"\"\"A type that will accept any AMQP DSN.\n\n    * User info required\n    * TLD not required\n    * Host not required\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 804, "code": "class RedisDsn(AnyUrl):\n    _constraints = UrlConstraints(\n        allowed_schemes=['redis', 'rediss'],\n        default_host='localhost',\n        default_port=6379,\n        default_path='/0',\n        host_required=True,\n    )\n    @property", "documentation": "    \"\"\"A type that will accept any Redis DSN.\n\n    * User info required\n    * TLD not required\n    * Host required (e.g., `rediss://:pass@localhost`)\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 821, "code": "    def host(self) -> str:\n        return self._url.host  # pyright: ignore[reportReturnType]", "documentation": "        \"\"\"The required URL host.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 826, "code": "class MongoDsn(_BaseMultiHostUrl):\n    _constraints = UrlConstraints(allowed_schemes=['mongodb', 'mongodb+srv'], default_port=27017)", "documentation": "    \"\"\"A type that will accept any MongoDB DSN.\n\n    * User info not required\n    * Database name not required\n    * Port not required\n    * User info may be passed without user part (e.g., `mongodb://mongodb0.example.com:27017`).\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 838, "code": "class KafkaDsn(AnyUrl):\n    _constraints = UrlConstraints(allowed_schemes=['kafka'], default_host='localhost', default_port=9092)", "documentation": "    \"\"\"A type that will accept any Kafka DSN.\n\n    * User info required\n    * TLD not required\n    * Host not required\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 849, "code": "class NatsDsn(_BaseMultiHostUrl):\n    _constraints = UrlConstraints(\n        allowed_schemes=['nats', 'tls', 'ws', 'wss'], default_host='localhost', default_port=4222\n    )", "documentation": "    \"\"\"A type that will accept any NATS DSN.\n\n    NATS is a connective technology built for the ever increasingly hyper-connected world.\n    It is a single technology that enables applications to securely communicate across\n    any combination of cloud vendors, on-premise, edge, web and mobile, and devices.\n    More: https://nats.io\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 863, "code": "class MySQLDsn(AnyUrl):\n    _constraints = UrlConstraints(\n        allowed_schemes=[\n            'mysql',\n            'mysql+mysqlconnector',\n            'mysql+aiomysql',\n            'mysql+asyncmy',\n            'mysql+mysqldb',\n            'mysql+pymysql',\n            'mysql+cymysql',\n            'mysql+pyodbc',", "documentation": "    \"\"\"A type that will accept any MySQL DSN.\n\n    * User info required\n    * TLD not required\n    * Host not required\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 887, "code": "class MariaDBDsn(AnyUrl):\n    _constraints = UrlConstraints(\n        allowed_schemes=['mariadb', 'mariadb+mariadbconnector', 'mariadb+pymysql'],\n        default_port=3306,\n    )", "documentation": "    \"\"\"A type that will accept any MariaDB DSN.\n\n    * User info required\n    * TLD not required\n    * Host not required\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 901, "code": "class ClickHouseDsn(AnyUrl):\n    _constraints = UrlConstraints(\n        allowed_schemes=[\n            'clickhouse+native',\n            'clickhouse+asynch',\n            'clickhouse+http',\n            'clickhouse',\n            'clickhouses',\n            'clickhousedb',\n        ],\n        default_host='localhost',", "documentation": "    \"\"\"A type that will accept any ClickHouse DSN.\n\n    * User info required\n    * TLD not required\n    * Host not required\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 923, "code": "class SnowflakeDsn(AnyUrl):\n    _constraints = UrlConstraints(\n        allowed_schemes=['snowflake'],\n        host_required=True,\n    )\n    @property", "documentation": "    \"\"\"A type that will accept any Snowflake DSN.\n\n    * User info required\n    * TLD not required\n    * Host required\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 937, "code": "    def host(self) -> str:\n        return self._url.host  # pyright: ignore[reportReturnType]", "documentation": "        \"\"\"The required URL host.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 956, "code": "    class EmailStr:\n        @classmethod", "documentation": "        \"\"\"\n        Info:\n            To use this type, you need to install the optional\n            [`email-validator`](https://github.com/JoshData/python-email-validator) package:\n\n            ```bash\n            pip install email-validator\n            ```\n\n        Validate email addresses.\n\n        ```python\n        from pydantic import BaseModel, EmailStr\n\n        class Model(BaseModel):\n            email: EmailStr\n\n        print(Model(email='contact@mail.com'))\n        #> email='contact@mail.com'\n        ```\n        \"\"\"  # noqa: D212"}, {"filename": "pydantic/networks.py", "start_line": 1001, "code": "class NameEmail(_repr.Representation):\n    __slots__ = 'name', 'email'", "documentation": "    \"\"\"\n    Info:\n        To use this type, you need to install the optional\n        [`email-validator`](https://github.com/JoshData/python-email-validator) package:\n\n        ```bash\n        pip install email-validator\n        ```\n\n    Validate a name and email address combination, as specified by\n    [RFC 5322](https://datatracker.ietf.org/doc/html/rfc5322#section-3.4).\n\n    The `NameEmail` has two properties: `name` and `email`.\n    In case the `name` is not provided, it's inferred from the email address.\n\n    ```python\n    from pydantic import BaseModel, NameEmail\n\n    class User(BaseModel):\n        email: NameEmail\n\n    user = User(email='Fred Bloggs <fred.bloggs@example.com>')\n    print(user.email)\n    #> Fred Bloggs <fred.bloggs@example.com>\n    print(user.email.name)\n    #> Fred Bloggs\n\n    user = User(email='fred.bloggs@example.com')\n    print(user.email)\n    #> fred.bloggs <fred.bloggs@example.com>\n    print(user.email.name)\n    #> fred.bloggs\n    ```\n    \"\"\"  # noqa: D212"}, {"filename": "pydantic/networks.py", "start_line": 1100, "code": "    class IPvAnyAddress:\n        __slots__ = ()", "documentation": "        \"\"\"Validate an IPv4 or IPv6 address.\n\n        ```python\n        from pydantic import BaseModel\n        from pydantic.networks import IPvAnyAddress\n\n        class IpModel(BaseModel):\n            ip: IPvAnyAddress\n\n        print(IpModel(ip='127.0.0.1'))\n        #> ip=IPv4Address('127.0.0.1')\n\n        try:\n            IpModel(ip='http://www.example.com')\n        except ValueError as e:\n            print(e.errors())\n            '''\n            [\n                {\n                    'type': 'ip_any_address',\n                    'loc': ('ip',),\n                    'msg': 'value is not a valid IPv4 or IPv6 address',\n                    'input': 'http://www.example.com',\n                }\n            ]\n            '''\n        ```\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 1132, "code": "        def __new__(cls, value: Any) -> IPvAnyAddressType:\n            try:\n                return IPv4Address(value)\n            except ValueError:\n                pass\n            try:\n                return IPv6Address(value)\n            except ValueError:\n                raise PydanticCustomError('ip_any_address', 'value is not a valid IPv4 or IPv6 address')\n        @classmethod", "documentation": "            \"\"\"Validate an IPv4 or IPv6 address.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 1166, "code": "    class IPvAnyInterface:\n        __slots__ = ()", "documentation": "        \"\"\"Validate an IPv4 or IPv6 interface.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 1171, "code": "        def __new__(cls, value: NetworkType) -> IPvAnyInterfaceType:\n            try:\n                return IPv4Interface(value)\n            except ValueError:\n                pass\n            try:\n                return IPv6Interface(value)\n            except ValueError:\n                raise PydanticCustomError('ip_any_interface', 'value is not a valid IPv4 or IPv6 interface')\n        @classmethod", "documentation": "            \"\"\"Validate an IPv4 or IPv6 interface.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 1205, "code": "    class IPvAnyNetwork:\n        __slots__ = ()", "documentation": "        \"\"\"Validate an IPv4 or IPv6 network.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 1210, "code": "        def __new__(cls, value: NetworkType) -> IPvAnyNetworkType:\n            try:\n                return IPv4Network(value)\n            except ValueError:\n                pass\n            try:\n                return IPv6Network(value)\n            except ValueError:\n                raise PydanticCustomError('ip_any_network', 'value is not a valid IPv4 or IPv6 network')\n        @classmethod", "documentation": "            \"\"\"Validate an IPv4 or IPv6 network.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 1263, "code": "def validate_email(value: str) -> tuple[str, str]:\n    if email_validator is None:\n        import_email_validator()\n    if len(value) > MAX_EMAIL_LENGTH:\n        raise PydanticCustomError(\n            'value_error',\n            'value is not a valid email address: {reason}',\n            {'reason': f'Length must not exceed {MAX_EMAIL_LENGTH} characters'},\n        )\n    m = pretty_email_regex.fullmatch(value)\n    name: str | None = None", "documentation": "    \"\"\"Email address validation using [email-validator](https://pypi.org/project/email-validator/).\n\n    Returns:\n        A tuple containing the local part of the email (or the name for \"pretty\" email addresses)\n            and the normalized email.\n\n    Raises:\n        PydanticCustomError: If the email is invalid.\n\n    Note:\n        Note that:\n\n        * Raw IP address (literal) domain parts are not allowed.\n        * `\"John Doe <local_part@domain.com>\"` style \"pretty\" email addresses are processed.\n        * Spaces are striped from the beginning and end of addresses, but no error is raised.\n    \"\"\""}], "after_segments": [{"filename": "pydantic/networks.py", "start_line": 70, "code": "class UrlConstraints:\n    max_length: int | None = None\n    allowed_schemes: list[str] | None = None\n    host_required: bool | None = None\n    default_host: str | None = None\n    default_port: int | None = None\n    default_path: str | None = None\n    preserve_empty_path: bool | None = None", "documentation": "    \"\"\"Url constraints.\n\n    Attributes:\n        max_length: The maximum length of the url. Defaults to `None`.\n        allowed_schemes: The allowed schemes. Defaults to `None`.\n        host_required: Whether the host is required. Defaults to `None`.\n        default_host: The default host. Defaults to `None`.\n        default_port: The default port. Defaults to `None`.\n        default_path: The default path. Defaults to `None`.\n        preserve_empty_path: Whether to preserve empty URL paths. Defaults to `None`.\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 105, "code": "    def defined_constraints(self) -> dict[str, Any]:\n        return {field.name: value for field in fields(self) if (value := getattr(self, field.name)) is not None}", "documentation": "        \"\"\"Fetch a key / value mapping of constraints to values that are not None. Used for core schema updates.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 133, "code": "    def scheme(self) -> str:\n        return self._url.scheme\n    @property", "documentation": "        \"\"\"The scheme part of the URL.\n\n        e.g. `https` in `https://user:pass@host:port/path?query#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 141, "code": "    def username(self) -> str | None:\n        return self._url.username\n    @property", "documentation": "        \"\"\"The username part of the URL, or `None`.\n\n        e.g. `user` in `https://user:pass@host:port/path?query#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 149, "code": "    def password(self) -> str | None:\n        return self._url.password\n    @property", "documentation": "        \"\"\"The password part of the URL, or `None`.\n\n        e.g. `pass` in `https://user:pass@host:port/path?query#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 157, "code": "    def host(self) -> str | None:\n        return self._url.host", "documentation": "        \"\"\"The host part of the URL, or `None`.\n\n        If the URL must be punycode encoded, this is the encoded host, e.g if the input URL is `https://\u00a3\u00a3\u00a3.com`,\n        `host` will be `xn--9aaa.com`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 165, "code": "    def unicode_host(self) -> str | None:\n        return self._url.unicode_host()\n    @property", "documentation": "        \"\"\"The host part of the URL as a unicode string, or `None`.\n\n        e.g. `host` in `https://user:pass@host:port/path?query#fragment`\n\n        If the URL must be punycode encoded, this is the decoded host, e.g if the input URL is `https://\u00a3\u00a3\u00a3.com`,\n        `unicode_host()` will be `\u00a3\u00a3\u00a3.com`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 176, "code": "    def port(self) -> int | None:\n        return self._url.port\n    @property", "documentation": "        \"\"\"The port part of the URL, or `None`.\n\n        e.g. `port` in `https://user:pass@host:port/path?query#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 184, "code": "    def path(self) -> str | None:\n        return self._url.path\n    @property", "documentation": "        \"\"\"The path part of the URL, or `None`.\n\n        e.g. `/path` in `https://user:pass@host:port/path?query#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 192, "code": "    def query(self) -> str | None:\n        return self._url.query", "documentation": "        \"\"\"The query part of the URL, or `None`.\n\n        e.g. `query` in `https://user:pass@host:port/path?query#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 199, "code": "    def query_params(self) -> list[tuple[str, str]]:\n        return self._url.query_params()\n    @property", "documentation": "        \"\"\"The query part of the URL as a list of key-value pairs.\n\n        e.g. `[('foo', 'bar')]` in `https://user:pass@host:port/path?foo=bar#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 207, "code": "    def fragment(self) -> str | None:\n        return self._url.fragment", "documentation": "        \"\"\"The fragment part of the URL, or `None`.\n\n        e.g. `fragment` in `https://user:pass@host:port/path?query#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 214, "code": "    def unicode_string(self) -> str:\n        return self._url.unicode_string()", "documentation": "        \"\"\"The URL as a unicode string, unlike `__str__()` this will not punycode encode the host.\n\n        If the URL must be punycode encoded, this is the decoded string, e.g if the input URL is `https://\u00a3\u00a3\u00a3.com`,\n        `unicode_string()` will be `https://\u00a3\u00a3\u00a3.com`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 222, "code": "    def encoded_string(self) -> str:\n        return str(self)", "documentation": "        \"\"\"The URL's encoded string representation via __str__().\n\n        This returns the punycode-encoded host version of the URL as a string.\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 229, "code": "    def __str__(self) -> str:\n        return str(self._url)", "documentation": "        \"\"\"The URL as a string, this will punycode encode the host if required.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 353, "code": "    def scheme(self) -> str:\n        return self._url.scheme\n    @property", "documentation": "        \"\"\"The scheme part of the URL.\n\n        e.g. `https` in `https://foo.com,bar.com/path?query#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 361, "code": "    def path(self) -> str | None:\n        return self._url.path\n    @property", "documentation": "        \"\"\"The path part of the URL, or `None`.\n\n        e.g. `/path` in `https://foo.com,bar.com/path?query#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 369, "code": "    def query(self) -> str | None:\n        return self._url.query", "documentation": "        \"\"\"The query part of the URL, or `None`.\n\n        e.g. `query` in `https://foo.com,bar.com/path?query#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 376, "code": "    def query_params(self) -> list[tuple[str, str]]:\n        return self._url.query_params()\n    @property", "documentation": "        \"\"\"The query part of the URL as a list of key-value pairs.\n\n        e.g. `[('foo', 'bar')]` in `https://foo.com,bar.com/path?foo=bar#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 384, "code": "    def fragment(self) -> str | None:\n        return self._url.fragment", "documentation": "        \"\"\"The fragment part of the URL, or `None`.\n\n        e.g. `fragment` in `https://foo.com,bar.com/path?query#fragment`\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 391, "code": "    def hosts(self) -> list[MultiHostHost]:\n        return self._url.hosts()", "documentation": "        '''The hosts of the `MultiHostUrl` as [`MultiHostHost`][pydantic_core.MultiHostHost] typed dicts.\n\n        ```python\n        from pydantic_core import MultiHostUrl\n\n        mhu = MultiHostUrl('https://foo.com:123,foo:bar@bar.com/path')\n        print(mhu.hosts())\n        \"\"\"\n        [\n            {'username': None, 'password': None, 'host': 'foo.com', 'port': 123},\n            {'username': 'foo', 'password': 'bar', 'host': 'bar.com', 'port': 443}\n        ]\n        ```\n        Returns:\n            A list of dicts, each representing a host.\n        '''"}, {"filename": "pydantic/networks.py", "start_line": 410, "code": "    def encoded_string(self) -> str:\n        return str(self)", "documentation": "        \"\"\"The URL's encoded string representation via __str__().\n\n        This returns the punycode-encoded host version of the URL as a string.\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 417, "code": "    def unicode_string(self) -> str:\n        return self._url.unicode_string()", "documentation": "        \"\"\"The URL as a unicode string, unlike `__str__()` this will not punycode encode the hosts.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 421, "code": "    def __str__(self) -> str:\n        return str(self._url)", "documentation": "        \"\"\"The URL as a string, this will punycode encode the host if required.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 536, "code": "class AnyUrl(_BaseUrl):", "documentation": "    \"\"\"Base type for all URLs.\n\n    * Any scheme allowed\n    * Top-level domain (TLD) not required\n    * Host not required\n\n    Assuming an input URL of `http://samuel:pass@example.com:8000/the/path/?query=here#fragment=is;this=bit`,\n    the types export the following properties:\n\n    - `scheme`: the URL scheme (`http`), always set.\n    - `host`: the URL host (`example.com`).\n    - `username`: optional username if included (`samuel`).\n    - `password`: optional password if included (`pass`).\n    - `port`: optional port (`8000`).\n    - `path`: optional path (`/the/path/`).\n    - `query`: optional URL query (for example, `GET` arguments or \"search string\", such as `query=here`).\n    - `fragment`: optional fragment (`fragment=is;this=bit`).\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 561, "code": "class AnyHttpUrl(AnyUrl):\n    _constraints = UrlConstraints(allowed_schemes=['http', 'https'])", "documentation": "    \"\"\"A type that will accept any http or https URL.\n\n    * TLD not required\n    * Host not required\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 571, "code": "class HttpUrl(AnyUrl):\n    _constraints = UrlConstraints(max_length=2083, allowed_schemes=['http', 'https'])", "documentation": "    \"\"\"A type that will accept any http or https URL.\n\n    * TLD not required\n    * Host not required\n    * Max length 2083\n\n    ```python\n    from pydantic import BaseModel, HttpUrl, ValidationError\n\n    class MyModel(BaseModel):\n        url: HttpUrl\n\n    m = MyModel(url='http://www.example.com')  # (1)!\n    print(m.url)\n    #> http://www.example.com/\n\n    try:\n        MyModel(url='ftp://invalid.url')\n    except ValidationError as e:\n        print(e)\n        '''\n        1 validation error for MyModel\n        url\n          URL scheme should be 'http' or 'https' [type=url_scheme, input_value='ftp://invalid.url', input_type=str]\n        '''\n\n    try:\n        MyModel(url='not a url')\n    except ValidationError as e:\n        print(e)\n        '''\n        1 validation error for MyModel\n        url\n          Input should be a valid URL, relative URL without a base [type=url_parsing, input_value='not a url', input_type=str]\n        '''\n    ```\n\n    1. Note: mypy would prefer `m = MyModel(url=HttpUrl('http://www.example.com'))`, but Pydantic will convert the string to an HttpUrl instance anyway.\n\n    \"International domains\" (e.g. a URL where the host or TLD includes non-ascii characters) will be encoded via\n    [punycode](https://en.wikipedia.org/wiki/Punycode) (see\n    [this article](https://www.xudongz.com/blog/2017/idn-phishing/) for a good description of why this is important):\n\n    ```python\n    from pydantic import BaseModel, HttpUrl\n\n    class MyModel(BaseModel):\n        url: HttpUrl\n\n    m1 = MyModel(url='http://puny\u00a3code.com')\n    print(m1.url)\n    #> http://xn--punycode-eja.com/\n    m2 = MyModel(url='https://www.\u0430\u0440\u0440\u04cf\u0435.com/')\n    print(m2.url)\n    #> https://www.xn--80ak6aa92e.com/\n    m3 = MyModel(url='https://www.example.\u73e0\u5b9d/')\n    print(m3.url)\n    #> https://www.example.xn--pbt977c/\n    ```\n\n\n    !!! warning \"Underscores in Hostnames\"\n        In Pydantic, underscores are allowed in all parts of a domain except the TLD.\n        Technically this might be wrong - in theory the hostname cannot have underscores, but subdomains can.\n\n        To explain this; consider the following two cases:\n\n        - `exam_ple.co.uk`: the hostname is `exam_ple`, which should not be allowed since it contains an underscore.\n        - `foo_bar.example.com` the hostname is `example`, which should be allowed since the underscore is in the subdomain.\n\n        Without having an exhaustive list of TLDs, it would be impossible to differentiate between these two. Therefore\n        underscores are allowed, but you can always do further validation in a validator if desired.\n\n        Also, Chrome, Firefox, and Safari all currently accept `http://exam_ple.com` as a URL, so we're in good\n        (or at least big) company.\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 652, "code": "class AnyWebsocketUrl(AnyUrl):\n    _constraints = UrlConstraints(allowed_schemes=['ws', 'wss'])", "documentation": "    \"\"\"A type that will accept any ws or wss URL.\n\n    * TLD not required\n    * Host not required\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 662, "code": "class WebsocketUrl(AnyUrl):\n    _constraints = UrlConstraints(max_length=2083, allowed_schemes=['ws', 'wss'])", "documentation": "    \"\"\"A type that will accept any ws or wss URL.\n\n    * TLD not required\n    * Host not required\n    * Max length 2083\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 673, "code": "class FileUrl(AnyUrl):\n    _constraints = UrlConstraints(allowed_schemes=['file'])", "documentation": "    \"\"\"A type that will accept any file URL.\n\n    * Host not required\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 682, "code": "class FtpUrl(AnyUrl):\n    _constraints = UrlConstraints(allowed_schemes=['ftp'])", "documentation": "    \"\"\"A type that will accept ftp URL.\n\n    * TLD not required\n    * Host not required\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 692, "code": "class PostgresDsn(_BaseMultiHostUrl):\n    _constraints = UrlConstraints(\n        host_required=True,\n        allowed_schemes=[\n            'postgres',\n            'postgresql',\n            'postgresql+asyncpg',\n            'postgresql+pg8000',\n            'postgresql+psycopg',\n            'postgresql+psycopg2',\n            'postgresql+psycopg2cffi',", "documentation": "    \"\"\"A type that will accept any Postgres DSN.\n\n    * User info required\n    * TLD not required\n    * Host required\n    * Supports multiple hosts\n\n    If further validation is required, these properties can be used by validators to enforce specific behaviour:\n\n    ```python\n    from pydantic import (\n        BaseModel,\n        HttpUrl,\n        PostgresDsn,\n        ValidationError,\n        field_validator,\n    )\n\n    class MyModel(BaseModel):\n        url: HttpUrl\n\n    m = MyModel(url='http://www.example.com')\n\n    # the repr() method for a url will display all properties of the url\n    print(repr(m.url))\n    #> HttpUrl('http://www.example.com/')\n    print(m.url.scheme)\n    #> http\n    print(m.url.host)\n    #> www.example.com\n    print(m.url.port)\n    #> 80\n\n    class MyDatabaseModel(BaseModel):\n        db: PostgresDsn\n\n        @field_validator('db')\n        def check_db_name(cls, v):\n            assert v.path and len(v.path) > 1, 'database must be provided'\n            return v\n\n    m = MyDatabaseModel(db='postgres://user:pass@localhost:5432/foobar')\n    print(m.db)\n    #> postgres://user:pass@localhost:5432/foobar\n\n    try:\n        MyDatabaseModel(db='postgres://user:pass@localhost:5432')\n    except ValidationError as e:\n        print(e)\n        '''\n        1 validation error for MyDatabaseModel\n        db\n          Assertion failed, database must be provided\n        assert (None)\n         +  where None = PostgresDsn('postgres://user:pass@localhost:5432').path [type=assertion_error, input_value='postgres://user:pass@localhost:5432', input_type=str]\n        '''\n    ```\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 768, "code": "    def host(self) -> str:\n        return self._url.host  # pyright: ignore[reportAttributeAccessIssue]", "documentation": "        \"\"\"The required URL host.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 773, "code": "class CockroachDsn(AnyUrl):\n    _constraints = UrlConstraints(\n        host_required=True,\n        allowed_schemes=[\n            'cockroachdb',\n            'cockroachdb+psycopg2',\n            'cockroachdb+asyncpg',\n        ],\n    )\n    @property", "documentation": "    \"\"\"A type that will accept any Cockroach DSN.\n\n    * User info required\n    * TLD not required\n    * Host required\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 791, "code": "    def host(self) -> str:\n        return self._url.host  # pyright: ignore[reportReturnType]", "documentation": "        \"\"\"The required URL host.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 796, "code": "class AmqpDsn(AnyUrl):\n    _constraints = UrlConstraints(allowed_schemes=['amqp', 'amqps'])", "documentation": "    \"\"\"A type that will accept any AMQP DSN.\n\n    * User info required\n    * TLD not required\n    * Host not required\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 807, "code": "class RedisDsn(AnyUrl):\n    _constraints = UrlConstraints(\n        allowed_schemes=['redis', 'rediss'],\n        default_host='localhost',\n        default_port=6379,\n        default_path='/0',\n        host_required=True,\n    )\n    @property", "documentation": "    \"\"\"A type that will accept any Redis DSN.\n\n    * User info required\n    * TLD not required\n    * Host required (e.g., `rediss://:pass@localhost`)\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 824, "code": "    def host(self) -> str:\n        return self._url.host  # pyright: ignore[reportReturnType]", "documentation": "        \"\"\"The required URL host.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 829, "code": "class MongoDsn(_BaseMultiHostUrl):\n    _constraints = UrlConstraints(allowed_schemes=['mongodb', 'mongodb+srv'], default_port=27017)", "documentation": "    \"\"\"A type that will accept any MongoDB DSN.\n\n    * User info not required\n    * Database name not required\n    * Port not required\n    * User info may be passed without user part (e.g., `mongodb://mongodb0.example.com:27017`).\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 841, "code": "class KafkaDsn(AnyUrl):\n    _constraints = UrlConstraints(allowed_schemes=['kafka'], default_host='localhost', default_port=9092)", "documentation": "    \"\"\"A type that will accept any Kafka DSN.\n\n    * User info required\n    * TLD not required\n    * Host not required\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 852, "code": "class NatsDsn(_BaseMultiHostUrl):\n    _constraints = UrlConstraints(\n        allowed_schemes=['nats', 'tls', 'ws', 'wss'], default_host='localhost', default_port=4222\n    )", "documentation": "    \"\"\"A type that will accept any NATS DSN.\n\n    NATS is a connective technology built for the ever increasingly hyper-connected world.\n    It is a single technology that enables applications to securely communicate across\n    any combination of cloud vendors, on-premise, edge, web and mobile, and devices.\n    More: https://nats.io\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 866, "code": "class MySQLDsn(AnyUrl):\n    _constraints = UrlConstraints(\n        allowed_schemes=[\n            'mysql',\n            'mysql+mysqlconnector',\n            'mysql+aiomysql',\n            'mysql+asyncmy',\n            'mysql+mysqldb',\n            'mysql+pymysql',\n            'mysql+cymysql',\n            'mysql+pyodbc',", "documentation": "    \"\"\"A type that will accept any MySQL DSN.\n\n    * User info required\n    * TLD not required\n    * Host not required\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 890, "code": "class MariaDBDsn(AnyUrl):\n    _constraints = UrlConstraints(\n        allowed_schemes=['mariadb', 'mariadb+mariadbconnector', 'mariadb+pymysql'],\n        default_port=3306,\n    )", "documentation": "    \"\"\"A type that will accept any MariaDB DSN.\n\n    * User info required\n    * TLD not required\n    * Host not required\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 904, "code": "class ClickHouseDsn(AnyUrl):\n    _constraints = UrlConstraints(\n        allowed_schemes=[\n            'clickhouse+native',\n            'clickhouse+asynch',\n            'clickhouse+http',\n            'clickhouse',\n            'clickhouses',\n            'clickhousedb',\n        ],\n        default_host='localhost',", "documentation": "    \"\"\"A type that will accept any ClickHouse DSN.\n\n    * User info required\n    * TLD not required\n    * Host not required\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 926, "code": "class SnowflakeDsn(AnyUrl):\n    _constraints = UrlConstraints(\n        allowed_schemes=['snowflake'],\n        host_required=True,\n    )\n    @property", "documentation": "    \"\"\"A type that will accept any Snowflake DSN.\n\n    * User info required\n    * TLD not required\n    * Host required\n    \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 940, "code": "    def host(self) -> str:\n        return self._url.host  # pyright: ignore[reportReturnType]", "documentation": "        \"\"\"The required URL host.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 959, "code": "    class EmailStr:\n        @classmethod", "documentation": "        \"\"\"\n        Info:\n            To use this type, you need to install the optional\n            [`email-validator`](https://github.com/JoshData/python-email-validator) package:\n\n            ```bash\n            pip install email-validator\n            ```\n\n        Validate email addresses.\n\n        ```python\n        from pydantic import BaseModel, EmailStr\n\n        class Model(BaseModel):\n            email: EmailStr\n\n        print(Model(email='contact@mail.com'))\n        #> email='contact@mail.com'\n        ```\n        \"\"\"  # noqa: D212"}, {"filename": "pydantic/networks.py", "start_line": 1004, "code": "class NameEmail(_repr.Representation):\n    __slots__ = 'name', 'email'", "documentation": "    \"\"\"\n    Info:\n        To use this type, you need to install the optional\n        [`email-validator`](https://github.com/JoshData/python-email-validator) package:\n\n        ```bash\n        pip install email-validator\n        ```\n\n    Validate a name and email address combination, as specified by\n    [RFC 5322](https://datatracker.ietf.org/doc/html/rfc5322#section-3.4).\n\n    The `NameEmail` has two properties: `name` and `email`.\n    In case the `name` is not provided, it's inferred from the email address.\n\n    ```python\n    from pydantic import BaseModel, NameEmail\n\n    class User(BaseModel):\n        email: NameEmail\n\n    user = User(email='Fred Bloggs <fred.bloggs@example.com>')\n    print(user.email)\n    #> Fred Bloggs <fred.bloggs@example.com>\n    print(user.email.name)\n    #> Fred Bloggs\n\n    user = User(email='fred.bloggs@example.com')\n    print(user.email)\n    #> fred.bloggs <fred.bloggs@example.com>\n    print(user.email.name)\n    #> fred.bloggs\n    ```\n    \"\"\"  # noqa: D212"}, {"filename": "pydantic/networks.py", "start_line": 1103, "code": "    class IPvAnyAddress:\n        __slots__ = ()", "documentation": "        \"\"\"Validate an IPv4 or IPv6 address.\n\n        ```python\n        from pydantic import BaseModel\n        from pydantic.networks import IPvAnyAddress\n\n        class IpModel(BaseModel):\n            ip: IPvAnyAddress\n\n        print(IpModel(ip='127.0.0.1'))\n        #> ip=IPv4Address('127.0.0.1')\n\n        try:\n            IpModel(ip='http://www.example.com')\n        except ValueError as e:\n            print(e.errors())\n            '''\n            [\n                {\n                    'type': 'ip_any_address',\n                    'loc': ('ip',),\n                    'msg': 'value is not a valid IPv4 or IPv6 address',\n                    'input': 'http://www.example.com',\n                }\n            ]\n            '''\n        ```\n        \"\"\""}, {"filename": "pydantic/networks.py", "start_line": 1135, "code": "        def __new__(cls, value: Any) -> IPvAnyAddressType:\n            try:\n                return IPv4Address(value)\n            except ValueError:\n                pass\n            try:\n                return IPv6Address(value)\n            except ValueError:\n                raise PydanticCustomError('ip_any_address', 'value is not a valid IPv4 or IPv6 address')\n        @classmethod", "documentation": "            \"\"\"Validate an IPv4 or IPv6 address.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 1169, "code": "    class IPvAnyInterface:\n        __slots__ = ()", "documentation": "        \"\"\"Validate an IPv4 or IPv6 interface.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 1174, "code": "        def __new__(cls, value: NetworkType) -> IPvAnyInterfaceType:\n            try:\n                return IPv4Interface(value)\n            except ValueError:\n                pass\n            try:\n                return IPv6Interface(value)\n            except ValueError:\n                raise PydanticCustomError('ip_any_interface', 'value is not a valid IPv4 or IPv6 interface')\n        @classmethod", "documentation": "            \"\"\"Validate an IPv4 or IPv6 interface.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 1208, "code": "    class IPvAnyNetwork:\n        __slots__ = ()", "documentation": "        \"\"\"Validate an IPv4 or IPv6 network.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 1213, "code": "        def __new__(cls, value: NetworkType) -> IPvAnyNetworkType:\n            try:\n                return IPv4Network(value)\n            except ValueError:\n                pass\n            try:\n                return IPv6Network(value)\n            except ValueError:\n                raise PydanticCustomError('ip_any_network', 'value is not a valid IPv4 or IPv6 network')\n        @classmethod", "documentation": "            \"\"\"Validate an IPv4 or IPv6 network.\"\"\""}, {"filename": "pydantic/networks.py", "start_line": 1266, "code": "def validate_email(value: str) -> tuple[str, str]:\n    if email_validator is None:\n        import_email_validator()\n    if len(value) > MAX_EMAIL_LENGTH:\n        raise PydanticCustomError(\n            'value_error',\n            'value is not a valid email address: {reason}',\n            {'reason': f'Length must not exceed {MAX_EMAIL_LENGTH} characters'},\n        )\n    m = pretty_email_regex.fullmatch(value)\n    name: str | None = None", "documentation": "    \"\"\"Email address validation using [email-validator](https://pypi.org/project/email-validator/).\n\n    Returns:\n        A tuple containing the local part of the email (or the name for \"pretty\" email addresses)\n            and the normalized email.\n\n    Raises:\n        PydanticCustomError: If the email is invalid.\n\n    Note:\n        Note that:\n\n        * Raw IP address (literal) domain parts are not allowed.\n        * `\"John Doe <local_part@domain.com>\"` style \"pretty\" email addresses are processed.\n        * Spaces are striped from the beginning and end of addresses, but no error is raised.\n    \"\"\""}]}
{"repository": "pydantic/pydantic", "commit_sha": "495b03f92dc9aedd18af454b69f2605ac402a580", "commit_message": "Add `preverse_empty_path` URL options (#12336)\n\nUpgrade `pydantic-core` to v2.41.1.\nAlso sync documentation about added error code after `pydantic-core` upgrade.", "commit_date": "2025-10-07T14:05:44+00:00", "author": "Victorien", "file": "pydantic/version.py", "patch": "@@ -19,7 +19,7 @@\n \"\"\"\n \n # Keep this in sync with the version constraint in the `pyproject.toml` dependencies:\n-_COMPATIBLE_PYDANTIC_CORE_VERSION = '2.40.1'\n+_COMPATIBLE_PYDANTIC_CORE_VERSION = '2.41.1'\n \n \n def version_short() -> str:", "before_segments": [{"filename": "pydantic/version.py", "start_line": 24, "code": "def version_short() -> str:\n    return '.'.join(VERSION.split('.')[:2])", "documentation": "    \"\"\"Return the `major.minor` part of Pydantic version.\n\n    It returns '2.1' if Pydantic version is '2.1.1'.\n    \"\"\""}, {"filename": "pydantic/version.py", "start_line": 32, "code": "def version_info() -> str:\n    import importlib.metadata\n    import platform\n    from pathlib import Path\n    import pydantic_core._pydantic_core as pdc\n    from ._internal import _git as git\n    package_names = {\n        'email-validator',\n        'fastapi',\n        'mypy',\n        'pydantic-extra-types',", "documentation": "    \"\"\"Return complete version information for Pydantic and its dependencies.\"\"\""}, {"filename": "pydantic/version.py", "start_line": 76, "code": "def check_pydantic_core_version() -> bool:\n    return __pydantic_core_version__ == _COMPATIBLE_PYDANTIC_CORE_VERSION", "documentation": "    \"\"\"Check that the installed `pydantic-core` dependency is compatible.\"\"\""}, {"filename": "pydantic/version.py", "start_line": 100, "code": "def parse_mypy_version(version: str) -> tuple[int, int, int]:\n    return tuple(map(int, version.partition('+')[0].split('.')))  # pyright: ignore[reportReturnType]", "documentation": "    \"\"\"Parse `mypy` string version to a 3-tuple of ints.\n\n    It parses normal version like `1.11.0` and extra info followed by a `+` sign\n    like `1.11.0+dev.d6d9d8cd4f27c52edac1f537e236ec48a01e54cb.dirty`.\n\n    Args:\n        version: The mypy version string.\n\n    Returns:\n        A triple of ints, e.g. `(1, 11, 0)`.\n    \"\"\""}], "after_segments": [{"filename": "pydantic/version.py", "start_line": 24, "code": "def version_short() -> str:\n    return '.'.join(VERSION.split('.')[:2])", "documentation": "    \"\"\"Return the `major.minor` part of Pydantic version.\n\n    It returns '2.1' if Pydantic version is '2.1.1'.\n    \"\"\""}, {"filename": "pydantic/version.py", "start_line": 32, "code": "def version_info() -> str:\n    import importlib.metadata\n    import platform\n    from pathlib import Path\n    import pydantic_core._pydantic_core as pdc\n    from ._internal import _git as git\n    package_names = {\n        'email-validator',\n        'fastapi',\n        'mypy',\n        'pydantic-extra-types',", "documentation": "    \"\"\"Return complete version information for Pydantic and its dependencies.\"\"\""}, {"filename": "pydantic/version.py", "start_line": 76, "code": "def check_pydantic_core_version() -> bool:\n    return __pydantic_core_version__ == _COMPATIBLE_PYDANTIC_CORE_VERSION", "documentation": "    \"\"\"Check that the installed `pydantic-core` dependency is compatible.\"\"\""}, {"filename": "pydantic/version.py", "start_line": 100, "code": "def parse_mypy_version(version: str) -> tuple[int, int, int]:\n    return tuple(map(int, version.partition('+')[0].split('.')))  # pyright: ignore[reportReturnType]", "documentation": "    \"\"\"Parse `mypy` string version to a 3-tuple of ints.\n\n    It parses normal version like `1.11.0` and extra info followed by a `+` sign\n    like `1.11.0+dev.d6d9d8cd4f27c52edac1f537e236ec48a01e54cb.dirty`.\n\n    Args:\n        version: The mypy version string.\n\n    Returns:\n        A triple of ints, e.g. `(1, 11, 0)`.\n    \"\"\""}]}
{"repository": "pydantic/pydantic", "commit_sha": "495b03f92dc9aedd18af454b69f2605ac402a580", "commit_message": "Add `preverse_empty_path` URL options (#12336)\n\nUpgrade `pydantic-core` to v2.41.1.\nAlso sync documentation about added error code after `pydantic-core` upgrade.", "commit_date": "2025-10-07T14:05:44+00:00", "author": "Victorien", "file": "tests/test_networks.py", "patch": "@@ -1216,3 +1216,17 @@ def test_url_ser_as_any() -> None:\n     ta = TypeAdapter(Any)\n     assert ta.dump_python(HttpUrl('http://example.com')) == HttpUrl('http://example.com')\n     assert ta.dump_json(HttpUrl('http://example.com')) == b'\"http://example.com/\"'\n+\n+\n+@pytest.mark.parametrize(\n+    'type',\n+    [Url, AnyUrl, HttpUrl],\n+)\n+def test_url_preserve_empty_path(type) -> None:\n+    ta_config = TypeAdapter(type, config={'url_preserve_empty_path': True})\n+\n+    assert str(ta_config.validate_python('http://example.com')) == 'http://example.com'\n+\n+    ta_constraint = TypeAdapter(Annotated[type, UrlConstraints(preserve_empty_path=True)])\n+\n+    assert str(ta_constraint.validate_python('http://example.com')) == 'http://example.com'", "before_segments": [{"filename": "tests/test_networks.py", "start_line": 1112, "code": "    def remove_trailing_slash(url: AnyUrl) -> str:\n        return str(url._url).rstrip('/')\n    HttpUrl = Annotated[\n        AnyUrl,\n        UrlConstraints(allowed_schemes=['http', 'https']),\n        AfterValidator(lambda url: remove_trailing_slash(url)),\n    ]\n    ta = TypeAdapter(HttpUrl)\n    assert ta.validate_python('https://example.com/') == 'https://example.com'", "documentation": "        \"\"\"Custom url -> str transformer that removes trailing slash.\"\"\""}], "after_segments": [{"filename": "tests/test_networks.py", "start_line": 1112, "code": "    def remove_trailing_slash(url: AnyUrl) -> str:\n        return str(url._url).rstrip('/')\n    HttpUrl = Annotated[\n        AnyUrl,\n        UrlConstraints(allowed_schemes=['http', 'https']),\n        AfterValidator(lambda url: remove_trailing_slash(url)),\n    ]\n    ta = TypeAdapter(HttpUrl)\n    assert ta.validate_python('https://example.com/') == 'https://example.com'", "documentation": "        \"\"\"Custom url -> str transformer that removes trailing slash.\"\"\""}]}
{"repository": "pydantic/pydantic", "commit_sha": "0d33ece12cd7c03af997c2ecde6dd7718d1bd044", "commit_message": "Fix check for stdlib dataclasses (#11822)\n\n- Rename function to `is_stdlib_dataclass()`\n- Instead of hacking around with annotations, check the\n  cls' `__dict__`\n- Update docstring, return type\n- Add and update existing tests\n- Improve some parts of the documentation.\n\nThanks @karta9821 for initially investigating.", "commit_date": "2025-05-01T20:23:14+00:00", "author": "Victorien", "file": "pydantic/_internal/_dataclasses.py", "patch": "@@ -2,7 +2,6 @@\n \n from __future__ import annotations as _annotations\n \n-import dataclasses\n import typing\n import warnings\n from functools import partial\n@@ -14,7 +13,7 @@\n     SchemaValidator,\n     core_schema,\n )\n-from typing_extensions import TypeGuard\n+from typing_extensions import TypeIs\n \n from ..errors import PydanticUndefinedAnnotation\n from ..plugin._schema_validator import PluggableSchemaValidator, create_schema_validator\n@@ -188,38 +187,16 @@ def __init__(__dataclass_self__: PydanticDataclass, *args: Any, **kwargs: Any) -\n     return True\n \n \n-def is_builtin_dataclass(_cls: type[Any]) -> TypeGuard[type[StandardDataclass]]:\n-    \"\"\"Returns True if a class is a stdlib dataclass and *not* a pydantic dataclass.\n+def is_stdlib_dataclass(cls: type[Any], /) -> TypeIs[type[StandardDataclass]]:\n+    \"\"\"Returns `True` if the class is a stdlib dataclass and *not* a Pydantic dataclass.\n \n-    We check that\n-    - `_cls` is a dataclass\n-    - `_cls` does not inherit from a processed pydantic dataclass (and thus have a `__pydantic_validator__`)\n-    - `_cls` does not have any annotations that are not dataclass fields\n-    e.g.\n-    ```python\n-    import dataclasses\n-\n-    import pydantic.dataclasses\n-\n-    @dataclasses.dataclass\n-    class A:\n-        x: int\n-\n-    @pydantic.dataclasses.dataclass\n-    class B(A):\n-        y: int\n-    ```\n-    In this case, when we first check `B`, we make an extra check and look at the annotations ('y'),\n-    which won't be a superset of all the dataclass fields (only the stdlib fields i.e. 'x')\n+    Unlike the stdlib `dataclasses.is_dataclass()` function, this does *not* include subclasses\n+    of a dataclass that are themselves not dataclasses.\n \n     Args:\n         cls: The class.\n \n     Returns:\n         `True` if the class is a stdlib dataclass, `False` otherwise.\n     \"\"\"\n-    return (\n-        dataclasses.is_dataclass(_cls)\n-        and not hasattr(_cls, '__pydantic_validator__')\n-        and set(_cls.__dataclass_fields__).issuperset(set(getattr(_cls, '__annotations__', {})))\n-    )\n+    return '__dataclass_fields__' in cls.__dict__ and not hasattr(cls, '__pydantic_validator__')", "before_segments": [{"filename": "pydantic/_internal/_dataclasses.py", "start_line": 36, "code": "    class PydanticDataclass(StandardDataclass, typing.Protocol):\n        __pydantic_config__: ClassVar[ConfigDict]\n        __pydantic_complete__: ClassVar[bool]\n        __pydantic_core_schema__: ClassVar[core_schema.CoreSchema]\n        __pydantic_decorators__: ClassVar[_decorators.DecoratorInfos]\n        __pydantic_fields__: ClassVar[dict[str, FieldInfo]]\n        __pydantic_serializer__: ClassVar[SchemaSerializer]\n        __pydantic_validator__: ClassVar[SchemaValidator | PluggableSchemaValidator]\nelse:\n    DeprecationWarning = PydanticDeprecatedSince20", "documentation": "        \"\"\"A protocol containing attributes only available once a class has been decorated as a Pydantic dataclass.\n\n        Attributes:\n            __pydantic_config__: Pydantic-specific configuration settings for the dataclass.\n            __pydantic_complete__: Whether dataclass building is completed, or if there are still undefined fields.\n            __pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n            __pydantic_decorators__: Metadata containing the decorators defined on the dataclass.\n            __pydantic_fields__: Metadata about the fields defined on the dataclass.\n            __pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the dataclass.\n            __pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the dataclass.\n        \"\"\""}, {"filename": "pydantic/_internal/_dataclasses.py", "start_line": 190, "code": "def is_builtin_dataclass(_cls: type[Any]) -> TypeGuard[type[StandardDataclass]]:\n    return (\n        dataclasses.is_dataclass(_cls)\n        and not hasattr(_cls, '__pydantic_validator__')\n        and set(_cls.__dataclass_fields__).issuperset(set(getattr(_cls, '__annotations__', {})))\n    )", "documentation": "    \"\"\"Returns True if a class is a stdlib dataclass and *not* a pydantic dataclass.\n\n    We check that\n    - `_cls` is a dataclass\n    - `_cls` does not inherit from a processed pydantic dataclass (and thus have a `__pydantic_validator__`)\n    - `_cls` does not have any annotations that are not dataclass fields\n    e.g.\n    ```python\n    import dataclasses\n\n    import pydantic.dataclasses\n\n    @dataclasses.dataclass\n    class A:\n        x: int\n\n    @pydantic.dataclasses.dataclass\n    class B(A):\n        y: int\n    ```\n    In this case, when we first check `B`, we make an extra check and look at the annotations ('y'),\n    which won't be a superset of all the dataclass fields (only the stdlib fields i.e. 'x')\n\n    Args:\n        cls: The class.\n\n    Returns:\n        `True` if the class is a stdlib dataclass, `False` otherwise.\n    \"\"\""}], "after_segments": [{"filename": "pydantic/_internal/_dataclasses.py", "start_line": 35, "code": "    class PydanticDataclass(StandardDataclass, typing.Protocol):\n        __pydantic_config__: ClassVar[ConfigDict]\n        __pydantic_complete__: ClassVar[bool]\n        __pydantic_core_schema__: ClassVar[core_schema.CoreSchema]\n        __pydantic_decorators__: ClassVar[_decorators.DecoratorInfos]\n        __pydantic_fields__: ClassVar[dict[str, FieldInfo]]\n        __pydantic_serializer__: ClassVar[SchemaSerializer]\n        __pydantic_validator__: ClassVar[SchemaValidator | PluggableSchemaValidator]\nelse:\n    DeprecationWarning = PydanticDeprecatedSince20", "documentation": "        \"\"\"A protocol containing attributes only available once a class has been decorated as a Pydantic dataclass.\n\n        Attributes:\n            __pydantic_config__: Pydantic-specific configuration settings for the dataclass.\n            __pydantic_complete__: Whether dataclass building is completed, or if there are still undefined fields.\n            __pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n            __pydantic_decorators__: Metadata containing the decorators defined on the dataclass.\n            __pydantic_fields__: Metadata about the fields defined on the dataclass.\n            __pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the dataclass.\n            __pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the dataclass.\n        \"\"\""}, {"filename": "pydantic/_internal/_dataclasses.py", "start_line": 189, "code": "def is_stdlib_dataclass(cls: type[Any], /) -> TypeIs[type[StandardDataclass]]:\n    return '__dataclass_fields__' in cls.__dict__ and not hasattr(cls, '__pydantic_validator__')", "documentation": "    \"\"\"Returns `True` if the class is a stdlib dataclass and *not* a Pydantic dataclass.\n\n    Unlike the stdlib `dataclasses.is_dataclass()` function, this does *not* include subclasses\n    of a dataclass that are themselves not dataclasses.\n\n    Args:\n        cls: The class.\n\n    Returns:\n        `True` if the class is a stdlib dataclass, `False` otherwise.\n    \"\"\""}]}
{"repository": "pydantic/pydantic", "commit_sha": "0d33ece12cd7c03af997c2ecde6dd7718d1bd044", "commit_message": "Fix check for stdlib dataclasses (#11822)\n\n- Rename function to `is_stdlib_dataclass()`\n- Instead of hacking around with annotations, check the\n  cls' `__dict__`\n- Update docstring, return type\n- Add and update existing tests\n- Improve some parts of the documentation.\n\nThanks @karta9821 for initially investigating.", "commit_date": "2025-05-01T20:23:14+00:00", "author": "Victorien", "file": "pydantic/dataclasses.py", "patch": "@@ -224,9 +224,9 @@ def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n         # since dataclasses.dataclass will set this as the __doc__\n         original_doc = cls.__doc__\n \n-        if _pydantic_dataclasses.is_builtin_dataclass(cls):\n-            # Don't preserve the docstring for vanilla dataclasses, as it may include the signature\n-            # This matches v1 behavior, and there was an explicit test for it\n+        if _pydantic_dataclasses.is_stdlib_dataclass(cls):\n+            # Vanilla dataclasses include a default docstring (representing the class signature),\n+            # which we don't want to preserve.\n             original_doc = None\n \n             # We don't want to add validation to the existing std lib dataclass, so we will subclass it", "before_segments": [{"filename": "pydantic/dataclasses.py", "start_line": 152, "code": "    def make_pydantic_fields_compatible(cls: type[Any]) -> None:\n        for annotation_cls in cls.__mro__:\n            annotations: dict[str, Any] = getattr(annotation_cls, '__annotations__', {})\n            for field_name in annotations:\n                field_value = getattr(cls, field_name, None)\n                if not isinstance(field_value, FieldInfo):\n                    continue\n                field_args: dict = {'default': field_value}\n                if sys.version_info >= (3, 10) and field_value.kw_only:\n                    field_args['kw_only'] = True\n                if field_value.repr is not True:", "documentation": "        \"\"\"Make sure that stdlib `dataclasses` understands `Field` kwargs like `kw_only`\n        To do that, we simply change\n          `x: int = pydantic.Field(..., kw_only=True)`\n        into\n          `x: int = dataclasses.field(default=pydantic.Field(..., kw_only=True), kw_only=True)`\n        \"\"\""}, {"filename": "pydantic/dataclasses.py", "start_line": 185, "code": "    def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(", "documentation": "        \"\"\"Create a Pydantic dataclass from a regular dataclass.\n\n        Args:\n            cls: The class to create the Pydantic dataclass from.\n\n        Returns:\n            A Pydantic dataclass.\n        \"\"\""}, {"filename": "pydantic/dataclasses.py", "start_line": 321, "code": "    def _call_initvar(*args: Any, **kwargs: Any) -> NoReturn:\n        raise TypeError(\"'InitVar' object is not callable\")\n    dataclasses.InitVar.__call__ = _call_initvar", "documentation": "        \"\"\"This function does nothing but raise an error that is as similar as possible to what you'd get\n        if you were to try calling `InitVar[int]()` without this monkeypatch. The whole purpose is just\n        to ensure typing._type_check does not error if the type hint evaluates to `InitVar[<parameter>]`.\n        \"\"\""}, {"filename": "pydantic/dataclasses.py", "start_line": 392, "code": "def is_pydantic_dataclass(class_: type[Any], /) -> TypeGuard[type[PydanticDataclass]]:\n    try:\n        return '__is_pydantic_dataclass__' in class_.__dict__ and dataclasses.is_dataclass(class_)\n    except AttributeError:\n        return False", "documentation": "    \"\"\"Whether a class is a pydantic dataclass.\n\n    Args:\n        class_: The class.\n\n    Returns:\n        `True` if the class is a pydantic dataclass, `False` otherwise.\n    \"\"\""}], "after_segments": [{"filename": "pydantic/dataclasses.py", "start_line": 152, "code": "    def make_pydantic_fields_compatible(cls: type[Any]) -> None:\n        for annotation_cls in cls.__mro__:\n            annotations: dict[str, Any] = getattr(annotation_cls, '__annotations__', {})\n            for field_name in annotations:\n                field_value = getattr(cls, field_name, None)\n                if not isinstance(field_value, FieldInfo):\n                    continue\n                field_args: dict = {'default': field_value}\n                if sys.version_info >= (3, 10) and field_value.kw_only:\n                    field_args['kw_only'] = True\n                if field_value.repr is not True:", "documentation": "        \"\"\"Make sure that stdlib `dataclasses` understands `Field` kwargs like `kw_only`\n        To do that, we simply change\n          `x: int = pydantic.Field(..., kw_only=True)`\n        into\n          `x: int = dataclasses.field(default=pydantic.Field(..., kw_only=True), kw_only=True)`\n        \"\"\""}, {"filename": "pydantic/dataclasses.py", "start_line": 185, "code": "    def create_dataclass(cls: type[Any]) -> type[PydanticDataclass]:\n        from ._internal._utils import is_model_class\n        if is_model_class(cls):\n            raise PydanticUserError(\n                f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n                code='dataclass-on-model',\n            )\n        original_cls = cls\n        has_dataclass_base = any(dataclasses.is_dataclass(base) for base in cls.__bases__)\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\n            warn(", "documentation": "        \"\"\"Create a Pydantic dataclass from a regular dataclass.\n\n        Args:\n            cls: The class to create the Pydantic dataclass from.\n\n        Returns:\n            A Pydantic dataclass.\n        \"\"\""}, {"filename": "pydantic/dataclasses.py", "start_line": 321, "code": "    def _call_initvar(*args: Any, **kwargs: Any) -> NoReturn:\n        raise TypeError(\"'InitVar' object is not callable\")\n    dataclasses.InitVar.__call__ = _call_initvar", "documentation": "        \"\"\"This function does nothing but raise an error that is as similar as possible to what you'd get\n        if you were to try calling `InitVar[int]()` without this monkeypatch. The whole purpose is just\n        to ensure typing._type_check does not error if the type hint evaluates to `InitVar[<parameter>]`.\n        \"\"\""}, {"filename": "pydantic/dataclasses.py", "start_line": 392, "code": "def is_pydantic_dataclass(class_: type[Any], /) -> TypeGuard[type[PydanticDataclass]]:\n    try:\n        return '__is_pydantic_dataclass__' in class_.__dict__ and dataclasses.is_dataclass(class_)\n    except AttributeError:\n        return False", "documentation": "    \"\"\"Whether a class is a pydantic dataclass.\n\n    Args:\n        class_: The class.\n\n    Returns:\n        `True` if the class is a pydantic dataclass, `False` otherwise.\n    \"\"\""}]}
{"repository": "pydantic/pydantic", "commit_sha": "0d33ece12cd7c03af997c2ecde6dd7718d1bd044", "commit_message": "Fix check for stdlib dataclasses (#11822)\n\n- Rename function to `is_stdlib_dataclass()`\n- Instead of hacking around with annotations, check the\n  cls' `__dict__`\n- Update docstring, return type\n- Add and update existing tests\n- Improve some parts of the documentation.\n\nThanks @karta9821 for initially investigating.", "commit_date": "2025-05-01T20:23:14+00:00", "author": "Victorien", "file": "pydantic/json_schema.py", "patch": "@@ -1675,7 +1675,7 @@ def dataclass_schema(self, schema: core_schema.DataclassSchema) -> JsonSchemaVal\n         Returns:\n             The generated JSON schema.\n         \"\"\"\n-        from ._internal._dataclasses import is_builtin_dataclass\n+        from ._internal._dataclasses import is_stdlib_dataclass\n \n         cls = schema['cls']\n         config: ConfigDict = getattr(cls, '__pydantic_config__', cast('ConfigDict', {}))\n@@ -1686,7 +1686,7 @@ def dataclass_schema(self, schema: core_schema.DataclassSchema) -> JsonSchemaVal\n         self._update_class_schema(json_schema, cls, config)\n \n         # Dataclass-specific handling of description\n-        if is_builtin_dataclass(cls):\n+        if is_stdlib_dataclass(cls):\n             # vanilla dataclass; don't use cls.__doc__ as it will contain the class signature by default\n             description = None\n         else:", "before_segments": [{"filename": "pydantic/json_schema.py", "start_line": 100, "code": "class PydanticJsonSchemaWarning(UserWarning):\nNoDefault = object()\n\"\"\"A sentinel value used to indicate that no default value should be used when generating a JSON Schema\nfor a core schema with a default value.\n\"\"\"\nDEFAULT_REF_TEMPLATE = '#/$defs/{model}'\n\"\"\"The default format string used to generate reference names.\"\"\"\nCoreRef = NewType('CoreRef', str)\nDefsRef = NewType('DefsRef', str)\nJsonRef = NewType('JsonRef', str)\nCoreModeRef = tuple[CoreRef, JsonSchemaMode]", "documentation": "    \"\"\"This class is used to emit warnings produced during JSON schema generation.\n    See the [`GenerateJsonSchema.emit_warning`][pydantic.json_schema.GenerateJsonSchema.emit_warning] and\n    [`GenerateJsonSchema.render_warning_message`][pydantic.json_schema.GenerateJsonSchema.render_warning_message]\n    methods for more details; these can be overridden to control warning behavior.\n    \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 192, "code": "    def remap_json_schema(self, schema: Any) -> Any:\n        if isinstance(schema, str):\n            return self.remap_json_ref(JsonRef(schema))\n        elif isinstance(schema, list):\n            return [self.remap_json_schema(item) for item in schema]\n        elif isinstance(schema, dict):\n            for key, value in schema.items():\n                if key == '$ref' and isinstance(value, str):\n                    schema['$ref'] = self.remap_json_ref(JsonRef(value))\n                elif key == '$defs':\n                    schema['$defs'] = {", "documentation": "        \"\"\"\n        Recursively update the JSON schema replacing all $refs\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 215, "code": "class GenerateJsonSchema:\n    schema_dialect = 'https://json-schema.org/draft/2020-12/schema'\n    ignored_warning_kinds: set[JsonSchemaWarningKind] = {'skipped-choice'}", "documentation": "    \"\"\"!!! abstract \"Usage Documentation\"\n        [Customizing the JSON Schema Generation Process](../concepts/json_schema.md#customizing-the-json-schema-generation-process)\n\n    A class for generating JSON schemas.\n\n    This class generates JSON schemas based on configured parameters. The default schema dialect\n    is [https://json-schema.org/draft/2020-12/schema](https://json-schema.org/draft/2020-12/schema).\n    The class uses `by_alias` to configure how fields with\n    multiple names are handled and `ref_template` to format reference names.\n\n    Attributes:\n        schema_dialect: The JSON schema dialect used to generate the schema. See\n            [Declaring a Dialect](https://json-schema.org/understanding-json-schema/reference/schema.html#id4)\n            in the JSON Schema documentation for more information about dialects.\n        ignored_warning_kinds: Warnings to ignore when generating the schema. `self.render_warning_message` will\n            do nothing if its argument `kind` is in `ignored_warning_kinds`;\n            this value can be modified on subclasses to easily control which warnings are emitted.\n        by_alias: Whether to use field aliases when generating the schema.\n        ref_template: The format string used when generating reference names.\n        core_to_json_refs: A mapping of core refs to JSON refs.\n        core_to_defs_refs: A mapping of core refs to definition refs.\n        defs_to_core_refs: A mapping of definition refs to core refs.\n        json_to_defs_refs: A mapping of JSON refs to definition refs.\n        definitions: Definitions in the schema.\n\n    Args:\n        by_alias: Whether to use field aliases in the generated schemas.\n        ref_template: The format string to use when generating reference names.\n\n    Raises:\n        JsonSchemaError: If the instance of the class is inadvertently reused after generating a schema.\n    \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 377, "code": "    def generate(self, schema: CoreSchema, mode: JsonSchemaMode = 'validation') -> JsonSchemaValue:\n        self._mode = mode\n        if self._used:\n            raise PydanticUserError(\n                'This JSON schema generator has already been used to generate a JSON schema. '\n                f'You must create a new instance of {type(self).__name__} to generate a new JSON schema.',\n                code='json-schema-already-used',\n            )\n        json_schema: JsonSchemaValue = self.generate_inner(schema)\n        json_ref_counts = self.get_json_ref_counts(json_schema)\n        ref = cast(JsonRef, json_schema.get('$ref'))", "documentation": "        \"\"\"Generates a JSON schema for a specified schema in a specified mode.\n\n        Args:\n            schema: A Pydantic model.\n            mode: The mode in which to generate the schema. Defaults to 'validation'.\n\n        Returns:\n            A JSON schema representing the specified schema.\n\n        Raises:\n            PydanticUserError: If the JSON schema generator has already been used to generate a JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 426, "code": "    def generate_inner(self, schema: CoreSchemaOrField) -> JsonSchemaValue:  # noqa: C901\n        if 'ref' in schema:\n            core_ref = CoreRef(schema['ref'])  # type: ignore[typeddict-item]\n            core_mode_ref = (core_ref, self.mode)\n            if core_mode_ref in self.core_to_defs_refs and self.core_to_defs_refs[core_mode_ref] in self.definitions:\n                return {'$ref': self.core_to_json_refs[core_mode_ref]}", "documentation": "        \"\"\"Generates a JSON schema for a given core schema.\n\n        Args:\n            schema: The given core schema.\n\n        Returns:\n            The generated JSON schema.\n\n        TODO: the nested function definitions here seem like bad practice, I'd like to unpack these\n        in a future PR. It'd be great if we could shorten the call stack a bit for JSON schema generation,\n        and I think there's potential for that here.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 462, "code": "        def handler_func(schema_or_field: CoreSchemaOrField) -> JsonSchemaValue:\n            json_schema: JsonSchemaValue | None = None\n            if self.mode == 'serialization' and 'serialization' in schema_or_field:\n                ser_schema = schema_or_field['serialization']  # type: ignore\n                json_schema = self.ser_schema(ser_schema)\n                if (\n                    json_schema is not None\n                    and ser_schema.get('when_used') in ('unless-none', 'json-unless-none')\n                    and schema_or_field['type'] == 'nullable'\n                ):\n                    json_schema = self.get_flattened_anyof([{'type': 'null'}, json_schema])", "documentation": "            \"\"\"Generate a JSON schema based on the input schema.\n\n            Args:\n                schema_or_field: The core schema to generate a JSON schema from.\n\n            Returns:\n                The generated JSON schema.\n\n            Raises:\n                TypeError: If an unexpected schema type is encountered.\n            \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 567, "code": "    def sort(self, value: JsonSchemaValue, parent_key: str | None = None) -> JsonSchemaValue:\n        sorted_dict: dict[str, JsonSchemaValue] = {}\n        keys = value.keys()\n        if parent_key not in ('properties', 'default'):\n            keys = sorted(keys)\n        for key in keys:\n            sorted_dict[key] = self._sort_recursive(value[key], parent_key=key)\n        return sorted_dict", "documentation": "        \"\"\"Override this method to customize the sorting of the JSON schema (e.g., don't sort at all, sort all keys unconditionally, etc.)\n\n        By default, alphabetically sort the keys in the JSON schema, skipping the 'properties' and 'default' keys to preserve field definition order.\n        This sort is recursive, so it will sort all nested dictionaries as well.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 581, "code": "    def _sort_recursive(self, value: Any, parent_key: str | None = None) -> Any:\n        if isinstance(value, dict):\n            sorted_dict: dict[str, JsonSchemaValue] = {}\n            keys = value.keys()\n            if parent_key not in ('properties', 'default'):\n                keys = sorted(keys)\n            for key in keys:\n                sorted_dict[key] = self._sort_recursive(value[key], parent_key=key)\n            return sorted_dict\n        elif isinstance(value, list):\n            sorted_list: list[JsonSchemaValue] = []", "documentation": "        \"\"\"Recursively sort a JSON schema value.\"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 601, "code": "    def invalid_schema(self, schema: core_schema.InvalidSchema) -> JsonSchemaValue:\n        raise RuntimeError('Cannot generate schema for invalid_schema. This is a bug! Please report it.')", "documentation": "        \"\"\"Placeholder - should never be called.\"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 606, "code": "    def any_schema(self, schema: core_schema.AnySchema) -> JsonSchemaValue:\n        return {}", "documentation": "        \"\"\"Generates a JSON schema that matches any value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 617, "code": "    def none_schema(self, schema: core_schema.NoneSchema) -> JsonSchemaValue:\n        return {'type': 'null'}", "documentation": "        \"\"\"Generates a JSON schema that matches `None`.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 628, "code": "    def bool_schema(self, schema: core_schema.BoolSchema) -> JsonSchemaValue:\n        return {'type': 'boolean'}", "documentation": "        \"\"\"Generates a JSON schema that matches a bool value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 639, "code": "    def int_schema(self, schema: core_schema.IntSchema) -> JsonSchemaValue:\n        json_schema: dict[str, Any] = {'type': 'integer'}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.numeric)\n        json_schema = {k: v for k, v in json_schema.items() if v not in {math.inf, -math.inf}}\n        return json_schema", "documentation": "        \"\"\"Generates a JSON schema that matches an int value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 653, "code": "    def float_schema(self, schema: core_schema.FloatSchema) -> JsonSchemaValue:\n        json_schema: dict[str, Any] = {'type': 'number'}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.numeric)\n        json_schema = {k: v for k, v in json_schema.items() if v not in {math.inf, -math.inf}}\n        return json_schema", "documentation": "        \"\"\"Generates a JSON schema that matches a float value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 667, "code": "    def decimal_schema(self, schema: core_schema.DecimalSchema) -> JsonSchemaValue:\n        json_schema = self.str_schema(core_schema.str_schema())\n        if self.mode == 'validation':\n            multiple_of = schema.get('multiple_of')\n            le = schema.get('le')\n            ge = schema.get('ge')\n            lt = schema.get('lt')\n            gt = schema.get('gt')\n            json_schema = {\n                'anyOf': [\n                    self.float_schema(", "documentation": "        \"\"\"Generates a JSON schema that matches a decimal value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 700, "code": "    def str_schema(self, schema: core_schema.StringSchema) -> JsonSchemaValue:\n        json_schema = {'type': 'string'}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.string)\n        if isinstance(json_schema.get('pattern'), Pattern):\n            json_schema['pattern'] = json_schema.get('pattern').pattern  # type: ignore\n        return json_schema", "documentation": "        \"\"\"Generates a JSON schema that matches a string value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 716, "code": "    def bytes_schema(self, schema: core_schema.BytesSchema) -> JsonSchemaValue:\n        json_schema = {'type': 'string', 'format': 'base64url' if self._config.ser_json_bytes == 'base64' else 'binary'}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.bytes)\n        return json_schema", "documentation": "        \"\"\"Generates a JSON schema that matches a bytes value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 729, "code": "    def date_schema(self, schema: core_schema.DateSchema) -> JsonSchemaValue:\n        return {'type': 'string', 'format': 'date'}", "documentation": "        \"\"\"Generates a JSON schema that matches a date value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 740, "code": "    def time_schema(self, schema: core_schema.TimeSchema) -> JsonSchemaValue:\n        return {'type': 'string', 'format': 'time'}", "documentation": "        \"\"\"Generates a JSON schema that matches a time value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 751, "code": "    def datetime_schema(self, schema: core_schema.DatetimeSchema) -> JsonSchemaValue:\n        return {'type': 'string', 'format': 'date-time'}", "documentation": "        \"\"\"Generates a JSON schema that matches a datetime value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 762, "code": "    def timedelta_schema(self, schema: core_schema.TimedeltaSchema) -> JsonSchemaValue:\n        if self._config.ser_json_timedelta == 'float':\n            return {'type': 'number'}\n        return {'type': 'string', 'format': 'duration'}", "documentation": "        \"\"\"Generates a JSON schema that matches a timedelta value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 775, "code": "    def literal_schema(self, schema: core_schema.LiteralSchema) -> JsonSchemaValue:\n        expected = [to_jsonable_python(v.value if isinstance(v, Enum) else v) for v in schema['expected']]\n        result: dict[str, Any] = {}\n        if len(expected) == 1:\n            result['const'] = expected[0]\n        else:\n            result['enum'] = expected\n        types = {type(e) for e in expected}\n        if types == {str}:\n            result['type'] = 'string'\n        elif types == {int}:", "documentation": "        \"\"\"Generates a JSON schema that matches a literal value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 807, "code": "    def enum_schema(self, schema: core_schema.EnumSchema) -> JsonSchemaValue:\n        enum_type = schema['cls']\n        description = None if not enum_type.__doc__ else inspect.cleandoc(enum_type.__doc__)\n        if (\n            description == 'An enumeration.'\n        ):  # This is the default value provided by enum.EnumMeta.__new__; don't use it\n            description = None\n        result: dict[str, Any] = {'title': enum_type.__name__, 'description': description}\n        result = {k: v for k, v in result.items() if v is not None}\n        expected = [to_jsonable_python(v.value) for v in schema['members']]\n        result['enum'] = expected", "documentation": "        \"\"\"Generates a JSON schema that matches an Enum value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 843, "code": "    def is_instance_schema(self, schema: core_schema.IsInstanceSchema) -> JsonSchemaValue:\n        return self.handle_invalid_for_json_schema(schema, f'core_schema.IsInstanceSchema ({schema[\"cls\"]})')", "documentation": "        \"\"\"Handles JSON schema generation for a core schema that checks if a value is an instance of a class.\n\n        Unless overridden in a subclass, this raises an error.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 856, "code": "    def is_subclass_schema(self, schema: core_schema.IsSubclassSchema) -> JsonSchemaValue:\n        return {}", "documentation": "        \"\"\"Handles JSON schema generation for a core schema that checks if a value is a subclass of a class.\n\n        For backwards compatibility with v1, this does not raise an error, but can be overridden to change this.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 870, "code": "    def callable_schema(self, schema: core_schema.CallableSchema) -> JsonSchemaValue:\n        return self.handle_invalid_for_json_schema(schema, 'core_schema.CallableSchema')", "documentation": "        \"\"\"Generates a JSON schema that matches a callable value.\n\n        Unless overridden in a subclass, this raises an error.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 883, "code": "    def list_schema(self, schema: core_schema.ListSchema) -> JsonSchemaValue:\n        items_schema = {} if 'items_schema' not in schema else self.generate_inner(schema['items_schema'])\n        json_schema = {'type': 'array', 'items': items_schema}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.array)\n        return json_schema\n    @deprecated('`tuple_positional_schema` is deprecated. Use `tuple_schema` instead.', category=None)\n    @final", "documentation": "        \"\"\"Returns a schema that matches a list schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 899, "code": "    def tuple_positional_schema(self, schema: core_schema.TupleSchema) -> JsonSchemaValue:\n        warnings.warn(\n            '`tuple_positional_schema` is deprecated. Use `tuple_schema` instead.',\n            PydanticDeprecatedSince26,\n            stacklevel=2,\n        )\n        return self.tuple_schema(schema)\n    @deprecated('`tuple_variable_schema` is deprecated. Use `tuple_schema` instead.', category=None)\n    @final", "documentation": "        \"\"\"Replaced by `tuple_schema`.\"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 910, "code": "    def tuple_variable_schema(self, schema: core_schema.TupleSchema) -> JsonSchemaValue:\n        warnings.warn(\n            '`tuple_variable_schema` is deprecated. Use `tuple_schema` instead.',\n            PydanticDeprecatedSince26,\n            stacklevel=2,\n        )\n        return self.tuple_schema(schema)", "documentation": "        \"\"\"Replaced by `tuple_schema`.\"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 919, "code": "    def tuple_schema(self, schema: core_schema.TupleSchema) -> JsonSchemaValue:\n        json_schema: JsonSchemaValue = {'type': 'array'}\n        if 'variadic_item_index' in schema:\n            variadic_item_index = schema['variadic_item_index']\n            if variadic_item_index > 0:\n                json_schema['minItems'] = variadic_item_index\n                json_schema['prefixItems'] = [\n                    self.generate_inner(item) for item in schema['items_schema'][:variadic_item_index]\n                ]\n            if variadic_item_index + 1 == len(schema['items_schema']):\n                json_schema['items'] = self.generate_inner(schema['items_schema'][variadic_item_index])", "documentation": "        \"\"\"Generates a JSON schema that matches a tuple schema e.g. `tuple[int,\n        str, bool]` or `tuple[int, ...]`.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 954, "code": "    def set_schema(self, schema: core_schema.SetSchema) -> JsonSchemaValue:\n        return self._common_set_schema(schema)", "documentation": "        \"\"\"Generates a JSON schema that matches a set schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 965, "code": "    def frozenset_schema(self, schema: core_schema.FrozenSetSchema) -> JsonSchemaValue:\n        return self._common_set_schema(schema)", "documentation": "        \"\"\"Generates a JSON schema that matches a frozenset schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 982, "code": "    def generator_schema(self, schema: core_schema.GeneratorSchema) -> JsonSchemaValue:\n        items_schema = {} if 'items_schema' not in schema else self.generate_inner(schema['items_schema'])\n        json_schema = {'type': 'array', 'items': items_schema}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.array)\n        return json_schema", "documentation": "        \"\"\"Returns a JSON schema that represents the provided GeneratorSchema.\n\n        Args:\n            schema: The schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 996, "code": "    def dict_schema(self, schema: core_schema.DictSchema) -> JsonSchemaValue:\n        json_schema: JsonSchemaValue = {'type': 'object'}\n        keys_schema = self.generate_inner(schema['keys_schema']).copy() if 'keys_schema' in schema else {}\n        if '$ref' not in keys_schema:\n            keys_pattern = keys_schema.pop('pattern', None)\n            keys_schema.pop('title', None)\n        else:\n            keys_pattern = None\n        values_schema = self.generate_inner(schema['values_schema']).copy() if 'values_schema' in schema else {}\n        values_schema.pop('title', None)\n        if values_schema or keys_pattern is not None:", "documentation": "        \"\"\"Generates a JSON schema that matches a dict schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1044, "code": "    def function_before_schema(self, schema: core_schema.BeforeValidatorFunctionSchema) -> JsonSchemaValue:\n        if self.mode == 'validation' and (input_schema := schema.get('json_schema_input_schema')):\n            return self.generate_inner(input_schema)\n        return self.generate_inner(schema['schema'])", "documentation": "        \"\"\"Generates a JSON schema that matches a function-before schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1058, "code": "    def function_after_schema(self, schema: core_schema.AfterValidatorFunctionSchema) -> JsonSchemaValue:\n        return self.generate_inner(schema['schema'])", "documentation": "        \"\"\"Generates a JSON schema that matches a function-after schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1069, "code": "    def function_plain_schema(self, schema: core_schema.PlainValidatorFunctionSchema) -> JsonSchemaValue:\n        if self.mode == 'validation' and (input_schema := schema.get('json_schema_input_schema')):\n            return self.generate_inner(input_schema)\n        return self.handle_invalid_for_json_schema(\n            schema, f'core_schema.PlainValidatorFunctionSchema ({schema[\"function\"]})'\n        )", "documentation": "        \"\"\"Generates a JSON schema that matches a function-plain schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1085, "code": "    def function_wrap_schema(self, schema: core_schema.WrapValidatorFunctionSchema) -> JsonSchemaValue:\n        if self.mode == 'validation' and (input_schema := schema.get('json_schema_input_schema')):\n            return self.generate_inner(input_schema)\n        return self.generate_inner(schema['schema'])", "documentation": "        \"\"\"Generates a JSON schema that matches a function-wrap schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1099, "code": "    def default_schema(self, schema: core_schema.WithDefaultSchema) -> JsonSchemaValue:\n        json_schema = self.generate_inner(schema['schema'])\n        default = self.get_default_value(schema)\n        if default is NoDefault:\n            return json_schema\n        if self.mode == 'serialization':\n            ser_schema = _get_ser_schema_for_default_value(schema['schema'])\n            if (\n                ser_schema is not None\n                and (ser_func := ser_schema.get('function'))\n                and not (default is None and ser_schema.get('when_used') in ('unless-none', 'json-unless-none'))", "documentation": "        \"\"\"Generates a JSON schema that matches a schema with a default value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1153, "code": "    def get_default_value(self, schema: core_schema.WithDefaultSchema) -> Any:\n        return schema.get('default', NoDefault)", "documentation": "        \"\"\"Get the default value to be used when generating a JSON Schema for a core schema with a default.\n\n        The default implementation is to use the statically defined default value. This method can be overridden\n        if you want to make use of the default factory.\n\n        Args:\n            schema: The `'with-default'` core schema.\n\n        Returns:\n            The default value to use, or [`NoDefault`][pydantic.json_schema.NoDefault] if no default\n                value is available.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1168, "code": "    def nullable_schema(self, schema: core_schema.NullableSchema) -> JsonSchemaValue:\n        null_schema = {'type': 'null'}\n        inner_json_schema = self.generate_inner(schema['schema'])\n        if inner_json_schema == null_schema:\n            return null_schema\n        else:\n            return self.get_flattened_anyof([inner_json_schema, null_schema])", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that allows null values.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1187, "code": "    def union_schema(self, schema: core_schema.UnionSchema) -> JsonSchemaValue:\n        generated: list[JsonSchemaValue] = []\n        choices = schema['choices']\n        for choice in choices:\n            choice_schema = choice[0] if isinstance(choice, tuple) else choice\n            try:\n                generated.append(self.generate_inner(choice_schema))\n            except PydanticOmit:\n                continue\n            except PydanticInvalidForJsonSchema as exc:\n                self.emit_warning('skipped-choice', exc.message)", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that allows values matching any of the given schemas.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1212, "code": "    def tagged_union_schema(self, schema: core_schema.TaggedUnionSchema) -> JsonSchemaValue:\n        generated: dict[str, JsonSchemaValue] = {}\n        for k, v in schema['choices'].items():\n            if isinstance(k, Enum):\n                k = k.value\n            try:\n                generated[str(k)] = self.generate_inner(v).copy()\n            except PydanticOmit:\n                continue\n            except PydanticInvalidForJsonSchema as exc:\n                self.emit_warning('skipped-choice', exc.message)", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that allows values matching any of the given schemas, where\n        the schemas are tagged with a discriminator field that indicates which schema should be used to validate\n        the value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1294, "code": "    def chain_schema(self, schema: core_schema.ChainSchema) -> JsonSchemaValue:\n        step_index = 0 if self.mode == 'validation' else -1  # use first step for validation, last for serialization\n        return self.generate_inner(schema['steps'][step_index])", "documentation": "        \"\"\"Generates a JSON schema that matches a core_schema.ChainSchema.\n\n        When generating a schema for validation, we return the validation JSON schema for the first step in the chain.\n        For serialization, we return the serialization JSON schema for the last step in the chain.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1309, "code": "    def lax_or_strict_schema(self, schema: core_schema.LaxOrStrictSchema) -> JsonSchemaValue:\n        use_strict = schema.get('strict', False)  # TODO: replace this default False\n        if use_strict:\n            return self.generate_inner(schema['strict_schema'])\n        else:\n            return self.generate_inner(schema['lax_schema'])", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that allows values matching either the lax schema or the\n        strict schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1328, "code": "    def json_or_python_schema(self, schema: core_schema.JsonOrPythonSchema) -> JsonSchemaValue:\n        return self.generate_inner(schema['json_schema'])", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that allows values matching either the JSON schema or the\n        Python schema.\n\n        The JSON schema is used instead of the Python schema. If you want to use the Python schema, you should override\n        this method.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1343, "code": "    def typed_dict_schema(self, schema: core_schema.TypedDictSchema) -> JsonSchemaValue:\n        total = schema.get('total', True)\n        named_required_fields: list[tuple[str, bool, CoreSchemaField]] = [\n            (name, self.field_is_required(field, total), field)\n            for name, field in schema['fields'].items()\n            if self.field_is_present(field)\n        ]\n        if self.mode == 'serialization':\n            named_required_fields.extend(self._name_required_computed_fields(schema.get('computed_fields', [])))\n        cls = schema.get('cls')\n        config = _get_typed_dict_config(cls)", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a typed dict.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1428, "code": "    def typed_dict_field_schema(self, schema: core_schema.TypedDictField) -> JsonSchemaValue:\n        return self.generate_inner(schema['schema'])", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a typed dict field.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1439, "code": "    def dataclass_field_schema(self, schema: core_schema.DataclassField) -> JsonSchemaValue:\n        return self.generate_inner(schema['schema'])", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a dataclass field.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1450, "code": "    def model_field_schema(self, schema: core_schema.ModelField) -> JsonSchemaValue:\n        return self.generate_inner(schema['schema'])", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a model field.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1461, "code": "    def computed_field_schema(self, schema: core_schema.ComputedField) -> JsonSchemaValue:\n        return self.generate_inner(schema['return_schema'])", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a computed field.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1472, "code": "    def model_schema(self, schema: core_schema.ModelSchema) -> JsonSchemaValue:\n        cls = cast('type[BaseModel]', schema['cls'])\n        config = cls.model_config\n        with self._config_wrapper_stack.push(config):\n            json_schema = self.generate_inner(schema['schema'])\n        self._update_class_schema(json_schema, cls, config)\n        return json_schema", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a model.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1493, "code": "    def _update_class_schema(self, json_schema: JsonSchemaValue, cls: type[Any], config: ConfigDict) -> None:\n        from .main import BaseModel\n        from .root_model import RootModel\n        if (config_title := config.get('title')) is not None:\n            json_schema.setdefault('title', config_title)\n        elif model_title_generator := config.get('model_title_generator'):\n            title = model_title_generator(cls)\n            if not isinstance(title, str):\n                raise TypeError(f'model_title_generator {model_title_generator} must return str, not {title.__class__}')\n            json_schema.setdefault('title', title)\n        if 'title' not in json_schema:", "documentation": "        \"\"\"Update json_schema with the following, extracted from `config` and `cls`:\n\n        * title\n        * description\n        * additional properties\n        * json_schema_extra\n        * deprecated\n\n        Done in place, hence there's no return value as the original json_schema is mutated.\n        No ref resolving is involved here, as that's not appropriate for simple updates.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1564, "code": "    def resolve_ref_schema(self, json_schema: JsonSchemaValue) -> JsonSchemaValue:\n        while '$ref' in json_schema:\n            ref = json_schema['$ref']\n            schema_to_update = self.get_schema_from_definitions(JsonRef(ref))\n            if schema_to_update is None:\n                raise RuntimeError(f'Cannot update undefined schema for $ref={ref}')\n            json_schema = schema_to_update\n        return json_schema", "documentation": "        \"\"\"Resolve a JsonSchemaValue to the non-ref schema if it is a $ref schema.\n\n        Args:\n            json_schema: The schema to resolve.\n\n        Returns:\n            The resolved schema.\n\n        Raises:\n            RuntimeError: If the schema reference can't be found in definitions.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1584, "code": "    def model_fields_schema(self, schema: core_schema.ModelFieldsSchema) -> JsonSchemaValue:\n        named_required_fields: list[tuple[str, bool, CoreSchemaField]] = [\n            (name, self.field_is_required(field, total=True), field)\n            for name, field in schema['fields'].items()\n            if self.field_is_present(field)\n        ]\n        if self.mode == 'serialization':\n            named_required_fields.extend(self._name_required_computed_fields(schema.get('computed_fields', [])))\n        json_schema = self._named_required_fields_schema(named_required_fields)\n        extras_schema = schema.get('extras_schema', None)\n        if extras_schema is not None:", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a model's fields.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1607, "code": "    def field_is_present(self, field: CoreSchemaField) -> bool:\n        if self.mode == 'serialization':\n            return not field.get('serialization_exclude')\n        elif self.mode == 'validation':\n            return True\n        else:\n            assert_never(self.mode)", "documentation": "        \"\"\"Whether the field should be included in the generated JSON schema.\n\n        Args:\n            field: The schema for the field itself.\n\n        Returns:\n            `True` if the field should be included in the generated JSON schema, `False` otherwise.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1650, "code": "    def dataclass_args_schema(self, schema: core_schema.DataclassArgsSchema) -> JsonSchemaValue:\n        named_required_fields: list[tuple[str, bool, CoreSchemaField]] = [\n            (field['name'], self.field_is_required(field, total=True), field)\n            for field in schema['fields']\n            if self.field_is_present(field)\n        ]\n        if self.mode == 'serialization':\n            named_required_fields.extend(self._name_required_computed_fields(schema.get('computed_fields', [])))\n        return self._named_required_fields_schema(named_required_fields)", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a dataclass's constructor arguments.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1668, "code": "    def dataclass_schema(self, schema: core_schema.DataclassSchema) -> JsonSchemaValue:\n        from ._internal._dataclasses import is_builtin_dataclass\n        cls = schema['cls']\n        config: ConfigDict = getattr(cls, '__pydantic_config__', cast('ConfigDict', {}))\n        with self._config_wrapper_stack.push(config):\n            json_schema = self.generate_inner(schema['schema']).copy()\n        self._update_class_schema(json_schema, cls, config)\n        if is_builtin_dataclass(cls):\n            description = None\n        else:\n            description = None if cls.__doc__ is None else inspect.cleandoc(cls.__doc__)", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a dataclass.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1698, "code": "    def arguments_schema(self, schema: core_schema.ArgumentsSchema) -> JsonSchemaValue:\n        prefer_positional = schema.get('metadata', {}).get('pydantic_js_prefer_positional_arguments')\n        arguments = schema['arguments_schema']\n        kw_only_arguments = [a for a in arguments if a.get('mode') == 'keyword_only']\n        kw_or_p_arguments = [a for a in arguments if a.get('mode') in {'positional_or_keyword', None}]\n        p_only_arguments = [a for a in arguments if a.get('mode') == 'positional_only']\n        var_args_schema = schema.get('var_args_schema')\n        var_kwargs_schema = schema.get('var_kwargs_schema')\n        if prefer_positional:\n            positional_possible = not kw_only_arguments and not var_kwargs_schema\n            if positional_possible:", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a function's arguments.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1813, "code": "    def get_argument_name(self, argument: core_schema.ArgumentsParameter | core_schema.ArgumentsV3Parameter) -> str:\n        name = argument['name']\n        if self.by_alias:\n            alias = argument.get('alias')\n            if isinstance(alias, str):\n                name = alias\n            else:\n                pass  # might want to do something else?\n        return name", "documentation": "        \"\"\"Retrieves the name of an argument.\n\n        Args:\n            argument: The core schema.\n\n        Returns:\n            The name of the argument.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1831, "code": "    def arguments_v3_schema(self, schema: core_schema.ArgumentsV3Schema) -> JsonSchemaValue:\n        arguments = schema['arguments_schema']\n        properties: dict[str, JsonSchemaValue] = {}\n        required: list[str] = []\n        for argument in arguments:\n            mode = argument.get('mode', 'positional_or_keyword')\n            name = self.get_argument_name(argument)\n            argument_schema = self.generate_inner(argument['schema']).copy()\n            if mode == 'var_args':\n                argument_schema = {'type': 'array', 'items': argument_schema}\n            elif mode == 'var_kwargs_uniform':", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a function's arguments.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1870, "code": "    def call_schema(self, schema: core_schema.CallSchema) -> JsonSchemaValue:\n        return self.generate_inner(schema['arguments_schema'])", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a function call.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1881, "code": "    def custom_error_schema(self, schema: core_schema.CustomErrorSchema) -> JsonSchemaValue:\n        return self.generate_inner(schema['schema'])", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a custom error.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1892, "code": "    def json_schema(self, schema: core_schema.JsonSchema) -> JsonSchemaValue:\n        content_core_schema = schema.get('schema') or core_schema.any_schema()\n        content_json_schema = self.generate_inner(content_core_schema)\n        if self.mode == 'validation':\n            return {'type': 'string', 'contentMediaType': 'application/json', 'contentSchema': content_json_schema}\n        else:\n            return content_json_schema", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a JSON object.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1909, "code": "    def url_schema(self, schema: core_schema.UrlSchema) -> JsonSchemaValue:\n        json_schema = {'type': 'string', 'format': 'uri', 'minLength': 1}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.string)\n        return json_schema", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a URL.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1922, "code": "    def multi_host_url_schema(self, schema: core_schema.MultiHostUrlSchema) -> JsonSchemaValue:\n        json_schema = {'type': 'string', 'format': 'multi-host-uri', 'minLength': 1}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.string)\n        return json_schema", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a URL that can be used with multiple hosts.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1936, "code": "    def uuid_schema(self, schema: core_schema.UuidSchema) -> JsonSchemaValue:\n        return {'type': 'string', 'format': 'uuid'}", "documentation": "        \"\"\"Generates a JSON schema that matches a UUID.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1947, "code": "    def definitions_schema(self, schema: core_schema.DefinitionsSchema) -> JsonSchemaValue:\n        for definition in schema['definitions']:\n            try:\n                self.generate_inner(definition)\n            except PydanticInvalidForJsonSchema as e:\n                core_ref: CoreRef = CoreRef(definition['ref'])  # type: ignore\n                self._core_defs_invalid_for_json_schema[self.get_defs_ref((core_ref, self.mode))] = e\n                continue\n        return self.generate_inner(schema['schema'])", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a JSON object with definitions.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1965, "code": "    def definition_ref_schema(self, schema: core_schema.DefinitionReferenceSchema) -> JsonSchemaValue:\n        core_ref = CoreRef(schema['schema_ref'])\n        _, ref_json_schema = self.get_cache_defs_ref_schema(core_ref)\n        return ref_json_schema", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that references a definition.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2003, "code": "    def complex_schema(self, schema: core_schema.ComplexSchema) -> JsonSchemaValue:\n        return {'type': 'string'}", "documentation": "        \"\"\"Generates a JSON schema that matches a complex number.\n\n        JSON has no standard way to represent complex numbers. Complex number is not a numeric\n        type. Here we represent complex number as strings following the rule defined by Python.\n        For instance, '1+2j' is an accepted complex string. Details can be found in\n        [Python's `complex` documentation][complex].\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2021, "code": "    def get_title_from_name(self, name: str) -> str:\n        return name.title().replace('_', ' ').strip()", "documentation": "        \"\"\"Retrieves a title from a name.\n\n        Args:\n            name: The name to retrieve a title from.\n\n        Returns:\n            The title.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2032, "code": "    def field_title_should_be_set(self, schema: CoreSchemaOrField) -> bool:\n        if _core_utils.is_core_schema_field(schema):\n            if schema['type'] == 'computed-field':\n                field_schema = schema['return_schema']\n            else:\n                field_schema = schema['schema']\n            return self.field_title_should_be_set(field_schema)\n        elif _core_utils.is_core_schema(schema):\n            if schema.get('ref'):  # things with refs, such as models and enums, should not have titles set\n                return False\n            if schema['type'] in {'default', 'nullable', 'definitions'}:", "documentation": "        \"\"\"Returns true if a field with the given schema should have a title set based on the field name.\n\n        Intuitively, we want this to return true for schemas that wouldn't otherwise provide their own title\n        (e.g., int, float, str), and false for those that would (e.g., BaseModel subclasses).\n\n        Args:\n            schema: The schema to check.\n\n        Returns:\n            `True` if the field should have a title set, `False` otherwise.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2067, "code": "    def normalize_name(self, name: str) -> str:\n        return re.sub(r'[^a-zA-Z0-9.\\-_]', '_', name).replace('.', '__')", "documentation": "        \"\"\"Normalizes a name to be used as a key in a dictionary.\n\n        Args:\n            name: The name to normalize.\n\n        Returns:\n            The normalized name.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2078, "code": "    def get_defs_ref(self, core_mode_ref: CoreModeRef) -> DefsRef:\n        core_ref, mode = core_mode_ref\n        components = re.split(r'([\\][,])', core_ref)\n        components = [x.rsplit(':', 1)[0] for x in components]\n        core_ref_no_id = ''.join(components)\n        components = [re.sub(r'(?:[^.[\\]]+\\.)+((?:[^.[\\]]+))', r'\\1', x) for x in components]\n        short_ref = ''.join(components)\n        mode_title = _MODE_TITLE_MAPPING[mode]\n        name = DefsRef(self.normalize_name(short_ref))\n        name_mode = DefsRef(self.normalize_name(short_ref) + f'-{mode_title}')\n        module_qualname = DefsRef(self.normalize_name(core_ref_no_id))", "documentation": "        \"\"\"Override this method to change the way that definitions keys are generated from a core reference.\n\n        Args:\n            core_mode_ref: The core reference.\n\n        Returns:\n            The definitions key.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2126, "code": "    def get_cache_defs_ref_schema(self, core_ref: CoreRef) -> tuple[DefsRef, JsonSchemaValue]:\n        core_mode_ref = (core_ref, self.mode)\n        maybe_defs_ref = self.core_to_defs_refs.get(core_mode_ref)\n        if maybe_defs_ref is not None:\n            json_ref = self.core_to_json_refs[core_mode_ref]\n            return maybe_defs_ref, {'$ref': json_ref}\n        defs_ref = self.get_defs_ref(core_mode_ref)\n        self.core_to_defs_refs[core_mode_ref] = defs_ref\n        self.defs_to_core_refs[defs_ref] = core_mode_ref\n        json_ref = JsonRef(self.ref_template.format(model=defs_ref))\n        self.core_to_json_refs[core_mode_ref] = json_ref", "documentation": "        \"\"\"This method wraps the get_defs_ref method with some cache-lookup/population logic,\n        and returns both the produced defs_ref and the JSON schema that will refer to the right definition.\n\n        Args:\n            core_ref: The core reference to get the definitions reference for.\n\n        Returns:\n            A tuple of the definitions reference and the JSON schema that will refer to it.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2154, "code": "    def handle_ref_overrides(self, json_schema: JsonSchemaValue) -> JsonSchemaValue:\n        if '$ref' in json_schema:\n            json_schema = json_schema.copy()\n            referenced_json_schema = self.get_schema_from_definitions(JsonRef(json_schema['$ref']))\n            if referenced_json_schema is None:\n                return json_schema\n            for k, v in list(json_schema.items()):\n                if k == '$ref':\n                    continue\n                if k in referenced_json_schema and referenced_json_schema[k] == v:\n                    del json_schema[k]  # redundant key", "documentation": "        \"\"\"Remove any sibling keys that are redundant with the referenced schema.\n\n        Args:\n            json_schema: The schema to remove redundant sibling keys from.\n\n        Returns:\n            The schema with redundant sibling keys removed.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2192, "code": "    def encode_default(self, dft: Any) -> Any:\n        from .type_adapter import TypeAdapter, _type_has_config\n        config = self._config\n        try:\n            default = (\n                dft\n                if _type_has_config(type(dft))\n                else TypeAdapter(type(dft), config=config.config_dict).dump_python(\n                    dft, by_alias=self.by_alias, mode='json'\n                )\n            )", "documentation": "        \"\"\"Encode a default value to a JSON-serializable value.\n\n        This is used to encode default values for fields in the generated JSON schema.\n\n        Args:\n            dft: The default value to encode.\n\n        Returns:\n            The encoded default value.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2236, "code": "    class ValidationsMapping:\n        numeric = {\n            'multiple_of': 'multipleOf',\n            'le': 'maximum',\n            'ge': 'minimum',\n            'lt': 'exclusiveMaximum',\n            'gt': 'exclusiveMinimum',\n        }\n        bytes = {\n            'min_length': 'minLength',\n            'max_length': 'maxLength',", "documentation": "        \"\"\"This class just contains mappings from core_schema attribute names to the corresponding\n        JSON schema attribute names. While I suspect it is unlikely to be necessary, you can in\n        principle override this class in a subclass of GenerateJsonSchema (by inheriting from\n        GenerateJsonSchema.ValidationsMapping) to change these mappings.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2280, "code": "    def get_json_ref_counts(self, json_schema: JsonSchemaValue) -> dict[JsonRef, int]:\n        json_refs: dict[JsonRef, int] = Counter()", "documentation": "        \"\"\"Get all values corresponding to the key '$ref' anywhere in the json_schema.\"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2319, "code": "    def emit_warning(self, kind: JsonSchemaWarningKind, detail: str) -> None:\n        message = self.render_warning_message(kind, detail)\n        if message is not None:\n            warnings.warn(message, PydanticJsonSchemaWarning)", "documentation": "        \"\"\"This method simply emits PydanticJsonSchemaWarnings based on handling in the `warning_message` method.\"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2325, "code": "    def render_warning_message(self, kind: JsonSchemaWarningKind, detail: str) -> str | None:\n        if kind in self.ignored_warning_kinds:\n            return None\n        return f'{detail} [{kind}]'", "documentation": "        \"\"\"This method is responsible for ignoring warnings as desired, and for formatting the warning messages.\n\n        You can override the value of `ignored_warning_kinds` in a subclass of GenerateJsonSchema\n        to modify what warnings are generated. If you want more control, you can override this method;\n        just return None in situations where you don't want warnings to be emitted.\n\n        Args:\n            kind: The kind of warning to render. It can be one of the following:\n\n                - 'skipped-choice': A choice field was skipped because it had no valid choices.\n                - 'non-serializable-default': A default value was skipped because it was not JSON-serializable.\n            detail: A string with additional details about the warning.\n\n        Returns:\n            The formatted warning message, or `None` if no warning should be emitted.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2485, "code": "class WithJsonSchema:\n    json_schema: JsonSchemaValue | None\n    mode: Literal['validation', 'serialization'] | None = None", "documentation": "    \"\"\"!!! abstract \"Usage Documentation\"\n        [`WithJsonSchema` Annotation](../concepts/json_schema.md#withjsonschema-annotation)\n\n    Add this as an annotation on a field to override the (base) JSON schema that would be generated for that field.\n    This provides a way to set a JSON schema for types that would otherwise raise errors when producing a JSON schema,\n    such as Callable, or types that have an is-instance core schema, without needing to go so far as creating a\n    custom subclass of pydantic.json_schema.GenerateJsonSchema.\n    Note that any _modifications_ to the schema that would normally be made (such as setting the title for model fields)\n    will still be performed.\n\n    If `mode` is set this will only apply to that schema generation mode, allowing you\n    to set different json schemas for validation and serialization.\n    \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2519, "code": "class Examples:\n    @overload\n    @deprecated('Using a dict for `examples` is deprecated since v2.9 and will be removed in v3.0. Use a list instead.')", "documentation": "    \"\"\"Add examples to a JSON schema.\n\n    If the JSON Schema already contains examples, the provided examples\n    will be appended.\n\n    If `mode` is set this will only apply to that schema generation mode,\n    allowing you to add different examples for validation and serialization.\n    \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2593, "code": "def _get_all_json_refs(item: Any) -> set[JsonRef]:\n    refs: set[JsonRef] = set()\n    stack = [item]\n    while stack:\n        current = stack.pop()\n        if isinstance(current, dict):\n            for key, value in current.items():\n                if key == 'examples' and isinstance(value, list):\n                    continue\n                if key == '$ref' and isinstance(value, str):\n                    refs.add(JsonRef(value))", "documentation": "    \"\"\"Get all the definitions references from a JSON schema.\"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2628, "code": "    class SkipJsonSchema:", "documentation": "        \"\"\"!!! abstract \"Usage Documentation\"\n            [`SkipJsonSchema` Annotation](../concepts/json_schema.md#skipjsonschema-annotation)\n\n        Add this as an annotation on a field to skip generating a JSON schema for that field.\n\n        Example:\n            ```python\n            from pprint import pprint\n            from typing import Union\n\n            from pydantic import BaseModel\n            from pydantic.json_schema import SkipJsonSchema\n\n            class Model(BaseModel):\n                a: Union[int, None] = None  # (1)!\n                b: Union[int, SkipJsonSchema[None]] = None  # (2)!\n                c: SkipJsonSchema[Union[int, None]] = None  # (3)!\n\n            pprint(Model.model_json_schema())\n            '''\n            {\n                'properties': {\n                    'a': {\n                        'anyOf': [\n                            {'type': 'integer'},\n                            {'type': 'null'}\n                        ],\n                        'default': None,\n                        'title': 'A'\n                    },\n                    'b': {\n                        'default': None,\n                        'title': 'B',\n                        'type': 'integer'\n                    }\n                },\n                'title': 'Model',\n                'type': 'object'\n            }\n            '''\n            ```\n\n            1. The integer and null types are both included in the schema for `a`.\n            2. The integer type is the only type included in the schema for `b`.\n            3. The entirety of the `c` field is omitted from the schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2697, "code": "def _get_ser_schema_for_default_value(schema: CoreSchema) -> core_schema.PlainSerializerFunctionSerSchema | None:\n    if (\n        (ser_schema := schema.get('serialization'))\n        and ser_schema['type'] == 'function-plain'\n        and not ser_schema.get('info_arg')\n    ):\n        return ser_schema\n    if _core_utils.is_function_with_inner_schema(schema):\n        return _get_ser_schema_for_default_value(schema['schema'])", "documentation": "    \"\"\"Get a `'function-plain'` serialization schema that can be used to serialize a default value.\n\n    This takes into account having the serialization schema nested under validation schema(s).\n    \"\"\""}], "after_segments": [{"filename": "pydantic/json_schema.py", "start_line": 100, "code": "class PydanticJsonSchemaWarning(UserWarning):\nNoDefault = object()\n\"\"\"A sentinel value used to indicate that no default value should be used when generating a JSON Schema\nfor a core schema with a default value.\n\"\"\"\nDEFAULT_REF_TEMPLATE = '#/$defs/{model}'\n\"\"\"The default format string used to generate reference names.\"\"\"\nCoreRef = NewType('CoreRef', str)\nDefsRef = NewType('DefsRef', str)\nJsonRef = NewType('JsonRef', str)\nCoreModeRef = tuple[CoreRef, JsonSchemaMode]", "documentation": "    \"\"\"This class is used to emit warnings produced during JSON schema generation.\n    See the [`GenerateJsonSchema.emit_warning`][pydantic.json_schema.GenerateJsonSchema.emit_warning] and\n    [`GenerateJsonSchema.render_warning_message`][pydantic.json_schema.GenerateJsonSchema.render_warning_message]\n    methods for more details; these can be overridden to control warning behavior.\n    \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 192, "code": "    def remap_json_schema(self, schema: Any) -> Any:\n        if isinstance(schema, str):\n            return self.remap_json_ref(JsonRef(schema))\n        elif isinstance(schema, list):\n            return [self.remap_json_schema(item) for item in schema]\n        elif isinstance(schema, dict):\n            for key, value in schema.items():\n                if key == '$ref' and isinstance(value, str):\n                    schema['$ref'] = self.remap_json_ref(JsonRef(value))\n                elif key == '$defs':\n                    schema['$defs'] = {", "documentation": "        \"\"\"\n        Recursively update the JSON schema replacing all $refs\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 215, "code": "class GenerateJsonSchema:\n    schema_dialect = 'https://json-schema.org/draft/2020-12/schema'\n    ignored_warning_kinds: set[JsonSchemaWarningKind] = {'skipped-choice'}", "documentation": "    \"\"\"!!! abstract \"Usage Documentation\"\n        [Customizing the JSON Schema Generation Process](../concepts/json_schema.md#customizing-the-json-schema-generation-process)\n\n    A class for generating JSON schemas.\n\n    This class generates JSON schemas based on configured parameters. The default schema dialect\n    is [https://json-schema.org/draft/2020-12/schema](https://json-schema.org/draft/2020-12/schema).\n    The class uses `by_alias` to configure how fields with\n    multiple names are handled and `ref_template` to format reference names.\n\n    Attributes:\n        schema_dialect: The JSON schema dialect used to generate the schema. See\n            [Declaring a Dialect](https://json-schema.org/understanding-json-schema/reference/schema.html#id4)\n            in the JSON Schema documentation for more information about dialects.\n        ignored_warning_kinds: Warnings to ignore when generating the schema. `self.render_warning_message` will\n            do nothing if its argument `kind` is in `ignored_warning_kinds`;\n            this value can be modified on subclasses to easily control which warnings are emitted.\n        by_alias: Whether to use field aliases when generating the schema.\n        ref_template: The format string used when generating reference names.\n        core_to_json_refs: A mapping of core refs to JSON refs.\n        core_to_defs_refs: A mapping of core refs to definition refs.\n        defs_to_core_refs: A mapping of definition refs to core refs.\n        json_to_defs_refs: A mapping of JSON refs to definition refs.\n        definitions: Definitions in the schema.\n\n    Args:\n        by_alias: Whether to use field aliases in the generated schemas.\n        ref_template: The format string to use when generating reference names.\n\n    Raises:\n        JsonSchemaError: If the instance of the class is inadvertently reused after generating a schema.\n    \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 377, "code": "    def generate(self, schema: CoreSchema, mode: JsonSchemaMode = 'validation') -> JsonSchemaValue:\n        self._mode = mode\n        if self._used:\n            raise PydanticUserError(\n                'This JSON schema generator has already been used to generate a JSON schema. '\n                f'You must create a new instance of {type(self).__name__} to generate a new JSON schema.',\n                code='json-schema-already-used',\n            )\n        json_schema: JsonSchemaValue = self.generate_inner(schema)\n        json_ref_counts = self.get_json_ref_counts(json_schema)\n        ref = cast(JsonRef, json_schema.get('$ref'))", "documentation": "        \"\"\"Generates a JSON schema for a specified schema in a specified mode.\n\n        Args:\n            schema: A Pydantic model.\n            mode: The mode in which to generate the schema. Defaults to 'validation'.\n\n        Returns:\n            A JSON schema representing the specified schema.\n\n        Raises:\n            PydanticUserError: If the JSON schema generator has already been used to generate a JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 426, "code": "    def generate_inner(self, schema: CoreSchemaOrField) -> JsonSchemaValue:  # noqa: C901\n        if 'ref' in schema:\n            core_ref = CoreRef(schema['ref'])  # type: ignore[typeddict-item]\n            core_mode_ref = (core_ref, self.mode)\n            if core_mode_ref in self.core_to_defs_refs and self.core_to_defs_refs[core_mode_ref] in self.definitions:\n                return {'$ref': self.core_to_json_refs[core_mode_ref]}", "documentation": "        \"\"\"Generates a JSON schema for a given core schema.\n\n        Args:\n            schema: The given core schema.\n\n        Returns:\n            The generated JSON schema.\n\n        TODO: the nested function definitions here seem like bad practice, I'd like to unpack these\n        in a future PR. It'd be great if we could shorten the call stack a bit for JSON schema generation,\n        and I think there's potential for that here.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 462, "code": "        def handler_func(schema_or_field: CoreSchemaOrField) -> JsonSchemaValue:\n            json_schema: JsonSchemaValue | None = None\n            if self.mode == 'serialization' and 'serialization' in schema_or_field:\n                ser_schema = schema_or_field['serialization']  # type: ignore\n                json_schema = self.ser_schema(ser_schema)\n                if (\n                    json_schema is not None\n                    and ser_schema.get('when_used') in ('unless-none', 'json-unless-none')\n                    and schema_or_field['type'] == 'nullable'\n                ):\n                    json_schema = self.get_flattened_anyof([{'type': 'null'}, json_schema])", "documentation": "            \"\"\"Generate a JSON schema based on the input schema.\n\n            Args:\n                schema_or_field: The core schema to generate a JSON schema from.\n\n            Returns:\n                The generated JSON schema.\n\n            Raises:\n                TypeError: If an unexpected schema type is encountered.\n            \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 567, "code": "    def sort(self, value: JsonSchemaValue, parent_key: str | None = None) -> JsonSchemaValue:\n        sorted_dict: dict[str, JsonSchemaValue] = {}\n        keys = value.keys()\n        if parent_key not in ('properties', 'default'):\n            keys = sorted(keys)\n        for key in keys:\n            sorted_dict[key] = self._sort_recursive(value[key], parent_key=key)\n        return sorted_dict", "documentation": "        \"\"\"Override this method to customize the sorting of the JSON schema (e.g., don't sort at all, sort all keys unconditionally, etc.)\n\n        By default, alphabetically sort the keys in the JSON schema, skipping the 'properties' and 'default' keys to preserve field definition order.\n        This sort is recursive, so it will sort all nested dictionaries as well.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 581, "code": "    def _sort_recursive(self, value: Any, parent_key: str | None = None) -> Any:\n        if isinstance(value, dict):\n            sorted_dict: dict[str, JsonSchemaValue] = {}\n            keys = value.keys()\n            if parent_key not in ('properties', 'default'):\n                keys = sorted(keys)\n            for key in keys:\n                sorted_dict[key] = self._sort_recursive(value[key], parent_key=key)\n            return sorted_dict\n        elif isinstance(value, list):\n            sorted_list: list[JsonSchemaValue] = []", "documentation": "        \"\"\"Recursively sort a JSON schema value.\"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 601, "code": "    def invalid_schema(self, schema: core_schema.InvalidSchema) -> JsonSchemaValue:\n        raise RuntimeError('Cannot generate schema for invalid_schema. This is a bug! Please report it.')", "documentation": "        \"\"\"Placeholder - should never be called.\"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 606, "code": "    def any_schema(self, schema: core_schema.AnySchema) -> JsonSchemaValue:\n        return {}", "documentation": "        \"\"\"Generates a JSON schema that matches any value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 617, "code": "    def none_schema(self, schema: core_schema.NoneSchema) -> JsonSchemaValue:\n        return {'type': 'null'}", "documentation": "        \"\"\"Generates a JSON schema that matches `None`.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 628, "code": "    def bool_schema(self, schema: core_schema.BoolSchema) -> JsonSchemaValue:\n        return {'type': 'boolean'}", "documentation": "        \"\"\"Generates a JSON schema that matches a bool value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 639, "code": "    def int_schema(self, schema: core_schema.IntSchema) -> JsonSchemaValue:\n        json_schema: dict[str, Any] = {'type': 'integer'}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.numeric)\n        json_schema = {k: v for k, v in json_schema.items() if v not in {math.inf, -math.inf}}\n        return json_schema", "documentation": "        \"\"\"Generates a JSON schema that matches an int value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 653, "code": "    def float_schema(self, schema: core_schema.FloatSchema) -> JsonSchemaValue:\n        json_schema: dict[str, Any] = {'type': 'number'}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.numeric)\n        json_schema = {k: v for k, v in json_schema.items() if v not in {math.inf, -math.inf}}\n        return json_schema", "documentation": "        \"\"\"Generates a JSON schema that matches a float value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 667, "code": "    def decimal_schema(self, schema: core_schema.DecimalSchema) -> JsonSchemaValue:\n        json_schema = self.str_schema(core_schema.str_schema())\n        if self.mode == 'validation':\n            multiple_of = schema.get('multiple_of')\n            le = schema.get('le')\n            ge = schema.get('ge')\n            lt = schema.get('lt')\n            gt = schema.get('gt')\n            json_schema = {\n                'anyOf': [\n                    self.float_schema(", "documentation": "        \"\"\"Generates a JSON schema that matches a decimal value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 700, "code": "    def str_schema(self, schema: core_schema.StringSchema) -> JsonSchemaValue:\n        json_schema = {'type': 'string'}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.string)\n        if isinstance(json_schema.get('pattern'), Pattern):\n            json_schema['pattern'] = json_schema.get('pattern').pattern  # type: ignore\n        return json_schema", "documentation": "        \"\"\"Generates a JSON schema that matches a string value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 716, "code": "    def bytes_schema(self, schema: core_schema.BytesSchema) -> JsonSchemaValue:\n        json_schema = {'type': 'string', 'format': 'base64url' if self._config.ser_json_bytes == 'base64' else 'binary'}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.bytes)\n        return json_schema", "documentation": "        \"\"\"Generates a JSON schema that matches a bytes value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 729, "code": "    def date_schema(self, schema: core_schema.DateSchema) -> JsonSchemaValue:\n        return {'type': 'string', 'format': 'date'}", "documentation": "        \"\"\"Generates a JSON schema that matches a date value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 740, "code": "    def time_schema(self, schema: core_schema.TimeSchema) -> JsonSchemaValue:\n        return {'type': 'string', 'format': 'time'}", "documentation": "        \"\"\"Generates a JSON schema that matches a time value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 751, "code": "    def datetime_schema(self, schema: core_schema.DatetimeSchema) -> JsonSchemaValue:\n        return {'type': 'string', 'format': 'date-time'}", "documentation": "        \"\"\"Generates a JSON schema that matches a datetime value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 762, "code": "    def timedelta_schema(self, schema: core_schema.TimedeltaSchema) -> JsonSchemaValue:\n        if self._config.ser_json_timedelta == 'float':\n            return {'type': 'number'}\n        return {'type': 'string', 'format': 'duration'}", "documentation": "        \"\"\"Generates a JSON schema that matches a timedelta value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 775, "code": "    def literal_schema(self, schema: core_schema.LiteralSchema) -> JsonSchemaValue:\n        expected = [to_jsonable_python(v.value if isinstance(v, Enum) else v) for v in schema['expected']]\n        result: dict[str, Any] = {}\n        if len(expected) == 1:\n            result['const'] = expected[0]\n        else:\n            result['enum'] = expected\n        types = {type(e) for e in expected}\n        if types == {str}:\n            result['type'] = 'string'\n        elif types == {int}:", "documentation": "        \"\"\"Generates a JSON schema that matches a literal value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 807, "code": "    def enum_schema(self, schema: core_schema.EnumSchema) -> JsonSchemaValue:\n        enum_type = schema['cls']\n        description = None if not enum_type.__doc__ else inspect.cleandoc(enum_type.__doc__)\n        if (\n            description == 'An enumeration.'\n        ):  # This is the default value provided by enum.EnumMeta.__new__; don't use it\n            description = None\n        result: dict[str, Any] = {'title': enum_type.__name__, 'description': description}\n        result = {k: v for k, v in result.items() if v is not None}\n        expected = [to_jsonable_python(v.value) for v in schema['members']]\n        result['enum'] = expected", "documentation": "        \"\"\"Generates a JSON schema that matches an Enum value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 843, "code": "    def is_instance_schema(self, schema: core_schema.IsInstanceSchema) -> JsonSchemaValue:\n        return self.handle_invalid_for_json_schema(schema, f'core_schema.IsInstanceSchema ({schema[\"cls\"]})')", "documentation": "        \"\"\"Handles JSON schema generation for a core schema that checks if a value is an instance of a class.\n\n        Unless overridden in a subclass, this raises an error.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 856, "code": "    def is_subclass_schema(self, schema: core_schema.IsSubclassSchema) -> JsonSchemaValue:\n        return {}", "documentation": "        \"\"\"Handles JSON schema generation for a core schema that checks if a value is a subclass of a class.\n\n        For backwards compatibility with v1, this does not raise an error, but can be overridden to change this.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 870, "code": "    def callable_schema(self, schema: core_schema.CallableSchema) -> JsonSchemaValue:\n        return self.handle_invalid_for_json_schema(schema, 'core_schema.CallableSchema')", "documentation": "        \"\"\"Generates a JSON schema that matches a callable value.\n\n        Unless overridden in a subclass, this raises an error.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 883, "code": "    def list_schema(self, schema: core_schema.ListSchema) -> JsonSchemaValue:\n        items_schema = {} if 'items_schema' not in schema else self.generate_inner(schema['items_schema'])\n        json_schema = {'type': 'array', 'items': items_schema}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.array)\n        return json_schema\n    @deprecated('`tuple_positional_schema` is deprecated. Use `tuple_schema` instead.', category=None)\n    @final", "documentation": "        \"\"\"Returns a schema that matches a list schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 899, "code": "    def tuple_positional_schema(self, schema: core_schema.TupleSchema) -> JsonSchemaValue:\n        warnings.warn(\n            '`tuple_positional_schema` is deprecated. Use `tuple_schema` instead.',\n            PydanticDeprecatedSince26,\n            stacklevel=2,\n        )\n        return self.tuple_schema(schema)\n    @deprecated('`tuple_variable_schema` is deprecated. Use `tuple_schema` instead.', category=None)\n    @final", "documentation": "        \"\"\"Replaced by `tuple_schema`.\"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 910, "code": "    def tuple_variable_schema(self, schema: core_schema.TupleSchema) -> JsonSchemaValue:\n        warnings.warn(\n            '`tuple_variable_schema` is deprecated. Use `tuple_schema` instead.',\n            PydanticDeprecatedSince26,\n            stacklevel=2,\n        )\n        return self.tuple_schema(schema)", "documentation": "        \"\"\"Replaced by `tuple_schema`.\"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 919, "code": "    def tuple_schema(self, schema: core_schema.TupleSchema) -> JsonSchemaValue:\n        json_schema: JsonSchemaValue = {'type': 'array'}\n        if 'variadic_item_index' in schema:\n            variadic_item_index = schema['variadic_item_index']\n            if variadic_item_index > 0:\n                json_schema['minItems'] = variadic_item_index\n                json_schema['prefixItems'] = [\n                    self.generate_inner(item) for item in schema['items_schema'][:variadic_item_index]\n                ]\n            if variadic_item_index + 1 == len(schema['items_schema']):\n                json_schema['items'] = self.generate_inner(schema['items_schema'][variadic_item_index])", "documentation": "        \"\"\"Generates a JSON schema that matches a tuple schema e.g. `tuple[int,\n        str, bool]` or `tuple[int, ...]`.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 954, "code": "    def set_schema(self, schema: core_schema.SetSchema) -> JsonSchemaValue:\n        return self._common_set_schema(schema)", "documentation": "        \"\"\"Generates a JSON schema that matches a set schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 965, "code": "    def frozenset_schema(self, schema: core_schema.FrozenSetSchema) -> JsonSchemaValue:\n        return self._common_set_schema(schema)", "documentation": "        \"\"\"Generates a JSON schema that matches a frozenset schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 982, "code": "    def generator_schema(self, schema: core_schema.GeneratorSchema) -> JsonSchemaValue:\n        items_schema = {} if 'items_schema' not in schema else self.generate_inner(schema['items_schema'])\n        json_schema = {'type': 'array', 'items': items_schema}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.array)\n        return json_schema", "documentation": "        \"\"\"Returns a JSON schema that represents the provided GeneratorSchema.\n\n        Args:\n            schema: The schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 996, "code": "    def dict_schema(self, schema: core_schema.DictSchema) -> JsonSchemaValue:\n        json_schema: JsonSchemaValue = {'type': 'object'}\n        keys_schema = self.generate_inner(schema['keys_schema']).copy() if 'keys_schema' in schema else {}\n        if '$ref' not in keys_schema:\n            keys_pattern = keys_schema.pop('pattern', None)\n            keys_schema.pop('title', None)\n        else:\n            keys_pattern = None\n        values_schema = self.generate_inner(schema['values_schema']).copy() if 'values_schema' in schema else {}\n        values_schema.pop('title', None)\n        if values_schema or keys_pattern is not None:", "documentation": "        \"\"\"Generates a JSON schema that matches a dict schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1044, "code": "    def function_before_schema(self, schema: core_schema.BeforeValidatorFunctionSchema) -> JsonSchemaValue:\n        if self.mode == 'validation' and (input_schema := schema.get('json_schema_input_schema')):\n            return self.generate_inner(input_schema)\n        return self.generate_inner(schema['schema'])", "documentation": "        \"\"\"Generates a JSON schema that matches a function-before schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1058, "code": "    def function_after_schema(self, schema: core_schema.AfterValidatorFunctionSchema) -> JsonSchemaValue:\n        return self.generate_inner(schema['schema'])", "documentation": "        \"\"\"Generates a JSON schema that matches a function-after schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1069, "code": "    def function_plain_schema(self, schema: core_schema.PlainValidatorFunctionSchema) -> JsonSchemaValue:\n        if self.mode == 'validation' and (input_schema := schema.get('json_schema_input_schema')):\n            return self.generate_inner(input_schema)\n        return self.handle_invalid_for_json_schema(\n            schema, f'core_schema.PlainValidatorFunctionSchema ({schema[\"function\"]})'\n        )", "documentation": "        \"\"\"Generates a JSON schema that matches a function-plain schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1085, "code": "    def function_wrap_schema(self, schema: core_schema.WrapValidatorFunctionSchema) -> JsonSchemaValue:\n        if self.mode == 'validation' and (input_schema := schema.get('json_schema_input_schema')):\n            return self.generate_inner(input_schema)\n        return self.generate_inner(schema['schema'])", "documentation": "        \"\"\"Generates a JSON schema that matches a function-wrap schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1099, "code": "    def default_schema(self, schema: core_schema.WithDefaultSchema) -> JsonSchemaValue:\n        json_schema = self.generate_inner(schema['schema'])\n        default = self.get_default_value(schema)\n        if default is NoDefault:\n            return json_schema\n        if self.mode == 'serialization':\n            ser_schema = _get_ser_schema_for_default_value(schema['schema'])\n            if (\n                ser_schema is not None\n                and (ser_func := ser_schema.get('function'))\n                and not (default is None and ser_schema.get('when_used') in ('unless-none', 'json-unless-none'))", "documentation": "        \"\"\"Generates a JSON schema that matches a schema with a default value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1153, "code": "    def get_default_value(self, schema: core_schema.WithDefaultSchema) -> Any:\n        return schema.get('default', NoDefault)", "documentation": "        \"\"\"Get the default value to be used when generating a JSON Schema for a core schema with a default.\n\n        The default implementation is to use the statically defined default value. This method can be overridden\n        if you want to make use of the default factory.\n\n        Args:\n            schema: The `'with-default'` core schema.\n\n        Returns:\n            The default value to use, or [`NoDefault`][pydantic.json_schema.NoDefault] if no default\n                value is available.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1168, "code": "    def nullable_schema(self, schema: core_schema.NullableSchema) -> JsonSchemaValue:\n        null_schema = {'type': 'null'}\n        inner_json_schema = self.generate_inner(schema['schema'])\n        if inner_json_schema == null_schema:\n            return null_schema\n        else:\n            return self.get_flattened_anyof([inner_json_schema, null_schema])", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that allows null values.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1187, "code": "    def union_schema(self, schema: core_schema.UnionSchema) -> JsonSchemaValue:\n        generated: list[JsonSchemaValue] = []\n        choices = schema['choices']\n        for choice in choices:\n            choice_schema = choice[0] if isinstance(choice, tuple) else choice\n            try:\n                generated.append(self.generate_inner(choice_schema))\n            except PydanticOmit:\n                continue\n            except PydanticInvalidForJsonSchema as exc:\n                self.emit_warning('skipped-choice', exc.message)", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that allows values matching any of the given schemas.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1212, "code": "    def tagged_union_schema(self, schema: core_schema.TaggedUnionSchema) -> JsonSchemaValue:\n        generated: dict[str, JsonSchemaValue] = {}\n        for k, v in schema['choices'].items():\n            if isinstance(k, Enum):\n                k = k.value\n            try:\n                generated[str(k)] = self.generate_inner(v).copy()\n            except PydanticOmit:\n                continue\n            except PydanticInvalidForJsonSchema as exc:\n                self.emit_warning('skipped-choice', exc.message)", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that allows values matching any of the given schemas, where\n        the schemas are tagged with a discriminator field that indicates which schema should be used to validate\n        the value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1294, "code": "    def chain_schema(self, schema: core_schema.ChainSchema) -> JsonSchemaValue:\n        step_index = 0 if self.mode == 'validation' else -1  # use first step for validation, last for serialization\n        return self.generate_inner(schema['steps'][step_index])", "documentation": "        \"\"\"Generates a JSON schema that matches a core_schema.ChainSchema.\n\n        When generating a schema for validation, we return the validation JSON schema for the first step in the chain.\n        For serialization, we return the serialization JSON schema for the last step in the chain.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1309, "code": "    def lax_or_strict_schema(self, schema: core_schema.LaxOrStrictSchema) -> JsonSchemaValue:\n        use_strict = schema.get('strict', False)  # TODO: replace this default False\n        if use_strict:\n            return self.generate_inner(schema['strict_schema'])\n        else:\n            return self.generate_inner(schema['lax_schema'])", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that allows values matching either the lax schema or the\n        strict schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1328, "code": "    def json_or_python_schema(self, schema: core_schema.JsonOrPythonSchema) -> JsonSchemaValue:\n        return self.generate_inner(schema['json_schema'])", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that allows values matching either the JSON schema or the\n        Python schema.\n\n        The JSON schema is used instead of the Python schema. If you want to use the Python schema, you should override\n        this method.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1343, "code": "    def typed_dict_schema(self, schema: core_schema.TypedDictSchema) -> JsonSchemaValue:\n        total = schema.get('total', True)\n        named_required_fields: list[tuple[str, bool, CoreSchemaField]] = [\n            (name, self.field_is_required(field, total), field)\n            for name, field in schema['fields'].items()\n            if self.field_is_present(field)\n        ]\n        if self.mode == 'serialization':\n            named_required_fields.extend(self._name_required_computed_fields(schema.get('computed_fields', [])))\n        cls = schema.get('cls')\n        config = _get_typed_dict_config(cls)", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a typed dict.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1428, "code": "    def typed_dict_field_schema(self, schema: core_schema.TypedDictField) -> JsonSchemaValue:\n        return self.generate_inner(schema['schema'])", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a typed dict field.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1439, "code": "    def dataclass_field_schema(self, schema: core_schema.DataclassField) -> JsonSchemaValue:\n        return self.generate_inner(schema['schema'])", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a dataclass field.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1450, "code": "    def model_field_schema(self, schema: core_schema.ModelField) -> JsonSchemaValue:\n        return self.generate_inner(schema['schema'])", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a model field.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1461, "code": "    def computed_field_schema(self, schema: core_schema.ComputedField) -> JsonSchemaValue:\n        return self.generate_inner(schema['return_schema'])", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a computed field.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1472, "code": "    def model_schema(self, schema: core_schema.ModelSchema) -> JsonSchemaValue:\n        cls = cast('type[BaseModel]', schema['cls'])\n        config = cls.model_config\n        with self._config_wrapper_stack.push(config):\n            json_schema = self.generate_inner(schema['schema'])\n        self._update_class_schema(json_schema, cls, config)\n        return json_schema", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a model.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1493, "code": "    def _update_class_schema(self, json_schema: JsonSchemaValue, cls: type[Any], config: ConfigDict) -> None:\n        from .main import BaseModel\n        from .root_model import RootModel\n        if (config_title := config.get('title')) is not None:\n            json_schema.setdefault('title', config_title)\n        elif model_title_generator := config.get('model_title_generator'):\n            title = model_title_generator(cls)\n            if not isinstance(title, str):\n                raise TypeError(f'model_title_generator {model_title_generator} must return str, not {title.__class__}')\n            json_schema.setdefault('title', title)\n        if 'title' not in json_schema:", "documentation": "        \"\"\"Update json_schema with the following, extracted from `config` and `cls`:\n\n        * title\n        * description\n        * additional properties\n        * json_schema_extra\n        * deprecated\n\n        Done in place, hence there's no return value as the original json_schema is mutated.\n        No ref resolving is involved here, as that's not appropriate for simple updates.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1564, "code": "    def resolve_ref_schema(self, json_schema: JsonSchemaValue) -> JsonSchemaValue:\n        while '$ref' in json_schema:\n            ref = json_schema['$ref']\n            schema_to_update = self.get_schema_from_definitions(JsonRef(ref))\n            if schema_to_update is None:\n                raise RuntimeError(f'Cannot update undefined schema for $ref={ref}')\n            json_schema = schema_to_update\n        return json_schema", "documentation": "        \"\"\"Resolve a JsonSchemaValue to the non-ref schema if it is a $ref schema.\n\n        Args:\n            json_schema: The schema to resolve.\n\n        Returns:\n            The resolved schema.\n\n        Raises:\n            RuntimeError: If the schema reference can't be found in definitions.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1584, "code": "    def model_fields_schema(self, schema: core_schema.ModelFieldsSchema) -> JsonSchemaValue:\n        named_required_fields: list[tuple[str, bool, CoreSchemaField]] = [\n            (name, self.field_is_required(field, total=True), field)\n            for name, field in schema['fields'].items()\n            if self.field_is_present(field)\n        ]\n        if self.mode == 'serialization':\n            named_required_fields.extend(self._name_required_computed_fields(schema.get('computed_fields', [])))\n        json_schema = self._named_required_fields_schema(named_required_fields)\n        extras_schema = schema.get('extras_schema', None)\n        if extras_schema is not None:", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a model's fields.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1607, "code": "    def field_is_present(self, field: CoreSchemaField) -> bool:\n        if self.mode == 'serialization':\n            return not field.get('serialization_exclude')\n        elif self.mode == 'validation':\n            return True\n        else:\n            assert_never(self.mode)", "documentation": "        \"\"\"Whether the field should be included in the generated JSON schema.\n\n        Args:\n            field: The schema for the field itself.\n\n        Returns:\n            `True` if the field should be included in the generated JSON schema, `False` otherwise.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1650, "code": "    def dataclass_args_schema(self, schema: core_schema.DataclassArgsSchema) -> JsonSchemaValue:\n        named_required_fields: list[tuple[str, bool, CoreSchemaField]] = [\n            (field['name'], self.field_is_required(field, total=True), field)\n            for field in schema['fields']\n            if self.field_is_present(field)\n        ]\n        if self.mode == 'serialization':\n            named_required_fields.extend(self._name_required_computed_fields(schema.get('computed_fields', [])))\n        return self._named_required_fields_schema(named_required_fields)", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a dataclass's constructor arguments.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1668, "code": "    def dataclass_schema(self, schema: core_schema.DataclassSchema) -> JsonSchemaValue:\n        from ._internal._dataclasses import is_stdlib_dataclass\n        cls = schema['cls']\n        config: ConfigDict = getattr(cls, '__pydantic_config__', cast('ConfigDict', {}))\n        with self._config_wrapper_stack.push(config):\n            json_schema = self.generate_inner(schema['schema']).copy()\n        self._update_class_schema(json_schema, cls, config)\n        if is_stdlib_dataclass(cls):\n            description = None\n        else:\n            description = None if cls.__doc__ is None else inspect.cleandoc(cls.__doc__)", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a dataclass.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1698, "code": "    def arguments_schema(self, schema: core_schema.ArgumentsSchema) -> JsonSchemaValue:\n        prefer_positional = schema.get('metadata', {}).get('pydantic_js_prefer_positional_arguments')\n        arguments = schema['arguments_schema']\n        kw_only_arguments = [a for a in arguments if a.get('mode') == 'keyword_only']\n        kw_or_p_arguments = [a for a in arguments if a.get('mode') in {'positional_or_keyword', None}]\n        p_only_arguments = [a for a in arguments if a.get('mode') == 'positional_only']\n        var_args_schema = schema.get('var_args_schema')\n        var_kwargs_schema = schema.get('var_kwargs_schema')\n        if prefer_positional:\n            positional_possible = not kw_only_arguments and not var_kwargs_schema\n            if positional_possible:", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a function's arguments.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1813, "code": "    def get_argument_name(self, argument: core_schema.ArgumentsParameter | core_schema.ArgumentsV3Parameter) -> str:\n        name = argument['name']\n        if self.by_alias:\n            alias = argument.get('alias')\n            if isinstance(alias, str):\n                name = alias\n            else:\n                pass  # might want to do something else?\n        return name", "documentation": "        \"\"\"Retrieves the name of an argument.\n\n        Args:\n            argument: The core schema.\n\n        Returns:\n            The name of the argument.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1831, "code": "    def arguments_v3_schema(self, schema: core_schema.ArgumentsV3Schema) -> JsonSchemaValue:\n        arguments = schema['arguments_schema']\n        properties: dict[str, JsonSchemaValue] = {}\n        required: list[str] = []\n        for argument in arguments:\n            mode = argument.get('mode', 'positional_or_keyword')\n            name = self.get_argument_name(argument)\n            argument_schema = self.generate_inner(argument['schema']).copy()\n            if mode == 'var_args':\n                argument_schema = {'type': 'array', 'items': argument_schema}\n            elif mode == 'var_kwargs_uniform':", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a function's arguments.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1870, "code": "    def call_schema(self, schema: core_schema.CallSchema) -> JsonSchemaValue:\n        return self.generate_inner(schema['arguments_schema'])", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a function call.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1881, "code": "    def custom_error_schema(self, schema: core_schema.CustomErrorSchema) -> JsonSchemaValue:\n        return self.generate_inner(schema['schema'])", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a custom error.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1892, "code": "    def json_schema(self, schema: core_schema.JsonSchema) -> JsonSchemaValue:\n        content_core_schema = schema.get('schema') or core_schema.any_schema()\n        content_json_schema = self.generate_inner(content_core_schema)\n        if self.mode == 'validation':\n            return {'type': 'string', 'contentMediaType': 'application/json', 'contentSchema': content_json_schema}\n        else:\n            return content_json_schema", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a JSON object.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1909, "code": "    def url_schema(self, schema: core_schema.UrlSchema) -> JsonSchemaValue:\n        json_schema = {'type': 'string', 'format': 'uri', 'minLength': 1}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.string)\n        return json_schema", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a URL.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1922, "code": "    def multi_host_url_schema(self, schema: core_schema.MultiHostUrlSchema) -> JsonSchemaValue:\n        json_schema = {'type': 'string', 'format': 'multi-host-uri', 'minLength': 1}\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.string)\n        return json_schema", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a URL that can be used with multiple hosts.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1936, "code": "    def uuid_schema(self, schema: core_schema.UuidSchema) -> JsonSchemaValue:\n        return {'type': 'string', 'format': 'uuid'}", "documentation": "        \"\"\"Generates a JSON schema that matches a UUID.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1947, "code": "    def definitions_schema(self, schema: core_schema.DefinitionsSchema) -> JsonSchemaValue:\n        for definition in schema['definitions']:\n            try:\n                self.generate_inner(definition)\n            except PydanticInvalidForJsonSchema as e:\n                core_ref: CoreRef = CoreRef(definition['ref'])  # type: ignore\n                self._core_defs_invalid_for_json_schema[self.get_defs_ref((core_ref, self.mode))] = e\n                continue\n        return self.generate_inner(schema['schema'])", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that defines a JSON object with definitions.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 1965, "code": "    def definition_ref_schema(self, schema: core_schema.DefinitionReferenceSchema) -> JsonSchemaValue:\n        core_ref = CoreRef(schema['schema_ref'])\n        _, ref_json_schema = self.get_cache_defs_ref_schema(core_ref)\n        return ref_json_schema", "documentation": "        \"\"\"Generates a JSON schema that matches a schema that references a definition.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2003, "code": "    def complex_schema(self, schema: core_schema.ComplexSchema) -> JsonSchemaValue:\n        return {'type': 'string'}", "documentation": "        \"\"\"Generates a JSON schema that matches a complex number.\n\n        JSON has no standard way to represent complex numbers. Complex number is not a numeric\n        type. Here we represent complex number as strings following the rule defined by Python.\n        For instance, '1+2j' is an accepted complex string. Details can be found in\n        [Python's `complex` documentation][complex].\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2021, "code": "    def get_title_from_name(self, name: str) -> str:\n        return name.title().replace('_', ' ').strip()", "documentation": "        \"\"\"Retrieves a title from a name.\n\n        Args:\n            name: The name to retrieve a title from.\n\n        Returns:\n            The title.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2032, "code": "    def field_title_should_be_set(self, schema: CoreSchemaOrField) -> bool:\n        if _core_utils.is_core_schema_field(schema):\n            if schema['type'] == 'computed-field':\n                field_schema = schema['return_schema']\n            else:\n                field_schema = schema['schema']\n            return self.field_title_should_be_set(field_schema)\n        elif _core_utils.is_core_schema(schema):\n            if schema.get('ref'):  # things with refs, such as models and enums, should not have titles set\n                return False\n            if schema['type'] in {'default', 'nullable', 'definitions'}:", "documentation": "        \"\"\"Returns true if a field with the given schema should have a title set based on the field name.\n\n        Intuitively, we want this to return true for schemas that wouldn't otherwise provide their own title\n        (e.g., int, float, str), and false for those that would (e.g., BaseModel subclasses).\n\n        Args:\n            schema: The schema to check.\n\n        Returns:\n            `True` if the field should have a title set, `False` otherwise.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2067, "code": "    def normalize_name(self, name: str) -> str:\n        return re.sub(r'[^a-zA-Z0-9.\\-_]', '_', name).replace('.', '__')", "documentation": "        \"\"\"Normalizes a name to be used as a key in a dictionary.\n\n        Args:\n            name: The name to normalize.\n\n        Returns:\n            The normalized name.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2078, "code": "    def get_defs_ref(self, core_mode_ref: CoreModeRef) -> DefsRef:\n        core_ref, mode = core_mode_ref\n        components = re.split(r'([\\][,])', core_ref)\n        components = [x.rsplit(':', 1)[0] for x in components]\n        core_ref_no_id = ''.join(components)\n        components = [re.sub(r'(?:[^.[\\]]+\\.)+((?:[^.[\\]]+))', r'\\1', x) for x in components]\n        short_ref = ''.join(components)\n        mode_title = _MODE_TITLE_MAPPING[mode]\n        name = DefsRef(self.normalize_name(short_ref))\n        name_mode = DefsRef(self.normalize_name(short_ref) + f'-{mode_title}')\n        module_qualname = DefsRef(self.normalize_name(core_ref_no_id))", "documentation": "        \"\"\"Override this method to change the way that definitions keys are generated from a core reference.\n\n        Args:\n            core_mode_ref: The core reference.\n\n        Returns:\n            The definitions key.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2126, "code": "    def get_cache_defs_ref_schema(self, core_ref: CoreRef) -> tuple[DefsRef, JsonSchemaValue]:\n        core_mode_ref = (core_ref, self.mode)\n        maybe_defs_ref = self.core_to_defs_refs.get(core_mode_ref)\n        if maybe_defs_ref is not None:\n            json_ref = self.core_to_json_refs[core_mode_ref]\n            return maybe_defs_ref, {'$ref': json_ref}\n        defs_ref = self.get_defs_ref(core_mode_ref)\n        self.core_to_defs_refs[core_mode_ref] = defs_ref\n        self.defs_to_core_refs[defs_ref] = core_mode_ref\n        json_ref = JsonRef(self.ref_template.format(model=defs_ref))\n        self.core_to_json_refs[core_mode_ref] = json_ref", "documentation": "        \"\"\"This method wraps the get_defs_ref method with some cache-lookup/population logic,\n        and returns both the produced defs_ref and the JSON schema that will refer to the right definition.\n\n        Args:\n            core_ref: The core reference to get the definitions reference for.\n\n        Returns:\n            A tuple of the definitions reference and the JSON schema that will refer to it.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2154, "code": "    def handle_ref_overrides(self, json_schema: JsonSchemaValue) -> JsonSchemaValue:\n        if '$ref' in json_schema:\n            json_schema = json_schema.copy()\n            referenced_json_schema = self.get_schema_from_definitions(JsonRef(json_schema['$ref']))\n            if referenced_json_schema is None:\n                return json_schema\n            for k, v in list(json_schema.items()):\n                if k == '$ref':\n                    continue\n                if k in referenced_json_schema and referenced_json_schema[k] == v:\n                    del json_schema[k]  # redundant key", "documentation": "        \"\"\"Remove any sibling keys that are redundant with the referenced schema.\n\n        Args:\n            json_schema: The schema to remove redundant sibling keys from.\n\n        Returns:\n            The schema with redundant sibling keys removed.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2192, "code": "    def encode_default(self, dft: Any) -> Any:\n        from .type_adapter import TypeAdapter, _type_has_config\n        config = self._config\n        try:\n            default = (\n                dft\n                if _type_has_config(type(dft))\n                else TypeAdapter(type(dft), config=config.config_dict).dump_python(\n                    dft, by_alias=self.by_alias, mode='json'\n                )\n            )", "documentation": "        \"\"\"Encode a default value to a JSON-serializable value.\n\n        This is used to encode default values for fields in the generated JSON schema.\n\n        Args:\n            dft: The default value to encode.\n\n        Returns:\n            The encoded default value.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2236, "code": "    class ValidationsMapping:\n        numeric = {\n            'multiple_of': 'multipleOf',\n            'le': 'maximum',\n            'ge': 'minimum',\n            'lt': 'exclusiveMaximum',\n            'gt': 'exclusiveMinimum',\n        }\n        bytes = {\n            'min_length': 'minLength',\n            'max_length': 'maxLength',", "documentation": "        \"\"\"This class just contains mappings from core_schema attribute names to the corresponding\n        JSON schema attribute names. While I suspect it is unlikely to be necessary, you can in\n        principle override this class in a subclass of GenerateJsonSchema (by inheriting from\n        GenerateJsonSchema.ValidationsMapping) to change these mappings.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2280, "code": "    def get_json_ref_counts(self, json_schema: JsonSchemaValue) -> dict[JsonRef, int]:\n        json_refs: dict[JsonRef, int] = Counter()", "documentation": "        \"\"\"Get all values corresponding to the key '$ref' anywhere in the json_schema.\"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2319, "code": "    def emit_warning(self, kind: JsonSchemaWarningKind, detail: str) -> None:\n        message = self.render_warning_message(kind, detail)\n        if message is not None:\n            warnings.warn(message, PydanticJsonSchemaWarning)", "documentation": "        \"\"\"This method simply emits PydanticJsonSchemaWarnings based on handling in the `warning_message` method.\"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2325, "code": "    def render_warning_message(self, kind: JsonSchemaWarningKind, detail: str) -> str | None:\n        if kind in self.ignored_warning_kinds:\n            return None\n        return f'{detail} [{kind}]'", "documentation": "        \"\"\"This method is responsible for ignoring warnings as desired, and for formatting the warning messages.\n\n        You can override the value of `ignored_warning_kinds` in a subclass of GenerateJsonSchema\n        to modify what warnings are generated. If you want more control, you can override this method;\n        just return None in situations where you don't want warnings to be emitted.\n\n        Args:\n            kind: The kind of warning to render. It can be one of the following:\n\n                - 'skipped-choice': A choice field was skipped because it had no valid choices.\n                - 'non-serializable-default': A default value was skipped because it was not JSON-serializable.\n            detail: A string with additional details about the warning.\n\n        Returns:\n            The formatted warning message, or `None` if no warning should be emitted.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2485, "code": "class WithJsonSchema:\n    json_schema: JsonSchemaValue | None\n    mode: Literal['validation', 'serialization'] | None = None", "documentation": "    \"\"\"!!! abstract \"Usage Documentation\"\n        [`WithJsonSchema` Annotation](../concepts/json_schema.md#withjsonschema-annotation)\n\n    Add this as an annotation on a field to override the (base) JSON schema that would be generated for that field.\n    This provides a way to set a JSON schema for types that would otherwise raise errors when producing a JSON schema,\n    such as Callable, or types that have an is-instance core schema, without needing to go so far as creating a\n    custom subclass of pydantic.json_schema.GenerateJsonSchema.\n    Note that any _modifications_ to the schema that would normally be made (such as setting the title for model fields)\n    will still be performed.\n\n    If `mode` is set this will only apply to that schema generation mode, allowing you\n    to set different json schemas for validation and serialization.\n    \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2519, "code": "class Examples:\n    @overload\n    @deprecated('Using a dict for `examples` is deprecated since v2.9 and will be removed in v3.0. Use a list instead.')", "documentation": "    \"\"\"Add examples to a JSON schema.\n\n    If the JSON Schema already contains examples, the provided examples\n    will be appended.\n\n    If `mode` is set this will only apply to that schema generation mode,\n    allowing you to add different examples for validation and serialization.\n    \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2593, "code": "def _get_all_json_refs(item: Any) -> set[JsonRef]:\n    refs: set[JsonRef] = set()\n    stack = [item]\n    while stack:\n        current = stack.pop()\n        if isinstance(current, dict):\n            for key, value in current.items():\n                if key == 'examples' and isinstance(value, list):\n                    continue\n                if key == '$ref' and isinstance(value, str):\n                    refs.add(JsonRef(value))", "documentation": "    \"\"\"Get all the definitions references from a JSON schema.\"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2628, "code": "    class SkipJsonSchema:", "documentation": "        \"\"\"!!! abstract \"Usage Documentation\"\n            [`SkipJsonSchema` Annotation](../concepts/json_schema.md#skipjsonschema-annotation)\n\n        Add this as an annotation on a field to skip generating a JSON schema for that field.\n\n        Example:\n            ```python\n            from pprint import pprint\n            from typing import Union\n\n            from pydantic import BaseModel\n            from pydantic.json_schema import SkipJsonSchema\n\n            class Model(BaseModel):\n                a: Union[int, None] = None  # (1)!\n                b: Union[int, SkipJsonSchema[None]] = None  # (2)!\n                c: SkipJsonSchema[Union[int, None]] = None  # (3)!\n\n            pprint(Model.model_json_schema())\n            '''\n            {\n                'properties': {\n                    'a': {\n                        'anyOf': [\n                            {'type': 'integer'},\n                            {'type': 'null'}\n                        ],\n                        'default': None,\n                        'title': 'A'\n                    },\n                    'b': {\n                        'default': None,\n                        'title': 'B',\n                        'type': 'integer'\n                    }\n                },\n                'title': 'Model',\n                'type': 'object'\n            }\n            '''\n            ```\n\n            1. The integer and null types are both included in the schema for `a`.\n            2. The integer type is the only type included in the schema for `b`.\n            3. The entirety of the `c` field is omitted from the schema.\n        \"\"\""}, {"filename": "pydantic/json_schema.py", "start_line": 2697, "code": "def _get_ser_schema_for_default_value(schema: CoreSchema) -> core_schema.PlainSerializerFunctionSerSchema | None:\n    if (\n        (ser_schema := schema.get('serialization'))\n        and ser_schema['type'] == 'function-plain'\n        and not ser_schema.get('info_arg')\n    ):\n        return ser_schema\n    if _core_utils.is_function_with_inner_schema(schema):\n        return _get_ser_schema_for_default_value(schema['schema'])", "documentation": "    \"\"\"Get a `'function-plain'` serialization schema that can be used to serialize a default value.\n\n    This takes into account having the serialization schema nested under validation schema(s).\n    \"\"\""}]}
{"repository": "pydantic/pydantic", "commit_sha": "0d33ece12cd7c03af997c2ecde6dd7718d1bd044", "commit_message": "Fix check for stdlib dataclasses (#11822)\n\n- Rename function to `is_stdlib_dataclass()`\n- Instead of hacking around with annotations, check the\n  cls' `__dict__`\n- Update docstring, return type\n- Add and update existing tests\n- Improve some parts of the documentation.\n\nThanks @karta9821 for initially investigating.", "commit_date": "2025-05-01T20:23:14+00:00", "author": "Victorien", "file": "tests/test_dataclasses.py", "patch": "@@ -1093,7 +1093,9 @@ def test_dataclass_equality_for_field_values(foo_bar_getter):\n     assert foo == bar.c\n \n \n-def test_issue_2383():\n+def test_hash_method_preserved() -> None:\n+    \"\"\"https://github.com/pydantic/pydantic/issues/2383\"\"\"\n+\n     @dataclasses.dataclass\n     class A:\n         s: str\n@@ -1111,7 +1113,9 @@ class B(pydantic.BaseModel):\n     assert hash(b.a) == 123\n \n \n-def test_issue_2398():\n+def test_order_preserved() -> None:\n+    \"\"\"https://github.com/pydantic/pydantic/issues/2398\"\"\"\n+\n     @dataclasses.dataclass(order=True)\n     class DC:\n         num: int = 42\n@@ -1128,7 +1132,9 @@ class Model(pydantic.BaseModel):\n     assert real_dc <= model.dc\n \n \n-def test_issue_2424():\n+def test_default_factory_works_on_subclasses() -> None:\n+    \"\"\"https://github.com/pydantic/pydantic/issues/2424\"\"\"\n+\n     @dataclasses.dataclass\n     class Base:\n         x: str\n@@ -1147,7 +1153,23 @@ class ValidatedThing(Base):\n     assert ValidatedThing(x='hi').y == ''\n \n \n-def test_issue_2541():\n+def test_override_default_stdlib_dataclass() -> None:\n+    \"\"\"https://github.com/pydantic/pydantic/issues/11816\"\"\"\n+\n+    @dataclasses.dataclass\n+    class Test:\n+        value: int = 1\n+\n+    @pydantic.dataclasses.dataclass\n+    class Sub(Test):\n+        value: int = 2\n+\n+    assert Sub().value == 2\n+\n+\n+def test_frozen_preserved_on_model_field() -> None:\n+    \"\"\"https://github.com/pydantic/pydantic/issues/2541\"\"\"\n+\n     @dataclasses.dataclass(frozen=True)\n     class Infos:\n         id: int\n@@ -1272,7 +1294,9 @@ class Model(BaseModel):\n     }\n \n \n-def test_issue_2594():\n+def test_supports_stdlib_dataclass_without_annotations() -> None:\n+    \"\"\"https://github.com/pydantic/pydantic/issues/2594\"\"\"\n+\n     @dataclasses.dataclass\n     class Empty:\n         pass\n@@ -1318,8 +1342,8 @@ class B:\n     assert model_json_schema(A)['description'] == 'my description'\n \n \n-def test_issue_3011():\n-    \"\"\"Validation of a subclass of a dataclass\"\"\"\n+def test_subclass_of_a_dataclass_supported() -> None:\n+    \"\"\"https://github.com/pydantic/pydantic/issues/3011\"\"\"\n \n     @dataclasses.dataclass\n     class A:\n@@ -1337,7 +1361,9 @@ class C:\n     assert c.thing.thing_a == 'Thing A'\n \n \n-def test_issue_3162():\n+def test_dataclass_referenced_twice() -> None:\n+    \"\"\"https://github.com/pydantic/pydantic/issues/3162\"\"\"\n+\n     @dataclasses.dataclass\n     class User:\n         id: int", "before_segments": [{"filename": "tests/test_dataclasses.py", "start_line": 758, "code": "def test_default_value_ellipsis():\n    @pydantic.dataclasses.dataclass", "documentation": "    \"\"\"\n    https://github.com/pydantic/pydantic/issues/5488\n    \"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 942, "code": "def test_pydantic_callable_field():", "documentation": "    \"\"\"pydantic callable fields behaviour should be the same as stdlib dataclass\"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 1022, "code": "def lazy_cases_for_dataclass_equality_checks():\n    cases = []", "documentation": "    \"\"\"\n    The reason for the convoluted structure of this function is to avoid\n    creating the classes while collecting tests, which may trigger breakpoints\n    etc. while working on one specific test.\n    \"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 1303, "code": "    class A:\n        x: int\n    assert model_json_schema(A)['description'] == 'my description'\n    @pydantic.dataclasses.dataclass\n    @dataclasses.dataclass", "documentation": "        \"\"\"my description\"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 1312, "code": "    class B:\n        x: int\n    assert model_json_schema(A)['description'] == 'my description'", "documentation": "        \"\"\"my description\"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 1320, "code": "def test_issue_3011():\n    @dataclasses.dataclass", "documentation": "    \"\"\"Validation of a subclass of a dataclass\"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 1427, "code": "def test_new_not_called():", "documentation": "    \"\"\"\n    pydantic dataclasses do not preserve sunder attributes set in __new__\n    \"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 1432, "code": "    class StandardClass:\n        a: str", "documentation": "        \"\"\"Class which modifies instance creation.\"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 1777, "code": "        def combined_decorator(cls):\n            return pydantic.dataclasses.dataclass(dataclasses.dataclass(cls))\n        decorators.append(combined_decorator)\n        ids.append('combined')\n    if include_identity:", "documentation": "            \"\"\"\n            Should be equivalent to:\n            @pydantic.dataclasses.dataclass\n            @dataclasses.dataclass\n            \"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 1885, "code": "def test_parent_post_init():\n    @dataclasses.dataclass", "documentation": "    \"\"\"\n    Test that the parent's __post_init__ gets called\n    and the order in which it gets called relative to validation.\n\n    In V1 we called it before validation, in V2 it gets called after.\n    \"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 1971, "code": "def test_validator_info_field_name_data_before():\n    @pydantic.dataclasses.dataclass", "documentation": "    \"\"\"\n    Test accessing info.field_name and info.data\n    We only test the `before` validator because they\n    all share the same implementation.\n    \"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 2006, "code": "def test_inheritance_replace(decorator1: Callable[[Any], Any], expected_parent: list[str], expected_child: list[str]):\n    @decorator1", "documentation": "    \"\"\"We promise that if you add a validator\n    with the same _function_ name as an existing validator\n    it replaces the existing validator and is run instead of it.\n    \"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 2222, "code": "    class FooStd:\n    FooPydantic = pydantic.dataclasses.dataclass(FooStd)\n    assert FooPydantic.__module__ == FooStd.__module__\n    assert FooPydantic.__name__ == FooStd.__name__\n    assert FooPydantic.__qualname__ == FooStd.__qualname__", "documentation": "        \"\"\"Docstring\"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 2439, "code": "def test_dataclass_slots_validate_assignment():\n    m = DataclassSlotsValidateAssignment(1)\n    m_pickle = pickle.loads(pickle.dumps(m))\n    assert m_pickle.a == 1\n    with pytest.raises(ValidationError):\n        m.a = 'not_an_int'\n@pytest.mark.parametrize(\n    'dataclass_decorator',\n    [\n        pydantic.dataclasses.dataclass,\n        dataclasses.dataclass,", "documentation": "    \"\"\"https://github.com/pydantic/pydantic/issues/11768\"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 2583, "code": "def test_combined_field_annotations():\n    @pydantic.dataclasses.dataclass", "documentation": "    \"\"\"\n    This test is included to document the fact that `Field` and `field` can be used together.\n    That said, if you mix them like this, there is a good chance you'll run into surprising behavior/bugs.\n\n    (E.g., `x: Annotated[int, Field(gt=1, validate_default=True)] = field(default=0)` doesn't cause an error)\n\n    I would generally advise against doing this, and if we do change the behavior in the future to somehow merge\n    pydantic.FieldInfo and dataclasses.Field in a way that changes runtime behavior for existing code, I would probably\n    consider it a bugfix rather than a breaking change.\n    \"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 2738, "code": "def test_can_inherit_stdlib_dataclasses_default_factories_and_use_them():\n    @dataclasses.dataclass", "documentation": "    \"\"\"This test documents that default factories are not supported\"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 2774, "code": "def test_alias_with_dashes():\n    @pydantic.dataclasses.dataclass", "documentation": "    \"\"\"Test for fix issue #7226.\"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 2981, "code": "def test_annotated_with_field_default_factory() -> None:\n    field = dataclasses.field\n    @pydantic.dataclasses.dataclass()", "documentation": "    \"\"\"\n    https://github.com/pydantic/pydantic/issues/9947\n    \"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 3039, "code": "def test_frozen_with_validate_assignment() -> None:\n    @pydantic.dataclasses.dataclass(frozen=True, config=ConfigDict(validate_assignment=True))", "documentation": "    \"\"\"Test for https://github.com/pydantic/pydantic/issues/10041.\"\"\""}], "after_segments": [{"filename": "tests/test_dataclasses.py", "start_line": 758, "code": "def test_default_value_ellipsis():\n    @pydantic.dataclasses.dataclass", "documentation": "    \"\"\"\n    https://github.com/pydantic/pydantic/issues/5488\n    \"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 942, "code": "def test_pydantic_callable_field():", "documentation": "    \"\"\"pydantic callable fields behaviour should be the same as stdlib dataclass\"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 1022, "code": "def lazy_cases_for_dataclass_equality_checks():\n    cases = []", "documentation": "    \"\"\"\n    The reason for the convoluted structure of this function is to avoid\n    creating the classes while collecting tests, which may trigger breakpoints\n    etc. while working on one specific test.\n    \"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 1095, "code": "def test_hash_method_preserved() -> None:\n    @dataclasses.dataclass", "documentation": "    \"\"\"https://github.com/pydantic/pydantic/issues/2383\"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 1115, "code": "def test_order_preserved() -> None:\n    @dataclasses.dataclass(order=True)", "documentation": "    \"\"\"https://github.com/pydantic/pydantic/issues/2398\"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 1134, "code": "def test_default_factory_works_on_subclasses() -> None:\n    @dataclasses.dataclass", "documentation": "    \"\"\"https://github.com/pydantic/pydantic/issues/2424\"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 1155, "code": "def test_override_default_stdlib_dataclass() -> None:\n    @dataclasses.dataclass", "documentation": "    \"\"\"https://github.com/pydantic/pydantic/issues/11816\"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 1169, "code": "def test_frozen_preserved_on_model_field() -> None:\n    @dataclasses.dataclass(frozen=True)", "documentation": "    \"\"\"https://github.com/pydantic/pydantic/issues/2541\"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 1296, "code": "def test_supports_stdlib_dataclass_without_annotations() -> None:\n    @dataclasses.dataclass", "documentation": "    \"\"\"https://github.com/pydantic/pydantic/issues/2594\"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 1327, "code": "    class A:\n        x: int\n    assert model_json_schema(A)['description'] == 'my description'\n    @pydantic.dataclasses.dataclass\n    @dataclasses.dataclass", "documentation": "        \"\"\"my description\"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 1336, "code": "    class B:\n        x: int\n    assert model_json_schema(A)['description'] == 'my description'", "documentation": "        \"\"\"my description\"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 1344, "code": "def test_subclass_of_a_dataclass_supported() -> None:\n    @dataclasses.dataclass", "documentation": "    \"\"\"https://github.com/pydantic/pydantic/issues/3011\"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 1363, "code": "def test_dataclass_referenced_twice() -> None:\n    @dataclasses.dataclass", "documentation": "    \"\"\"https://github.com/pydantic/pydantic/issues/3162\"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 1453, "code": "def test_new_not_called():", "documentation": "    \"\"\"\n    pydantic dataclasses do not preserve sunder attributes set in __new__\n    \"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 1458, "code": "    class StandardClass:\n        a: str", "documentation": "        \"\"\"Class which modifies instance creation.\"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 1803, "code": "        def combined_decorator(cls):\n            return pydantic.dataclasses.dataclass(dataclasses.dataclass(cls))\n        decorators.append(combined_decorator)\n        ids.append('combined')\n    if include_identity:", "documentation": "            \"\"\"\n            Should be equivalent to:\n            @pydantic.dataclasses.dataclass\n            @dataclasses.dataclass\n            \"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 1911, "code": "def test_parent_post_init():\n    @dataclasses.dataclass", "documentation": "    \"\"\"\n    Test that the parent's __post_init__ gets called\n    and the order in which it gets called relative to validation.\n\n    In V1 we called it before validation, in V2 it gets called after.\n    \"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 1997, "code": "def test_validator_info_field_name_data_before():\n    @pydantic.dataclasses.dataclass", "documentation": "    \"\"\"\n    Test accessing info.field_name and info.data\n    We only test the `before` validator because they\n    all share the same implementation.\n    \"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 2032, "code": "def test_inheritance_replace(decorator1: Callable[[Any], Any], expected_parent: list[str], expected_child: list[str]):\n    @decorator1", "documentation": "    \"\"\"We promise that if you add a validator\n    with the same _function_ name as an existing validator\n    it replaces the existing validator and is run instead of it.\n    \"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 2248, "code": "    class FooStd:\n    FooPydantic = pydantic.dataclasses.dataclass(FooStd)\n    assert FooPydantic.__module__ == FooStd.__module__\n    assert FooPydantic.__name__ == FooStd.__name__\n    assert FooPydantic.__qualname__ == FooStd.__qualname__", "documentation": "        \"\"\"Docstring\"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 2465, "code": "def test_dataclass_slots_validate_assignment():\n    m = DataclassSlotsValidateAssignment(1)\n    m_pickle = pickle.loads(pickle.dumps(m))\n    assert m_pickle.a == 1\n    with pytest.raises(ValidationError):\n        m.a = 'not_an_int'\n@pytest.mark.parametrize(\n    'dataclass_decorator',\n    [\n        pydantic.dataclasses.dataclass,\n        dataclasses.dataclass,", "documentation": "    \"\"\"https://github.com/pydantic/pydantic/issues/11768\"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 2609, "code": "def test_combined_field_annotations():\n    @pydantic.dataclasses.dataclass", "documentation": "    \"\"\"\n    This test is included to document the fact that `Field` and `field` can be used together.\n    That said, if you mix them like this, there is a good chance you'll run into surprising behavior/bugs.\n\n    (E.g., `x: Annotated[int, Field(gt=1, validate_default=True)] = field(default=0)` doesn't cause an error)\n\n    I would generally advise against doing this, and if we do change the behavior in the future to somehow merge\n    pydantic.FieldInfo and dataclasses.Field in a way that changes runtime behavior for existing code, I would probably\n    consider it a bugfix rather than a breaking change.\n    \"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 2764, "code": "def test_can_inherit_stdlib_dataclasses_default_factories_and_use_them():\n    @dataclasses.dataclass", "documentation": "    \"\"\"This test documents that default factories are not supported\"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 2800, "code": "def test_alias_with_dashes():\n    @pydantic.dataclasses.dataclass", "documentation": "    \"\"\"Test for fix issue #7226.\"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 3007, "code": "def test_annotated_with_field_default_factory() -> None:\n    field = dataclasses.field\n    @pydantic.dataclasses.dataclass()", "documentation": "    \"\"\"\n    https://github.com/pydantic/pydantic/issues/9947\n    \"\"\""}, {"filename": "tests/test_dataclasses.py", "start_line": 3065, "code": "def test_frozen_with_validate_assignment() -> None:\n    @pydantic.dataclasses.dataclass(frozen=True, config=ConfigDict(validate_assignment=True))", "documentation": "    \"\"\"Test for https://github.com/pydantic/pydantic/issues/10041.\"\"\""}]}
{"repository": "pydantic/pydantic", "commit_sha": "12554318bbc3efe1ff40a2c20e5735e11249fb88", "commit_message": "Update documentation of `allow_inf_nan` (#11568)", "commit_date": "2025-03-17T11:07:30+00:00", "author": "Victorien", "file": "pydantic/fields.py", "patch": "@@ -1019,7 +1019,7 @@ def Field(  # noqa: C901\n         min_length: Minimum length for iterables.\n         max_length: Maximum length for iterables.\n         pattern: Pattern for strings (a regular expression).\n-        allow_inf_nan: Allow `inf`, `-inf`, `nan`. Only applicable to numbers.\n+        allow_inf_nan: Allow `inf`, `-inf`, `nan`. Only applicable to float and [`Decimal`][decimal.Decimal] numbers.\n         max_digits: Maximum number of allow digits for strings.\n         decimal_places: Maximum number of decimal places allowed for numbers.\n         union_mode: The strategy to apply when validating a union. Can be `smart` (the default), or `left_to_right`.", "before_segments": [{"filename": "pydantic/fields.py", "start_line": 51, "code": "class _FromFieldInfoInputs(typing_extensions.TypedDict, total=False):\n    annotation: type[Any] | None\n    default_factory: Callable[[], Any] | Callable[[dict[str, Any]], Any] | None\n    alias: str | None\n    alias_priority: int | None\n    validation_alias: str | AliasPath | AliasChoices | None\n    serialization_alias: str | None\n    title: str | None\n    field_title_generator: Callable[[str, FieldInfo], str] | None\n    description: str | None\n    examples: list[Any] | None", "documentation": "    \"\"\"This class exists solely to add type checking for the `**kwargs` in `FieldInfo.from_field`.\"\"\""}, {"filename": "pydantic/fields.py", "start_line": 92, "code": "class _FieldInfoInputs(_FromFieldInfoInputs, total=False):\n    default: Any", "documentation": "    \"\"\"This class exists solely to add type checking for the `**kwargs` in `FieldInfo.__init__`.\"\"\""}, {"filename": "pydantic/fields.py", "start_line": 98, "code": "class FieldInfo(_repr.Representation):\n    annotation: type[Any] | None\n    default: Any\n    default_factory: Callable[[], Any] | Callable[[dict[str, Any]], Any] | None\n    alias: str | None\n    alias_priority: int | None\n    validation_alias: str | AliasPath | AliasChoices | None\n    serialization_alias: str | None\n    title: str | None\n    field_title_generator: Callable[[str, FieldInfo], str] | None\n    description: str | None", "documentation": "    \"\"\"This class holds information about a field.\n\n    `FieldInfo` is used for any field definition regardless of whether the [`Field()`][pydantic.fields.Field]\n    function is explicitly used.\n\n    !!! warning\n        You generally shouldn't be creating `FieldInfo` directly, you'll only need to use it when accessing\n        [`BaseModel`][pydantic.main.BaseModel] `.model_fields` internals.\n\n    Attributes:\n        annotation: The type annotation of the field.\n        default: The default value of the field.\n        default_factory: A callable to generate the default value. The callable can either take 0 arguments\n            (in which case it is called as is) or a single argument containing the already validated data.\n        alias: The alias name of the field.\n        alias_priority: The priority of the field's alias.\n        validation_alias: The validation alias of the field.\n        serialization_alias: The serialization alias of the field.\n        title: The title of the field.\n        field_title_generator: A callable that takes a field name and returns title for it.\n        description: The description of the field.\n        examples: List of examples of the field.\n        exclude: Whether to exclude the field from the model serialization.\n        discriminator: Field name or Discriminator for discriminating the type in a tagged union.\n        deprecated: A deprecation message, an instance of `warnings.deprecated` or the `typing_extensions.deprecated` backport,\n            or a boolean. If `True`, a default deprecation message will be emitted when accessing the field.\n        json_schema_extra: A dict or callable to provide extra JSON schema properties.\n        frozen: Whether the field is frozen.\n        validate_default: Whether to validate the default value of the field.\n        repr: Whether to include the field in representation of the model.\n        init: Whether the field should be included in the constructor of the dataclass.\n        init_var: Whether the field should _only_ be included in the constructor of the dataclass, and not stored.\n        kw_only: Whether the field should be a keyword-only argument in the constructor of the dataclass.\n        metadata: List of metadata constraints.\n    \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 208, "code": "    def __init__(self, **kwargs: Unpack[_FieldInfoInputs]) -> None:\n        self._attributes_set = {k: v for k, v in kwargs.items() if v is not _Unset}\n        kwargs = {k: _DefaultValues.get(k) if v is _Unset else v for k, v in kwargs.items()}  # type: ignore\n        self.annotation = kwargs.get('annotation')\n        default = kwargs.pop('default', PydanticUndefined)\n        if default is Ellipsis:\n            self.default = PydanticUndefined\n            self._attributes_set.pop('default', None)\n        else:\n            self.default = default\n        self.default_factory = kwargs.pop('default_factory', None)", "documentation": "        \"\"\"This class should generally not be initialized directly; instead, use the `pydantic.fields.Field` function\n        or one of the constructor classmethods.\n\n        See the signature of `pydantic.fields.Field` for more details about the expected arguments.\n        \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 262, "code": "    def from_field(default: Any = PydanticUndefined, **kwargs: Unpack[_FromFieldInfoInputs]) -> FieldInfo:\n        if 'annotation' in kwargs:\n            raise TypeError('\"annotation\" is not permitted as a Field keyword argument')\n        return FieldInfo(default=default, **kwargs)\n    @staticmethod", "documentation": "        \"\"\"Create a new `FieldInfo` object with the `Field` function.\n\n        Args:\n            default: The default value for the field. Defaults to Undefined.\n            **kwargs: Additional arguments dictionary.\n\n        Raises:\n            TypeError: If 'annotation' is passed as a keyword argument.\n\n        Returns:\n            A new FieldInfo object with the given parameters.\n\n        Example:\n            This is how you can create a field with default value like this:\n\n            ```python\n            import pydantic\n\n            class MyModel(pydantic.BaseModel):\n                foo: int = pydantic.Field(4)\n            ```\n        \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 290, "code": "    def from_annotation(annotation: type[Any], *, _source: AnnotationSource = AnnotationSource.ANY) -> FieldInfo:\n        try:\n            inspected_ann = inspect_annotation(\n                annotation,\n                annotation_source=_source,\n                unpack_type_aliases='skip',\n            )\n        except ForbiddenQualifier as e:\n            raise PydanticForbiddenQualifier(e.qualifier, annotation)\n        type_expr: Any = Any if inspected_ann.type is UNKNOWN else inspected_ann.type\n        final = 'final' in inspected_ann.qualifiers", "documentation": "        \"\"\"Creates a `FieldInfo` instance from a bare annotation.\n\n        This function is used internally to create a `FieldInfo` from a bare annotation like this:\n\n        ```python\n        import pydantic\n\n        class MyModel(pydantic.BaseModel):\n            foo: int  # <-- like this\n        ```\n\n        We also account for the case where the annotation can be an instance of `Annotated` and where\n        one of the (not first) arguments in `Annotated` is an instance of `FieldInfo`, e.g.:\n\n        ```python\n        from typing import Annotated\n\n        import annotated_types\n\n        import pydantic\n\n        class MyModel(pydantic.BaseModel):\n            foo: Annotated[int, annotated_types.Gt(42)]\n            bar: Annotated[int, pydantic.Field(gt=42)]\n        ```\n\n        Args:\n            annotation: An annotation object.\n\n        Returns:\n            An instance of the field metadata.\n        \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 470, "code": "    def merge_field_infos(*field_infos: FieldInfo, **overrides: Any) -> FieldInfo:\n        if len(field_infos) == 1:\n            field_info = copy(field_infos[0])\n            field_info._attributes_set.update(overrides)\n            default_override = overrides.pop('default', PydanticUndefined)\n            if default_override is Ellipsis:\n                default_override = PydanticUndefined\n            if default_override is not PydanticUndefined:\n                field_info.default = default_override\n            for k, v in overrides.items():\n                setattr(field_info, k, v)", "documentation": "        \"\"\"Merge `FieldInfo` instances keeping only explicitly set attributes.\n\n        Later `FieldInfo` instances override earlier ones.\n\n        Returns:\n            FieldInfo: A merged FieldInfo instance.\n        \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 536, "code": "    def _from_dataclass_field(dc_field: DataclassField[Any]) -> FieldInfo:\n        default = dc_field.default\n        if default is dataclasses.MISSING:\n            default = _Unset\n        if dc_field.default_factory is dataclasses.MISSING:\n            default_factory = _Unset\n        else:\n            default_factory = dc_field.default_factory\n        dc_field_metadata = {k: v for k, v in dc_field.metadata.items() if k in _FIELD_ARG_NAMES}\n        return Field(default=default, default_factory=default_factory, repr=dc_field.repr, **dc_field_metadata)  # pyright: ignore[reportCallIssue]\n    @staticmethod", "documentation": "        \"\"\"Return a new `FieldInfo` instance from a `dataclasses.Field` instance.\n\n        Args:\n            dc_field: The `dataclasses.Field` instance to convert.\n\n        Returns:\n            The corresponding `FieldInfo` instance.\n\n        Raises:\n            TypeError: If any of the `FieldInfo` kwargs does not match the `dataclass.Field` kwargs.\n        \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 562, "code": "    def _collect_metadata(kwargs: dict[str, Any]) -> list[Any]:\n        metadata: list[Any] = []\n        general_metadata = {}\n        for key, value in list(kwargs.items()):\n            try:\n                marker = FieldInfo.metadata_lookup[key]\n            except KeyError:\n                continue\n            del kwargs[key]\n            if value is not None:\n                if marker is None:", "documentation": "        \"\"\"Collect annotations from kwargs.\n\n        Args:\n            kwargs: Keyword arguments passed to the function.\n\n        Returns:\n            A list of metadata objects - a combination of `annotated_types.BaseMetadata` and\n                `PydanticMetadata`.\n        \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 591, "code": "    def deprecation_message(self) -> str | None:\n        if self.deprecated is None:\n            return None\n        if isinstance(self.deprecated, bool):\n            return 'deprecated' if self.deprecated else None\n        return self.deprecated if isinstance(self.deprecated, str) else self.deprecated.message\n    @property", "documentation": "        \"\"\"The deprecation message to be emitted, or `None` if not set.\"\"\""}, {"filename": "pydantic/fields.py", "start_line": 600, "code": "    def default_factory_takes_validated_data(self) -> bool | None:\n        if self.default_factory is not None:\n            return _fields.takes_validated_data_argument(self.default_factory)\n    @overload", "documentation": "        \"\"\"Whether the provided default factory callable has a validated data parameter.\n\n        Returns `None` if no default factory is set.\n        \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 616, "code": "    def get_default(self, *, call_default_factory: bool = False, validated_data: dict[str, Any] | None = None) -> Any:\n        if self.default_factory is None:\n            return _utils.smart_deepcopy(self.default)\n        elif call_default_factory:\n            if self.default_factory_takes_validated_data:\n                fac = cast('Callable[[dict[str, Any]], Any]', self.default_factory)\n                if validated_data is None:\n                    raise ValueError(\n                        \"The default factory requires the 'validated_data' argument, which was not provided when calling 'get_default'.\"\n                    )\n                return fac(validated_data)", "documentation": "        \"\"\"Get the default value.\n\n        We expose an option for whether to call the default_factory (if present), as calling it may\n        result in side effects that we want to avoid. However, there are times when it really should\n        be called (namely, when instantiating a model via `model_construct`).\n\n        Args:\n            call_default_factory: Whether to call the default factory or not.\n            validated_data: The already validated data to be passed to the default factory.\n\n        Returns:\n            The default value, calling the default factory if requested or `None` if not set.\n        \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 646, "code": "    def is_required(self) -> bool:\n        return self.default is PydanticUndefined and self.default_factory is None", "documentation": "        \"\"\"Check if the field is required (i.e., does not have a default value or factory).\n\n        Returns:\n            `True` if the field is required, `False` otherwise.\n        \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 654, "code": "    def rebuild_annotation(self) -> Any:\n        if not self.metadata:\n            return self.annotation\n        else:\n            return Annotated[(self.annotation, *self.metadata)]  # type: ignore", "documentation": "        \"\"\"Attempts to rebuild the original annotation for use in function signatures.\n\n        If metadata is present, it adds it to the original annotation using\n        `Annotated`. Otherwise, it returns the original annotation as-is.\n\n        Note that because the metadata has been flattened, the original annotation\n        may not be reconstructed exactly as originally provided, e.g. if the original\n        type had unrecognized annotations, or was annotated with a call to `pydantic.Field`.\n\n        Returns:\n            The rebuilt annotation.\n        \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 733, "code": "class _EmptyKwargs(typing_extensions.TypedDict):\n_DefaultValues = {\n    'default': ...,\n    'default_factory': None,\n    'alias': None,\n    'alias_priority': None,\n    'validation_alias': None,\n    'serialization_alias': None,\n    'title': None,\n    'description': None,\n    'examples': None,", "documentation": "    \"\"\"This class exists solely to ensure that type checking warns about passing `**extra` in `Field`.\"\"\""}, {"filename": "pydantic/fields.py", "start_line": 1144, "code": "class ModelPrivateAttr(_repr.Representation):\n    __slots__ = ('default', 'default_factory')", "documentation": "    \"\"\"A descriptor for private attributes in class models.\n\n    !!! warning\n        You generally shouldn't be creating `ModelPrivateAttr` instances directly, instead use\n        `pydantic.fields.PrivateAttr`. (This is similar to `FieldInfo` vs. `Field`.)\n\n    Attributes:\n        default: The default value of the attribute if not provided.\n        default_factory: A callable function that generates the default value of the\n            attribute if not provided.\n    \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 1171, "code": "        def __getattr__(self, item: str) -> Any:\n            if item in {'__get__', '__set__', '__delete__'}:\n                if hasattr(self.default, item):\n                    return getattr(self.default, item)\n            raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')", "documentation": "            \"\"\"This function improves compatibility with custom descriptors by ensuring delegation happens\n            as expected when the default value of a private attribute is a descriptor.\n            \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 1180, "code": "    def __set_name__(self, cls: type[Any], name: str) -> None:\n        default = self.default\n        if default is PydanticUndefined:\n            return\n        set_name = getattr(default, '__set_name__', None)\n        if callable(set_name):\n            set_name(cls, name)", "documentation": "        \"\"\"Preserve `__set_name__` protocol defined in https://peps.python.org/pep-0487.\"\"\""}, {"filename": "pydantic/fields.py", "start_line": 1189, "code": "    def get_default(self) -> Any:\n        return _utils.smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()", "documentation": "        \"\"\"Retrieve the default value of the object.\n\n        If `self.default_factory` is `None`, the method will return a deep copy of the `self.default` object.\n\n        If `self.default_factory` is not `None`, it will call `self.default_factory` and return the value returned.\n\n        Returns:\n            The default value of the object.\n        \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 1265, "code": "class ComputedFieldInfo:\n    decorator_repr: ClassVar[str] = '@computed_field'\n    wrapped_property: property\n    return_type: Any\n    alias: str | None\n    alias_priority: int | None\n    title: str | None\n    field_title_generator: typing.Callable[[str, ComputedFieldInfo], str] | None\n    description: str | None\n    deprecated: Deprecated | str | bool | None\n    examples: list[Any] | None", "documentation": "    \"\"\"A container for data from `@computed_field` so that we can access it while building the pydantic-core schema.\n\n    Attributes:\n        decorator_repr: A class variable representing the decorator string, '@computed_field'.\n        wrapped_property: The wrapped computed field property.\n        return_type: The type of the computed field property's return value.\n        alias: The alias of the property to be used during serialization.\n        alias_priority: The priority of the alias. This affects whether an alias generator is used.\n        title: Title of the computed field to include in the serialization JSON schema.\n        field_title_generator: A callable that takes a field name and returns title for it.\n        description: Description of the computed field to include in the serialization JSON schema.\n        deprecated: A deprecation message, an instance of `warnings.deprecated` or the `typing_extensions.deprecated` backport,\n            or a boolean. If `True`, a default deprecation message will be emitted when accessing the field.\n        examples: Example values of the computed field to include in the serialization JSON schema.\n        json_schema_extra: A dict or callable to provide extra JSON schema properties.\n        repr: A boolean indicating whether to include the field in the __repr__ output.\n    \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 1298, "code": "    def deprecation_message(self) -> str | None:\n        if self.deprecated is None:\n            return None\n        if isinstance(self.deprecated, bool):\n            return 'deprecated' if self.deprecated else None\n        return self.deprecated if isinstance(self.deprecated, str) else self.deprecated.message", "documentation": "        \"\"\"The deprecation message to be emitted, or `None` if not set.\"\"\""}, {"filename": "pydantic/fields.py", "start_line": 1307, "code": "def _wrapped_property_is_private(property_: cached_property | property) -> bool:  # type: ignore\n    wrapped_name: str = ''\n    if isinstance(property_, property):\n        wrapped_name = getattr(property_.fget, '__name__', '')\n    elif isinstance(property_, cached_property):  # type: ignore\n        wrapped_name = getattr(property_.func, '__name__', '')  # type: ignore\n    return wrapped_name.startswith('_') and not wrapped_name.startswith('__')\nPropertyT = typing.TypeVar('PropertyT')\n@typing.overload", "documentation": "    \"\"\"Returns true if provided property is private, False otherwise.\"\"\""}], "after_segments": [{"filename": "pydantic/fields.py", "start_line": 51, "code": "class _FromFieldInfoInputs(typing_extensions.TypedDict, total=False):\n    annotation: type[Any] | None\n    default_factory: Callable[[], Any] | Callable[[dict[str, Any]], Any] | None\n    alias: str | None\n    alias_priority: int | None\n    validation_alias: str | AliasPath | AliasChoices | None\n    serialization_alias: str | None\n    title: str | None\n    field_title_generator: Callable[[str, FieldInfo], str] | None\n    description: str | None\n    examples: list[Any] | None", "documentation": "    \"\"\"This class exists solely to add type checking for the `**kwargs` in `FieldInfo.from_field`.\"\"\""}, {"filename": "pydantic/fields.py", "start_line": 92, "code": "class _FieldInfoInputs(_FromFieldInfoInputs, total=False):\n    default: Any", "documentation": "    \"\"\"This class exists solely to add type checking for the `**kwargs` in `FieldInfo.__init__`.\"\"\""}, {"filename": "pydantic/fields.py", "start_line": 98, "code": "class FieldInfo(_repr.Representation):\n    annotation: type[Any] | None\n    default: Any\n    default_factory: Callable[[], Any] | Callable[[dict[str, Any]], Any] | None\n    alias: str | None\n    alias_priority: int | None\n    validation_alias: str | AliasPath | AliasChoices | None\n    serialization_alias: str | None\n    title: str | None\n    field_title_generator: Callable[[str, FieldInfo], str] | None\n    description: str | None", "documentation": "    \"\"\"This class holds information about a field.\n\n    `FieldInfo` is used for any field definition regardless of whether the [`Field()`][pydantic.fields.Field]\n    function is explicitly used.\n\n    !!! warning\n        You generally shouldn't be creating `FieldInfo` directly, you'll only need to use it when accessing\n        [`BaseModel`][pydantic.main.BaseModel] `.model_fields` internals.\n\n    Attributes:\n        annotation: The type annotation of the field.\n        default: The default value of the field.\n        default_factory: A callable to generate the default value. The callable can either take 0 arguments\n            (in which case it is called as is) or a single argument containing the already validated data.\n        alias: The alias name of the field.\n        alias_priority: The priority of the field's alias.\n        validation_alias: The validation alias of the field.\n        serialization_alias: The serialization alias of the field.\n        title: The title of the field.\n        field_title_generator: A callable that takes a field name and returns title for it.\n        description: The description of the field.\n        examples: List of examples of the field.\n        exclude: Whether to exclude the field from the model serialization.\n        discriminator: Field name or Discriminator for discriminating the type in a tagged union.\n        deprecated: A deprecation message, an instance of `warnings.deprecated` or the `typing_extensions.deprecated` backport,\n            or a boolean. If `True`, a default deprecation message will be emitted when accessing the field.\n        json_schema_extra: A dict or callable to provide extra JSON schema properties.\n        frozen: Whether the field is frozen.\n        validate_default: Whether to validate the default value of the field.\n        repr: Whether to include the field in representation of the model.\n        init: Whether the field should be included in the constructor of the dataclass.\n        init_var: Whether the field should _only_ be included in the constructor of the dataclass, and not stored.\n        kw_only: Whether the field should be a keyword-only argument in the constructor of the dataclass.\n        metadata: List of metadata constraints.\n    \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 208, "code": "    def __init__(self, **kwargs: Unpack[_FieldInfoInputs]) -> None:\n        self._attributes_set = {k: v for k, v in kwargs.items() if v is not _Unset}\n        kwargs = {k: _DefaultValues.get(k) if v is _Unset else v for k, v in kwargs.items()}  # type: ignore\n        self.annotation = kwargs.get('annotation')\n        default = kwargs.pop('default', PydanticUndefined)\n        if default is Ellipsis:\n            self.default = PydanticUndefined\n            self._attributes_set.pop('default', None)\n        else:\n            self.default = default\n        self.default_factory = kwargs.pop('default_factory', None)", "documentation": "        \"\"\"This class should generally not be initialized directly; instead, use the `pydantic.fields.Field` function\n        or one of the constructor classmethods.\n\n        See the signature of `pydantic.fields.Field` for more details about the expected arguments.\n        \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 262, "code": "    def from_field(default: Any = PydanticUndefined, **kwargs: Unpack[_FromFieldInfoInputs]) -> FieldInfo:\n        if 'annotation' in kwargs:\n            raise TypeError('\"annotation\" is not permitted as a Field keyword argument')\n        return FieldInfo(default=default, **kwargs)\n    @staticmethod", "documentation": "        \"\"\"Create a new `FieldInfo` object with the `Field` function.\n\n        Args:\n            default: The default value for the field. Defaults to Undefined.\n            **kwargs: Additional arguments dictionary.\n\n        Raises:\n            TypeError: If 'annotation' is passed as a keyword argument.\n\n        Returns:\n            A new FieldInfo object with the given parameters.\n\n        Example:\n            This is how you can create a field with default value like this:\n\n            ```python\n            import pydantic\n\n            class MyModel(pydantic.BaseModel):\n                foo: int = pydantic.Field(4)\n            ```\n        \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 290, "code": "    def from_annotation(annotation: type[Any], *, _source: AnnotationSource = AnnotationSource.ANY) -> FieldInfo:\n        try:\n            inspected_ann = inspect_annotation(\n                annotation,\n                annotation_source=_source,\n                unpack_type_aliases='skip',\n            )\n        except ForbiddenQualifier as e:\n            raise PydanticForbiddenQualifier(e.qualifier, annotation)\n        type_expr: Any = Any if inspected_ann.type is UNKNOWN else inspected_ann.type\n        final = 'final' in inspected_ann.qualifiers", "documentation": "        \"\"\"Creates a `FieldInfo` instance from a bare annotation.\n\n        This function is used internally to create a `FieldInfo` from a bare annotation like this:\n\n        ```python\n        import pydantic\n\n        class MyModel(pydantic.BaseModel):\n            foo: int  # <-- like this\n        ```\n\n        We also account for the case where the annotation can be an instance of `Annotated` and where\n        one of the (not first) arguments in `Annotated` is an instance of `FieldInfo`, e.g.:\n\n        ```python\n        from typing import Annotated\n\n        import annotated_types\n\n        import pydantic\n\n        class MyModel(pydantic.BaseModel):\n            foo: Annotated[int, annotated_types.Gt(42)]\n            bar: Annotated[int, pydantic.Field(gt=42)]\n        ```\n\n        Args:\n            annotation: An annotation object.\n\n        Returns:\n            An instance of the field metadata.\n        \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 470, "code": "    def merge_field_infos(*field_infos: FieldInfo, **overrides: Any) -> FieldInfo:\n        if len(field_infos) == 1:\n            field_info = copy(field_infos[0])\n            field_info._attributes_set.update(overrides)\n            default_override = overrides.pop('default', PydanticUndefined)\n            if default_override is Ellipsis:\n                default_override = PydanticUndefined\n            if default_override is not PydanticUndefined:\n                field_info.default = default_override\n            for k, v in overrides.items():\n                setattr(field_info, k, v)", "documentation": "        \"\"\"Merge `FieldInfo` instances keeping only explicitly set attributes.\n\n        Later `FieldInfo` instances override earlier ones.\n\n        Returns:\n            FieldInfo: A merged FieldInfo instance.\n        \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 536, "code": "    def _from_dataclass_field(dc_field: DataclassField[Any]) -> FieldInfo:\n        default = dc_field.default\n        if default is dataclasses.MISSING:\n            default = _Unset\n        if dc_field.default_factory is dataclasses.MISSING:\n            default_factory = _Unset\n        else:\n            default_factory = dc_field.default_factory\n        dc_field_metadata = {k: v for k, v in dc_field.metadata.items() if k in _FIELD_ARG_NAMES}\n        return Field(default=default, default_factory=default_factory, repr=dc_field.repr, **dc_field_metadata)  # pyright: ignore[reportCallIssue]\n    @staticmethod", "documentation": "        \"\"\"Return a new `FieldInfo` instance from a `dataclasses.Field` instance.\n\n        Args:\n            dc_field: The `dataclasses.Field` instance to convert.\n\n        Returns:\n            The corresponding `FieldInfo` instance.\n\n        Raises:\n            TypeError: If any of the `FieldInfo` kwargs does not match the `dataclass.Field` kwargs.\n        \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 562, "code": "    def _collect_metadata(kwargs: dict[str, Any]) -> list[Any]:\n        metadata: list[Any] = []\n        general_metadata = {}\n        for key, value in list(kwargs.items()):\n            try:\n                marker = FieldInfo.metadata_lookup[key]\n            except KeyError:\n                continue\n            del kwargs[key]\n            if value is not None:\n                if marker is None:", "documentation": "        \"\"\"Collect annotations from kwargs.\n\n        Args:\n            kwargs: Keyword arguments passed to the function.\n\n        Returns:\n            A list of metadata objects - a combination of `annotated_types.BaseMetadata` and\n                `PydanticMetadata`.\n        \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 591, "code": "    def deprecation_message(self) -> str | None:\n        if self.deprecated is None:\n            return None\n        if isinstance(self.deprecated, bool):\n            return 'deprecated' if self.deprecated else None\n        return self.deprecated if isinstance(self.deprecated, str) else self.deprecated.message\n    @property", "documentation": "        \"\"\"The deprecation message to be emitted, or `None` if not set.\"\"\""}, {"filename": "pydantic/fields.py", "start_line": 600, "code": "    def default_factory_takes_validated_data(self) -> bool | None:\n        if self.default_factory is not None:\n            return _fields.takes_validated_data_argument(self.default_factory)\n    @overload", "documentation": "        \"\"\"Whether the provided default factory callable has a validated data parameter.\n\n        Returns `None` if no default factory is set.\n        \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 616, "code": "    def get_default(self, *, call_default_factory: bool = False, validated_data: dict[str, Any] | None = None) -> Any:\n        if self.default_factory is None:\n            return _utils.smart_deepcopy(self.default)\n        elif call_default_factory:\n            if self.default_factory_takes_validated_data:\n                fac = cast('Callable[[dict[str, Any]], Any]', self.default_factory)\n                if validated_data is None:\n                    raise ValueError(\n                        \"The default factory requires the 'validated_data' argument, which was not provided when calling 'get_default'.\"\n                    )\n                return fac(validated_data)", "documentation": "        \"\"\"Get the default value.\n\n        We expose an option for whether to call the default_factory (if present), as calling it may\n        result in side effects that we want to avoid. However, there are times when it really should\n        be called (namely, when instantiating a model via `model_construct`).\n\n        Args:\n            call_default_factory: Whether to call the default factory or not.\n            validated_data: The already validated data to be passed to the default factory.\n\n        Returns:\n            The default value, calling the default factory if requested or `None` if not set.\n        \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 646, "code": "    def is_required(self) -> bool:\n        return self.default is PydanticUndefined and self.default_factory is None", "documentation": "        \"\"\"Check if the field is required (i.e., does not have a default value or factory).\n\n        Returns:\n            `True` if the field is required, `False` otherwise.\n        \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 654, "code": "    def rebuild_annotation(self) -> Any:\n        if not self.metadata:\n            return self.annotation\n        else:\n            return Annotated[(self.annotation, *self.metadata)]  # type: ignore", "documentation": "        \"\"\"Attempts to rebuild the original annotation for use in function signatures.\n\n        If metadata is present, it adds it to the original annotation using\n        `Annotated`. Otherwise, it returns the original annotation as-is.\n\n        Note that because the metadata has been flattened, the original annotation\n        may not be reconstructed exactly as originally provided, e.g. if the original\n        type had unrecognized annotations, or was annotated with a call to `pydantic.Field`.\n\n        Returns:\n            The rebuilt annotation.\n        \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 733, "code": "class _EmptyKwargs(typing_extensions.TypedDict):\n_DefaultValues = {\n    'default': ...,\n    'default_factory': None,\n    'alias': None,\n    'alias_priority': None,\n    'validation_alias': None,\n    'serialization_alias': None,\n    'title': None,\n    'description': None,\n    'examples': None,", "documentation": "    \"\"\"This class exists solely to ensure that type checking warns about passing `**extra` in `Field`.\"\"\""}, {"filename": "pydantic/fields.py", "start_line": 1144, "code": "class ModelPrivateAttr(_repr.Representation):\n    __slots__ = ('default', 'default_factory')", "documentation": "    \"\"\"A descriptor for private attributes in class models.\n\n    !!! warning\n        You generally shouldn't be creating `ModelPrivateAttr` instances directly, instead use\n        `pydantic.fields.PrivateAttr`. (This is similar to `FieldInfo` vs. `Field`.)\n\n    Attributes:\n        default: The default value of the attribute if not provided.\n        default_factory: A callable function that generates the default value of the\n            attribute if not provided.\n    \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 1171, "code": "        def __getattr__(self, item: str) -> Any:\n            if item in {'__get__', '__set__', '__delete__'}:\n                if hasattr(self.default, item):\n                    return getattr(self.default, item)\n            raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')", "documentation": "            \"\"\"This function improves compatibility with custom descriptors by ensuring delegation happens\n            as expected when the default value of a private attribute is a descriptor.\n            \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 1180, "code": "    def __set_name__(self, cls: type[Any], name: str) -> None:\n        default = self.default\n        if default is PydanticUndefined:\n            return\n        set_name = getattr(default, '__set_name__', None)\n        if callable(set_name):\n            set_name(cls, name)", "documentation": "        \"\"\"Preserve `__set_name__` protocol defined in https://peps.python.org/pep-0487.\"\"\""}, {"filename": "pydantic/fields.py", "start_line": 1189, "code": "    def get_default(self) -> Any:\n        return _utils.smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()", "documentation": "        \"\"\"Retrieve the default value of the object.\n\n        If `self.default_factory` is `None`, the method will return a deep copy of the `self.default` object.\n\n        If `self.default_factory` is not `None`, it will call `self.default_factory` and return the value returned.\n\n        Returns:\n            The default value of the object.\n        \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 1265, "code": "class ComputedFieldInfo:\n    decorator_repr: ClassVar[str] = '@computed_field'\n    wrapped_property: property\n    return_type: Any\n    alias: str | None\n    alias_priority: int | None\n    title: str | None\n    field_title_generator: typing.Callable[[str, ComputedFieldInfo], str] | None\n    description: str | None\n    deprecated: Deprecated | str | bool | None\n    examples: list[Any] | None", "documentation": "    \"\"\"A container for data from `@computed_field` so that we can access it while building the pydantic-core schema.\n\n    Attributes:\n        decorator_repr: A class variable representing the decorator string, '@computed_field'.\n        wrapped_property: The wrapped computed field property.\n        return_type: The type of the computed field property's return value.\n        alias: The alias of the property to be used during serialization.\n        alias_priority: The priority of the alias. This affects whether an alias generator is used.\n        title: Title of the computed field to include in the serialization JSON schema.\n        field_title_generator: A callable that takes a field name and returns title for it.\n        description: Description of the computed field to include in the serialization JSON schema.\n        deprecated: A deprecation message, an instance of `warnings.deprecated` or the `typing_extensions.deprecated` backport,\n            or a boolean. If `True`, a default deprecation message will be emitted when accessing the field.\n        examples: Example values of the computed field to include in the serialization JSON schema.\n        json_schema_extra: A dict or callable to provide extra JSON schema properties.\n        repr: A boolean indicating whether to include the field in the __repr__ output.\n    \"\"\""}, {"filename": "pydantic/fields.py", "start_line": 1298, "code": "    def deprecation_message(self) -> str | None:\n        if self.deprecated is None:\n            return None\n        if isinstance(self.deprecated, bool):\n            return 'deprecated' if self.deprecated else None\n        return self.deprecated if isinstance(self.deprecated, str) else self.deprecated.message", "documentation": "        \"\"\"The deprecation message to be emitted, or `None` if not set.\"\"\""}, {"filename": "pydantic/fields.py", "start_line": 1307, "code": "def _wrapped_property_is_private(property_: cached_property | property) -> bool:  # type: ignore\n    wrapped_name: str = ''\n    if isinstance(property_, property):\n        wrapped_name = getattr(property_.fget, '__name__', '')\n    elif isinstance(property_, cached_property):  # type: ignore\n        wrapped_name = getattr(property_.func, '__name__', '')  # type: ignore\n    return wrapped_name.startswith('_') and not wrapped_name.startswith('__')\nPropertyT = typing.TypeVar('PropertyT')\n@typing.overload", "documentation": "    \"\"\"Returns true if provided property is private, False otherwise.\"\"\""}]}
{"repository": "encode/starlette", "commit_sha": "5668ce62872de48aa3f87c620eb9a7e25d40a782", "commit_message": "Add state generic to WebSocket (#3132)\n\n* add state generic to WebSocket\n\n* remove extra import\n\n* add tests\n\n* add docs\n\n* Fix docs formatting for WebSocket state example\n\n---------\n\nCo-authored-by: Lucas Granberg <lucas.granberg@sensofusion.com>\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>", "commit_date": "2026-02-14T10:41:32+00:00", "author": "Lucas Granberg", "file": "starlette/websockets.py", "patch": "@@ -5,7 +5,7 @@\n from collections.abc import AsyncIterator, Iterable\n from typing import Any, cast\n \n-from starlette.requests import HTTPConnection\n+from starlette.requests import HTTPConnection, StateT\n from starlette.responses import Response\n from starlette.types import Message, Receive, Scope, Send\n \n@@ -23,7 +23,7 @@ def __init__(self, code: int = 1000, reason: str | None = None) -> None:\n         self.reason = reason or \"\"\n \n \n-class WebSocket(HTTPConnection):\n+class WebSocket(HTTPConnection[StateT]):\n     def __init__(self, scope: Scope, receive: Receive, send: Send) -> None:\n         super().__init__(scope)\n         assert scope[\"type\"] == \"websocket\"", "before_segments": [], "after_segments": []}
{"repository": "encode/starlette", "commit_sha": "5668ce62872de48aa3f87c620eb9a7e25d40a782", "commit_message": "Add state generic to WebSocket (#3132)\n\n* add state generic to WebSocket\n\n* remove extra import\n\n* add tests\n\n* add docs\n\n* Fix docs formatting for WebSocket state example\n\n---------\n\nCo-authored-by: Lucas Granberg <lucas.granberg@sensofusion.com>\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>", "commit_date": "2026-02-14T10:41:32+00:00", "author": "Lucas Granberg", "file": "tests/test_applications.py", "patch": "@@ -91,6 +91,12 @@ async def websocket_raise_custom(websocket: WebSocket) -> None:\n     raise CustomWSException()\n \n \n+async def websocket_state(websocket: WebSocket[CustomState]) -> None:\n+    await websocket.accept()\n+    await websocket.send_json({\"count\": websocket.state[\"count\"]})\n+    await websocket.close()\n+\n+\n def custom_ws_exception_handler(websocket: WebSocket, exc: CustomWSException) -> None:\n     anyio.from_thread.run(websocket.close, status.WS_1013_TRY_AGAIN_LATER)\n \n@@ -141,6 +147,7 @@ async def state_count(request: Request[CustomState]) -> JSONResponse:\n         WebSocketRoute(\"/ws-raise-websocket\", endpoint=websocket_raise_websocket_exception),\n         WebSocketRoute(\"/ws-raise-http\", endpoint=websocket_raise_http_exception),\n         WebSocketRoute(\"/ws-raise-custom\", endpoint=websocket_raise_custom),\n+        WebSocketRoute(\"/ws-state\", endpoint=websocket_state),\n         Mount(\"/users\", app=users),\n         Host(\"{subdomain}.example.org\", app=subdomain),\n     ],\n@@ -247,6 +254,12 @@ def test_websocket_raise_websocket_exception(client: TestClient) -> None:\n         }\n \n \n+def test_websocket_state(client: TestClient) -> None:\n+    with client.websocket_connect(\"/ws-state\") as session:\n+        response = session.receive_json()\n+        assert response == {\"count\": 1}\n+\n+\n def test_websocket_raise_http_exception(client: TestClient) -> None:\n     with pytest.raises(WebSocketDenialResponse) as exc:\n         with client.websocket_connect(\"/ws-raise-http\"):\n@@ -283,6 +296,7 @@ def test_routes() -> None:\n         WebSocketRoute(\"/ws-raise-websocket\", endpoint=websocket_raise_websocket_exception),\n         WebSocketRoute(\"/ws-raise-http\", endpoint=websocket_raise_http_exception),\n         WebSocketRoute(\"/ws-raise-custom\", endpoint=websocket_raise_custom),\n+        WebSocketRoute(\"/ws-state\", endpoint=websocket_state),\n         Mount(\n             \"/users\",\n             app=Router(", "before_segments": [], "after_segments": []}
{"repository": "encode/starlette", "commit_sha": "edfb48f2b59a373c9a7a4fba26e5e6d04ba108b5", "commit_message": "Version 0.44.0 (#2819)\n\n* Version 0.44.0\r\n\r\n* Update docs/release-notes.md", "commit_date": "2024-12-28T07:31:23+00:00", "author": "Marcelo Trylesinski", "file": "starlette/__init__.py", "patch": "@@ -1 +1 @@\n-__version__ = \"0.43.0\"\n+__version__ = \"0.44.0\"", "before_segments": [], "after_segments": []}
{"repository": "encode/starlette", "commit_sha": "fa7b382a66cd99e3dc18f3baa44dae5ec68be76b", "commit_message": "Version 0.39.1 (#2706)\n\n* Version 0.39.1\r\n\r\n* Update docs/release-notes.md\r\n\r\n* Update docs/release-notes.md\r\n\r\n* Update docs/release-notes.md", "commit_date": "2024-09-25T15:24:24+00:00", "author": "Marcelo Trylesinski", "file": "starlette/__init__.py", "patch": "@@ -1 +1 @@\n-__version__ = \"0.39.0\"\n+__version__ = \"0.39.1\"", "before_segments": [], "after_segments": []}
{"repository": "encode/starlette", "commit_sha": "9f16bf5c25e126200701f6e04330864f4a91a898", "commit_message": "Fix documentation on client address type (#2580)", "commit_date": "2024-04-25T06:49:38+00:00", "author": "julien4215", "file": "starlette/requests.py", "patch": "@@ -145,7 +145,7 @@ def cookies(self) -> dict[str, str]:\n \n     @property\n     def client(self) -> Address | None:\n-        # client is a 2 item tuple of (host, port), None or missing\n+        # client is a 2 item tuple of (host, port), None if missing\n         host_port = self.scope.get(\"client\")\n         if host_port is not None:\n             return Address(*host_port)", "before_segments": [{"filename": "starlette/requests.py", "start_line": 33, "code": "def cookie_parser(cookie_string: str) -> dict[str, str]:\n    cookie_dict: dict[str, str] = {}\n    for chunk in cookie_string.split(\";\"):\n        if \"=\" in chunk:\n            key, val = chunk.split(\"=\", 1)\n        else:\n            key, val = \"\", chunk\n        key, val = key.strip(), val.strip()\n        if key or val:\n            cookie_dict[key] = http_cookies._unquote(val)\n    return cookie_dict", "documentation": "    \"\"\"\n    This function parses a ``Cookie`` HTTP header into a dict of key/value pairs.\n\n    It attempts to mimic browser cookie parsing behavior: browsers and web servers\n    frequently disregard the spec (RFC 6265) when setting and reading cookies,\n    so we attempt to suit the common scenarios here.\n\n    This function has been adapted from Django 3.1.0.\n    Note: we are explicitly _NOT_ using `SimpleCookie.load` because it is based\n    on an outdated spec and will fail on lots of input we want to support\n    \"\"\""}, {"filename": "starlette/requests.py", "start_line": 64, "code": "class HTTPConnection(typing.Mapping[str, typing.Any]):", "documentation": "    \"\"\"\n    A base class for incoming HTTP connections, that is used to provide\n    any functionality that is common to both `Request` and `WebSocket`.\n    \"\"\""}], "after_segments": [{"filename": "starlette/requests.py", "start_line": 33, "code": "def cookie_parser(cookie_string: str) -> dict[str, str]:\n    cookie_dict: dict[str, str] = {}\n    for chunk in cookie_string.split(\";\"):\n        if \"=\" in chunk:\n            key, val = chunk.split(\"=\", 1)\n        else:\n            key, val = \"\", chunk\n        key, val = key.strip(), val.strip()\n        if key or val:\n            cookie_dict[key] = http_cookies._unquote(val)\n    return cookie_dict", "documentation": "    \"\"\"\n    This function parses a ``Cookie`` HTTP header into a dict of key/value pairs.\n\n    It attempts to mimic browser cookie parsing behavior: browsers and web servers\n    frequently disregard the spec (RFC 6265) when setting and reading cookies,\n    so we attempt to suit the common scenarios here.\n\n    This function has been adapted from Django 3.1.0.\n    Note: we are explicitly _NOT_ using `SimpleCookie.load` because it is based\n    on an outdated spec and will fail on lots of input we want to support\n    \"\"\""}, {"filename": "starlette/requests.py", "start_line": 64, "code": "class HTTPConnection(typing.Mapping[str, typing.Any]):", "documentation": "    \"\"\"\n    A base class for incoming HTTP connections, that is used to provide\n    any functionality that is common to both `Request` and `WebSocket`.\n    \"\"\""}]}
{"repository": "encode/starlette", "commit_sha": "554f368809e0d891a699667faf0cfbb20057eeb2", "commit_message": "Version 0.37.2 (#2533)\n\n* Version 0.37.2\r\n\r\n* Update docs/release-notes.md", "commit_date": "2024-03-05T16:08:44+00:00", "author": "Marcelo Trylesinski", "file": "starlette/__init__.py", "patch": "@@ -1 +1 @@\n-__version__ = \"0.37.1\"\n+__version__ = \"0.37.2\"", "before_segments": [], "after_segments": []}
{"repository": "encode/starlette", "commit_sha": "74ccb961b3c1b1871aa7ed70a81dd3000e0194da", "commit_message": "Version 0.37.1 (#2498)\n\n* Version 0.37.1\r\n\r\n* Update docs/release-notes.md\r\n\r\n---------\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>", "commit_date": "2024-02-09T21:43:54+00:00", "author": "Timofey Vasenin", "file": "starlette/__init__.py", "patch": "@@ -1 +1 @@\n-__version__ = \"0.37.0\"\n+__version__ = \"0.37.1\"", "before_segments": [], "after_segments": []}
{"repository": "encode/starlette", "commit_sha": "93e74a4d2f171bc48c66133d04b071c28ea63562", "commit_message": "Support the WebSocket Denial Response ASGI extension (#2041)\n\n* supply asgi_extensions to TestClient\r\n\r\n* Add WebSocket.send_response()\r\n\r\n* Add response support for WebSocket testclient\r\n\r\n* fix test for filesystem line-endings\r\n\r\n* lintint\r\n\r\n* support websocket.http.response extension by default\r\n\r\n* Improve coverate\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>\r\n\r\n* Undo unrelated change\r\n\r\n* fix incorrect error message\r\n\r\n* Update starlette/websockets.py\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>\r\n\r\n* formatting\r\n\r\n* Re-introduce close-code and close-reason to WebSocketReject\r\n\r\n* Make sure the \"websocket.connect\" message is received in tests\r\n\r\n* Deliver a websocket.disconnect message to the app even if it closes/rejects itself.\r\n\r\n* Add test for filling out missing `websocket.disconnect` code\r\n\r\n* Add rejection headers.  Expand tests.\r\n\r\n* Fix types, headers in message are `bytes` tuples.\r\n\r\n* Minimal WebSocket Denial Response implementation\r\n\r\n* Revert \"Minimal WebSocket Denial Response implementation\"\r\n\r\nThis reverts commit 7af10ddcfa5423c18953cf5d1317cb5aa30a014c.\r\n\r\n* Rename to send_denial_response and update documentation\r\n\r\n* Remove the app_disconnect_msg.  This can be added later in a separate PR\r\n\r\n* Remove status code 1005 from this PR\r\n\r\n* Assume that the application has tested for the extension before sending websocket.http.response.start\r\n\r\n* Rename WebSocketReject to WebSocketDenialResponse\r\n\r\n* Remove code and status from WebSocketDenialResponse.\r\nJust send a regular WebSocketDisconnect even when connection is rejected with close()\r\n\r\n* Raise an exception if attempting to send a http response and server does not support it.\r\n\r\n* WebSocketDenialClose and WebSocketDenialResponse\r\nThese are both instances of WebSocketDenial.\r\n\r\n* Update starlette/testclient.py\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>\r\n\r\n* Revert \"WebSocketDenialClose and WebSocketDenialResponse\"\r\n\r\nThis reverts commit 71b76e3f1c87064fe8458ff9d4ad0b242cbf15e7.\r\n\r\n* Rename parameters, member variables\r\n\r\n* Use httpx.Response as the base for WebSocketDenialResponse.\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>\r\n\r\n* Update sanity check message\r\n\r\n* Remove un-needed function\r\n\r\n* Expand error message test regex\r\n\r\n* Add type hings to test methods\r\n\r\n* Add doc string to test.\r\n\r\n* Fix mypy complaining about mismatching parent methods.\r\n\r\n* nitpick & remove test\r\n\r\n* Simplify the documentation\r\n\r\n* Update starlette/testclient.py\r\n\r\n* Update starlette/testclient.py\r\n\r\n* Remove an unnecessary test\r\n\r\n* there is no special \"close because of rejection\" in the testclient anymore.\r\n\r\n---------\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>", "commit_date": "2024-02-04T20:16:10+00:00", "author": "Kristj\u00e1n Valur J\u00f3nsson", "file": "starlette/responses.py", "patch": "@@ -148,14 +148,15 @@ def delete_cookie(\n         )\n \n     async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n+        prefix = \"websocket.\" if scope[\"type\"] == \"websocket\" else \"\"\n         await send(\n             {\n-                \"type\": \"http.response.start\",\n+                \"type\": prefix + \"http.response.start\",\n                 \"status\": self.status_code,\n                 \"headers\": self.raw_headers,\n             }\n         )\n-        await send({\"type\": \"http.response.body\", \"body\": self.body})\n+        await send({\"type\": prefix + \"http.response.body\", \"body\": self.body})\n \n         if self.background is not None:\n             await self.background()", "before_segments": [], "after_segments": []}
{"repository": "encode/starlette", "commit_sha": "93e74a4d2f171bc48c66133d04b071c28ea63562", "commit_message": "Support the WebSocket Denial Response ASGI extension (#2041)\n\n* supply asgi_extensions to TestClient\r\n\r\n* Add WebSocket.send_response()\r\n\r\n* Add response support for WebSocket testclient\r\n\r\n* fix test for filesystem line-endings\r\n\r\n* lintint\r\n\r\n* support websocket.http.response extension by default\r\n\r\n* Improve coverate\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>\r\n\r\n* Undo unrelated change\r\n\r\n* fix incorrect error message\r\n\r\n* Update starlette/websockets.py\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>\r\n\r\n* formatting\r\n\r\n* Re-introduce close-code and close-reason to WebSocketReject\r\n\r\n* Make sure the \"websocket.connect\" message is received in tests\r\n\r\n* Deliver a websocket.disconnect message to the app even if it closes/rejects itself.\r\n\r\n* Add test for filling out missing `websocket.disconnect` code\r\n\r\n* Add rejection headers.  Expand tests.\r\n\r\n* Fix types, headers in message are `bytes` tuples.\r\n\r\n* Minimal WebSocket Denial Response implementation\r\n\r\n* Revert \"Minimal WebSocket Denial Response implementation\"\r\n\r\nThis reverts commit 7af10ddcfa5423c18953cf5d1317cb5aa30a014c.\r\n\r\n* Rename to send_denial_response and update documentation\r\n\r\n* Remove the app_disconnect_msg.  This can be added later in a separate PR\r\n\r\n* Remove status code 1005 from this PR\r\n\r\n* Assume that the application has tested for the extension before sending websocket.http.response.start\r\n\r\n* Rename WebSocketReject to WebSocketDenialResponse\r\n\r\n* Remove code and status from WebSocketDenialResponse.\r\nJust send a regular WebSocketDisconnect even when connection is rejected with close()\r\n\r\n* Raise an exception if attempting to send a http response and server does not support it.\r\n\r\n* WebSocketDenialClose and WebSocketDenialResponse\r\nThese are both instances of WebSocketDenial.\r\n\r\n* Update starlette/testclient.py\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>\r\n\r\n* Revert \"WebSocketDenialClose and WebSocketDenialResponse\"\r\n\r\nThis reverts commit 71b76e3f1c87064fe8458ff9d4ad0b242cbf15e7.\r\n\r\n* Rename parameters, member variables\r\n\r\n* Use httpx.Response as the base for WebSocketDenialResponse.\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>\r\n\r\n* Update sanity check message\r\n\r\n* Remove un-needed function\r\n\r\n* Expand error message test regex\r\n\r\n* Add type hings to test methods\r\n\r\n* Add doc string to test.\r\n\r\n* Fix mypy complaining about mismatching parent methods.\r\n\r\n* nitpick & remove test\r\n\r\n* Simplify the documentation\r\n\r\n* Update starlette/testclient.py\r\n\r\n* Update starlette/testclient.py\r\n\r\n* Remove an unnecessary test\r\n\r\n* there is no special \"close because of rejection\" in the testclient anymore.\r\n\r\n---------\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>", "commit_date": "2024-02-04T20:16:10+00:00", "author": "Kristj\u00e1n Valur J\u00f3nsson", "file": "starlette/testclient.py", "patch": "@@ -78,6 +78,16 @@ def __init__(self, session: WebSocketTestSession) -> None:\n         self.session = session\n \n \n+class WebSocketDenialResponse(  # type: ignore[misc]\n+    httpx.Response,\n+    WebSocketDisconnect,\n+):\n+    \"\"\"\n+    A special case of `WebSocketDisconnect`, raised in the `TestClient` if the\n+    `WebSocket` is closed before being accepted with a `send_denial_response()`.\n+    \"\"\"\n+\n+\n class WebSocketTestSession:\n     def __init__(\n         self,\n@@ -159,7 +169,22 @@ async def _asgi_send(self, message: Message) -> None:\n     def _raise_on_close(self, message: Message) -> None:\n         if message[\"type\"] == \"websocket.close\":\n             raise WebSocketDisconnect(\n-                message.get(\"code\", 1000), message.get(\"reason\", \"\")\n+                code=message.get(\"code\", 1000), reason=message.get(\"reason\", \"\")\n+            )\n+        elif message[\"type\"] == \"websocket.http.response.start\":\n+            status_code: int = message[\"status\"]\n+            headers: list[tuple[bytes, bytes]] = message[\"headers\"]\n+            body: list[bytes] = []\n+            while True:\n+                message = self.receive()\n+                assert message[\"type\"] == \"websocket.http.response.body\"\n+                body.append(message[\"body\"])\n+                if not message.get(\"more_body\", False):\n+                    break\n+            raise WebSocketDenialResponse(\n+                status_code=status_code,\n+                headers=headers,\n+                content=b\"\".join(body),\n             )\n \n     def send(self, message: Message) -> None:\n@@ -277,6 +302,7 @@ def handle_request(self, request: httpx.Request) -> httpx.Response:\n                 \"server\": [host, port],\n                 \"subprotocols\": subprotocols,\n                 \"state\": self.app_state.copy(),\n+                \"extensions\": {\"websocket.http.response\": {}},\n             }\n             session = WebSocketTestSession(self.app, scope, self.portal_factory)\n             raise _Upgrade(session)", "before_segments": [{"filename": "starlette/testclient.py", "start_line": 57, "code": "class _WrapASGI2:", "documentation": "    \"\"\"\n    Provide an ASGI3 interface onto an ASGI2 app.\n    \"\"\""}], "after_segments": [{"filename": "starlette/testclient.py", "start_line": 57, "code": "class _WrapASGI2:", "documentation": "    \"\"\"\n    Provide an ASGI3 interface onto an ASGI2 app.\n    \"\"\""}]}
{"repository": "encode/starlette", "commit_sha": "93e74a4d2f171bc48c66133d04b071c28ea63562", "commit_message": "Support the WebSocket Denial Response ASGI extension (#2041)\n\n* supply asgi_extensions to TestClient\r\n\r\n* Add WebSocket.send_response()\r\n\r\n* Add response support for WebSocket testclient\r\n\r\n* fix test for filesystem line-endings\r\n\r\n* lintint\r\n\r\n* support websocket.http.response extension by default\r\n\r\n* Improve coverate\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>\r\n\r\n* Undo unrelated change\r\n\r\n* fix incorrect error message\r\n\r\n* Update starlette/websockets.py\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>\r\n\r\n* formatting\r\n\r\n* Re-introduce close-code and close-reason to WebSocketReject\r\n\r\n* Make sure the \"websocket.connect\" message is received in tests\r\n\r\n* Deliver a websocket.disconnect message to the app even if it closes/rejects itself.\r\n\r\n* Add test for filling out missing `websocket.disconnect` code\r\n\r\n* Add rejection headers.  Expand tests.\r\n\r\n* Fix types, headers in message are `bytes` tuples.\r\n\r\n* Minimal WebSocket Denial Response implementation\r\n\r\n* Revert \"Minimal WebSocket Denial Response implementation\"\r\n\r\nThis reverts commit 7af10ddcfa5423c18953cf5d1317cb5aa30a014c.\r\n\r\n* Rename to send_denial_response and update documentation\r\n\r\n* Remove the app_disconnect_msg.  This can be added later in a separate PR\r\n\r\n* Remove status code 1005 from this PR\r\n\r\n* Assume that the application has tested for the extension before sending websocket.http.response.start\r\n\r\n* Rename WebSocketReject to WebSocketDenialResponse\r\n\r\n* Remove code and status from WebSocketDenialResponse.\r\nJust send a regular WebSocketDisconnect even when connection is rejected with close()\r\n\r\n* Raise an exception if attempting to send a http response and server does not support it.\r\n\r\n* WebSocketDenialClose and WebSocketDenialResponse\r\nThese are both instances of WebSocketDenial.\r\n\r\n* Update starlette/testclient.py\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>\r\n\r\n* Revert \"WebSocketDenialClose and WebSocketDenialResponse\"\r\n\r\nThis reverts commit 71b76e3f1c87064fe8458ff9d4ad0b242cbf15e7.\r\n\r\n* Rename parameters, member variables\r\n\r\n* Use httpx.Response as the base for WebSocketDenialResponse.\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>\r\n\r\n* Update sanity check message\r\n\r\n* Remove un-needed function\r\n\r\n* Expand error message test regex\r\n\r\n* Add type hings to test methods\r\n\r\n* Add doc string to test.\r\n\r\n* Fix mypy complaining about mismatching parent methods.\r\n\r\n* nitpick & remove test\r\n\r\n* Simplify the documentation\r\n\r\n* Update starlette/testclient.py\r\n\r\n* Update starlette/testclient.py\r\n\r\n* Remove an unnecessary test\r\n\r\n* there is no special \"close because of rejection\" in the testclient anymore.\r\n\r\n---------\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>", "commit_date": "2024-02-04T20:16:10+00:00", "author": "Kristj\u00e1n Valur J\u00f3nsson", "file": "starlette/websockets.py", "patch": "@@ -5,13 +5,15 @@\n import typing\n \n from starlette.requests import HTTPConnection\n+from starlette.responses import Response\n from starlette.types import Message, Receive, Scope, Send\n \n \n class WebSocketState(enum.Enum):\n     CONNECTING = 0\n     CONNECTED = 1\n     DISCONNECTED = 2\n+    RESPONSE = 3\n \n \n class WebSocketDisconnect(Exception):\n@@ -65,13 +67,20 @@ async def send(self, message: Message) -> None:\n         \"\"\"\n         if self.application_state == WebSocketState.CONNECTING:\n             message_type = message[\"type\"]\n-            if message_type not in {\"websocket.accept\", \"websocket.close\"}:\n+            if message_type not in {\n+                \"websocket.accept\",\n+                \"websocket.close\",\n+                \"websocket.http.response.start\",\n+            }:\n                 raise RuntimeError(\n-                    'Expected ASGI message \"websocket.accept\" or '\n-                    f'\"websocket.close\", but got {message_type!r}'\n+                    'Expected ASGI message \"websocket.accept\",'\n+                    '\"websocket.close\" or \"websocket.http.response.start\",'\n+                    f\"but got {message_type!r}\"\n                 )\n             if message_type == \"websocket.close\":\n                 self.application_state = WebSocketState.DISCONNECTED\n+            elif message_type == \"websocket.http.response.start\":\n+                self.application_state = WebSocketState.RESPONSE\n             else:\n                 self.application_state = WebSocketState.CONNECTED\n             await self._send(message)\n@@ -89,6 +98,16 @@ async def send(self, message: Message) -> None:\n             except IOError:\n                 self.application_state = WebSocketState.DISCONNECTED\n                 raise WebSocketDisconnect(code=1006)\n+        elif self.application_state == WebSocketState.RESPONSE:\n+            message_type = message[\"type\"]\n+            if message_type != \"websocket.http.response.body\":\n+                raise RuntimeError(\n+                    'Expected ASGI message \"websocket.http.response.body\", '\n+                    f\"but got {message_type!r}\"\n+                )\n+            if not message.get(\"more_body\", False):\n+                self.application_state = WebSocketState.DISCONNECTED\n+            await self._send(message)\n         else:\n             raise RuntimeError('Cannot call \"send\" once a close message has been sent.')\n \n@@ -185,6 +204,14 @@ async def close(self, code: int = 1000, reason: str | None = None) -> None:\n             {\"type\": \"websocket.close\", \"code\": code, \"reason\": reason or \"\"}\n         )\n \n+    async def send_denial_response(self, response: Response) -> None:\n+        if \"websocket.http.response\" in self.scope.get(\"extensions\", {}):\n+            await response(self.scope, self.receive, self.send)\n+        else:\n+            raise RuntimeError(\n+                \"The server doesn't support the Websocket Denial Response extension.\"\n+            )\n+\n \n class WebSocketClose:\n     def __init__(self, code: int = 1000, reason: str | None = None) -> None:", "before_segments": [], "after_segments": []}
{"repository": "encode/starlette", "commit_sha": "93e74a4d2f171bc48c66133d04b071c28ea63562", "commit_message": "Support the WebSocket Denial Response ASGI extension (#2041)\n\n* supply asgi_extensions to TestClient\r\n\r\n* Add WebSocket.send_response()\r\n\r\n* Add response support for WebSocket testclient\r\n\r\n* fix test for filesystem line-endings\r\n\r\n* lintint\r\n\r\n* support websocket.http.response extension by default\r\n\r\n* Improve coverate\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>\r\n\r\n* Undo unrelated change\r\n\r\n* fix incorrect error message\r\n\r\n* Update starlette/websockets.py\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>\r\n\r\n* formatting\r\n\r\n* Re-introduce close-code and close-reason to WebSocketReject\r\n\r\n* Make sure the \"websocket.connect\" message is received in tests\r\n\r\n* Deliver a websocket.disconnect message to the app even if it closes/rejects itself.\r\n\r\n* Add test for filling out missing `websocket.disconnect` code\r\n\r\n* Add rejection headers.  Expand tests.\r\n\r\n* Fix types, headers in message are `bytes` tuples.\r\n\r\n* Minimal WebSocket Denial Response implementation\r\n\r\n* Revert \"Minimal WebSocket Denial Response implementation\"\r\n\r\nThis reverts commit 7af10ddcfa5423c18953cf5d1317cb5aa30a014c.\r\n\r\n* Rename to send_denial_response and update documentation\r\n\r\n* Remove the app_disconnect_msg.  This can be added later in a separate PR\r\n\r\n* Remove status code 1005 from this PR\r\n\r\n* Assume that the application has tested for the extension before sending websocket.http.response.start\r\n\r\n* Rename WebSocketReject to WebSocketDenialResponse\r\n\r\n* Remove code and status from WebSocketDenialResponse.\r\nJust send a regular WebSocketDisconnect even when connection is rejected with close()\r\n\r\n* Raise an exception if attempting to send a http response and server does not support it.\r\n\r\n* WebSocketDenialClose and WebSocketDenialResponse\r\nThese are both instances of WebSocketDenial.\r\n\r\n* Update starlette/testclient.py\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>\r\n\r\n* Revert \"WebSocketDenialClose and WebSocketDenialResponse\"\r\n\r\nThis reverts commit 71b76e3f1c87064fe8458ff9d4ad0b242cbf15e7.\r\n\r\n* Rename parameters, member variables\r\n\r\n* Use httpx.Response as the base for WebSocketDenialResponse.\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>\r\n\r\n* Update sanity check message\r\n\r\n* Remove un-needed function\r\n\r\n* Expand error message test regex\r\n\r\n* Add type hings to test methods\r\n\r\n* Add doc string to test.\r\n\r\n* Fix mypy complaining about mismatching parent methods.\r\n\r\n* nitpick & remove test\r\n\r\n* Simplify the documentation\r\n\r\n* Update starlette/testclient.py\r\n\r\n* Update starlette/testclient.py\r\n\r\n* Remove an unnecessary test\r\n\r\n* there is no special \"close because of rejection\" in the testclient anymore.\r\n\r\n---------\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>", "commit_date": "2024-02-04T20:16:10+00:00", "author": "Kristj\u00e1n Valur J\u00f3nsson", "file": "tests/test_websockets.py", "patch": "@@ -6,7 +6,8 @@\n from anyio.abc import ObjectReceiveStream, ObjectSendStream\n \n from starlette import status\n-from starlette.testclient import TestClient\n+from starlette.responses import Response\n+from starlette.testclient import TestClient, WebSocketDenialResponse\n from starlette.types import Message, Receive, Scope, Send\n from starlette.websockets import WebSocket, WebSocketDisconnect, WebSocketState\n \n@@ -293,6 +294,8 @@ async def app(scope: Scope, receive: Receive, send: Send) -> None:\n def test_rejected_connection(test_client_factory: Callable[..., TestClient]):\n     async def app(scope: Scope, receive: Receive, send: Send) -> None:\n         websocket = WebSocket(scope, receive=receive, send=send)\n+        msg = await websocket.receive()\n+        assert msg == {\"type\": \"websocket.connect\"}\n         await websocket.close(status.WS_1001_GOING_AWAY)\n \n     client = test_client_factory(app)\n@@ -302,6 +305,111 @@ async def app(scope: Scope, receive: Receive, send: Send) -> None:\n     assert exc.value.code == status.WS_1001_GOING_AWAY\n \n \n+def test_send_denial_response(test_client_factory: Callable[..., TestClient]):\n+    async def app(scope: Scope, receive: Receive, send: Send) -> None:\n+        websocket = WebSocket(scope, receive=receive, send=send)\n+        msg = await websocket.receive()\n+        assert msg == {\"type\": \"websocket.connect\"}\n+        response = Response(status_code=404, content=\"foo\")\n+        await websocket.send_denial_response(response)\n+\n+    client = test_client_factory(app)\n+    with pytest.raises(WebSocketDenialResponse) as exc:\n+        with client.websocket_connect(\"/\"):\n+            pass  # pragma: no cover\n+    assert exc.value.status_code == 404\n+    assert exc.value.content == b\"foo\"\n+\n+\n+def test_send_response_multi(test_client_factory: Callable[..., TestClient]):\n+    async def app(scope: Scope, receive: Receive, send: Send) -> None:\n+        websocket = WebSocket(scope, receive=receive, send=send)\n+        msg = await websocket.receive()\n+        assert msg == {\"type\": \"websocket.connect\"}\n+        await websocket.send(\n+            {\n+                \"type\": \"websocket.http.response.start\",\n+                \"status\": 404,\n+                \"headers\": [(b\"content-type\", b\"text/plain\"), (b\"foo\", b\"bar\")],\n+            }\n+        )\n+        await websocket.send(\n+            {\n+                \"type\": \"websocket.http.response.body\",\n+                \"body\": b\"hard\",\n+                \"more_body\": True,\n+            }\n+        )\n+        await websocket.send(\n+            {\n+                \"type\": \"websocket.http.response.body\",\n+                \"body\": b\"body\",\n+            }\n+        )\n+\n+    client = test_client_factory(app)\n+    with pytest.raises(WebSocketDenialResponse) as exc:\n+        with client.websocket_connect(\"/\"):\n+            pass  # pragma: no cover\n+    assert exc.value.status_code == 404\n+    assert exc.value.content == b\"hardbody\"\n+    assert exc.value.headers[\"foo\"] == \"bar\"\n+\n+\n+def test_send_response_unsupported(test_client_factory: Callable[..., TestClient]):\n+    async def app(scope: Scope, receive: Receive, send: Send) -> None:\n+        del scope[\"extensions\"][\"websocket.http.response\"]\n+        websocket = WebSocket(scope, receive=receive, send=send)\n+        msg = await websocket.receive()\n+        assert msg == {\"type\": \"websocket.connect\"}\n+        response = Response(status_code=404, content=\"foo\")\n+        with pytest.raises(\n+            RuntimeError,\n+            match=\"The server doesn't support the Websocket Denial Response extension.\",\n+        ):\n+            await websocket.send_denial_response(response)\n+        await websocket.close()\n+\n+    client = test_client_factory(app)\n+    with pytest.raises(WebSocketDisconnect) as exc:\n+        with client.websocket_connect(\"/\"):\n+            pass  # pragma: no cover\n+    assert exc.value.code == status.WS_1000_NORMAL_CLOSURE\n+\n+\n+def test_send_response_duplicate_start(test_client_factory: Callable[..., TestClient]):\n+    async def app(scope: Scope, receive: Receive, send: Send) -> None:\n+        websocket = WebSocket(scope, receive=receive, send=send)\n+        msg = await websocket.receive()\n+        assert msg == {\"type\": \"websocket.connect\"}\n+        response = Response(status_code=404, content=\"foo\")\n+        await websocket.send(\n+            {\n+                \"type\": \"websocket.http.response.start\",\n+                \"status\": response.status_code,\n+                \"headers\": response.raw_headers,\n+            }\n+        )\n+        await websocket.send(\n+            {\n+                \"type\": \"websocket.http.response.start\",\n+                \"status\": response.status_code,\n+                \"headers\": response.raw_headers,\n+            }\n+        )\n+\n+    client = test_client_factory(app)\n+    with pytest.raises(\n+        RuntimeError,\n+        match=(\n+            'Expected ASGI message \"websocket.http.response.body\", but got '\n+            \"'websocket.http.response.start'\"\n+        ),\n+    ):\n+        with client.websocket_connect(\"/\"):\n+            pass  # pragma: no cover\n+\n+\n def test_subprotocol(test_client_factory: Callable[..., TestClient]):\n     async def app(scope: Scope, receive: Receive, send: Send) -> None:\n         websocket = WebSocket(scope, receive=receive, send=send)", "before_segments": [{"filename": "tests/test_websockets.py", "start_line": 375, "code": "def test_websocket_scope_interface():\n    async def mock_receive() -> Message:  # type: ignore\n        ...  # pragma: no cover\n    async def mock_send(message: Message) -> None:\n        ...  # pragma: no cover\n    websocket = WebSocket(\n        {\"type\": \"websocket\", \"path\": \"/abc/\", \"headers\": []},\n        receive=mock_receive,\n        send=mock_send,\n    )\n    assert websocket[\"type\"] == \"websocket\"", "documentation": "    \"\"\"\n    A WebSocket can be instantiated with a scope, and presents a `Mapping`\n    interface.\n    \"\"\""}], "after_segments": [{"filename": "tests/test_websockets.py", "start_line": 483, "code": "def test_websocket_scope_interface():\n    async def mock_receive() -> Message:  # type: ignore\n        ...  # pragma: no cover\n    async def mock_send(message: Message) -> None:\n        ...  # pragma: no cover\n    websocket = WebSocket(\n        {\"type\": \"websocket\", \"path\": \"/abc/\", \"headers\": []},\n        receive=mock_receive,\n        send=mock_send,\n    )\n    assert websocket[\"type\"] == \"websocket\"", "documentation": "    \"\"\"\n    A WebSocket can be instantiated with a scope, and presents a `Mapping`\n    interface.\n    \"\"\""}]}
{"repository": "encode/starlette", "commit_sha": "04a7d9d8d1db3734c028d89a6a3fd636ddcafedf", "commit_message": "Version 0.36.2 (#2456)\n\n* Version 0.36.2\r\n\r\n* Apply suggestions from code review\r\n\r\n* Update docs/release-notes.md", "commit_date": "2024-02-03T12:39:24+00:00", "author": "Marcelo Trylesinski", "file": "starlette/__init__.py", "patch": "@@ -1 +1 @@\n-__version__ = \"0.36.1\"\n+__version__ = \"0.36.2\"", "before_segments": [], "after_segments": []}
{"repository": "encode/starlette", "commit_sha": "b8eebef38711902769fc1846c9bb2a1fcd26960d", "commit_message": "Avoid duplicate charset on `Content-Type` (#2443)\n\n* fix(response): avoid duplicated charset\r\n\r\n* Update docs/responses.md\r\n\r\n---------\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>", "commit_date": "2024-02-03T08:39:01+00:00", "author": "Mikkel Duif", "file": "starlette/responses.py", "patch": "@@ -73,7 +73,10 @@ def init_headers(self, headers: typing.Mapping[str, str] | None = None) -> None:\n \n         content_type = self.media_type\n         if content_type is not None and populate_content_type:\n-            if content_type.startswith(\"text/\"):\n+            if (\n+                content_type.startswith(\"text/\")\n+                and \"charset=\" not in content_type.lower()\n+            ):\n                 content_type += \"; charset=\" + self.charset\n             raw_headers.append((b\"content-type\", content_type.encode(\"latin-1\")))\n ", "before_segments": [], "after_segments": []}
{"repository": "encode/starlette", "commit_sha": "b8eebef38711902769fc1846c9bb2a1fcd26960d", "commit_message": "Avoid duplicate charset on `Content-Type` (#2443)\n\n* fix(response): avoid duplicated charset\r\n\r\n* Update docs/responses.md\r\n\r\n---------\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>", "commit_date": "2024-02-03T08:39:01+00:00", "author": "Mikkel Duif", "file": "tests/test_responses.py", "patch": "@@ -473,6 +473,13 @@ def test_non_empty_response(test_client_factory):\n     assert response.headers[\"content-length\"] == \"2\"\n \n \n+def test_response_do_not_add_redundant_charset(test_client_factory):\n+    app = Response(media_type=\"text/plain; charset=utf-8\")\n+    client = test_client_factory(app)\n+    response = client.get(\"/\")\n+    assert response.headers[\"content-type\"] == \"text/plain; charset=utf-8\"\n+\n+\n def test_file_response_known_size(tmpdir, test_client_factory):\n     path = os.path.join(tmpdir, \"xyz\")\n     content = b\"<file content>\" * 1000", "before_segments": [], "after_segments": []}
{"repository": "encode/starlette", "commit_sha": "aa6caf986bdc7328b6fb416e2411a4d44afe2a6c", "commit_message": "Version 0.32.0 (#2324)\n\n* Version 0.32.0\r\n\r\n* Update docs/release-notes.md", "commit_date": "2023-11-04T12:16:33+00:00", "author": "Marcelo Trylesinski", "file": "starlette/__init__.py", "patch": "@@ -1 +1 @@\n-__version__ = \"0.31.1\"\n+__version__ = \"0.32.0\"", "before_segments": [], "after_segments": []}
{"repository": "encode/starlette", "commit_sha": "0c4b68ac4e179a788e38bac28f299dcad0f36e99", "commit_message": "Version 0.27.0 (#2147)\n\n* Version 0.27.0\r\n\r\n* Update release-notes.md\r\n\r\n* Update docs/release-notes.md\r\n\r\n* Update docs/release-notes.md\r\n\r\n* Update docs/release-notes.md\r\n\r\n* Update release-notes.md\r\n\r\n* Update docs/release-notes.md\r\n\r\n* Update release-notes.md\r\n\r\n* Update docs/release-notes.md\r\n\r\n* Update docs/release-notes.md\r\n\r\n* Update docs/release-notes.md\r\n\r\n---------\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>", "commit_date": "2023-05-16T10:56:45+00:00", "author": "Amin Alaee", "file": "starlette/__init__.py", "patch": "@@ -1 +1 @@\n-__version__ = \"0.26.1\"\n+__version__ = \"0.27.0\"", "before_segments": [], "after_segments": []}
{"repository": "encode/starlette", "commit_sha": "782256837879ece72a89c9343813b5b78898e099", "commit_message": "Add Config.env_prefix option (#1990)\n\n* add Config.env_prefix option\r\n\r\n* fix variable name in docs\r\n\r\n* simplify test case\r\n\r\n* rollback markdown formatting\r\n\r\n* Update docs/config.md\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>", "commit_date": "2023-01-22T21:24:35+00:00", "author": "Alex Oleshkevich", "file": "starlette/config.py", "patch": "@@ -54,8 +54,10 @@ def __init__(\n         self,\n         env_file: typing.Optional[typing.Union[str, Path]] = None,\n         environ: typing.Mapping[str, str] = environ,\n+        env_prefix: str = \"\",\n     ) -> None:\n         self.environ = environ\n+        self.env_prefix = env_prefix\n         self.file_values: typing.Dict[str, str] = {}\n         if env_file is not None and os.path.isfile(env_file):\n             self.file_values = self._read_file(env_file)\n@@ -103,6 +105,7 @@ def get(\n         cast: typing.Optional[typing.Callable] = None,\n         default: typing.Any = undefined,\n     ) -> typing.Any:\n+        key = self.env_prefix + key\n         if key in self.environ:\n             value = self.environ[key]\n             return self._perform_cast(key, value, cast)", "before_segments": [], "after_segments": []}
{"repository": "encode/starlette", "commit_sha": "782256837879ece72a89c9343813b5b78898e099", "commit_message": "Add Config.env_prefix option (#1990)\n\n* add Config.env_prefix option\r\n\r\n* fix variable name in docs\r\n\r\n* simplify test case\r\n\r\n* rollback markdown formatting\r\n\r\n* Update docs/config.md\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>", "commit_date": "2023-01-22T21:24:35+00:00", "author": "Alex Oleshkevich", "file": "tests/test_config.py", "patch": "@@ -127,3 +127,13 @@ def test_environ():\n     environ = Environ()\n     assert list(iter(environ)) == list(iter(os.environ))\n     assert len(environ) == len(os.environ)\n+\n+\n+def test_config_with_env_prefix(tmpdir, monkeypatch):\n+    config = Config(\n+        environ={\"APP_DEBUG\": \"value\", \"ENVIRONMENT\": \"dev\"}, env_prefix=\"APP_\"\n+    )\n+    assert config.get(\"DEBUG\") == \"value\"\n+\n+    with pytest.raises(KeyError):\n+        config.get(\"ENVIRONMENT\")", "before_segments": [{"filename": "tests/test_config.py", "start_line": 11, "code": "def test_config_types() -> None:\n    config = Config(\n        environ={\"STR\": \"some_str_value\", \"STR_CAST\": \"some_str_value\", \"BOOL\": \"true\"}\n    )\n    assert_type(config(\"STR\"), str)\n    assert_type(config(\"STR_DEFAULT\", default=\"\"), str)\n    assert_type(config(\"STR_CAST\", cast=str), str)\n    assert_type(config(\"STR_NONE\", default=None), Optional[str])\n    assert_type(config(\"STR_CAST_NONE\", cast=str, default=None), Optional[str])\n    assert_type(config(\"STR_CAST_STR\", cast=str, default=\"\"), str)\n    assert_type(config(\"BOOL\", cast=bool), bool)", "documentation": "    \"\"\"\n    We use `assert_type` to test the types returned by Config via mypy.\n    \"\"\""}], "after_segments": [{"filename": "tests/test_config.py", "start_line": 11, "code": "def test_config_types() -> None:\n    config = Config(\n        environ={\"STR\": \"some_str_value\", \"STR_CAST\": \"some_str_value\", \"BOOL\": \"true\"}\n    )\n    assert_type(config(\"STR\"), str)\n    assert_type(config(\"STR_DEFAULT\", default=\"\"), str)\n    assert_type(config(\"STR_CAST\", cast=str), str)\n    assert_type(config(\"STR_NONE\", default=None), Optional[str])\n    assert_type(config(\"STR_CAST_NONE\", cast=str, default=None), Optional[str])\n    assert_type(config(\"STR_CAST_STR\", cast=str, default=\"\"), str)\n    assert_type(config(\"BOOL\", cast=bool), bool)", "documentation": "    \"\"\"\n    We use `assert_type` to test the types returned by Config via mypy.\n    \"\"\""}]}
{"repository": "encode/starlette", "commit_sha": "4519fba9b557d89cbefda9b4de71f8b90a62b406", "commit_message": "Version 0.20.2 (#1658)\n\n* Version 0.20.2\r\n\r\n* Update release-notes.md\r\n\r\n* Update release-notes.md\r\n\r\n* Update docs/release-notes.md\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>\r\n\r\n* Update docs/release-notes.md\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>\r\n\r\n* Update docs/release-notes.md\r\n\r\n* Update docs/release-notes.md\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>", "commit_date": "2022-06-07T20:14:25+00:00", "author": "Amin Alaee", "file": "starlette/__init__.py", "patch": "@@ -1 +1 @@\n-__version__ = \"0.20.1\"\n+__version__ = \"0.20.2\"", "before_segments": [], "after_segments": []}
{"repository": "encode/starlette", "commit_sha": "d81545c71a7988cfd57c613be02f4661449c0793", "commit_message": "Version 0.20.1 (#1644)\n\n* Version 0.20.1\r\n\r\n* Update docs/release-notes.md\r\n\r\n* Update release-notes.md\r\n\r\n* Update release-notes.md\r\n\r\n* Update release-notes.md", "commit_date": "2022-05-28T07:58:34+00:00", "author": "Marcelo Trylesinski", "file": "starlette/__init__.py", "patch": "@@ -1 +1 @@\n-__version__ = \"0.20.0\"\n+__version__ = \"0.20.1\"", "before_segments": [], "after_segments": []}
{"repository": "encode/starlette", "commit_sha": "830f3486537916bae6b46948ff922adc14a22b7c", "commit_message": "Version 0.20.0 (#1600)\n\n* Version 0.20.0\r\n\r\n* Add the __version__\r\n\r\n* Update docs/release-notes.md\r\n\r\n* Update release-notes.md\r\n\r\n* Update docs/release-notes.md", "commit_date": "2022-05-03T05:32:27+00:00", "author": "Marcelo Trylesinski", "file": "starlette/__init__.py", "patch": "@@ -1 +1 @@\n-__version__ = \"0.19.1\"\n+__version__ = \"0.20.0\"", "before_segments": [], "after_segments": []}
{"repository": "encode/starlette", "commit_sha": "702fe816d73cdc4b7c2dc9db8b34e584fc815b66", "commit_message": "Version 0.19.1 (#1591)\n\n* Version 0.19.1\r\n\r\n* Apply suggestions from code review\r\n\r\n* Update docs/release-notes.md\r\n\r\n* Update docs/release-notes.md\r\n\r\nCo-authored-by: Adrian Garcia Badaracco <1755071+adriangb@users.noreply.github.com>\r\n\r\nCo-authored-by: Adrian Garcia Badaracco <1755071+adriangb@users.noreply.github.com>", "commit_date": "2022-04-22T05:26:15+00:00", "author": "Marcelo Trylesinski", "file": "starlette/__init__.py", "patch": "@@ -1 +1 @@\n-__version__ = \"0.19.0\"\n+__version__ = \"0.19.1\"", "before_segments": [], "after_segments": []}
{"repository": "encode/starlette", "commit_sha": "e086fc2da361767b532cf690e5203619bbae98aa", "commit_message": "Version 0.19.0 (#1439)\n\n* Version 0.19.0\r\n\r\n* Add missing links\r\n\r\n* Update release-notes.md\r\n\r\n* Update release-notes.md\r\n\r\n* Update docs/release-notes.md\r\n\r\n* Add PR 1459\r\n\r\n* Update docs/release-notes.md\r\n\r\n* Update docs/release-notes.md\r\n\r\n* Apply suggestions from code review\r\n\r\n* Update docs/release-notes.md\r\n\r\n* Update release-notes.md\r\n\r\n* Update docs/release-notes.md\r\n\r\n* Update __init__.py\r\n\r\nCo-authored-by: Tom Christie <tom@tomchristie.com>", "commit_date": "2022-03-09T18:43:11+00:00", "author": "Marcelo Trylesinski", "file": "starlette/__init__.py", "patch": "@@ -1 +1 @@\n-__version__ = \"0.18.0\"\n+__version__ = \"0.19.0\"", "before_segments": [], "after_segments": []}
{"repository": "encode/starlette", "commit_sha": "b314fd9491a2b9afbfea76cac4d2f8ec42656f93", "commit_message": "Add docstrings to discourage routing decorator style API (#1486)\n\n* Add docstrings for decorator style API\r\n\r\n* add nocover\r\n\r\n* revert old comment\r\n\r\n* update docstrings\r\n\r\n* lint\r\n\r\n* Update starlette/routing.py\r\n\r\n* Update applications.py\r\n\r\nCo-authored-by: Tom Christie <tom@tomchristie.com>", "commit_date": "2022-02-09T13:46:58+00:00", "author": "Amin Alaee", "file": "starlette/applications.py", "patch": "@@ -119,8 +119,8 @@ async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n         await self.middleware_stack(scope, receive, send)\n \n     # The following usages are now discouraged in favour of configuration\n-    # \u00a0during Starlette.__init__(...)\n-    def on_event(self, event_type: str) -> typing.Callable:\n+    # during Starlette.__init__(...)\n+    def on_event(self, event_type: str) -> typing.Callable:  # pragma: nocover\n         return self.router.on_event(event_type)\n \n     def mount(self, path: str, app: ASGIApp, name: str = None) -> None:\n@@ -163,7 +163,7 @@ def add_websocket_route(\n \n     def exception_handler(\n         self, exc_class_or_status_code: typing.Union[int, typing.Type[Exception]]\n-    ) -> typing.Callable:\n+    ) -> typing.Callable:  # pragma: nocover\n         def decorator(func: typing.Callable) -> typing.Callable:\n             self.add_exception_handler(exc_class_or_status_code, func)\n             return func\n@@ -176,7 +176,19 @@ def route(\n         methods: typing.List[str] = None,\n         name: str = None,\n         include_in_schema: bool = True,\n-    ) -> typing.Callable:\n+    ) -> typing.Callable:  # pragma: nocover\n+        \"\"\"\n+        We no longer document this decorator style API, and its usage is discouraged.\n+        Instead you should use the following approach:\n+\n+        routes = [\n+            Route(path, endpoint=..., ...),\n+            ...\n+        ]\n+\n+        app = Starlette(routes=routes)\n+        \"\"\"\n+\n         def decorator(func: typing.Callable) -> typing.Callable:\n             self.router.add_route(\n                 path,\n@@ -189,14 +201,40 @@ def decorator(func: typing.Callable) -> typing.Callable:\n \n         return decorator\n \n-    def websocket_route(self, path: str, name: str = None) -> typing.Callable:\n+    def websocket_route(\n+        self, path: str, name: str = None\n+    ) -> typing.Callable:  # pragma: nocover\n+        \"\"\"\n+        We no longer document this decorator style API, and its usage is discouraged.\n+        Instead you should use the following approach:\n+\n+        routes = [\n+            WebSocketRoute(path, endpoint=..., ...),\n+            ...\n+        ]\n+\n+        app = Starlette(routes=routes)\n+        \"\"\"\n+\n         def decorator(func: typing.Callable) -> typing.Callable:\n             self.router.add_websocket_route(path, func, name=name)\n             return func\n \n         return decorator\n \n-    def middleware(self, middleware_type: str) -> typing.Callable:\n+    def middleware(self, middleware_type: str) -> typing.Callable:  # pragma: nocover\n+        \"\"\"\n+        We no longer document this decorator style API, and its usage is discouraged.\n+        Instead you should use the following approach:\n+\n+        middleware = [\n+            Middleware(...),\n+            ...\n+        ]\n+\n+        app = Starlette(middleware=middleware)\n+        \"\"\"\n+\n         assert (\n             middleware_type == \"http\"\n         ), 'Currently only middleware(\"http\") is supported.'", "before_segments": [{"filename": "starlette/applications.py", "start_line": 13, "code": "class Starlette:", "documentation": "    \"\"\"\n    Creates an application instance.\n\n    **Parameters:**\n\n    * **debug** - Boolean indicating if debug tracebacks should be returned on errors.\n    * **routes** - A list of routes to serve incoming HTTP and WebSocket requests.\n    * **middleware** - A list of middleware to run for every request. A starlette\n    application will always automatically include two middleware classes.\n    `ServerErrorMiddleware` is added as the very outermost middleware, to handle\n    any uncaught errors occurring anywhere in the entire stack.\n    `ExceptionMiddleware` is added as the very innermost middleware, to deal\n    with handled exception cases occurring in the routing or endpoints.\n    * **exception_handlers** - A mapping of either integer status codes,\n    or exception class types onto callables which handle the exceptions.\n    Exception handler callables should be of the form\n    `handler(request, exc) -> response` and may be be either standard functions, or\n    async functions.\n    * **on_startup** - A list of callables to run on application startup.\n    Startup handler callables do not take any arguments, and may be be either\n    standard functions, or async functions.\n    * **on_shutdown** - A list of callables to run on application shutdown.\n    Shutdown handler callables do not take any arguments, and may be be either\n    standard functions, or async functions.\n    \"\"\""}], "after_segments": [{"filename": "starlette/applications.py", "start_line": 13, "code": "class Starlette:", "documentation": "    \"\"\"\n    Creates an application instance.\n\n    **Parameters:**\n\n    * **debug** - Boolean indicating if debug tracebacks should be returned on errors.\n    * **routes** - A list of routes to serve incoming HTTP and WebSocket requests.\n    * **middleware** - A list of middleware to run for every request. A starlette\n    application will always automatically include two middleware classes.\n    `ServerErrorMiddleware` is added as the very outermost middleware, to handle\n    any uncaught errors occurring anywhere in the entire stack.\n    `ExceptionMiddleware` is added as the very innermost middleware, to deal\n    with handled exception cases occurring in the routing or endpoints.\n    * **exception_handlers** - A mapping of either integer status codes,\n    or exception class types onto callables which handle the exceptions.\n    Exception handler callables should be of the form\n    `handler(request, exc) -> response` and may be be either standard functions, or\n    async functions.\n    * **on_startup** - A list of callables to run on application startup.\n    Startup handler callables do not take any arguments, and may be be either\n    standard functions, or async functions.\n    * **on_shutdown** - A list of callables to run on application shutdown.\n    Shutdown handler callables do not take any arguments, and may be be either\n    standard functions, or async functions.\n    \"\"\""}, {"filename": "starlette/applications.py", "start_line": 224, "code": "    def middleware(self, middleware_type: str) -> typing.Callable:  # pragma: nocover\n        assert (\n            middleware_type == \"http\"\n        ), 'Currently only middleware(\"http\") is supported.'", "documentation": "        \"\"\"\n        We no longer document this decorator style API, and its usage is discouraged.\n        Instead you should use the following approach:\n\n        middleware = [\n            Middleware(...),\n            ...\n        ]\n\n        app = Starlette(middleware=middleware)\n        \"\"\""}]}
{"repository": "encode/starlette", "commit_sha": "b314fd9491a2b9afbfea76cac4d2f8ec42656f93", "commit_message": "Add docstrings to discourage routing decorator style API (#1486)\n\n* Add docstrings for decorator style API\r\n\r\n* add nocover\r\n\r\n* revert old comment\r\n\r\n* update docstrings\r\n\r\n* lint\r\n\r\n* Update starlette/routing.py\r\n\r\n* Update applications.py\r\n\r\nCo-authored-by: Tom Christie <tom@tomchristie.com>", "commit_date": "2022-02-09T13:46:58+00:00", "author": "Amin Alaee", "file": "starlette/routing.py", "patch": "@@ -732,7 +732,19 @@ def route(\n         methods: typing.List[str] = None,\n         name: str = None,\n         include_in_schema: bool = True,\n-    ) -> typing.Callable:\n+    ) -> typing.Callable:  # pragma: nocover\n+        \"\"\"\n+        We no longer document this decorator style API, and its usage is discouraged.\n+        Instead you should use the following approach:\n+\n+        routes = [\n+            Route(path, endpoint=..., ...),\n+            ...\n+        ]\n+\n+        app = Starlette(routes=routes)\n+        \"\"\"\n+\n         def decorator(func: typing.Callable) -> typing.Callable:\n             self.add_route(\n                 path,\n@@ -745,7 +757,21 @@ def decorator(func: typing.Callable) -> typing.Callable:\n \n         return decorator\n \n-    def websocket_route(self, path: str, name: str = None) -> typing.Callable:\n+    def websocket_route(\n+        self, path: str, name: str = None\n+    ) -> typing.Callable:  # pragma: nocover\n+        \"\"\"\n+        We no longer document this decorator style API, and its usage is discouraged.\n+        Instead you should use the following approach:\n+\n+        routes = [\n+            WebSocketRoute(path, endpoint=..., ...),\n+            ...\n+        ]\n+\n+        app = Starlette(routes=routes)\n+        \"\"\"\n+\n         def decorator(func: typing.Callable) -> typing.Callable:\n             self.add_websocket_route(path, func, name=name)\n             return func\n@@ -760,7 +786,7 @@ def add_event_handler(self, event_type: str, func: typing.Callable) -> None:\n         else:\n             self.on_shutdown.append(func)\n \n-    def on_event(self, event_type: str) -> typing.Callable:\n+    def on_event(self, event_type: str) -> typing.Callable:  # pragma: nocover\n         def decorator(func: typing.Callable) -> typing.Callable:\n             self.add_event_handler(event_type, func)\n             return func", "before_segments": [{"filename": "starlette/routing.py", "start_line": 27, "code": "class NoMatchFound(Exception):", "documentation": "    \"\"\"\n    Raised by `.url_for(name, **path_params)` and `.url_path_for(name, **path_params)`\n    if no matching route exists.\n    \"\"\""}, {"filename": "starlette/routing.py", "start_line": 40, "code": "def iscoroutinefunction_or_partial(obj: typing.Any) -> bool:\n    while isinstance(obj, functools.partial):\n        obj = obj.func\n    return inspect.iscoroutinefunction(obj)", "documentation": "    \"\"\"\n    Correctly determines if an object is a coroutine function,\n    including those wrapped in functools.partial objects.\n    \"\"\""}, {"filename": "starlette/routing.py", "start_line": 50, "code": "def request_response(func: typing.Callable) -> ASGIApp:\n    is_coroutine = iscoroutinefunction_or_partial(func)\n    async def app(scope: Scope, receive: Receive, send: Send) -> None:\n        request = Request(scope, receive=receive, send=send)\n        if is_coroutine:\n            response = await func(request)\n        else:\n            response = await run_in_threadpool(func, request)\n        await response(scope, receive, send)\n    return app", "documentation": "    \"\"\"\n    Takes a function or coroutine `func(request) -> response`,\n    and returns an ASGI application.\n    \"\"\""}, {"filename": "starlette/routing.py", "start_line": 68, "code": "def websocket_session(func: typing.Callable) -> ASGIApp:\n    async def app(scope: Scope, receive: Receive, send: Send) -> None:\n        session = WebSocket(scope, receive=receive, send=send)\n        await func(session)\n    return app", "documentation": "    \"\"\"\n    Takes a coroutine `func(session)`, and returns an ASGI application.\n    \"\"\""}], "after_segments": [{"filename": "starlette/routing.py", "start_line": 27, "code": "class NoMatchFound(Exception):", "documentation": "    \"\"\"\n    Raised by `.url_for(name, **path_params)` and `.url_path_for(name, **path_params)`\n    if no matching route exists.\n    \"\"\""}, {"filename": "starlette/routing.py", "start_line": 40, "code": "def iscoroutinefunction_or_partial(obj: typing.Any) -> bool:\n    while isinstance(obj, functools.partial):\n        obj = obj.func\n    return inspect.iscoroutinefunction(obj)", "documentation": "    \"\"\"\n    Correctly determines if an object is a coroutine function,\n    including those wrapped in functools.partial objects.\n    \"\"\""}, {"filename": "starlette/routing.py", "start_line": 50, "code": "def request_response(func: typing.Callable) -> ASGIApp:\n    is_coroutine = iscoroutinefunction_or_partial(func)\n    async def app(scope: Scope, receive: Receive, send: Send) -> None:\n        request = Request(scope, receive=receive, send=send)\n        if is_coroutine:\n            response = await func(request)\n        else:\n            response = await run_in_threadpool(func, request)\n        await response(scope, receive, send)\n    return app", "documentation": "    \"\"\"\n    Takes a function or coroutine `func(request) -> response`,\n    and returns an ASGI application.\n    \"\"\""}, {"filename": "starlette/routing.py", "start_line": 68, "code": "def websocket_session(func: typing.Callable) -> ASGIApp:\n    async def app(scope: Scope, receive: Receive, send: Send) -> None:\n        session = WebSocket(scope, receive=receive, send=send)\n        await func(session)\n    return app", "documentation": "    \"\"\"\n    Takes a coroutine `func(session)`, and returns an ASGI application.\n    \"\"\""}]}
{"repository": "encode/starlette", "commit_sha": "a7c5a41396752c39a5a9b688e2dccfaca152a62f", "commit_message": "Allow Session scoped cookies. (#1387)\n\n* Allow Session scoped cookies.\r\n\r\n* Update docs/middleware.md\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>\r\n\r\n* Improve typing.\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>", "commit_date": "2022-01-12T09:57:47+00:00", "author": "Alex Oleshkevich", "file": "starlette/middleware/sessions.py", "patch": "@@ -16,7 +16,7 @@ def __init__(\n         app: ASGIApp,\n         secret_key: typing.Union[str, Secret],\n         session_cookie: str = \"session\",\n-        max_age: int = 14 * 24 * 60 * 60,  # 14 days, in seconds\n+        max_age: typing.Optional[int] = 14 * 24 * 60 * 60,  # 14 days, in seconds\n         same_site: str = \"lax\",\n         https_only: bool = False,\n     ) -> None:\n@@ -55,12 +55,12 @@ async def send_wrapper(message: Message) -> None:\n                     data = b64encode(json.dumps(scope[\"session\"]).encode(\"utf-8\"))\n                     data = self.signer.sign(data)\n                     headers = MutableHeaders(scope=message)\n-                    header_value = \"%s=%s; path=%s; Max-Age=%d; %s\" % (\n-                        self.session_cookie,\n-                        data.decode(\"utf-8\"),\n-                        path,\n-                        self.max_age,\n-                        self.security_flags,\n+                    header_value = \"{session_cookie}={data}; path={path}; {max_age}{security_flags}\".format(  # noqa E501\n+                        session_cookie=self.session_cookie,\n+                        data=data.decode(\"utf-8\"),\n+                        path=path,\n+                        max_age=f\"Max-Age={self.max_age}; \" if self.max_age else \"\",\n+                        security_flags=self.security_flags,\n                     )\n                     headers.append(\"Set-Cookie\", header_value)\n                 elif not initial_session_was_empty:", "before_segments": [], "after_segments": []}
{"repository": "encode/starlette", "commit_sha": "a7c5a41396752c39a5a9b688e2dccfaca152a62f", "commit_message": "Allow Session scoped cookies. (#1387)\n\n* Allow Session scoped cookies.\r\n\r\n* Update docs/middleware.md\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>\r\n\r\n* Improve typing.\r\n\r\nCo-authored-by: Marcelo Trylesinski <marcelotryle@gmail.com>", "commit_date": "2022-01-12T09:57:47+00:00", "author": "Alex Oleshkevich", "file": "tests/middleware/test_session.py", "patch": "@@ -129,3 +129,20 @@ def test_invalid_session_cookie(test_client_factory):\n     # we expect it to not raise an exception if we provide a bogus session cookie\n     response = client.get(\"/view_session\", cookies={\"session\": \"invalid\"})\n     assert response.json() == {\"session\": {}}\n+\n+\n+def test_session_cookie(test_client_factory):\n+    app = create_app()\n+    app.add_middleware(SessionMiddleware, secret_key=\"example\", max_age=None)\n+    client = test_client_factory(app)\n+\n+    response = client.post(\"/update_session\", json={\"some\": \"data\"})\n+    assert response.json() == {\"session\": {\"some\": \"data\"}}\n+\n+    # check cookie max-age\n+    set_cookie = response.headers[\"set-cookie\"]\n+    assert \"Max-Age\" not in set_cookie\n+\n+    client.cookies.clear_session_cookies()\n+    response = client.get(\"/view_session\")\n+    assert response.json() == {\"session\": {}}", "before_segments": [], "after_segments": []}
{"repository": "matplotlib/matplotlib", "commit_sha": "1c40b336f0a2819e711f7ad3c4316743c4ab8c4e", "commit_message": "docs: fix docstring formatting and line wrap", "commit_date": "2026-01-22T07:08:08+00:00", "author": "Moniza Kidwai", "file": "lib/matplotlib/contour.py", "patch": "@@ -1490,6 +1490,9 @@ def _initialize_x_y(self, z):\n     to automatically choose no more than *n+1* \"nice\" contour levels\n     between minimum and maximum numeric values of *Z*.\n \n+    If None (default), a reasonable default is chosen; for linear\n+    scales, *n*=7 is the default.\n+\n     If array-like, draw contour lines at the specified levels.\n     The values must be in increasing order.\n ", "before_segments": [{"filename": "lib/matplotlib/contour.py", "start_line": 58, "code": "class ContourLabeler:", "documentation": "    \"\"\"Mixin to provide labelling capability to `.ContourSet`.\"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 216, "code": "    def print_label(self, linecontour, labelwidth):\n        return (len(linecontour) > 10 * labelwidth\n                or (len(linecontour)\n                    and (np.ptp(linecontour, axis=0) > 1.2 * labelwidth).any()))", "documentation": "        \"\"\"Return whether a contour is long enough to hold a label.\"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 222, "code": "    def too_close(self, x, y, lw):\n        thresh = (1.2 * lw) ** 2\n        return any((x - loc[0]) ** 2 + (y - loc[1]) ** 2 < thresh\n                   for loc in self.labelXYs)", "documentation": "        \"\"\"Return whether a label is already near this location.\"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 228, "code": "    def _get_nth_label_width(self, nth):\n        fig = self.axes.get_figure(root=False)\n        renderer = fig.get_figure(root=True)._get_renderer()\n        return (Text(0, 0,\n                     self.get_text(self.labelLevelList[nth], self.labelFmt),\n                     figure=fig, fontproperties=self._label_font_props)\n                .get_window_extent(renderer).width)", "documentation": "        \"\"\"Return the width of the *nth* label, in pixels.\"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 237, "code": "    def get_text(self, lev, fmt):\n        if isinstance(lev, str):\n            return lev\n        elif isinstance(fmt, dict):\n            return fmt.get(lev, '%1.3f')\n        elif callable(getattr(fmt, \"format_ticks\", None)):\n            return fmt.format_ticks([*self.labelLevelList, lev])[-1]\n        elif callable(fmt):\n            return fmt(lev)\n        else:\n            return fmt % lev", "documentation": "        \"\"\"Get the text of the label.\"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 250, "code": "    def locate_label(self, linecontour, labelwidth):\n        ctr_size = len(linecontour)\n        n_blocks = int(np.ceil(ctr_size / labelwidth)) if labelwidth > 1 else 1\n        block_size = ctr_size if n_blocks == 1 else int(labelwidth)\n        xx = np.resize(linecontour[:, 0], (n_blocks, block_size))\n        yy = np.resize(linecontour[:, 1], (n_blocks, block_size))\n        yfirst = yy[:, :1]\n        ylast = yy[:, -1:]\n        xfirst = xx[:, :1]\n        xlast = xx[:, -1:]\n        s = (yfirst - yy) * (xlast - xfirst) - (xfirst - xx) * (ylast - yfirst)", "documentation": "        \"\"\"\n        Find good place to draw a label (relatively flat part of the contour).\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 284, "code": "    def _split_path_and_get_label_rotation(self, path, idx, screen_pos, lw, spacing=5):\n        xys = path.vertices\n        codes = path.codes\n        pos = self.get_transform().inverted().transform(screen_pos)\n        if not np.allclose(pos, xys[idx]):\n            xys = np.insert(xys, idx, pos, axis=0)\n            codes = np.insert(codes, idx, Path.LINETO)\n        movetos = (codes == Path.MOVETO).nonzero()[0]\n        start = movetos[movetos <= idx][-1]\n        try:\n            stop = movetos[movetos > idx][0]", "documentation": "        \"\"\"\n        Prepare for insertion of a label at index *idx* of *path*.\n\n        Parameters\n        ----------\n        path : Path\n            The path where the label will be inserted, in data space.\n        idx : int\n            The vertex index after which the label will be inserted.\n        screen_pos : (float, float)\n            The position where the label will be inserted, in screen space.\n        lw : float\n            The label width, in screen space.\n        spacing : float\n            Extra spacing around the label, in screen space.\n\n        Returns\n        -------\n        path : Path\n            The path, broken so that the label can be drawn over it.\n        angle : float\n            The rotation of the label.\n\n        Notes\n        -----\n        Both tasks are done together to avoid calculating path lengths multiple times,\n        which is relatively costly.\n\n        The method used here involves computing the path length along the contour in\n        pixel coordinates and then looking (label width / 2) away from central point to\n        determine rotation and then to break contour if desired.  The extra spacing is\n        taken into account when breaking the path, but not when computing the angle.\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 406, "code": "    def add_label(self, x, y, rotation, lev, cvalue):\n        data_x, data_y = self.axes.transData.inverted().transform((x, y))\n        t = Text(\n            data_x, data_y,\n            text=self.get_text(lev, self.labelFmt),\n            rotation=rotation,\n            horizontalalignment='center', verticalalignment='center',\n            zorder=self._clabel_zorder,\n            color=self.labelMappable.to_rgba(cvalue, alpha=self.get_alpha()),\n            fontproperties=self._label_font_props,\n            clip_box=self.axes.bbox)", "documentation": "        \"\"\"Add a contour label, respecting whether *use_clabeltext* was set.\"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 468, "code": "    def pop_label(self, index=-1):\n        self.labelCValues.pop(index)\n        t = self.labelTexts.pop(index)\n        t.remove()", "documentation": "        \"\"\"Defaults to removing last label, but any index can be supplied\"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 507, "code": "def _find_closest_point_on_path(xys, p):\n    if len(xys) == 1:\n        return (((p - xys[0]) ** 2).sum(), xys[0], (0, 0))\n    dxys = xys[1:] - xys[:-1]  # Individual segment vectors.\n    norms = (dxys ** 2).sum(axis=1)\n    norms[norms == 0] = 1  # For zero-length segment, replace 0/0 by 0/1.\n    rel_projs = np.clip(  # Project onto each segment in relative 0-1 coords.\n        ((p - xys[:-1]) * dxys).sum(axis=1) / norms,\n        0, 1)[:, None]\n    projs = xys[:-1] + rel_projs * dxys  # Projs. onto each segment, in (x, y).\n    d2s = ((projs - p) ** 2).sum(axis=1)  # Squared distances.", "documentation": "    \"\"\"\n    Parameters\n    ----------\n    xys : (N, 2) array-like\n        Coordinates of vertices.\n    p : (float, float)\n        Coordinates of point.\n\n    Returns\n    -------\n    d2min : float\n        Minimum square distance of *p* to *xys*.\n    proj : (float, float)\n        Projection of *p* onto *xys*.\n    imin : (int, int)\n        Consecutive indices of vertices of segment in *xys* where *proj* is.\n        Segments are considered as including their end-points; i.e. if the\n        closest point on the path is a node in *xys* with index *i*, this\n        returns ``(i-1, i)``.  For the special case where *xys* is a single\n        point, this returns ``(0, 0)``.\n    \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 556, "code": "class ContourSet(ContourLabeler, mcoll.Collection):", "documentation": "    \"\"\"\n    Store a set of contour lines or filled regions.\n\n    User-callable method: `~.Axes.clabel`\n\n    Parameters\n    ----------\n    ax : `~matplotlib.axes.Axes`\n\n    levels : [level0, level1, ..., leveln]\n        A list of floating point numbers indicating the contour levels.\n\n    allsegs : [level0segs, level1segs, ...]\n        List of all the polygon segments for all the *levels*.\n        For contour lines ``len(allsegs) == len(levels)``, and for\n        filled contour regions ``len(allsegs) = len(levels)-1``. The lists\n        should look like ::\n\n            level0segs = [polygon0, polygon1, ...]\n            polygon0 = [[x0, y0], [x1, y1], ...]\n\n    allkinds : ``None`` or [level0kinds, level1kinds, ...]\n        Optional list of all the polygon vertex kinds (code types), as\n        described and used in Path. This is used to allow multiply-\n        connected paths such as holes within filled polygons.\n        If not ``None``, ``len(allkinds) == len(allsegs)``. The lists\n        should look like ::\n\n            level0kinds = [polygon0kinds, ...]\n            polygon0kinds = [vertexcode0, vertexcode1, ...]\n\n        If *allkinds* is not ``None``, usually all polygons for a\n        particular contour level are grouped together so that\n        ``level0segs = [polygon0]`` and ``level0kinds = [polygon0kinds]``.\n\n    **kwargs\n        Keyword arguments are as described in the docstring of\n        `~.Axes.contour`.\n\n    %(contour_set_attributes)s\n    \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 797, "code": "    def get_transform(self):\n        if self._transform is None:\n            self._transform = self.axes.transData\n        elif (not isinstance(self._transform, mtransforms.Transform)\n              and hasattr(self._transform, '_as_mpl_transform')):\n            self._transform = self._transform._as_mpl_transform(self.axes)\n        return self._transform", "documentation": "        \"\"\"Return the `.Transform` instance used by this ContourSet.\"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 814, "code": "    def legend_elements(self, variable_name='x', str_format=str):\n        artists = []\n        labels = []\n        if self.filled:\n            lowers, uppers = self._get_lowers_and_uppers()\n            n_levels = len(self._paths)\n            for idx in range(n_levels):\n                artists.append(mpatches.Rectangle(\n                    (0, 0), 1, 1,\n                    facecolor=self.get_facecolor()[idx],\n                    hatch=self.hatches[idx % len(self.hatches)],", "documentation": "        \"\"\"\n        Return a list of artists and labels suitable for passing through\n        to `~.Axes.legend` which represent this ContourSet.\n\n        The labels have the form \"0 < x <= 1\" stating the data ranges which\n        the artists represent.\n\n        Parameters\n        ----------\n        variable_name : str\n            The string used inside the inequality used on the labels.\n        str_format : function: float -> str\n            Function used to format the numbers in the labels.\n\n        Returns\n        -------\n        artists : list[`.Artist`]\n            A list of the artists.\n        labels : list[str]\n            A list of the labels.\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 868, "code": "    def _process_args(self, *args, **kwargs):\n        self.levels = args[0]\n        allsegs = args[1]\n        allkinds = args[2] if len(args) > 2 else None\n        self.zmax = np.max(self.levels)\n        self.zmin = np.min(self.levels)\n        if allkinds is None:\n            allkinds = [[None] * len(segs) for segs in allsegs]\n        if self.filled:\n            if len(allsegs) != len(self.levels) - 1:\n                raise ValueError('must be one less number of segments as '", "documentation": "        \"\"\"\n        Process *args* and *kwargs*; override in derived classes.\n\n        Must set self.levels, self.zmin and self.zmax, and update Axes limits.\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 912, "code": "    def _make_paths_from_contour_generator(self):\n        if self._paths is not None:\n            return self._paths\n        cg = self._contour_generator\n        empty_path = Path(np.empty((0, 2)))\n        vertices_and_codes = (\n            map(cg.create_filled_contour, *self._get_lowers_and_uppers())\n            if self.filled else\n            map(cg.create_contour, self.levels))\n        return [Path(np.concatenate(vs), np.concatenate(cs)) if len(vs) else empty_path\n                for vs, cs in vertices_and_codes]", "documentation": "        \"\"\"Compute ``paths`` using C extension.\"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 925, "code": "    def _get_lowers_and_uppers(self):\n        lowers = self._levels[:-1]\n        if self.zmin == lowers[0]:\n            lowers = lowers.copy()  # so we don't change self._levels\n            if self.logscale:\n                lowers[0] = 0.99 * self.zmin\n            else:\n                lowers[0] -= 1\n        uppers = self._levels[1:]\n        return (lowers, uppers)", "documentation": "        \"\"\"\n        Return ``(lowers, uppers)`` for filled contours.\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 956, "code": "    def _ensure_locator_exists(self, N):\n        if self.locator is None:\n            if self.logscale:\n                self.locator = ticker.LogLocator(numticks=N)\n            else:\n                if N is None:\n                    N = 7  # Hard coded default\n                self.locator = ticker.MaxNLocator(N + 1, min_n_ticks=1)", "documentation": "        \"\"\"\n        Set a locator on this ContourSet if it's not already set.\n\n        Parameters\n        ----------\n        N : int or None\n            If *N* is an int, it is used as the target number of levels.\n            Otherwise when *N* is None, a reasonable default is chosen;\n            for logscales the LogLocator chooses, N=7 is the default\n            otherwise.\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 976, "code": "    def _autolev(self):\n        lev = self.locator.tick_values(self.zmin, self.zmax)\n        try:\n            if self.locator._symmetric:\n                return lev\n        except AttributeError:\n            pass\n        under = np.nonzero(lev < self.zmin)[0]\n        i0 = under[-1] if len(under) else 0\n        over = np.nonzero(lev > self.zmax)[0]\n        i1 = over[0] + 1 if len(over) else len(lev)", "documentation": "        \"\"\"\n        Select contour levels to span the data.\n\n        We need two more levels for filled contours than for\n        line contours, because for the latter we need to specify\n        the lower and upper boundary of each range. For example,\n        a single contour boundary, say at z = 0, requires only\n        one contour line, but two filled regions, and therefore\n        three levels to provide boundaries for both regions.\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 1010, "code": "    def _process_contour_level_args(self, args, z_dtype):\n        levels_arg = self.levels\n        if levels_arg is None:\n            if args:\n                levels_arg = args[0]\n            elif np.issubdtype(z_dtype, bool):\n                levels_arg = [0, .5, 1] if self.filled else [.5]\n        if isinstance(levels_arg, Integral) or levels_arg is None:\n            self._ensure_locator_exists(levels_arg)\n            self.levels = self._autolev()\n        else:", "documentation": "        \"\"\"\n        Determine the contour levels and store in self.levels.\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 1034, "code": "    def _process_levels(self):\n        self._levels = list(self.levels)\n        if self.logscale:\n            lower, upper = 1e-250, 1e250\n        else:\n            lower, upper = -1e250, 1e250\n        if self.extend in ('both', 'min'):\n            self._levels.insert(0, lower)\n        if self.extend in ('both', 'max'):\n            self._levels.append(upper)\n        self._levels = np.asarray(self._levels)", "documentation": "        \"\"\"\n        Assign values to :attr:`layers` based on :attr:`levels`,\n        adding extended layers as needed if contours are filled.\n\n        For line contours, layers simply coincide with levels;\n        a line is a thin layer.  No extended levels are needed\n        with line contours.\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 1071, "code": "    def _process_colors(self):\n        self.monochrome = self.cmap.monochrome\n        if self.colors is not None:\n            i0, i1 = 0, len(self.levels)\n            if self.filled:\n                i1 -= 1\n                if self.extend in ('both', 'min'):\n                    i0 -= 1\n                if self.extend in ('both', 'max'):\n                    i1 += 1\n            self.cvalues = list(range(i0, i1))", "documentation": "        \"\"\"\n        Color argument processing for contouring.\n\n        Note that we base the colormapping on the contour levels\n        and layers, not on the actual range of the Z values.  This\n        means we don't have to worry about bad values in Z, and we\n        always have the full dynamic range available for the selected\n        levels.\n\n        The color is based on the midpoint of the layer, except for\n        extended end layers.  By default, the norm vmin and vmax\n        are the extreme values of the non-extended levels.  Hence,\n        the layer color extremes are not the extreme values of\n        the colormap itself, but approach those values as the number\n        of levels increases.  An advantage of this scheme is that\n        line contours, when added to filled contours, take on\n        colors that are consistent with those of the filled regions;\n        for example, a contour line on the boundary between two\n        regions will have a color intermediate between those\n        of the regions.\n\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 1151, "code": "    def _find_nearest_contour(self, xy, indices=None):\n        if self.filled:\n            raise ValueError(\"Method does not support filled contours\")\n        if indices is None:\n            indices = range(len(self._paths))\n        d2min = np.inf\n        idx_level_min = idx_vtx_min = proj_min = None\n        for idx_level in indices:\n            path = self._paths[idx_level]\n            idx_vtx_start = 0\n            for subpath in path._iter_connected_components():", "documentation": "        \"\"\"\n        Find the point in the unfilled contour plot that is closest (in screen\n        space) to point *xy*.\n\n        Parameters\n        ----------\n        xy : tuple[float, float]\n            The reference point (in screen space).\n        indices : list of int or None, default: None\n            Indices of contour levels to consider.  If None (the default), all levels\n            are considered.\n\n        Returns\n        -------\n        idx_level_min : int\n            The index of the contour level closest to *xy*.\n        idx_vtx_min : int\n            The index of the `.Path` segment closest to *xy* (at that level).\n        proj : (float, float)\n            The point in the contour plot closest to *xy*.\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 1204, "code": "    def find_nearest_contour(self, x, y, indices=None, pixel=True):\n        segment = index = d2 = None\n        with ExitStack() as stack:\n            if not pixel:\n                stack.enter_context(self._cm_set(\n                    transform=mtransforms.IdentityTransform()))\n            i_level, i_vtx, (xmin, ymin) = self._find_nearest_contour((x, y), indices)\n        if i_level is not None:\n            cc_cumlens = np.cumsum(\n                [*map(len, self._paths[i_level]._iter_connected_components())])\n            segment = cc_cumlens.searchsorted(i_vtx, \"right\")", "documentation": "        \"\"\"\n        Find the point in the contour plot that is closest to ``(x, y)``.\n\n        This method does not support filled contours.\n\n        Parameters\n        ----------\n        x, y : float\n            The reference point.\n        indices : list of int or None, default: None\n            Indices of contour levels to consider.  If None (the default), all\n            levels are considered.\n        pixel : bool, default: True\n            If *True*, measure distance in pixel (screen) space, which is\n            useful for manual contour labeling; else, measure distance in axes\n            space.\n\n        Returns\n        -------\n        path : int\n            The index of the path that is closest to ``(x, y)``.  Each path corresponds\n            to one contour level.\n        subpath : int\n            The index within that closest path of the subpath that is closest to\n            ``(x, y)``.  Each subpath corresponds to one unbroken contour line.\n        index : int\n            The index of the vertices within that subpath that are closest to\n            ``(x, y)``.\n        xmin, ymin : float\n            The point in the contour plot that is closest to ``(x, y)``.\n        d2 : float\n            The squared distance from ``(xmin, ymin)`` to ``(x, y)``.\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 1282, "code": "class QuadContourSet(ContourSet):", "documentation": "    \"\"\"\n    Create and store a set of contour lines or filled regions.\n\n    This class is typically not instantiated directly by the user but by\n    `~.Axes.contour` and `~.Axes.contourf`.\n\n    %(contour_set_attributes)s\n    \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 1292, "code": "    def _process_args(self, *args, corner_mask=None, algorithm=None, **kwargs):\n        if args and isinstance(args[0], QuadContourSet):\n            if self.levels is None:\n                self.levels = args[0].levels\n            self.zmin = args[0].zmin\n            self.zmax = args[0].zmax\n            self._corner_mask = args[0]._corner_mask\n            contour_generator = args[0]._contour_generator\n            self._mins = args[0]._mins\n            self._maxs = args[0]._maxs\n            self._algorithm = args[0]._algorithm", "documentation": "        \"\"\"\n        Process args and kwargs.\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 1376, "code": "    def _check_xyz(self, x, y, z, kwargs):\n        x, y = self.axes._process_unit_info([(\"x\", x), (\"y\", y)], kwargs)\n        x = np.asarray(x, dtype=np.float64)\n        y = np.asarray(y, dtype=np.float64)\n        z = ma.asarray(z)\n        if z.ndim != 2:\n            raise TypeError(f\"Input z must be 2D, not {z.ndim}D\")\n        if z.shape[0] < 2 or z.shape[1] < 2:\n            raise TypeError(f\"Input z must be at least a (2, 2) shaped array, \"\n                            f\"but has shape {z.shape}\")\n        Ny, Nx = z.shape", "documentation": "        \"\"\"\n        Check that the shapes of the input arrays match; if x and y are 1D,\n        convert them to 2D using meshgrid.\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 1419, "code": "    def _initialize_x_y(self, z):\n        if z.ndim != 2:\n            raise TypeError(f\"Input z must be 2D, not {z.ndim}D\")\n        elif z.shape[0] < 2 or z.shape[1] < 2:\n            raise TypeError(f\"Input z must be at least a (2, 2) shaped array, \"\n                            f\"but has shape {z.shape}\")\n        else:\n            Ny, Nx = z.shape\n        if self.origin is None:  # Not for image-matching.\n            if self.extent is None:\n                return np.meshgrid(np.arange(Nx), np.arange(Ny))", "documentation": "        \"\"\"\n        Return X, Y arrays such that contour(Z) will match imshow(Z)\n        if origin is not None.\n        The center of pixel Z[i, j] depends on origin:\n        if origin is None, x = j, y = i;\n        if origin is 'lower', x = j + 0.5, y = i + 0.5;\n        if origin is 'upper', x = j + 0.5, y = Nrows - i - 0.5\n        If extent is not None, x and y will be scaled to match,\n        as in imshow.\n        If origin is None and extent is not None, then extent\n        will give the minimum and maximum values of x and y.\n        \"\"\""}], "after_segments": [{"filename": "lib/matplotlib/contour.py", "start_line": 58, "code": "class ContourLabeler:", "documentation": "    \"\"\"Mixin to provide labelling capability to `.ContourSet`.\"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 216, "code": "    def print_label(self, linecontour, labelwidth):\n        return (len(linecontour) > 10 * labelwidth\n                or (len(linecontour)\n                    and (np.ptp(linecontour, axis=0) > 1.2 * labelwidth).any()))", "documentation": "        \"\"\"Return whether a contour is long enough to hold a label.\"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 222, "code": "    def too_close(self, x, y, lw):\n        thresh = (1.2 * lw) ** 2\n        return any((x - loc[0]) ** 2 + (y - loc[1]) ** 2 < thresh\n                   for loc in self.labelXYs)", "documentation": "        \"\"\"Return whether a label is already near this location.\"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 228, "code": "    def _get_nth_label_width(self, nth):\n        fig = self.axes.get_figure(root=False)\n        renderer = fig.get_figure(root=True)._get_renderer()\n        return (Text(0, 0,\n                     self.get_text(self.labelLevelList[nth], self.labelFmt),\n                     figure=fig, fontproperties=self._label_font_props)\n                .get_window_extent(renderer).width)", "documentation": "        \"\"\"Return the width of the *nth* label, in pixels.\"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 237, "code": "    def get_text(self, lev, fmt):\n        if isinstance(lev, str):\n            return lev\n        elif isinstance(fmt, dict):\n            return fmt.get(lev, '%1.3f')\n        elif callable(getattr(fmt, \"format_ticks\", None)):\n            return fmt.format_ticks([*self.labelLevelList, lev])[-1]\n        elif callable(fmt):\n            return fmt(lev)\n        else:\n            return fmt % lev", "documentation": "        \"\"\"Get the text of the label.\"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 250, "code": "    def locate_label(self, linecontour, labelwidth):\n        ctr_size = len(linecontour)\n        n_blocks = int(np.ceil(ctr_size / labelwidth)) if labelwidth > 1 else 1\n        block_size = ctr_size if n_blocks == 1 else int(labelwidth)\n        xx = np.resize(linecontour[:, 0], (n_blocks, block_size))\n        yy = np.resize(linecontour[:, 1], (n_blocks, block_size))\n        yfirst = yy[:, :1]\n        ylast = yy[:, -1:]\n        xfirst = xx[:, :1]\n        xlast = xx[:, -1:]\n        s = (yfirst - yy) * (xlast - xfirst) - (xfirst - xx) * (ylast - yfirst)", "documentation": "        \"\"\"\n        Find good place to draw a label (relatively flat part of the contour).\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 284, "code": "    def _split_path_and_get_label_rotation(self, path, idx, screen_pos, lw, spacing=5):\n        xys = path.vertices\n        codes = path.codes\n        pos = self.get_transform().inverted().transform(screen_pos)\n        if not np.allclose(pos, xys[idx]):\n            xys = np.insert(xys, idx, pos, axis=0)\n            codes = np.insert(codes, idx, Path.LINETO)\n        movetos = (codes == Path.MOVETO).nonzero()[0]\n        start = movetos[movetos <= idx][-1]\n        try:\n            stop = movetos[movetos > idx][0]", "documentation": "        \"\"\"\n        Prepare for insertion of a label at index *idx* of *path*.\n\n        Parameters\n        ----------\n        path : Path\n            The path where the label will be inserted, in data space.\n        idx : int\n            The vertex index after which the label will be inserted.\n        screen_pos : (float, float)\n            The position where the label will be inserted, in screen space.\n        lw : float\n            The label width, in screen space.\n        spacing : float\n            Extra spacing around the label, in screen space.\n\n        Returns\n        -------\n        path : Path\n            The path, broken so that the label can be drawn over it.\n        angle : float\n            The rotation of the label.\n\n        Notes\n        -----\n        Both tasks are done together to avoid calculating path lengths multiple times,\n        which is relatively costly.\n\n        The method used here involves computing the path length along the contour in\n        pixel coordinates and then looking (label width / 2) away from central point to\n        determine rotation and then to break contour if desired.  The extra spacing is\n        taken into account when breaking the path, but not when computing the angle.\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 406, "code": "    def add_label(self, x, y, rotation, lev, cvalue):\n        data_x, data_y = self.axes.transData.inverted().transform((x, y))\n        t = Text(\n            data_x, data_y,\n            text=self.get_text(lev, self.labelFmt),\n            rotation=rotation,\n            horizontalalignment='center', verticalalignment='center',\n            zorder=self._clabel_zorder,\n            color=self.labelMappable.to_rgba(cvalue, alpha=self.get_alpha()),\n            fontproperties=self._label_font_props,\n            clip_box=self.axes.bbox)", "documentation": "        \"\"\"Add a contour label, respecting whether *use_clabeltext* was set.\"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 468, "code": "    def pop_label(self, index=-1):\n        self.labelCValues.pop(index)\n        t = self.labelTexts.pop(index)\n        t.remove()", "documentation": "        \"\"\"Defaults to removing last label, but any index can be supplied\"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 507, "code": "def _find_closest_point_on_path(xys, p):\n    if len(xys) == 1:\n        return (((p - xys[0]) ** 2).sum(), xys[0], (0, 0))\n    dxys = xys[1:] - xys[:-1]  # Individual segment vectors.\n    norms = (dxys ** 2).sum(axis=1)\n    norms[norms == 0] = 1  # For zero-length segment, replace 0/0 by 0/1.\n    rel_projs = np.clip(  # Project onto each segment in relative 0-1 coords.\n        ((p - xys[:-1]) * dxys).sum(axis=1) / norms,\n        0, 1)[:, None]\n    projs = xys[:-1] + rel_projs * dxys  # Projs. onto each segment, in (x, y).\n    d2s = ((projs - p) ** 2).sum(axis=1)  # Squared distances.", "documentation": "    \"\"\"\n    Parameters\n    ----------\n    xys : (N, 2) array-like\n        Coordinates of vertices.\n    p : (float, float)\n        Coordinates of point.\n\n    Returns\n    -------\n    d2min : float\n        Minimum square distance of *p* to *xys*.\n    proj : (float, float)\n        Projection of *p* onto *xys*.\n    imin : (int, int)\n        Consecutive indices of vertices of segment in *xys* where *proj* is.\n        Segments are considered as including their end-points; i.e. if the\n        closest point on the path is a node in *xys* with index *i*, this\n        returns ``(i-1, i)``.  For the special case where *xys* is a single\n        point, this returns ``(0, 0)``.\n    \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 556, "code": "class ContourSet(ContourLabeler, mcoll.Collection):", "documentation": "    \"\"\"\n    Store a set of contour lines or filled regions.\n\n    User-callable method: `~.Axes.clabel`\n\n    Parameters\n    ----------\n    ax : `~matplotlib.axes.Axes`\n\n    levels : [level0, level1, ..., leveln]\n        A list of floating point numbers indicating the contour levels.\n\n    allsegs : [level0segs, level1segs, ...]\n        List of all the polygon segments for all the *levels*.\n        For contour lines ``len(allsegs) == len(levels)``, and for\n        filled contour regions ``len(allsegs) = len(levels)-1``. The lists\n        should look like ::\n\n            level0segs = [polygon0, polygon1, ...]\n            polygon0 = [[x0, y0], [x1, y1], ...]\n\n    allkinds : ``None`` or [level0kinds, level1kinds, ...]\n        Optional list of all the polygon vertex kinds (code types), as\n        described and used in Path. This is used to allow multiply-\n        connected paths such as holes within filled polygons.\n        If not ``None``, ``len(allkinds) == len(allsegs)``. The lists\n        should look like ::\n\n            level0kinds = [polygon0kinds, ...]\n            polygon0kinds = [vertexcode0, vertexcode1, ...]\n\n        If *allkinds* is not ``None``, usually all polygons for a\n        particular contour level are grouped together so that\n        ``level0segs = [polygon0]`` and ``level0kinds = [polygon0kinds]``.\n\n    **kwargs\n        Keyword arguments are as described in the docstring of\n        `~.Axes.contour`.\n\n    %(contour_set_attributes)s\n    \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 797, "code": "    def get_transform(self):\n        if self._transform is None:\n            self._transform = self.axes.transData\n        elif (not isinstance(self._transform, mtransforms.Transform)\n              and hasattr(self._transform, '_as_mpl_transform')):\n            self._transform = self._transform._as_mpl_transform(self.axes)\n        return self._transform", "documentation": "        \"\"\"Return the `.Transform` instance used by this ContourSet.\"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 814, "code": "    def legend_elements(self, variable_name='x', str_format=str):\n        artists = []\n        labels = []\n        if self.filled:\n            lowers, uppers = self._get_lowers_and_uppers()\n            n_levels = len(self._paths)\n            for idx in range(n_levels):\n                artists.append(mpatches.Rectangle(\n                    (0, 0), 1, 1,\n                    facecolor=self.get_facecolor()[idx],\n                    hatch=self.hatches[idx % len(self.hatches)],", "documentation": "        \"\"\"\n        Return a list of artists and labels suitable for passing through\n        to `~.Axes.legend` which represent this ContourSet.\n\n        The labels have the form \"0 < x <= 1\" stating the data ranges which\n        the artists represent.\n\n        Parameters\n        ----------\n        variable_name : str\n            The string used inside the inequality used on the labels.\n        str_format : function: float -> str\n            Function used to format the numbers in the labels.\n\n        Returns\n        -------\n        artists : list[`.Artist`]\n            A list of the artists.\n        labels : list[str]\n            A list of the labels.\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 868, "code": "    def _process_args(self, *args, **kwargs):\n        self.levels = args[0]\n        allsegs = args[1]\n        allkinds = args[2] if len(args) > 2 else None\n        self.zmax = np.max(self.levels)\n        self.zmin = np.min(self.levels)\n        if allkinds is None:\n            allkinds = [[None] * len(segs) for segs in allsegs]\n        if self.filled:\n            if len(allsegs) != len(self.levels) - 1:\n                raise ValueError('must be one less number of segments as '", "documentation": "        \"\"\"\n        Process *args* and *kwargs*; override in derived classes.\n\n        Must set self.levels, self.zmin and self.zmax, and update Axes limits.\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 912, "code": "    def _make_paths_from_contour_generator(self):\n        if self._paths is not None:\n            return self._paths\n        cg = self._contour_generator\n        empty_path = Path(np.empty((0, 2)))\n        vertices_and_codes = (\n            map(cg.create_filled_contour, *self._get_lowers_and_uppers())\n            if self.filled else\n            map(cg.create_contour, self.levels))\n        return [Path(np.concatenate(vs), np.concatenate(cs)) if len(vs) else empty_path\n                for vs, cs in vertices_and_codes]", "documentation": "        \"\"\"Compute ``paths`` using C extension.\"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 925, "code": "    def _get_lowers_and_uppers(self):\n        lowers = self._levels[:-1]\n        if self.zmin == lowers[0]:\n            lowers = lowers.copy()  # so we don't change self._levels\n            if self.logscale:\n                lowers[0] = 0.99 * self.zmin\n            else:\n                lowers[0] -= 1\n        uppers = self._levels[1:]\n        return (lowers, uppers)", "documentation": "        \"\"\"\n        Return ``(lowers, uppers)`` for filled contours.\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 956, "code": "    def _ensure_locator_exists(self, N):\n        if self.locator is None:\n            if self.logscale:\n                self.locator = ticker.LogLocator(numticks=N)\n            else:\n                if N is None:\n                    N = 7  # Hard coded default\n                self.locator = ticker.MaxNLocator(N + 1, min_n_ticks=1)", "documentation": "        \"\"\"\n        Set a locator on this ContourSet if it's not already set.\n\n        Parameters\n        ----------\n        N : int or None\n            If *N* is an int, it is used as the target number of levels.\n            Otherwise when *N* is None, a reasonable default is chosen;\n            for logscales the LogLocator chooses, N=7 is the default\n            otherwise.\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 976, "code": "    def _autolev(self):\n        lev = self.locator.tick_values(self.zmin, self.zmax)\n        try:\n            if self.locator._symmetric:\n                return lev\n        except AttributeError:\n            pass\n        under = np.nonzero(lev < self.zmin)[0]\n        i0 = under[-1] if len(under) else 0\n        over = np.nonzero(lev > self.zmax)[0]\n        i1 = over[0] + 1 if len(over) else len(lev)", "documentation": "        \"\"\"\n        Select contour levels to span the data.\n\n        We need two more levels for filled contours than for\n        line contours, because for the latter we need to specify\n        the lower and upper boundary of each range. For example,\n        a single contour boundary, say at z = 0, requires only\n        one contour line, but two filled regions, and therefore\n        three levels to provide boundaries for both regions.\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 1010, "code": "    def _process_contour_level_args(self, args, z_dtype):\n        levels_arg = self.levels\n        if levels_arg is None:\n            if args:\n                levels_arg = args[0]\n            elif np.issubdtype(z_dtype, bool):\n                levels_arg = [0, .5, 1] if self.filled else [.5]\n        if isinstance(levels_arg, Integral) or levels_arg is None:\n            self._ensure_locator_exists(levels_arg)\n            self.levels = self._autolev()\n        else:", "documentation": "        \"\"\"\n        Determine the contour levels and store in self.levels.\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 1034, "code": "    def _process_levels(self):\n        self._levels = list(self.levels)\n        if self.logscale:\n            lower, upper = 1e-250, 1e250\n        else:\n            lower, upper = -1e250, 1e250\n        if self.extend in ('both', 'min'):\n            self._levels.insert(0, lower)\n        if self.extend in ('both', 'max'):\n            self._levels.append(upper)\n        self._levels = np.asarray(self._levels)", "documentation": "        \"\"\"\n        Assign values to :attr:`layers` based on :attr:`levels`,\n        adding extended layers as needed if contours are filled.\n\n        For line contours, layers simply coincide with levels;\n        a line is a thin layer.  No extended levels are needed\n        with line contours.\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 1071, "code": "    def _process_colors(self):\n        self.monochrome = self.cmap.monochrome\n        if self.colors is not None:\n            i0, i1 = 0, len(self.levels)\n            if self.filled:\n                i1 -= 1\n                if self.extend in ('both', 'min'):\n                    i0 -= 1\n                if self.extend in ('both', 'max'):\n                    i1 += 1\n            self.cvalues = list(range(i0, i1))", "documentation": "        \"\"\"\n        Color argument processing for contouring.\n\n        Note that we base the colormapping on the contour levels\n        and layers, not on the actual range of the Z values.  This\n        means we don't have to worry about bad values in Z, and we\n        always have the full dynamic range available for the selected\n        levels.\n\n        The color is based on the midpoint of the layer, except for\n        extended end layers.  By default, the norm vmin and vmax\n        are the extreme values of the non-extended levels.  Hence,\n        the layer color extremes are not the extreme values of\n        the colormap itself, but approach those values as the number\n        of levels increases.  An advantage of this scheme is that\n        line contours, when added to filled contours, take on\n        colors that are consistent with those of the filled regions;\n        for example, a contour line on the boundary between two\n        regions will have a color intermediate between those\n        of the regions.\n\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 1151, "code": "    def _find_nearest_contour(self, xy, indices=None):\n        if self.filled:\n            raise ValueError(\"Method does not support filled contours\")\n        if indices is None:\n            indices = range(len(self._paths))\n        d2min = np.inf\n        idx_level_min = idx_vtx_min = proj_min = None\n        for idx_level in indices:\n            path = self._paths[idx_level]\n            idx_vtx_start = 0\n            for subpath in path._iter_connected_components():", "documentation": "        \"\"\"\n        Find the point in the unfilled contour plot that is closest (in screen\n        space) to point *xy*.\n\n        Parameters\n        ----------\n        xy : tuple[float, float]\n            The reference point (in screen space).\n        indices : list of int or None, default: None\n            Indices of contour levels to consider.  If None (the default), all levels\n            are considered.\n\n        Returns\n        -------\n        idx_level_min : int\n            The index of the contour level closest to *xy*.\n        idx_vtx_min : int\n            The index of the `.Path` segment closest to *xy* (at that level).\n        proj : (float, float)\n            The point in the contour plot closest to *xy*.\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 1204, "code": "    def find_nearest_contour(self, x, y, indices=None, pixel=True):\n        segment = index = d2 = None\n        with ExitStack() as stack:\n            if not pixel:\n                stack.enter_context(self._cm_set(\n                    transform=mtransforms.IdentityTransform()))\n            i_level, i_vtx, (xmin, ymin) = self._find_nearest_contour((x, y), indices)\n        if i_level is not None:\n            cc_cumlens = np.cumsum(\n                [*map(len, self._paths[i_level]._iter_connected_components())])\n            segment = cc_cumlens.searchsorted(i_vtx, \"right\")", "documentation": "        \"\"\"\n        Find the point in the contour plot that is closest to ``(x, y)``.\n\n        This method does not support filled contours.\n\n        Parameters\n        ----------\n        x, y : float\n            The reference point.\n        indices : list of int or None, default: None\n            Indices of contour levels to consider.  If None (the default), all\n            levels are considered.\n        pixel : bool, default: True\n            If *True*, measure distance in pixel (screen) space, which is\n            useful for manual contour labeling; else, measure distance in axes\n            space.\n\n        Returns\n        -------\n        path : int\n            The index of the path that is closest to ``(x, y)``.  Each path corresponds\n            to one contour level.\n        subpath : int\n            The index within that closest path of the subpath that is closest to\n            ``(x, y)``.  Each subpath corresponds to one unbroken contour line.\n        index : int\n            The index of the vertices within that subpath that are closest to\n            ``(x, y)``.\n        xmin, ymin : float\n            The point in the contour plot that is closest to ``(x, y)``.\n        d2 : float\n            The squared distance from ``(xmin, ymin)`` to ``(x, y)``.\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 1282, "code": "class QuadContourSet(ContourSet):", "documentation": "    \"\"\"\n    Create and store a set of contour lines or filled regions.\n\n    This class is typically not instantiated directly by the user but by\n    `~.Axes.contour` and `~.Axes.contourf`.\n\n    %(contour_set_attributes)s\n    \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 1292, "code": "    def _process_args(self, *args, corner_mask=None, algorithm=None, **kwargs):\n        if args and isinstance(args[0], QuadContourSet):\n            if self.levels is None:\n                self.levels = args[0].levels\n            self.zmin = args[0].zmin\n            self.zmax = args[0].zmax\n            self._corner_mask = args[0]._corner_mask\n            contour_generator = args[0]._contour_generator\n            self._mins = args[0]._mins\n            self._maxs = args[0]._maxs\n            self._algorithm = args[0]._algorithm", "documentation": "        \"\"\"\n        Process args and kwargs.\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 1376, "code": "    def _check_xyz(self, x, y, z, kwargs):\n        x, y = self.axes._process_unit_info([(\"x\", x), (\"y\", y)], kwargs)\n        x = np.asarray(x, dtype=np.float64)\n        y = np.asarray(y, dtype=np.float64)\n        z = ma.asarray(z)\n        if z.ndim != 2:\n            raise TypeError(f\"Input z must be 2D, not {z.ndim}D\")\n        if z.shape[0] < 2 or z.shape[1] < 2:\n            raise TypeError(f\"Input z must be at least a (2, 2) shaped array, \"\n                            f\"but has shape {z.shape}\")\n        Ny, Nx = z.shape", "documentation": "        \"\"\"\n        Check that the shapes of the input arrays match; if x and y are 1D,\n        convert them to 2D using meshgrid.\n        \"\"\""}, {"filename": "lib/matplotlib/contour.py", "start_line": 1419, "code": "    def _initialize_x_y(self, z):\n        if z.ndim != 2:\n            raise TypeError(f\"Input z must be 2D, not {z.ndim}D\")\n        elif z.shape[0] < 2 or z.shape[1] < 2:\n            raise TypeError(f\"Input z must be at least a (2, 2) shaped array, \"\n                            f\"but has shape {z.shape}\")\n        else:\n            Ny, Nx = z.shape\n        if self.origin is None:  # Not for image-matching.\n            if self.extent is None:\n                return np.meshgrid(np.arange(Nx), np.arange(Ny))", "documentation": "        \"\"\"\n        Return X, Y arrays such that contour(Z) will match imshow(Z)\n        if origin is not None.\n        The center of pixel Z[i, j] depends on origin:\n        if origin is None, x = j, y = i;\n        if origin is 'lower', x = j + 0.5, y = i + 0.5;\n        if origin is 'upper', x = j + 0.5, y = Nrows - i - 0.5\n        If extent is not None, x and y will be scaled to match,\n        as in imshow.\n        If origin is None and extent is not None, then extent\n        will give the minimum and maximum values of x and y.\n        \"\"\""}]}
{"repository": "matplotlib/matplotlib", "commit_sha": "88e62bd75d0ce91ac28f39a953595177a1068e2f", "commit_message": "DOC: Fix docstring formatting for FFMpegFileWriter\n\n- Move Parameters section to correct location\n- Add blank line before Parameters section\n- Follow NumPy docstring conventions", "commit_date": "2025-12-07T16:56:19+00:00", "author": "Saumya", "file": "lib/matplotlib/animation.py", "patch": "@@ -606,16 +606,17 @@ class FFMpegFileWriter(FFMpegBase, FileMovieWriter):\n     \"\"\"\n     File-based ffmpeg writer.\n \n-    Parameters\n-    ----------\n-    *args, **kwargs\n-        All arguments are forwarded to `FileMovieWriter`.\n-\n     Frames are written to temporary files on disk and then stitched together at the end.\n \n     This effectively works as a slideshow input to ffmpeg with the fps passed as\n     ``-framerate``, so see also `their notes on frame rates`_ for further details.\n \n+    Parameters\n+    ----------\n+    *args, **kwargs\n+        All arguments are forwarded to `FileMovieWriter`. See\n+        `FileMovieWriter` for a list of all possible parameters.\n+\n     .. _their notes on frame rates: https://trac.ffmpeg.org/wiki/Slideshow#Framerates\n     \"\"\"\n     supported_formats = ['png', 'jpeg', 'tiff', 'raw', 'rgba']", "before_segments": [{"filename": "lib/matplotlib/animation.py", "start_line": 32, "code": "def adjusted_figsize(w, h, dpi, n):", "documentation": "    \"\"\"\n    Compute figure size so that pixels are a multiple of n.\n\n    Parameters\n    ----------\n    w, h : float\n        Size in inches.\n\n    dpi : float\n        The dpi.\n\n    n : int\n        The target multiple.\n\n    Returns\n    -------\n    wnew, hnew : float\n        The new figure size in inches.\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 68, "code": "class MovieWriterRegistry:", "documentation": "    \"\"\"Registry of available writer classes by human readable name.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 74, "code": "    def register(self, name):", "documentation": "        \"\"\"\n        Decorator for registering a class under a name.\n\n        Example use::\n\n            @registry.register(name)\n            class Foo:\n                pass\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 89, "code": "    def is_available(self, name):\n        try:\n            cls = self._registered[name]\n        except KeyError:\n            return False\n        return cls.isAvailable()", "documentation": "        \"\"\"\n        Check if given writer is available by name.\n\n        Parameters\n        ----------\n        name : str\n\n        Returns\n        -------\n        bool\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 107, "code": "    def __iter__(self):\n        for name in self._registered:\n            if self.is_available(name):\n                yield name", "documentation": "        \"\"\"Iterate over names of available writer class.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 113, "code": "    def list(self):\n        return [*self]", "documentation": "        \"\"\"Get a list of available MovieWriters.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 117, "code": "    def __getitem__(self, name):\n        if self.is_available(name):\n            return self._registered[name]\n        raise RuntimeError(f\"Requested MovieWriter ({name}) not available\")\nwriters = MovieWriterRegistry()", "documentation": "        \"\"\"Get an available writer class from its name.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 127, "code": "class AbstractMovieWriter(abc.ABC):", "documentation": "    \"\"\"\n    Abstract base class for writing movies, providing a way to grab frames by\n    calling `~AbstractMovieWriter.grab_frame`.\n\n    `setup` is called to start the process and `finish` is called afterwards.\n    `saving` is provided as a context manager to facilitate this process as ::\n\n        with moviewriter.saving(fig, outfile='myfile.mp4', dpi=100):\n            # Iterate over frames\n            moviewriter.grab_frame(**savefig_kwargs)\n\n    The use of the context manager ensures that `setup` and `finish` are\n    performed as necessary.\n\n    An instance of a concrete subclass of this class can be given as the\n    ``writer`` argument of `Animation.save()`.\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 153, "code": "    def setup(self, fig, outfile, dpi=None):\n        Path(outfile).parent.resolve(strict=True)\n        self.outfile = outfile\n        self.fig = fig\n        if dpi is None:\n            dpi = self.fig.dpi\n        self.dpi = dpi\n    @property", "documentation": "        \"\"\"\n        Setup for writing the movie file.\n\n        Parameters\n        ----------\n        fig : `~matplotlib.figure.Figure`\n            The figure object that contains the information for frames.\n        outfile : str\n            The filename of the resulting movie file.\n        dpi : float, default: ``fig.dpi``\n            The DPI (or resolution) for the file.  This controls the size\n            in pixels of the resulting movie file.\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 176, "code": "    def frame_size(self):\n        w, h = self.fig.get_size_inches()\n        return int(w * self.dpi), int(h * self.dpi)", "documentation": "        \"\"\"A tuple ``(width, height)`` in pixels of a movie frame.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 181, "code": "    def _supports_transparency(self):\n        return False\n    @abc.abstractmethod", "documentation": "        \"\"\"\n        Whether this writer supports transparency.\n\n        Writers may consult output file type and codec to determine this at runtime.\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 190, "code": "    def grab_frame(self, **savefig_kwargs):\n    @abc.abstractmethod", "documentation": "        \"\"\"\n        Grab the image information from the figure and save as a movie frame.\n\n        All keyword arguments in *savefig_kwargs* are passed on to the\n        `~.Figure.savefig` call that saves the figure.  However, several\n        keyword arguments that are supported by `~.Figure.savefig` may not be\n        passed as they are controlled by the MovieWriter:\n\n        - *dpi*, *bbox_inches*:  These may not be passed because each frame of the\n           animation much be exactly the same size in pixels.\n        - *format*: This is controlled by the MovieWriter.\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 205, "code": "    def finish(self):\n    @contextlib.contextmanager", "documentation": "        \"\"\"Finish any processing for writing the movie.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 209, "code": "    def saving(self, fig, outfile, dpi, *args, **kwargs):\n        if mpl.rcParams['savefig.bbox'] == 'tight':\n            _log.info(\"Disabling savefig.bbox = 'tight', as it may cause \"\n                      \"frame size to vary, which is inappropriate for \"\n                      \"animation.\")\n        self.setup(fig, outfile, dpi, *args, **kwargs)\n        with mpl.rc_context({'savefig.bbox': None}):\n            try:\n                yield self\n            finally:\n                self.finish()", "documentation": "        \"\"\"\n        Context manager to facilitate writing the movie file.\n\n        ``*args, **kw`` are any parameters that should be passed to `setup`.\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 229, "code": "class MovieWriter(AbstractMovieWriter):\n    supported_formats = [\"rgba\"]", "documentation": "    \"\"\"\n    Base class for writing movies.\n\n    This is a base class for MovieWriter subclasses that write a movie frame\n    data to a pipe. You cannot instantiate this class directly.\n    See examples for how to use its subclasses.\n\n    Attributes\n    ----------\n    frame_format : str\n        The format used in writing frame data, defaults to 'rgba'.\n    fig : `~matplotlib.figure.Figure`\n        The figure to capture data from.\n        This must be provided by the subclasses.\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 326, "code": "    def finish(self):\n        out, err = self._proc.communicate()\n        out = TextIOWrapper(BytesIO(out)).read()\n        err = TextIOWrapper(BytesIO(err)).read()\n        if out:\n            _log.log(\n                logging.WARNING if self._proc.returncode else logging.DEBUG,\n                \"MovieWriter stdout:\\n%s\", out)\n        if err:\n            _log.log(\n                logging.WARNING if self._proc.returncode else logging.DEBUG,", "documentation": "        \"\"\"Finish any processing for writing the movie.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 355, "code": "    def _args(self):\n        return NotImplementedError(\"args needs to be implemented by subclass.\")\n    @classmethod", "documentation": "        \"\"\"Assemble list of encoder-specific command-line arguments.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 360, "code": "    def bin_path(cls):\n        return str(mpl.rcParams[cls._exec_key])\n    @classmethod", "documentation": "        \"\"\"\n        Return the binary path to the commandline tool used by a specific\n        subclass. This is a class method so that the tool can be looked for\n        before making a particular MovieWriter subclass available.\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 369, "code": "    def isAvailable(cls):\n        return shutil.which(cls.bin_path()) is not None", "documentation": "        \"\"\"Return whether a MovieWriter subclass is actually available.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 374, "code": "class FileMovieWriter(MovieWriter):", "documentation": "    \"\"\"\n    `MovieWriter` for writing to individual files and stitching at the end.\n\n    This must be sub-classed to be useful.\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 384, "code": "    def setup(self, fig, outfile, dpi=None, frame_prefix=None):\n        Path(outfile).parent.resolve(strict=True)\n        self.fig = fig\n        self.outfile = outfile\n        if dpi is None:\n            dpi = self.fig.dpi\n        self.dpi = dpi\n        self._adjust_frame_size()\n        if frame_prefix is None:\n            self._tmpdir = TemporaryDirectory()\n            self.temp_prefix = str(Path(self._tmpdir.name, 'tmp'))", "documentation": "        \"\"\"\n        Setup for writing the movie file.\n\n        Parameters\n        ----------\n        fig : `~matplotlib.figure.Figure`\n            The figure to grab the rendered frames from.\n        outfile : str\n            The filename of the resulting movie file.\n        dpi : float, default: ``fig.dpi``\n            The dpi of the output file. This, with the figure size,\n            controls the size in pixels of the resulting movie file.\n        frame_prefix : str, optional\n            The filename prefix to use for temporary files.  If *None* (the\n            default), files are written to a temporary directory which is\n            deleted by `finish`; if not *None*, no temporary files are\n            deleted.\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 427, "code": "    def frame_format(self):\n        return self._frame_format\n    @frame_format.setter", "documentation": "        \"\"\"\n        Format (png, jpeg, etc.) to use for saving the frames, which can be\n        decided by the individual subclasses.\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 513, "code": "class FFMpegBase:\n    _exec_key = 'animation.ffmpeg_path'\n    _args_key = 'animation.ffmpeg_args'", "documentation": "    \"\"\"\n    Mixin class for FFMpeg output.\n\n    This is a base class for the concrete `FFMpegWriter` and `FFMpegFileWriter`\n    classes.\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 576, "code": "class FFMpegWriter(FFMpegBase, MovieWriter):", "documentation": "    \"\"\"\n    Pipe-based ffmpeg writer.\n\n    Frames are streamed directly to ffmpeg via a pipe and written in a single pass.\n\n    This effectively works as a slideshow input to ffmpeg with the fps passed as\n    ``-framerate``, so see also `their notes on frame rates`_ for further details.\n\n    .. _their notes on frame rates: https://trac.ffmpeg.org/wiki/Slideshow#Framerates\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 604, "code": "class FFMpegFileWriter(FFMpegBase, FileMovieWriter):\n    supported_formats = ['png', 'jpeg', 'tiff', 'raw', 'rgba']", "documentation": "    \"\"\"\n    File-based ffmpeg writer.\n\n    Parameters\n    ----------\n    *args, **kwargs\n        All arguments are forwarded to `FileMovieWriter`.\n\n    Frames are written to temporary files on disk and then stitched together at the end.\n\n    This effectively works as a slideshow input to ffmpeg with the fps passed as\n    ``-framerate``, so see also `their notes on frame rates`_ for further details.\n\n    .. _their notes on frame rates: https://trac.ffmpeg.org/wiki/Slideshow#Framerates\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 645, "code": "class ImageMagickBase:\n    _exec_key = 'animation.convert_path'\n    _args_key = 'animation.convert_args'", "documentation": "    \"\"\"\n    Mixin class for ImageMagick output.\n\n    This is a base class for the concrete `ImageMagickWriter` and\n    `ImageMagickFileWriter` classes, which define an ``input_names`` attribute\n    (or property) specifying the input names passed to ImageMagick.\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 696, "code": "class ImageMagickWriter(ImageMagickBase, MovieWriter):\n    input_names = \"-\"  # stdin\n@writers.register('imagemagick_file')", "documentation": "    \"\"\"\n    Pipe-based animated gif writer.\n\n    Frames are streamed directly to ImageMagick via a pipe and written\n    in a single pass.\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 709, "code": "class ImageMagickFileWriter(ImageMagickBase, FileMovieWriter):\n    supported_formats = ['png', 'jpeg', 'tiff', 'raw', 'rgba']\n    input_names = property(\n        lambda self: f'{self.temp_prefix}*.{self.frame_format}')", "documentation": "    \"\"\"\n    File-based animated gif writer.\n\n    Frames are written to temporary files on disk and then stitched\n    together at the end.\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 730, "code": "def _embedded_frames(frame_list, frame_format):\n    if frame_format == 'svg':\n        frame_format = 'svg+xml'\n    template = '  frames[{0}] = \"data:image/{1};base64,{2}\"\\n'\n    return \"\\n\" + \"\".join(\n        template.format(i, frame_format, frame_data.replace('\\n', '\\\\\\n'))\n        for i, frame_data in enumerate(frame_list))\n@writers.register('html')", "documentation": "    \"\"\"frame_list should be a list of base64-encoded png files\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 742, "code": "class HTMLWriter(FileMovieWriter):\n    supported_formats = ['png', 'jpeg', 'tiff', 'svg']\n    @classmethod", "documentation": "    \"\"\"Writer for JavaScript-based HTML movies.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 850, "code": "class Animation:", "documentation": "    \"\"\"\n    A base class for Animations.\n\n    This class is not usable as is, and should be subclassed to provide needed\n    behavior.\n\n    .. note::\n\n        You must store the created Animation in a variable that lives as long\n        as the animation should run. Otherwise, the Animation object will be\n        garbage-collected and the animation stops.\n\n    Parameters\n    ----------\n    fig : `~matplotlib.figure.Figure`\n        The figure object used to get needed events, such as draw or resize.\n\n    event_source : object\n        A class that can run a callback when desired events\n        are generated, as well as be stopped and started.\n\n        Examples include timers (see `TimedAnimation`) and file\n        system notifications.\n\n    blit : bool, default: False\n        Whether blitting is used to optimize drawing.  If the backend does not\n        support blitting, then this parameter has no effect.\n\n    See Also\n    --------\n    FuncAnimation,  ArtistAnimation\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 922, "code": "    def _start(self, *args):\n        if self._fig.canvas.is_saving():\n            return\n        self._fig.canvas.mpl_disconnect(self._first_draw_id)\n        self._init_draw()\n        self.event_source.start()", "documentation": "        \"\"\"\n        Starts interactive animation. Adds the draw frame command to the GUI\n        handler, calls show to start the event loop.\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1130, "code": "    def _step(self, *args):\n        try:\n            framedata = next(self.frame_seq)\n            self._draw_next_frame(framedata, self._blit)\n            return True\n        except StopIteration:\n            return False", "documentation": "        \"\"\"\n        Handler for getting events. By default, gets the next frame in the\n        sequence and hands the data off to be drawn.\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1145, "code": "    def new_frame_seq(self):\n        return iter(self._framedata)", "documentation": "        \"\"\"Return a new sequence of frame information.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1150, "code": "    def new_saved_frame_seq(self):\n        return self.new_frame_seq()", "documentation": "        \"\"\"Return a new sequence of saved/cached frame information.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1261, "code": "    def to_html5_video(self, embed_limit=None):\n        VIDEO_TAG = r'''<video {size} {options}>\n  <source type=\"video/mp4\" src=\"data:video/mp4;base64,{video}\">\n  Your browser does not support the video tag.\n</video>'''\n        if not hasattr(self, '_base64_video'):\n            embed_limit = mpl._val_or_rc(embed_limit, 'animation.embed_limit')\n            embed_limit *= 1024 * 1024\n            with TemporaryDirectory() as tmpdir:\n                path = Path(tmpdir, \"temp.m4v\")\n                Writer = writers[mpl.rcParams['animation.writer']]", "documentation": "        \"\"\"\n        Convert the animation to an HTML5 ``<video>`` tag.\n\n        This saves the animation as an h264 video, encoded in base64\n        directly into the HTML5 video tag. This respects :rc:`animation.writer`\n        and :rc:`animation.bitrate`. This also makes use of the\n        *interval* to control the speed, and uses the *repeat*\n        parameter to decide whether to loop.\n\n        Parameters\n        ----------\n        embed_limit : float, optional\n            Limit, in MB, of the returned animation. No animation is created\n            if the limit is exceeded.\n            Defaults to :rc:`animation.embed_limit` = 20.0.\n\n        Returns\n        -------\n        str\n            An HTML5 video tag with the animation embedded as base64 encoded\n            h264 video.\n            If the *embed_limit* is exceeded, this returns the string\n            \"Video too large to embed.\"\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1339, "code": "    def to_jshtml(self, fps=None, embed_frames=True, default_mode=None):\n        if fps is None and hasattr(self, '_interval'):\n            fps = 1000 / self._interval\n        if default_mode is None:\n            default_mode = 'loop' if getattr(self, '_repeat',\n                                             False) else 'once'\n        if not hasattr(self, \"_html_representation\"):\n            with TemporaryDirectory() as tmpdir:\n                path = Path(tmpdir, \"temp.html\")\n                writer = HTMLWriter(fps=fps,\n                                    embed_frames=embed_frames,", "documentation": "        \"\"\"\n        Generate HTML representation of the animation.\n\n        Parameters\n        ----------\n        fps : int, optional\n            Movie frame rate (per second). If not set, the frame rate from\n            the animation's frame interval.\n        embed_frames : bool, optional\n        default_mode : str, optional\n            What to do when the animation ends. Must be one of ``{'loop',\n            'once', 'reflect'}``. Defaults to ``'loop'`` if the *repeat*\n            parameter is True, otherwise ``'once'``.\n\n        Returns\n        -------\n        str\n            An HTML representation of the animation embedded as a js object as\n            produced with the `.HTMLWriter`.\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1383, "code": "    def _repr_html_(self):\n        fmt = mpl.rcParams['animation.html']\n        if fmt == 'html5':\n            return self.to_html5_video()\n        elif fmt == 'jshtml':\n            return self.to_jshtml()", "documentation": "        \"\"\"IPython display hook for rendering.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1391, "code": "    def pause(self):\n        self.event_source.stop()\n        if self._blit:\n            for artist in self._drawn_artists:\n                artist.set_animated(False)", "documentation": "        \"\"\"Pause the animation.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1398, "code": "    def resume(self):\n        self.event_source.start()\n        if self._blit:\n            for artist in self._drawn_artists:\n                artist.set_animated(True)", "documentation": "        \"\"\"Resume the animation.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1406, "code": "class TimedAnimation(Animation):", "documentation": "    \"\"\"\n    `Animation` subclass for time-based animation.\n\n    A new frame is drawn every *interval* milliseconds.\n\n    .. note::\n\n        You must store the created Animation in a variable that lives as long\n        as the animation should run. Otherwise, the Animation object will be\n        garbage-collected and the animation stops.\n\n    Parameters\n    ----------\n    fig : `~matplotlib.figure.Figure`\n        The figure object used to get needed events, such as draw or resize.\n    interval : int, default: 200\n        Delay between frames in milliseconds.\n    repeat_delay : int, default: 0\n        The delay in milliseconds between consecutive animation runs, if\n        *repeat* is True.\n    repeat : bool, default: True\n        Whether the animation repeats when the sequence of frames is completed.\n    blit : bool, default: False\n        Whether blitting is used to optimize drawing.\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1444, "code": "    def _step(self, *args):\n        still_going = super()._step(*args)\n        if not still_going:\n            if self._repeat:\n                self._init_draw()\n                self.frame_seq = self.new_frame_seq()\n                self.event_source.interval = self._repeat_delay\n                return True\n            else:\n                self.pause()\n                if self._blit:", "documentation": "        \"\"\"Handler for getting events.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1475, "code": "class ArtistAnimation(TimedAnimation):", "documentation": "    \"\"\"\n    `TimedAnimation` subclass that creates an animation by using a fixed\n    set of `.Artist` objects.\n\n    Before creating an instance, all plotting should have taken place\n    and the relevant artists saved.\n\n    .. note::\n\n        You must store the created Animation in a variable that lives as long\n        as the animation should run. Otherwise, the Animation object will be\n        garbage-collected and the animation stops.\n\n    Parameters\n    ----------\n    fig : `~matplotlib.figure.Figure`\n        The figure object used to get needed events, such as draw or resize.\n    artists : list\n        Each list entry is a collection of `.Artist` objects that are made\n        visible on the corresponding frame.  Other artists are made invisible.\n    interval : int, default: 200\n        Delay between frames in milliseconds.\n    repeat_delay : int, default: 0\n        The delay in milliseconds between consecutive animation runs, if\n        *repeat* is True.\n    repeat : bool, default: True\n        Whether the animation repeats when the sequence of frames is completed.\n    blit : bool, default: False\n        Whether blitting is used to optimize drawing.\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1532, "code": "    def _pre_draw(self, framedata, blit):\n        if blit:\n            self._blit_clear(self._drawn_artists)\n        else:\n            for artist in self._drawn_artists:\n                artist.set_visible(False)", "documentation": "        \"\"\"Clears artists from the last frame.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1552, "code": "class FuncAnimation(TimedAnimation):", "documentation": "    \"\"\"\n    `TimedAnimation` subclass that makes an animation by repeatedly calling\n    a function *func*.\n\n    .. note::\n\n        You must store the created Animation in a variable that lives as long\n        as the animation should run. Otherwise, the Animation object will be\n        garbage-collected and the animation stops.\n\n    Parameters\n    ----------\n    fig : `~matplotlib.figure.Figure`\n        The figure object used to get needed events, such as draw or resize.\n\n    func : callable\n        The function to call at each frame.  The first argument will\n        be the next value in *frames*.   Any additional positional\n        arguments can be supplied using `functools.partial` or via the *fargs*\n        parameter.\n\n        The required signature is::\n\n            def func(frame, *fargs) -> iterable_of_artists\n\n        It is often more convenient to provide the arguments using\n        `functools.partial`. In this way it is also possible to pass keyword\n        arguments. To pass a function with both positional and keyword\n        arguments, set all arguments as keyword arguments, just leaving the\n        *frame* argument unset::\n\n            def func(frame, art, *, y=None):\n                ...\n\n            ani = FuncAnimation(fig, partial(func, art=ln, y='foo'))\n\n        If ``blit == True``, *func* must return an iterable of all artists\n        that were modified or created. This information is used by the blitting\n        algorithm to determine which parts of the figure have to be updated.\n        The return value is unused if ``blit == False`` and may be omitted in\n        that case.\n\n    frames : iterable, int, generator function, or None, optional\n        Source of data to pass *func* and each frame of the animation\n\n        - If an iterable, then simply use the values provided.  If the\n          iterable has a length, it will override the *save_count* kwarg.\n\n        - If an integer, then equivalent to passing ``range(frames)``\n\n        - If a generator function, then must have the signature::\n\n             def gen_function() -> obj\n\n        - If *None*, then equivalent to passing ``itertools.count``.\n\n        In all of these cases, the values in *frames* is simply passed through\n        to the user-supplied *func* and thus can be of any type.\n\n    init_func : callable, optional\n        A function used to draw a clear frame. If not given, the results of\n        drawing from the first item in the frames sequence will be used. This\n        function will be called once before the first frame.\n\n        The required signature is::\n\n            def init_func() -> iterable_of_artists\n\n        If ``blit == True``, *init_func* must return an iterable of artists\n        to be re-drawn. This information is used by the blitting algorithm to\n        determine which parts of the figure have to be updated.  The return\n        value is unused if ``blit == False`` and may be omitted in that case.\n\n    fargs : tuple or None, optional\n        Additional arguments to pass to each call to *func*. Note: the use of\n        `functools.partial` is preferred over *fargs*. See *func* for details.\n\n    save_count : int, optional\n        Fallback for the number of values from *frames* to cache. This is\n        only used if the number of frames cannot be inferred from *frames*,\n        i.e. when it's an iterator without length or a generator.\n\n    interval : int, default: 200\n        Delay between frames in milliseconds.\n\n    repeat_delay : int, default: 0\n        The delay in milliseconds between consecutive animation runs, if\n        *repeat* is True.\n\n    repeat : bool, default: True\n        Whether the animation repeats when the sequence of frames is completed.\n\n    blit : bool, default: False\n        Whether blitting is used to optimize drawing.  Note: when using\n        blitting, any animated artists will be drawn according to their zorder;\n        however, they will be drawn on top of any previous artists, regardless\n        of their zorder.\n\n    cache_frame_data : bool, default: True\n        Whether frame data is cached.  Disabling cache might be helpful when\n        frames contain large objects.\n    \"\"\""}], "after_segments": [{"filename": "lib/matplotlib/animation.py", "start_line": 32, "code": "def adjusted_figsize(w, h, dpi, n):", "documentation": "    \"\"\"\n    Compute figure size so that pixels are a multiple of n.\n\n    Parameters\n    ----------\n    w, h : float\n        Size in inches.\n\n    dpi : float\n        The dpi.\n\n    n : int\n        The target multiple.\n\n    Returns\n    -------\n    wnew, hnew : float\n        The new figure size in inches.\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 68, "code": "class MovieWriterRegistry:", "documentation": "    \"\"\"Registry of available writer classes by human readable name.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 74, "code": "    def register(self, name):", "documentation": "        \"\"\"\n        Decorator for registering a class under a name.\n\n        Example use::\n\n            @registry.register(name)\n            class Foo:\n                pass\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 89, "code": "    def is_available(self, name):\n        try:\n            cls = self._registered[name]\n        except KeyError:\n            return False\n        return cls.isAvailable()", "documentation": "        \"\"\"\n        Check if given writer is available by name.\n\n        Parameters\n        ----------\n        name : str\n\n        Returns\n        -------\n        bool\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 107, "code": "    def __iter__(self):\n        for name in self._registered:\n            if self.is_available(name):\n                yield name", "documentation": "        \"\"\"Iterate over names of available writer class.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 113, "code": "    def list(self):\n        return [*self]", "documentation": "        \"\"\"Get a list of available MovieWriters.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 117, "code": "    def __getitem__(self, name):\n        if self.is_available(name):\n            return self._registered[name]\n        raise RuntimeError(f\"Requested MovieWriter ({name}) not available\")\nwriters = MovieWriterRegistry()", "documentation": "        \"\"\"Get an available writer class from its name.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 127, "code": "class AbstractMovieWriter(abc.ABC):", "documentation": "    \"\"\"\n    Abstract base class for writing movies, providing a way to grab frames by\n    calling `~AbstractMovieWriter.grab_frame`.\n\n    `setup` is called to start the process and `finish` is called afterwards.\n    `saving` is provided as a context manager to facilitate this process as ::\n\n        with moviewriter.saving(fig, outfile='myfile.mp4', dpi=100):\n            # Iterate over frames\n            moviewriter.grab_frame(**savefig_kwargs)\n\n    The use of the context manager ensures that `setup` and `finish` are\n    performed as necessary.\n\n    An instance of a concrete subclass of this class can be given as the\n    ``writer`` argument of `Animation.save()`.\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 153, "code": "    def setup(self, fig, outfile, dpi=None):\n        Path(outfile).parent.resolve(strict=True)\n        self.outfile = outfile\n        self.fig = fig\n        if dpi is None:\n            dpi = self.fig.dpi\n        self.dpi = dpi\n    @property", "documentation": "        \"\"\"\n        Setup for writing the movie file.\n\n        Parameters\n        ----------\n        fig : `~matplotlib.figure.Figure`\n            The figure object that contains the information for frames.\n        outfile : str\n            The filename of the resulting movie file.\n        dpi : float, default: ``fig.dpi``\n            The DPI (or resolution) for the file.  This controls the size\n            in pixels of the resulting movie file.\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 176, "code": "    def frame_size(self):\n        w, h = self.fig.get_size_inches()\n        return int(w * self.dpi), int(h * self.dpi)", "documentation": "        \"\"\"A tuple ``(width, height)`` in pixels of a movie frame.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 181, "code": "    def _supports_transparency(self):\n        return False\n    @abc.abstractmethod", "documentation": "        \"\"\"\n        Whether this writer supports transparency.\n\n        Writers may consult output file type and codec to determine this at runtime.\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 190, "code": "    def grab_frame(self, **savefig_kwargs):\n    @abc.abstractmethod", "documentation": "        \"\"\"\n        Grab the image information from the figure and save as a movie frame.\n\n        All keyword arguments in *savefig_kwargs* are passed on to the\n        `~.Figure.savefig` call that saves the figure.  However, several\n        keyword arguments that are supported by `~.Figure.savefig` may not be\n        passed as they are controlled by the MovieWriter:\n\n        - *dpi*, *bbox_inches*:  These may not be passed because each frame of the\n           animation much be exactly the same size in pixels.\n        - *format*: This is controlled by the MovieWriter.\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 205, "code": "    def finish(self):\n    @contextlib.contextmanager", "documentation": "        \"\"\"Finish any processing for writing the movie.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 209, "code": "    def saving(self, fig, outfile, dpi, *args, **kwargs):\n        if mpl.rcParams['savefig.bbox'] == 'tight':\n            _log.info(\"Disabling savefig.bbox = 'tight', as it may cause \"\n                      \"frame size to vary, which is inappropriate for \"\n                      \"animation.\")\n        self.setup(fig, outfile, dpi, *args, **kwargs)\n        with mpl.rc_context({'savefig.bbox': None}):\n            try:\n                yield self\n            finally:\n                self.finish()", "documentation": "        \"\"\"\n        Context manager to facilitate writing the movie file.\n\n        ``*args, **kw`` are any parameters that should be passed to `setup`.\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 229, "code": "class MovieWriter(AbstractMovieWriter):\n    supported_formats = [\"rgba\"]", "documentation": "    \"\"\"\n    Base class for writing movies.\n\n    This is a base class for MovieWriter subclasses that write a movie frame\n    data to a pipe. You cannot instantiate this class directly.\n    See examples for how to use its subclasses.\n\n    Attributes\n    ----------\n    frame_format : str\n        The format used in writing frame data, defaults to 'rgba'.\n    fig : `~matplotlib.figure.Figure`\n        The figure to capture data from.\n        This must be provided by the subclasses.\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 326, "code": "    def finish(self):\n        out, err = self._proc.communicate()\n        out = TextIOWrapper(BytesIO(out)).read()\n        err = TextIOWrapper(BytesIO(err)).read()\n        if out:\n            _log.log(\n                logging.WARNING if self._proc.returncode else logging.DEBUG,\n                \"MovieWriter stdout:\\n%s\", out)\n        if err:\n            _log.log(\n                logging.WARNING if self._proc.returncode else logging.DEBUG,", "documentation": "        \"\"\"Finish any processing for writing the movie.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 355, "code": "    def _args(self):\n        return NotImplementedError(\"args needs to be implemented by subclass.\")\n    @classmethod", "documentation": "        \"\"\"Assemble list of encoder-specific command-line arguments.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 360, "code": "    def bin_path(cls):\n        return str(mpl.rcParams[cls._exec_key])\n    @classmethod", "documentation": "        \"\"\"\n        Return the binary path to the commandline tool used by a specific\n        subclass. This is a class method so that the tool can be looked for\n        before making a particular MovieWriter subclass available.\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 369, "code": "    def isAvailable(cls):\n        return shutil.which(cls.bin_path()) is not None", "documentation": "        \"\"\"Return whether a MovieWriter subclass is actually available.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 374, "code": "class FileMovieWriter(MovieWriter):", "documentation": "    \"\"\"\n    `MovieWriter` for writing to individual files and stitching at the end.\n\n    This must be sub-classed to be useful.\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 384, "code": "    def setup(self, fig, outfile, dpi=None, frame_prefix=None):\n        Path(outfile).parent.resolve(strict=True)\n        self.fig = fig\n        self.outfile = outfile\n        if dpi is None:\n            dpi = self.fig.dpi\n        self.dpi = dpi\n        self._adjust_frame_size()\n        if frame_prefix is None:\n            self._tmpdir = TemporaryDirectory()\n            self.temp_prefix = str(Path(self._tmpdir.name, 'tmp'))", "documentation": "        \"\"\"\n        Setup for writing the movie file.\n\n        Parameters\n        ----------\n        fig : `~matplotlib.figure.Figure`\n            The figure to grab the rendered frames from.\n        outfile : str\n            The filename of the resulting movie file.\n        dpi : float, default: ``fig.dpi``\n            The dpi of the output file. This, with the figure size,\n            controls the size in pixels of the resulting movie file.\n        frame_prefix : str, optional\n            The filename prefix to use for temporary files.  If *None* (the\n            default), files are written to a temporary directory which is\n            deleted by `finish`; if not *None*, no temporary files are\n            deleted.\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 427, "code": "    def frame_format(self):\n        return self._frame_format\n    @frame_format.setter", "documentation": "        \"\"\"\n        Format (png, jpeg, etc.) to use for saving the frames, which can be\n        decided by the individual subclasses.\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 513, "code": "class FFMpegBase:\n    _exec_key = 'animation.ffmpeg_path'\n    _args_key = 'animation.ffmpeg_args'", "documentation": "    \"\"\"\n    Mixin class for FFMpeg output.\n\n    This is a base class for the concrete `FFMpegWriter` and `FFMpegFileWriter`\n    classes.\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 576, "code": "class FFMpegWriter(FFMpegBase, MovieWriter):", "documentation": "    \"\"\"\n    Pipe-based ffmpeg writer.\n\n    Frames are streamed directly to ffmpeg via a pipe and written in a single pass.\n\n    This effectively works as a slideshow input to ffmpeg with the fps passed as\n    ``-framerate``, so see also `their notes on frame rates`_ for further details.\n\n    .. _their notes on frame rates: https://trac.ffmpeg.org/wiki/Slideshow#Framerates\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 604, "code": "class FFMpegFileWriter(FFMpegBase, FileMovieWriter):\n    supported_formats = ['png', 'jpeg', 'tiff', 'raw', 'rgba']", "documentation": "    \"\"\"\n    File-based ffmpeg writer.\n\n    Frames are written to temporary files on disk and then stitched together at the end.\n\n    This effectively works as a slideshow input to ffmpeg with the fps passed as\n    ``-framerate``, so see also `their notes on frame rates`_ for further details.\n\n    Parameters\n    ----------\n    *args, **kwargs\n        All arguments are forwarded to `FileMovieWriter`. See\n        `FileMovieWriter` for a list of all possible parameters.\n\n    .. _their notes on frame rates: https://trac.ffmpeg.org/wiki/Slideshow#Framerates\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 646, "code": "class ImageMagickBase:\n    _exec_key = 'animation.convert_path'\n    _args_key = 'animation.convert_args'", "documentation": "    \"\"\"\n    Mixin class for ImageMagick output.\n\n    This is a base class for the concrete `ImageMagickWriter` and\n    `ImageMagickFileWriter` classes, which define an ``input_names`` attribute\n    (or property) specifying the input names passed to ImageMagick.\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 697, "code": "class ImageMagickWriter(ImageMagickBase, MovieWriter):\n    input_names = \"-\"  # stdin\n@writers.register('imagemagick_file')", "documentation": "    \"\"\"\n    Pipe-based animated gif writer.\n\n    Frames are streamed directly to ImageMagick via a pipe and written\n    in a single pass.\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 710, "code": "class ImageMagickFileWriter(ImageMagickBase, FileMovieWriter):\n    supported_formats = ['png', 'jpeg', 'tiff', 'raw', 'rgba']\n    input_names = property(\n        lambda self: f'{self.temp_prefix}*.{self.frame_format}')", "documentation": "    \"\"\"\n    File-based animated gif writer.\n\n    Frames are written to temporary files on disk and then stitched\n    together at the end.\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 731, "code": "def _embedded_frames(frame_list, frame_format):\n    if frame_format == 'svg':\n        frame_format = 'svg+xml'\n    template = '  frames[{0}] = \"data:image/{1};base64,{2}\"\\n'\n    return \"\\n\" + \"\".join(\n        template.format(i, frame_format, frame_data.replace('\\n', '\\\\\\n'))\n        for i, frame_data in enumerate(frame_list))\n@writers.register('html')", "documentation": "    \"\"\"frame_list should be a list of base64-encoded png files\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 743, "code": "class HTMLWriter(FileMovieWriter):\n    supported_formats = ['png', 'jpeg', 'tiff', 'svg']\n    @classmethod", "documentation": "    \"\"\"Writer for JavaScript-based HTML movies.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 851, "code": "class Animation:", "documentation": "    \"\"\"\n    A base class for Animations.\n\n    This class is not usable as is, and should be subclassed to provide needed\n    behavior.\n\n    .. note::\n\n        You must store the created Animation in a variable that lives as long\n        as the animation should run. Otherwise, the Animation object will be\n        garbage-collected and the animation stops.\n\n    Parameters\n    ----------\n    fig : `~matplotlib.figure.Figure`\n        The figure object used to get needed events, such as draw or resize.\n\n    event_source : object\n        A class that can run a callback when desired events\n        are generated, as well as be stopped and started.\n\n        Examples include timers (see `TimedAnimation`) and file\n        system notifications.\n\n    blit : bool, default: False\n        Whether blitting is used to optimize drawing.  If the backend does not\n        support blitting, then this parameter has no effect.\n\n    See Also\n    --------\n    FuncAnimation,  ArtistAnimation\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 923, "code": "    def _start(self, *args):\n        if self._fig.canvas.is_saving():\n            return\n        self._fig.canvas.mpl_disconnect(self._first_draw_id)\n        self._init_draw()\n        self.event_source.start()", "documentation": "        \"\"\"\n        Starts interactive animation. Adds the draw frame command to the GUI\n        handler, calls show to start the event loop.\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1131, "code": "    def _step(self, *args):\n        try:\n            framedata = next(self.frame_seq)\n            self._draw_next_frame(framedata, self._blit)\n            return True\n        except StopIteration:\n            return False", "documentation": "        \"\"\"\n        Handler for getting events. By default, gets the next frame in the\n        sequence and hands the data off to be drawn.\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1146, "code": "    def new_frame_seq(self):\n        return iter(self._framedata)", "documentation": "        \"\"\"Return a new sequence of frame information.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1151, "code": "    def new_saved_frame_seq(self):\n        return self.new_frame_seq()", "documentation": "        \"\"\"Return a new sequence of saved/cached frame information.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1262, "code": "    def to_html5_video(self, embed_limit=None):\n        VIDEO_TAG = r'''<video {size} {options}>\n  <source type=\"video/mp4\" src=\"data:video/mp4;base64,{video}\">\n  Your browser does not support the video tag.\n</video>'''\n        if not hasattr(self, '_base64_video'):\n            embed_limit = mpl._val_or_rc(embed_limit, 'animation.embed_limit')\n            embed_limit *= 1024 * 1024\n            with TemporaryDirectory() as tmpdir:\n                path = Path(tmpdir, \"temp.m4v\")\n                Writer = writers[mpl.rcParams['animation.writer']]", "documentation": "        \"\"\"\n        Convert the animation to an HTML5 ``<video>`` tag.\n\n        This saves the animation as an h264 video, encoded in base64\n        directly into the HTML5 video tag. This respects :rc:`animation.writer`\n        and :rc:`animation.bitrate`. This also makes use of the\n        *interval* to control the speed, and uses the *repeat*\n        parameter to decide whether to loop.\n\n        Parameters\n        ----------\n        embed_limit : float, optional\n            Limit, in MB, of the returned animation. No animation is created\n            if the limit is exceeded.\n            Defaults to :rc:`animation.embed_limit` = 20.0.\n\n        Returns\n        -------\n        str\n            An HTML5 video tag with the animation embedded as base64 encoded\n            h264 video.\n            If the *embed_limit* is exceeded, this returns the string\n            \"Video too large to embed.\"\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1340, "code": "    def to_jshtml(self, fps=None, embed_frames=True, default_mode=None):\n        if fps is None and hasattr(self, '_interval'):\n            fps = 1000 / self._interval\n        if default_mode is None:\n            default_mode = 'loop' if getattr(self, '_repeat',\n                                             False) else 'once'\n        if not hasattr(self, \"_html_representation\"):\n            with TemporaryDirectory() as tmpdir:\n                path = Path(tmpdir, \"temp.html\")\n                writer = HTMLWriter(fps=fps,\n                                    embed_frames=embed_frames,", "documentation": "        \"\"\"\n        Generate HTML representation of the animation.\n\n        Parameters\n        ----------\n        fps : int, optional\n            Movie frame rate (per second). If not set, the frame rate from\n            the animation's frame interval.\n        embed_frames : bool, optional\n        default_mode : str, optional\n            What to do when the animation ends. Must be one of ``{'loop',\n            'once', 'reflect'}``. Defaults to ``'loop'`` if the *repeat*\n            parameter is True, otherwise ``'once'``.\n\n        Returns\n        -------\n        str\n            An HTML representation of the animation embedded as a js object as\n            produced with the `.HTMLWriter`.\n        \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1384, "code": "    def _repr_html_(self):\n        fmt = mpl.rcParams['animation.html']\n        if fmt == 'html5':\n            return self.to_html5_video()\n        elif fmt == 'jshtml':\n            return self.to_jshtml()", "documentation": "        \"\"\"IPython display hook for rendering.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1392, "code": "    def pause(self):\n        self.event_source.stop()\n        if self._blit:\n            for artist in self._drawn_artists:\n                artist.set_animated(False)", "documentation": "        \"\"\"Pause the animation.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1399, "code": "    def resume(self):\n        self.event_source.start()\n        if self._blit:\n            for artist in self._drawn_artists:\n                artist.set_animated(True)", "documentation": "        \"\"\"Resume the animation.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1407, "code": "class TimedAnimation(Animation):", "documentation": "    \"\"\"\n    `Animation` subclass for time-based animation.\n\n    A new frame is drawn every *interval* milliseconds.\n\n    .. note::\n\n        You must store the created Animation in a variable that lives as long\n        as the animation should run. Otherwise, the Animation object will be\n        garbage-collected and the animation stops.\n\n    Parameters\n    ----------\n    fig : `~matplotlib.figure.Figure`\n        The figure object used to get needed events, such as draw or resize.\n    interval : int, default: 200\n        Delay between frames in milliseconds.\n    repeat_delay : int, default: 0\n        The delay in milliseconds between consecutive animation runs, if\n        *repeat* is True.\n    repeat : bool, default: True\n        Whether the animation repeats when the sequence of frames is completed.\n    blit : bool, default: False\n        Whether blitting is used to optimize drawing.\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1445, "code": "    def _step(self, *args):\n        still_going = super()._step(*args)\n        if not still_going:\n            if self._repeat:\n                self._init_draw()\n                self.frame_seq = self.new_frame_seq()\n                self.event_source.interval = self._repeat_delay\n                return True\n            else:\n                self.pause()\n                if self._blit:", "documentation": "        \"\"\"Handler for getting events.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1476, "code": "class ArtistAnimation(TimedAnimation):", "documentation": "    \"\"\"\n    `TimedAnimation` subclass that creates an animation by using a fixed\n    set of `.Artist` objects.\n\n    Before creating an instance, all plotting should have taken place\n    and the relevant artists saved.\n\n    .. note::\n\n        You must store the created Animation in a variable that lives as long\n        as the animation should run. Otherwise, the Animation object will be\n        garbage-collected and the animation stops.\n\n    Parameters\n    ----------\n    fig : `~matplotlib.figure.Figure`\n        The figure object used to get needed events, such as draw or resize.\n    artists : list\n        Each list entry is a collection of `.Artist` objects that are made\n        visible on the corresponding frame.  Other artists are made invisible.\n    interval : int, default: 200\n        Delay between frames in milliseconds.\n    repeat_delay : int, default: 0\n        The delay in milliseconds between consecutive animation runs, if\n        *repeat* is True.\n    repeat : bool, default: True\n        Whether the animation repeats when the sequence of frames is completed.\n    blit : bool, default: False\n        Whether blitting is used to optimize drawing.\n    \"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1533, "code": "    def _pre_draw(self, framedata, blit):\n        if blit:\n            self._blit_clear(self._drawn_artists)\n        else:\n            for artist in self._drawn_artists:\n                artist.set_visible(False)", "documentation": "        \"\"\"Clears artists from the last frame.\"\"\""}, {"filename": "lib/matplotlib/animation.py", "start_line": 1553, "code": "class FuncAnimation(TimedAnimation):", "documentation": "    \"\"\"\n    `TimedAnimation` subclass that makes an animation by repeatedly calling\n    a function *func*.\n\n    .. note::\n\n        You must store the created Animation in a variable that lives as long\n        as the animation should run. Otherwise, the Animation object will be\n        garbage-collected and the animation stops.\n\n    Parameters\n    ----------\n    fig : `~matplotlib.figure.Figure`\n        The figure object used to get needed events, such as draw or resize.\n\n    func : callable\n        The function to call at each frame.  The first argument will\n        be the next value in *frames*.   Any additional positional\n        arguments can be supplied using `functools.partial` or via the *fargs*\n        parameter.\n\n        The required signature is::\n\n            def func(frame, *fargs) -> iterable_of_artists\n\n        It is often more convenient to provide the arguments using\n        `functools.partial`. In this way it is also possible to pass keyword\n        arguments. To pass a function with both positional and keyword\n        arguments, set all arguments as keyword arguments, just leaving the\n        *frame* argument unset::\n\n            def func(frame, art, *, y=None):\n                ...\n\n            ani = FuncAnimation(fig, partial(func, art=ln, y='foo'))\n\n        If ``blit == True``, *func* must return an iterable of all artists\n        that were modified or created. This information is used by the blitting\n        algorithm to determine which parts of the figure have to be updated.\n        The return value is unused if ``blit == False`` and may be omitted in\n        that case.\n\n    frames : iterable, int, generator function, or None, optional\n        Source of data to pass *func* and each frame of the animation\n\n        - If an iterable, then simply use the values provided.  If the\n          iterable has a length, it will override the *save_count* kwarg.\n\n        - If an integer, then equivalent to passing ``range(frames)``\n\n        - If a generator function, then must have the signature::\n\n             def gen_function() -> obj\n\n        - If *None*, then equivalent to passing ``itertools.count``.\n\n        In all of these cases, the values in *frames* is simply passed through\n        to the user-supplied *func* and thus can be of any type.\n\n    init_func : callable, optional\n        A function used to draw a clear frame. If not given, the results of\n        drawing from the first item in the frames sequence will be used. This\n        function will be called once before the first frame.\n\n        The required signature is::\n\n            def init_func() -> iterable_of_artists\n\n        If ``blit == True``, *init_func* must return an iterable of artists\n        to be re-drawn. This information is used by the blitting algorithm to\n        determine which parts of the figure have to be updated.  The return\n        value is unused if ``blit == False`` and may be omitted in that case.\n\n    fargs : tuple or None, optional\n        Additional arguments to pass to each call to *func*. Note: the use of\n        `functools.partial` is preferred over *fargs*. See *func* for details.\n\n    save_count : int, optional\n        Fallback for the number of values from *frames* to cache. This is\n        only used if the number of frames cannot be inferred from *frames*,\n        i.e. when it's an iterator without length or a generator.\n\n    interval : int, default: 200\n        Delay between frames in milliseconds.\n\n    repeat_delay : int, default: 0\n        The delay in milliseconds between consecutive animation runs, if\n        *repeat* is True.\n\n    repeat : bool, default: True\n        Whether the animation repeats when the sequence of frames is completed.\n\n    blit : bool, default: False\n        Whether blitting is used to optimize drawing.  Note: when using\n        blitting, any animated artists will be drawn according to their zorder;\n        however, they will be drawn on top of any previous artists, regardless\n        of their zorder.\n\n    cache_frame_data : bool, default: True\n        Whether frame data is cached.  Disabling cache might be helpful when\n        frames contain large objects.\n    \"\"\""}]}
{"repository": "matplotlib/matplotlib", "commit_sha": "7a7a38882b57e8a358944c494b0ed368f3aa6ad8", "commit_message": "Add legend support for PatchCollection (#30756)\n\n* Add support for PatchCollection legends and update documentation\n* use HandlerPolyCollection directly since PatchCollection and PolyCollection have identical APIs, we can\n  register PatchCollection to use the existing handler without creating a new class.", "commit_date": "2025-11-26T18:25:45+00:00", "author": "Fazeel Usmani", "file": "lib/matplotlib/legend.py", "patch": "@@ -37,7 +37,7 @@\n from matplotlib.patches import (Patch, Rectangle, Shadow, FancyBboxPatch,\n                                 StepPatch)\n from matplotlib.collections import (\n-    Collection, CircleCollection, LineCollection, PathCollection,\n+    Collection, CircleCollection, LineCollection, PatchCollection, PathCollection,\n     PolyCollection, RegularPolyCollection)\n from matplotlib.text import Text\n from matplotlib.transforms import Bbox, BboxBase, TransformedBbox\n@@ -787,6 +787,7 @@ def draw(self, renderer):\n         BarContainer: legend_handler.HandlerPatch(\n             update_func=legend_handler.update_from_first_child),\n         tuple: legend_handler.HandlerTuple(),\n+        PatchCollection: legend_handler.HandlerPolyCollection(),\n         PathCollection: legend_handler.HandlerPathCollection(),\n         PolyCollection: legend_handler.HandlerPolyCollection()\n         }", "before_segments": [{"filename": "lib/matplotlib/legend.py", "start_line": 54, "code": "    def __init__(self, legend, use_blit=False, update=\"loc\"):\n        self.legend = legend\n        _api.check_in_list([\"loc\", \"bbox\"], update=update)\n        self._update = update\n        super().__init__(legend, legend._legend_box, use_blit=use_blit)", "documentation": "        \"\"\"\n        Wrapper around a `.Legend` to support mouse dragging.\n\n        Parameters\n        ----------\n        legend : `.Legend`\n            The `.Legend` instance to wrap.\n        use_blit : bool, optional\n            Use blitting for faster image composition. For details see\n            :ref:`func-animation`.\n        update : {'loc', 'bbox'}, optional\n            If \"loc\", update the *loc* parameter of the legend upon finalizing.\n            If \"bbox\", update the *bbox_to_anchor* parameter.\n        \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 341, "code": "class Legend(Artist):\n    codes = {'best': 0, **AnchoredOffsetbox.codes}\n    zorder = 5", "documentation": "    \"\"\"\n    Place a legend on the figure/axes.\n    \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 628, "code": "    def _set_artist_props(self, a):\n        a.set_figure(self.get_figure(root=False))\n        if self.isaxes:\n            a.axes = self.axes\n        a.set_transform(self.get_transform())\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Set the boilerplate props for artists added to Axes.\n        \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 639, "code": "    def set_loc(self, loc=None):\n        loc0 = loc\n        self._loc_used_default = loc is None\n        if loc is None:\n            loc = mpl.rcParams[\"legend.loc\"]\n            if not self.isaxes and loc in [0, 'best']:\n                loc = 'upper right'\n        type_err_message = (\"loc must be string, coordinate tuple, or\"\n                            f\" an integer 0-10, not {loc!r}\")\n        self._outside_loc = None\n        if isinstance(loc, str):", "documentation": "        \"\"\"\n        Set the location of the legend.\n\n        .. versionadded:: 3.8\n\n        Parameters\n        ----------\n        %(_legend_kw_set_loc_doc)s\n        \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 714, "code": "    def set_ncols(self, ncols):\n        self._ncols = ncols", "documentation": "        \"\"\"Set the number of columns.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 723, "code": "    def _findoffset(self, width, height, xdescent, ydescent, renderer):\n        if self._loc == 0:  # \"best\".\n            x, y = self._find_best_position(width, height, renderer)\n        elif self._loc in Legend.codes.values():  # Fixed location.\n            bbox = Bbox.from_bounds(0, 0, width, height)\n            x, y = self._get_anchored_bbox(self._loc, bbox,\n                                           self.get_bbox_to_anchor(),\n                                           renderer)\n        else:  # Axes or figure coordinates.\n            fx, fy = self._loc\n            bbox = self.get_bbox_to_anchor()", "documentation": "        \"\"\"Helper function to locate the legend.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 797, "code": "    def get_default_handler_map(cls):\n        return cls._default_handler_map\n    @classmethod", "documentation": "        \"\"\"Return the global default handler map, shared by all legends.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 802, "code": "    def set_default_handler_map(cls, handler_map):\n        cls._default_handler_map = handler_map\n    @classmethod", "documentation": "        \"\"\"Set the global default handler map, shared by all legends.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 807, "code": "    def update_default_handler_map(cls, handler_map):\n        cls._default_handler_map.update(handler_map)", "documentation": "        \"\"\"Update the global default handler map, shared by all legends.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 811, "code": "    def get_legend_handler_map(self):\n        default_handler_map = self.get_default_handler_map()\n        return ({**default_handler_map, **self._custom_handler_map}\n                if self._custom_handler_map else default_handler_map)\n    @staticmethod", "documentation": "        \"\"\"Return this legend instance's handler map.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 818, "code": "    def get_legend_handler(legend_handler_map, orig_handle):\n        try:\n            return legend_handler_map[orig_handle]\n        except (TypeError, KeyError):  # TypeError if unhashable.\n            pass\n        for handle_type in type(orig_handle).mro():\n            try:\n                return legend_handler_map[handle_type]\n            except KeyError:\n                pass\n        return None", "documentation": "        \"\"\"\n        Return a legend handler from *legend_handler_map* that\n        corresponds to *orig_handler*.\n\n        *legend_handler_map* should be a dictionary object (that is\n        returned by the get_legend_handler_map method).\n\n        It first checks if the *orig_handle* itself is a key in the\n        *legend_handler_map* and return the associated value.\n        Otherwise, it checks for each of the classes in its\n        method-resolution-order. If no matching key is found, it\n        returns ``None``.\n        \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 843, "code": "    def _init_legend_box(self, handles, labels, markerfirst=True):\n        fontsize = self._fontsize\n        text_list = []  # the list of text instances\n        handle_list = []  # the list of handle instances\n        handles_and_labels = []\n        descent = 0.35 * fontsize * (self.handleheight - 0.7)  # heuristic.\n        height = fontsize * self.handleheight - descent\n        legend_handler_map = self.get_legend_handler_map()\n        for orig_handle, label in zip(handles, labels):\n            handler = self.get_legend_handler(legend_handler_map, orig_handle)\n            if handler is None:", "documentation": "        \"\"\"\n        Initialize the legend_box. The legend_box is an instance of\n        the OffsetBox, which is packed with legend handles and\n        texts. Once packed, their location is calculated during the\n        drawing time.\n        \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 943, "code": "    def _auto_legend_data(self, renderer):\n        assert self.isaxes  # always holds, as this is only called internally\n        bboxes = []\n        lines = []\n        offsets = []\n        for artist in self.parent._children:\n            if isinstance(artist, Line2D):\n                lines.append(\n                    artist.get_transform().transform_path(artist.get_path()))\n            elif isinstance(artist, Rectangle):\n                bboxes.append(", "documentation": "        \"\"\"\n        Return display coordinates for hit testing for \"best\" positioning.\n\n        Returns\n        -------\n        bboxes\n            List of bounding boxes of all patches.\n        lines\n            List of `.Path` corresponding to each line.\n        offsets\n            List of (x, y) offsets of all collection.\n        \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 986, "code": "    def get_frame(self):\n        return self.legendPatch", "documentation": "        \"\"\"Return the `~.patches.Rectangle` used to frame the legend.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1004, "code": "    def set_alignment(self, alignment):\n        _api.check_in_list([\"center\", \"left\", \"right\"], alignment=alignment)\n        self._alignment = alignment\n        self._legend_box.align = alignment", "documentation": "        \"\"\"\n        Set the alignment of the legend title and the box of entries.\n\n        The entries are aligned as a single block, so that markers always\n        lined up.\n\n        Parameters\n        ----------\n        alignment : {'center', 'left', 'right'}.\n\n        \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1020, "code": "    def get_alignment(self):\n        return self._legend_box.align", "documentation": "        \"\"\"Get the alignment value of the legend box\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1024, "code": "    def set_title(self, title, prop=None):\n        self._legend_title_box._text.set_text(title)\n        if title:\n            self._legend_title_box._text.set_visible(True)\n            self._legend_title_box.set_visible(True)\n        else:\n            self._legend_title_box._text.set_visible(False)\n            self._legend_title_box.set_visible(False)\n        if prop is not None:\n            self._legend_title_box._text.set_fontproperties(prop)\n        self.stale = True", "documentation": "        \"\"\"\n        Set legend title and title style.\n\n        Parameters\n        ----------\n        title : str\n            The legend title.\n\n        prop : `.font_manager.FontProperties` or `str` or `pathlib.Path`\n            The font properties of the legend title.\n            If a `str`, it is interpreted as a fontconfig pattern parsed by\n            `.FontProperties`.  If a `pathlib.Path`, it is interpreted as the\n            absolute path to a font file.\n\n        \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1053, "code": "    def get_title(self):\n        return self._legend_title_box._text", "documentation": "        \"\"\"Return the `.Text` instance for the legend title.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1067, "code": "    def get_frame_on(self):\n        return self.legendPatch.get_visible()", "documentation": "        \"\"\"Get whether the legend box patch is drawn.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1071, "code": "    def set_frame_on(self, b):\n        self.legendPatch.set_visible(b)\n        self.stale = True\n    draw_frame = set_frame_on  # Backcompat alias.", "documentation": "        \"\"\"\n        Set whether the legend box patch is drawn.\n\n        Parameters\n        ----------\n        b : bool\n        \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1084, "code": "    def get_bbox_to_anchor(self):\n        if self._bbox_to_anchor is None:\n            return self.parent.bbox\n        else:\n            return self._bbox_to_anchor", "documentation": "        \"\"\"Return the bbox that the legend will be anchored to.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1091, "code": "    def set_bbox_to_anchor(self, bbox, transform=None):\n        if bbox is None:\n            self._bbox_to_anchor = None\n            return\n        elif isinstance(bbox, BboxBase):\n            self._bbox_to_anchor = bbox\n        else:\n            try:\n                l = len(bbox)\n            except TypeError as err:\n                raise ValueError(f\"Invalid bbox: {bbox}\") from err", "documentation": "        \"\"\"\n        Set the bbox that the legend will be anchored to.\n\n        Parameters\n        ----------\n        bbox : `~matplotlib.transforms.BboxBase` or tuple\n            The bounding box can be specified in the following ways:\n\n            - A `.BboxBase` instance\n            - A tuple of ``(left, bottom, width, height)`` in the given\n              transform (normalized axes coordinate if None)\n            - A tuple of ``(left, bottom)`` where the width and height will be\n              assumed to be zero.\n            - *None*, to remove the bbox anchoring, and use the parent bbox.\n\n        transform : `~matplotlib.transforms.Transform`, optional\n            A transform to apply to the bounding box. If not specified, this\n            will use a transform to the bounding box of the parent.\n        \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1134, "code": "    def _get_anchored_bbox(self, loc, bbox, parentbbox, renderer):\n        pad = self.borderaxespad * renderer.points_to_pixels(self._fontsize)\n        return offsetbox._get_anchored_bbox(\n            loc, bbox, parentbbox,\n            pad, pad)", "documentation": "        \"\"\"\n        Place the *bbox* inside the *parentbbox* according to a given\n        location code. Return the (x, y) coordinate of the bbox.\n\n        Parameters\n        ----------\n        loc : int\n            A location code in range(1, 11). This corresponds to the possible\n            values for ``self._loc``, excluding \"best\".\n        bbox : `~matplotlib.transforms.Bbox`\n            bbox to be placed, in display coordinates.\n        parentbbox : `~matplotlib.transforms.Bbox`\n            A parent box which will contain the bbox, in display coordinates.\n        \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1154, "code": "    def _find_best_position(self, width, height, renderer):\n        assert self.isaxes  # always holds, as this is only called internally\n        start_time = time.perf_counter()\n        bboxes, lines, offsets = self._auto_legend_data(renderer)\n        bbox = Bbox.from_bounds(0, 0, width, height)\n        candidates = []\n        for idx in range(1, len(self.codes)):\n            l, b = self._get_anchored_bbox(idx, bbox,\n                                           self.get_bbox_to_anchor(),\n                                           renderer)\n            legendBox = Bbox.from_bounds(l, b, width, height)", "documentation": "        \"\"\"Determine the best location to place the legend.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1195, "code": "    def set_draggable(self, state, use_blit=False, update='loc'):\n        if state:\n            if self._draggable is None:\n                self._draggable = DraggableLegend(self,\n                                                  use_blit,\n                                                  update=update)\n        else:\n            if self._draggable is not None:\n                self._draggable.disconnect()\n            self._draggable = None\n        return self._draggable", "documentation": "        \"\"\"\n        Enable or disable mouse dragging support of the legend.\n\n        Parameters\n        ----------\n        state : bool\n            Whether mouse dragging is enabled.\n        use_blit : bool, optional\n            Use blitting for faster image composition. For details see\n            :ref:`func-animation`.\n        update : {'loc', 'bbox'}, optional\n            The legend parameter to be changed when dragged:\n\n            - 'loc': update the *loc* parameter of the legend\n            - 'bbox': update the *bbox_to_anchor* parameter of the legend\n\n        Returns\n        -------\n        `.DraggableLegend` or *None*\n            If *state* is ``True`` this returns the `.DraggableLegend` helper\n            instance. Otherwise this returns *None*.\n        \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1229, "code": "    def get_draggable(self):\n        return self._draggable is not None", "documentation": "        \"\"\"Return ``True`` if the legend is draggable, ``False`` otherwise.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1236, "code": "def _get_legend_handles(axs, legend_handler_map=None):\n    handles_original = []\n    for ax in axs:\n        handles_original += [\n            *(a for a in ax._children\n              if isinstance(a, (Line2D, Patch, Collection, Text))),\n            *ax.containers]\n        if hasattr(ax, 'parasites'):\n            for axx in ax.parasites:\n                handles_original += [\n                    *(a for a in axx._children", "documentation": "    \"\"\"Yield artists that can be used as handles in a legend.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1270, "code": "def _get_legend_handles_labels(axs, legend_handler_map=None):\n    handles = []\n    labels = []\n    for handle in _get_legend_handles(axs, legend_handler_map):\n        label = handle.get_label()\n        if label and not label.startswith('_'):\n            handles.append(handle)\n            labels.append(label)\n    return handles, labels", "documentation": "    \"\"\"Return handles and labels for legend.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1282, "code": "def _parse_legend_args(axs, *args, handles=None, labels=None, **kwargs):\n    log = logging.getLogger(__name__)\n    handlers = kwargs.get('handler_map')\n    if (handles is not None or labels is not None) and args:\n        raise TypeError(\"When passing handles and labels, they must both be \"\n                        \"passed positionally or both as keywords.\")\n    if (hasattr(handles, \"__len__\") and\n            hasattr(labels, \"__len__\") and\n            len(handles) != len(labels)):\n        _api.warn_external(f\"Mismatched number of handles and labels: \"\n                           f\"len(handles) = {len(handles)} \"", "documentation": "    \"\"\"\n    Get the handles and labels from the calls to either ``figure.legend``\n    or ``axes.legend``.\n\n    The parser is a bit involved because we support::\n\n        legend()\n        legend(labels)\n        legend(handles, labels)\n        legend(labels=labels)\n        legend(handles=handles)\n        legend(handles=handles, labels=labels)\n\n    The behavior for a mixture of positional and keyword handles and labels\n    is undefined and raises an error.\n\n    Parameters\n    ----------\n    axs : list of `.Axes`\n        If handles are not given explicitly, the artists in these Axes are\n        used as handles.\n    *args : tuple\n        Positional parameters passed to ``legend()``.\n    handles\n        The value of the keyword argument ``legend(handles=...)``, or *None*\n        if that keyword argument was not used.\n    labels\n        The value of the keyword argument ``legend(labels=...)``, or *None*\n        if that keyword argument was not used.\n    **kwargs\n        All other keyword arguments passed to ``legend()``.\n\n    Returns\n    -------\n    handles : list of (`.Artist` or tuple of `.Artist`)\n        The legend handles.\n    labels : list of str\n        The legend labels.\n    kwargs : dict\n        *kwargs* with keywords handles and labels removed.\n\n    \"\"\""}], "after_segments": [{"filename": "lib/matplotlib/legend.py", "start_line": 54, "code": "    def __init__(self, legend, use_blit=False, update=\"loc\"):\n        self.legend = legend\n        _api.check_in_list([\"loc\", \"bbox\"], update=update)\n        self._update = update\n        super().__init__(legend, legend._legend_box, use_blit=use_blit)", "documentation": "        \"\"\"\n        Wrapper around a `.Legend` to support mouse dragging.\n\n        Parameters\n        ----------\n        legend : `.Legend`\n            The `.Legend` instance to wrap.\n        use_blit : bool, optional\n            Use blitting for faster image composition. For details see\n            :ref:`func-animation`.\n        update : {'loc', 'bbox'}, optional\n            If \"loc\", update the *loc* parameter of the legend upon finalizing.\n            If \"bbox\", update the *bbox_to_anchor* parameter.\n        \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 341, "code": "class Legend(Artist):\n    codes = {'best': 0, **AnchoredOffsetbox.codes}\n    zorder = 5", "documentation": "    \"\"\"\n    Place a legend on the figure/axes.\n    \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 628, "code": "    def _set_artist_props(self, a):\n        a.set_figure(self.get_figure(root=False))\n        if self.isaxes:\n            a.axes = self.axes\n        a.set_transform(self.get_transform())\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Set the boilerplate props for artists added to Axes.\n        \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 639, "code": "    def set_loc(self, loc=None):\n        loc0 = loc\n        self._loc_used_default = loc is None\n        if loc is None:\n            loc = mpl.rcParams[\"legend.loc\"]\n            if not self.isaxes and loc in [0, 'best']:\n                loc = 'upper right'\n        type_err_message = (\"loc must be string, coordinate tuple, or\"\n                            f\" an integer 0-10, not {loc!r}\")\n        self._outside_loc = None\n        if isinstance(loc, str):", "documentation": "        \"\"\"\n        Set the location of the legend.\n\n        .. versionadded:: 3.8\n\n        Parameters\n        ----------\n        %(_legend_kw_set_loc_doc)s\n        \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 714, "code": "    def set_ncols(self, ncols):\n        self._ncols = ncols", "documentation": "        \"\"\"Set the number of columns.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 723, "code": "    def _findoffset(self, width, height, xdescent, ydescent, renderer):\n        if self._loc == 0:  # \"best\".\n            x, y = self._find_best_position(width, height, renderer)\n        elif self._loc in Legend.codes.values():  # Fixed location.\n            bbox = Bbox.from_bounds(0, 0, width, height)\n            x, y = self._get_anchored_bbox(self._loc, bbox,\n                                           self.get_bbox_to_anchor(),\n                                           renderer)\n        else:  # Axes or figure coordinates.\n            fx, fy = self._loc\n            bbox = self.get_bbox_to_anchor()", "documentation": "        \"\"\"Helper function to locate the legend.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 798, "code": "    def get_default_handler_map(cls):\n        return cls._default_handler_map\n    @classmethod", "documentation": "        \"\"\"Return the global default handler map, shared by all legends.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 803, "code": "    def set_default_handler_map(cls, handler_map):\n        cls._default_handler_map = handler_map\n    @classmethod", "documentation": "        \"\"\"Set the global default handler map, shared by all legends.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 808, "code": "    def update_default_handler_map(cls, handler_map):\n        cls._default_handler_map.update(handler_map)", "documentation": "        \"\"\"Update the global default handler map, shared by all legends.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 812, "code": "    def get_legend_handler_map(self):\n        default_handler_map = self.get_default_handler_map()\n        return ({**default_handler_map, **self._custom_handler_map}\n                if self._custom_handler_map else default_handler_map)\n    @staticmethod", "documentation": "        \"\"\"Return this legend instance's handler map.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 819, "code": "    def get_legend_handler(legend_handler_map, orig_handle):\n        try:\n            return legend_handler_map[orig_handle]\n        except (TypeError, KeyError):  # TypeError if unhashable.\n            pass\n        for handle_type in type(orig_handle).mro():\n            try:\n                return legend_handler_map[handle_type]\n            except KeyError:\n                pass\n        return None", "documentation": "        \"\"\"\n        Return a legend handler from *legend_handler_map* that\n        corresponds to *orig_handler*.\n\n        *legend_handler_map* should be a dictionary object (that is\n        returned by the get_legend_handler_map method).\n\n        It first checks if the *orig_handle* itself is a key in the\n        *legend_handler_map* and return the associated value.\n        Otherwise, it checks for each of the classes in its\n        method-resolution-order. If no matching key is found, it\n        returns ``None``.\n        \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 844, "code": "    def _init_legend_box(self, handles, labels, markerfirst=True):\n        fontsize = self._fontsize\n        text_list = []  # the list of text instances\n        handle_list = []  # the list of handle instances\n        handles_and_labels = []\n        descent = 0.35 * fontsize * (self.handleheight - 0.7)  # heuristic.\n        height = fontsize * self.handleheight - descent\n        legend_handler_map = self.get_legend_handler_map()\n        for orig_handle, label in zip(handles, labels):\n            handler = self.get_legend_handler(legend_handler_map, orig_handle)\n            if handler is None:", "documentation": "        \"\"\"\n        Initialize the legend_box. The legend_box is an instance of\n        the OffsetBox, which is packed with legend handles and\n        texts. Once packed, their location is calculated during the\n        drawing time.\n        \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 944, "code": "    def _auto_legend_data(self, renderer):\n        assert self.isaxes  # always holds, as this is only called internally\n        bboxes = []\n        lines = []\n        offsets = []\n        for artist in self.parent._children:\n            if isinstance(artist, Line2D):\n                lines.append(\n                    artist.get_transform().transform_path(artist.get_path()))\n            elif isinstance(artist, Rectangle):\n                bboxes.append(", "documentation": "        \"\"\"\n        Return display coordinates for hit testing for \"best\" positioning.\n\n        Returns\n        -------\n        bboxes\n            List of bounding boxes of all patches.\n        lines\n            List of `.Path` corresponding to each line.\n        offsets\n            List of (x, y) offsets of all collection.\n        \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 987, "code": "    def get_frame(self):\n        return self.legendPatch", "documentation": "        \"\"\"Return the `~.patches.Rectangle` used to frame the legend.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1005, "code": "    def set_alignment(self, alignment):\n        _api.check_in_list([\"center\", \"left\", \"right\"], alignment=alignment)\n        self._alignment = alignment\n        self._legend_box.align = alignment", "documentation": "        \"\"\"\n        Set the alignment of the legend title and the box of entries.\n\n        The entries are aligned as a single block, so that markers always\n        lined up.\n\n        Parameters\n        ----------\n        alignment : {'center', 'left', 'right'}.\n\n        \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1021, "code": "    def get_alignment(self):\n        return self._legend_box.align", "documentation": "        \"\"\"Get the alignment value of the legend box\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1025, "code": "    def set_title(self, title, prop=None):\n        self._legend_title_box._text.set_text(title)\n        if title:\n            self._legend_title_box._text.set_visible(True)\n            self._legend_title_box.set_visible(True)\n        else:\n            self._legend_title_box._text.set_visible(False)\n            self._legend_title_box.set_visible(False)\n        if prop is not None:\n            self._legend_title_box._text.set_fontproperties(prop)\n        self.stale = True", "documentation": "        \"\"\"\n        Set legend title and title style.\n\n        Parameters\n        ----------\n        title : str\n            The legend title.\n\n        prop : `.font_manager.FontProperties` or `str` or `pathlib.Path`\n            The font properties of the legend title.\n            If a `str`, it is interpreted as a fontconfig pattern parsed by\n            `.FontProperties`.  If a `pathlib.Path`, it is interpreted as the\n            absolute path to a font file.\n\n        \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1054, "code": "    def get_title(self):\n        return self._legend_title_box._text", "documentation": "        \"\"\"Return the `.Text` instance for the legend title.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1068, "code": "    def get_frame_on(self):\n        return self.legendPatch.get_visible()", "documentation": "        \"\"\"Get whether the legend box patch is drawn.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1072, "code": "    def set_frame_on(self, b):\n        self.legendPatch.set_visible(b)\n        self.stale = True\n    draw_frame = set_frame_on  # Backcompat alias.", "documentation": "        \"\"\"\n        Set whether the legend box patch is drawn.\n\n        Parameters\n        ----------\n        b : bool\n        \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1085, "code": "    def get_bbox_to_anchor(self):\n        if self._bbox_to_anchor is None:\n            return self.parent.bbox\n        else:\n            return self._bbox_to_anchor", "documentation": "        \"\"\"Return the bbox that the legend will be anchored to.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1092, "code": "    def set_bbox_to_anchor(self, bbox, transform=None):\n        if bbox is None:\n            self._bbox_to_anchor = None\n            return\n        elif isinstance(bbox, BboxBase):\n            self._bbox_to_anchor = bbox\n        else:\n            try:\n                l = len(bbox)\n            except TypeError as err:\n                raise ValueError(f\"Invalid bbox: {bbox}\") from err", "documentation": "        \"\"\"\n        Set the bbox that the legend will be anchored to.\n\n        Parameters\n        ----------\n        bbox : `~matplotlib.transforms.BboxBase` or tuple\n            The bounding box can be specified in the following ways:\n\n            - A `.BboxBase` instance\n            - A tuple of ``(left, bottom, width, height)`` in the given\n              transform (normalized axes coordinate if None)\n            - A tuple of ``(left, bottom)`` where the width and height will be\n              assumed to be zero.\n            - *None*, to remove the bbox anchoring, and use the parent bbox.\n\n        transform : `~matplotlib.transforms.Transform`, optional\n            A transform to apply to the bounding box. If not specified, this\n            will use a transform to the bounding box of the parent.\n        \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1135, "code": "    def _get_anchored_bbox(self, loc, bbox, parentbbox, renderer):\n        pad = self.borderaxespad * renderer.points_to_pixels(self._fontsize)\n        return offsetbox._get_anchored_bbox(\n            loc, bbox, parentbbox,\n            pad, pad)", "documentation": "        \"\"\"\n        Place the *bbox* inside the *parentbbox* according to a given\n        location code. Return the (x, y) coordinate of the bbox.\n\n        Parameters\n        ----------\n        loc : int\n            A location code in range(1, 11). This corresponds to the possible\n            values for ``self._loc``, excluding \"best\".\n        bbox : `~matplotlib.transforms.Bbox`\n            bbox to be placed, in display coordinates.\n        parentbbox : `~matplotlib.transforms.Bbox`\n            A parent box which will contain the bbox, in display coordinates.\n        \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1155, "code": "    def _find_best_position(self, width, height, renderer):\n        assert self.isaxes  # always holds, as this is only called internally\n        start_time = time.perf_counter()\n        bboxes, lines, offsets = self._auto_legend_data(renderer)\n        bbox = Bbox.from_bounds(0, 0, width, height)\n        candidates = []\n        for idx in range(1, len(self.codes)):\n            l, b = self._get_anchored_bbox(idx, bbox,\n                                           self.get_bbox_to_anchor(),\n                                           renderer)\n            legendBox = Bbox.from_bounds(l, b, width, height)", "documentation": "        \"\"\"Determine the best location to place the legend.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1196, "code": "    def set_draggable(self, state, use_blit=False, update='loc'):\n        if state:\n            if self._draggable is None:\n                self._draggable = DraggableLegend(self,\n                                                  use_blit,\n                                                  update=update)\n        else:\n            if self._draggable is not None:\n                self._draggable.disconnect()\n            self._draggable = None\n        return self._draggable", "documentation": "        \"\"\"\n        Enable or disable mouse dragging support of the legend.\n\n        Parameters\n        ----------\n        state : bool\n            Whether mouse dragging is enabled.\n        use_blit : bool, optional\n            Use blitting for faster image composition. For details see\n            :ref:`func-animation`.\n        update : {'loc', 'bbox'}, optional\n            The legend parameter to be changed when dragged:\n\n            - 'loc': update the *loc* parameter of the legend\n            - 'bbox': update the *bbox_to_anchor* parameter of the legend\n\n        Returns\n        -------\n        `.DraggableLegend` or *None*\n            If *state* is ``True`` this returns the `.DraggableLegend` helper\n            instance. Otherwise this returns *None*.\n        \"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1230, "code": "    def get_draggable(self):\n        return self._draggable is not None", "documentation": "        \"\"\"Return ``True`` if the legend is draggable, ``False`` otherwise.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1237, "code": "def _get_legend_handles(axs, legend_handler_map=None):\n    handles_original = []\n    for ax in axs:\n        handles_original += [\n            *(a for a in ax._children\n              if isinstance(a, (Line2D, Patch, Collection, Text))),\n            *ax.containers]\n        if hasattr(ax, 'parasites'):\n            for axx in ax.parasites:\n                handles_original += [\n                    *(a for a in axx._children", "documentation": "    \"\"\"Yield artists that can be used as handles in a legend.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1271, "code": "def _get_legend_handles_labels(axs, legend_handler_map=None):\n    handles = []\n    labels = []\n    for handle in _get_legend_handles(axs, legend_handler_map):\n        label = handle.get_label()\n        if label and not label.startswith('_'):\n            handles.append(handle)\n            labels.append(label)\n    return handles, labels", "documentation": "    \"\"\"Return handles and labels for legend.\"\"\""}, {"filename": "lib/matplotlib/legend.py", "start_line": 1283, "code": "def _parse_legend_args(axs, *args, handles=None, labels=None, **kwargs):\n    log = logging.getLogger(__name__)\n    handlers = kwargs.get('handler_map')\n    if (handles is not None or labels is not None) and args:\n        raise TypeError(\"When passing handles and labels, they must both be \"\n                        \"passed positionally or both as keywords.\")\n    if (hasattr(handles, \"__len__\") and\n            hasattr(labels, \"__len__\") and\n            len(handles) != len(labels)):\n        _api.warn_external(f\"Mismatched number of handles and labels: \"\n                           f\"len(handles) = {len(handles)} \"", "documentation": "    \"\"\"\n    Get the handles and labels from the calls to either ``figure.legend``\n    or ``axes.legend``.\n\n    The parser is a bit involved because we support::\n\n        legend()\n        legend(labels)\n        legend(handles, labels)\n        legend(labels=labels)\n        legend(handles=handles)\n        legend(handles=handles, labels=labels)\n\n    The behavior for a mixture of positional and keyword handles and labels\n    is undefined and raises an error.\n\n    Parameters\n    ----------\n    axs : list of `.Axes`\n        If handles are not given explicitly, the artists in these Axes are\n        used as handles.\n    *args : tuple\n        Positional parameters passed to ``legend()``.\n    handles\n        The value of the keyword argument ``legend(handles=...)``, or *None*\n        if that keyword argument was not used.\n    labels\n        The value of the keyword argument ``legend(labels=...)``, or *None*\n        if that keyword argument was not used.\n    **kwargs\n        All other keyword arguments passed to ``legend()``.\n\n    Returns\n    -------\n    handles : list of (`.Artist` or tuple of `.Artist`)\n        The legend handles.\n    labels : list of str\n        The legend labels.\n    kwargs : dict\n        *kwargs* with keywords handles and labels removed.\n\n    \"\"\""}]}
{"repository": "matplotlib/matplotlib", "commit_sha": "7a7a38882b57e8a358944c494b0ed368f3aa6ad8", "commit_message": "Add legend support for PatchCollection (#30756)\n\n* Add support for PatchCollection legends and update documentation\n* use HandlerPolyCollection directly since PatchCollection and PolyCollection have identical APIs, we can\n  register PatchCollection to use the existing handler without creating a new class.", "commit_date": "2025-11-26T18:25:45+00:00", "author": "Fazeel Usmani", "file": "lib/matplotlib/tests/test_legend.py", "patch": "@@ -1667,3 +1667,55 @@ def test_boxplot_legend_labels():\n     bp4 = axs[3].boxplot(data, label='box A')\n     assert bp4['medians'][0].get_label() == 'box A'\n     assert all(x.get_label().startswith(\"_\") for x in bp4['medians'][1:])\n+\n+\n+def test_patchcollection_legend():\n+    # Test that PatchCollection labels show up in legend and preserve visual\n+    # properties (issue #23998)\n+    fig, ax = plt.subplots()\n+\n+    pc = mcollections.PatchCollection(\n+        [mpatches.Circle((0, 0), 1), mpatches.Circle((2, 0), 1)],\n+        label=\"patch collection\",\n+        facecolor='red',\n+        edgecolor='blue',\n+        linewidths=3,\n+        linestyle='--',\n+    )\n+    ax.add_collection(pc)\n+    ax.autoscale_view()\n+\n+    leg = ax.legend()\n+\n+    # Check that the legend contains our label\n+    assert len(leg.get_texts()) == 1\n+    assert leg.get_texts()[0].get_text() == \"patch collection\"\n+\n+    # Check that the legend handle exists and has correct visual properties\n+    assert len(leg.legend_handles) == 1\n+    legend_patch = leg.legend_handles[0]\n+    assert mpl.colors.same_color(legend_patch.get_facecolor(),\n+                                  pc.get_facecolor()[0])\n+    assert mpl.colors.same_color(legend_patch.get_edgecolor(),\n+                                  pc.get_edgecolor()[0])\n+    assert legend_patch.get_linewidth() == pc.get_linewidths()[0]\n+    assert legend_patch.get_linestyle() == pc.get_linestyles()[0]\n+\n+\n+def test_patchcollection_legend_empty():\n+    # Test that empty PatchCollection doesn't crash\n+    fig, ax = plt.subplots()\n+\n+    # Create an empty PatchCollection\n+    pc = mcollections.PatchCollection([], label=\"empty collection\")\n+    ax.add_collection(pc)\n+\n+    # This should not crash\n+    leg = ax.legend()\n+\n+    # Check that the label still appears\n+    assert len(leg.get_texts()) == 1\n+    assert leg.get_texts()[0].get_text() == \"empty collection\"\n+\n+    # The legend handle should exist\n+    assert len(leg.legend_handles) == 1", "before_segments": [{"filename": "lib/matplotlib/tests/test_legend.py", "start_line": 57, "code": "def test_legend_auto1():\n    fig, ax = plt.subplots()\n    x = np.arange(100)\n    ax.plot(x, 50 - x, 'o', label='y=1')\n    ax.plot(x, x - 50, 'o', label='y=-1')\n    ax.legend(loc='best')\n@image_comparison(['legend_auto2.png'], remove_text=True)", "documentation": "    \"\"\"Test automatic legend placement\"\"\""}, {"filename": "lib/matplotlib/tests/test_legend.py", "start_line": 67, "code": "def test_legend_auto2():\n    fig, ax = plt.subplots()\n    x = np.arange(100)\n    b1 = ax.bar(x, x, align='edge', color='m')\n    b2 = ax.bar(x, x[::-1], align='edge', color='g')\n    ax.legend([b1[0], b2[0]], ['up', 'down'], loc='best')\n@image_comparison(['legend_auto3.png'])", "documentation": "    \"\"\"Test automatic legend placement\"\"\""}, {"filename": "lib/matplotlib/tests/test_legend.py", "start_line": 77, "code": "def test_legend_auto3():\n    fig, ax = plt.subplots()\n    x = [0.9, 0.1, 0.1, 0.9, 0.9, 0.5]\n    y = [0.95, 0.95, 0.05, 0.05, 0.5, 0.5]\n    ax.plot(x, y, 'o-', label='line')\n    ax.set_xlim(0.0, 1.0)\n    ax.set_ylim(0.0, 1.0)\n    ax.legend(loc='best')", "documentation": "    \"\"\"Test automatic legend placement\"\"\""}, {"filename": "lib/matplotlib/tests/test_legend.py", "start_line": 88, "code": "def test_legend_auto4():\n    fig, axs = plt.subplots(ncols=3, figsize=(6.4, 2.4))\n    leg_bboxes = []\n    for ax, ht in zip(axs.flat, ('bar', 'step', 'stepfilled')):\n        ax.set_title(ht)\n        ax.hist([0] + 5*[9], bins=range(10), label=\"Legend\", histtype=ht)\n        leg = ax.legend(loc=\"best\")\n        fig.canvas.draw()\n        leg_bboxes.append(\n            leg.get_window_extent().transformed(ax.transAxes.inverted()))\n    assert_allclose(leg_bboxes[1].bounds, leg_bboxes[0].bounds)", "documentation": "    \"\"\"\n    Check that the legend location with automatic placement is the same,\n    whatever the histogram type is. Related to issue #9580.\n    \"\"\""}, {"filename": "lib/matplotlib/tests/test_legend.py", "start_line": 110, "code": "def test_legend_auto5():\n    fig, axs = plt.subplots(ncols=2, figsize=(9.6, 4.8))\n    leg_bboxes = []\n    for ax, loc in zip(axs.flat, (\"center\", \"best\")):\n        for _patch in [\n                mpatches.Ellipse(\n                    xy=(0.5, 0.9), width=0.8, height=0.2, fc=\"C1\"),\n                mpatches.Polygon(np.array([\n                    [0, 1], [0, 0], [1, 0], [1, 1], [0.9, 1.0], [0.9, 0.1],\n                    [0.1, 0.1], [0.1, 1.0], [0.1, 1.0]]), fc=\"C1\"),\n                mpatches.Wedge((0.5, 0.5), 0.5, 0, 360, width=0.05, fc=\"C0\")", "documentation": "    \"\"\"\n    Check that the automatic placement handle a rather complex\n    case with non rectangular patch. Related to issue #9580.\n    \"\"\""}, {"filename": "lib/matplotlib/tests/test_legend.py", "start_line": 242, "code": "def test_legend_expand():\n    legend_modes = [None, \"expand\"]\n    fig, axs = plt.subplots(len(legend_modes), 1)\n    x = np.arange(100)\n    for ax, mode in zip(axs, legend_modes):\n        ax.plot(x, 50 - x, 'o', label='y=1')\n        l1 = ax.legend(loc='upper left', mode=mode)\n        ax.add_artist(l1)\n        ax.plot(x, x - 50, 'o', label='y=-1')\n        l2 = ax.legend(loc='right', mode=mode)\n        ax.add_artist(l2)", "documentation": "    \"\"\"Test expand mode\"\"\""}, {"filename": "lib/matplotlib/tests/test_legend.py", "start_line": 302, "code": "def test_reverse_legend_handles_and_labels():\n    fig, ax = plt.subplots()\n    x = 1\n    y = 1\n    labels = [\"First label\", \"Second label\", \"Third label\"]\n    markers = ['.', ',', 'o']\n    ax.plot(x, y, markers[0], label=labels[0])\n    ax.plot(x, y, markers[1], label=labels[1])\n    ax.plot(x, y, markers[2], label=labels[2])\n    leg = ax.legend(reverse=True)\n    actual_labels = [t.get_text() for t in leg.get_texts()]", "documentation": "    \"\"\"Check that the legend handles and labels are reversed.\"\"\""}, {"filename": "lib/matplotlib/tests/test_legend.py", "start_line": 321, "code": "def test_reverse_legend_display(fig_test, fig_ref):\n    ax = fig_test.subplots()\n    ax.plot([1], 'ro', label=\"first\")\n    ax.plot([2], 'bx', label=\"second\")\n    ax.legend(reverse=True)\n    ax = fig_ref.subplots()\n    ax.plot([2], 'bx', label=\"second\")\n    ax.plot([1], 'ro', label=\"first\")\n    ax.legend()", "documentation": "    \"\"\"Check that the rendered legend entries are reversed\"\"\""}, {"filename": "lib/matplotlib/tests/test_legend.py", "start_line": 523, "code": "def test_legend_stackplot():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 10, 10)\n    y1 = 1.0 * x\n    y2 = 2.0 * x + 1\n    y3 = 3.0 * x + 2\n    ax.stackplot(x, y1, y2, y3, labels=['y1', 'y2', 'y3'])\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 70)\n    ax.legend(loc='best')", "documentation": "    \"\"\"Test legend for PolyCollection using stackplot.\"\"\""}, {"filename": "lib/matplotlib/tests/test_legend.py", "start_line": 624, "code": "def test_handler_numpoints():\n    fig, ax = plt.subplots()\n    ax.plot(range(5), label='test')\n    ax.legend(numpoints=0.5)", "documentation": "    \"\"\"Test legend handler with numpoints <= 1.\"\"\""}, {"filename": "lib/matplotlib/tests/test_legend.py", "start_line": 632, "code": "def test_text_nohandler_warning():\n    fig, ax = plt.subplots()\n    ax.plot([0], label=\"mock data\")\n    ax.text(x=0, y=0, s=\"text\", label=\"label\")\n    with pytest.warns(UserWarning) as record:\n        ax.legend()\n    assert len(record) == 1\n    f, ax = plt.subplots()\n    ax.pcolormesh(np.random.uniform(0, 1, (10, 10)))\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\")", "documentation": "    \"\"\"Test that Text artists with labels raise a warning\"\"\""}, {"filename": "lib/matplotlib/tests/test_legend.py", "start_line": 649, "code": "def test_empty_bar_chart_with_legend():\n    plt.bar([], [], label='test')\n    plt.legend()\n@image_comparison(['shadow_argument_types.png'], remove_text=True, style='mpl20',\n                  tol=0 if platform.machine() == 'x86_64' else 0.028)", "documentation": "    \"\"\"Test legend when bar chart is empty with a label.\"\"\""}], "after_segments": [{"filename": "lib/matplotlib/tests/test_legend.py", "start_line": 57, "code": "def test_legend_auto1():\n    fig, ax = plt.subplots()\n    x = np.arange(100)\n    ax.plot(x, 50 - x, 'o', label='y=1')\n    ax.plot(x, x - 50, 'o', label='y=-1')\n    ax.legend(loc='best')\n@image_comparison(['legend_auto2.png'], remove_text=True)", "documentation": "    \"\"\"Test automatic legend placement\"\"\""}, {"filename": "lib/matplotlib/tests/test_legend.py", "start_line": 67, "code": "def test_legend_auto2():\n    fig, ax = plt.subplots()\n    x = np.arange(100)\n    b1 = ax.bar(x, x, align='edge', color='m')\n    b2 = ax.bar(x, x[::-1], align='edge', color='g')\n    ax.legend([b1[0], b2[0]], ['up', 'down'], loc='best')\n@image_comparison(['legend_auto3.png'])", "documentation": "    \"\"\"Test automatic legend placement\"\"\""}, {"filename": "lib/matplotlib/tests/test_legend.py", "start_line": 77, "code": "def test_legend_auto3():\n    fig, ax = plt.subplots()\n    x = [0.9, 0.1, 0.1, 0.9, 0.9, 0.5]\n    y = [0.95, 0.95, 0.05, 0.05, 0.5, 0.5]\n    ax.plot(x, y, 'o-', label='line')\n    ax.set_xlim(0.0, 1.0)\n    ax.set_ylim(0.0, 1.0)\n    ax.legend(loc='best')", "documentation": "    \"\"\"Test automatic legend placement\"\"\""}, {"filename": "lib/matplotlib/tests/test_legend.py", "start_line": 88, "code": "def test_legend_auto4():\n    fig, axs = plt.subplots(ncols=3, figsize=(6.4, 2.4))\n    leg_bboxes = []\n    for ax, ht in zip(axs.flat, ('bar', 'step', 'stepfilled')):\n        ax.set_title(ht)\n        ax.hist([0] + 5*[9], bins=range(10), label=\"Legend\", histtype=ht)\n        leg = ax.legend(loc=\"best\")\n        fig.canvas.draw()\n        leg_bboxes.append(\n            leg.get_window_extent().transformed(ax.transAxes.inverted()))\n    assert_allclose(leg_bboxes[1].bounds, leg_bboxes[0].bounds)", "documentation": "    \"\"\"\n    Check that the legend location with automatic placement is the same,\n    whatever the histogram type is. Related to issue #9580.\n    \"\"\""}, {"filename": "lib/matplotlib/tests/test_legend.py", "start_line": 110, "code": "def test_legend_auto5():\n    fig, axs = plt.subplots(ncols=2, figsize=(9.6, 4.8))\n    leg_bboxes = []\n    for ax, loc in zip(axs.flat, (\"center\", \"best\")):\n        for _patch in [\n                mpatches.Ellipse(\n                    xy=(0.5, 0.9), width=0.8, height=0.2, fc=\"C1\"),\n                mpatches.Polygon(np.array([\n                    [0, 1], [0, 0], [1, 0], [1, 1], [0.9, 1.0], [0.9, 0.1],\n                    [0.1, 0.1], [0.1, 1.0], [0.1, 1.0]]), fc=\"C1\"),\n                mpatches.Wedge((0.5, 0.5), 0.5, 0, 360, width=0.05, fc=\"C0\")", "documentation": "    \"\"\"\n    Check that the automatic placement handle a rather complex\n    case with non rectangular patch. Related to issue #9580.\n    \"\"\""}, {"filename": "lib/matplotlib/tests/test_legend.py", "start_line": 242, "code": "def test_legend_expand():\n    legend_modes = [None, \"expand\"]\n    fig, axs = plt.subplots(len(legend_modes), 1)\n    x = np.arange(100)\n    for ax, mode in zip(axs, legend_modes):\n        ax.plot(x, 50 - x, 'o', label='y=1')\n        l1 = ax.legend(loc='upper left', mode=mode)\n        ax.add_artist(l1)\n        ax.plot(x, x - 50, 'o', label='y=-1')\n        l2 = ax.legend(loc='right', mode=mode)\n        ax.add_artist(l2)", "documentation": "    \"\"\"Test expand mode\"\"\""}, {"filename": "lib/matplotlib/tests/test_legend.py", "start_line": 302, "code": "def test_reverse_legend_handles_and_labels():\n    fig, ax = plt.subplots()\n    x = 1\n    y = 1\n    labels = [\"First label\", \"Second label\", \"Third label\"]\n    markers = ['.', ',', 'o']\n    ax.plot(x, y, markers[0], label=labels[0])\n    ax.plot(x, y, markers[1], label=labels[1])\n    ax.plot(x, y, markers[2], label=labels[2])\n    leg = ax.legend(reverse=True)\n    actual_labels = [t.get_text() for t in leg.get_texts()]", "documentation": "    \"\"\"Check that the legend handles and labels are reversed.\"\"\""}, {"filename": "lib/matplotlib/tests/test_legend.py", "start_line": 321, "code": "def test_reverse_legend_display(fig_test, fig_ref):\n    ax = fig_test.subplots()\n    ax.plot([1], 'ro', label=\"first\")\n    ax.plot([2], 'bx', label=\"second\")\n    ax.legend(reverse=True)\n    ax = fig_ref.subplots()\n    ax.plot([2], 'bx', label=\"second\")\n    ax.plot([1], 'ro', label=\"first\")\n    ax.legend()", "documentation": "    \"\"\"Check that the rendered legend entries are reversed\"\"\""}, {"filename": "lib/matplotlib/tests/test_legend.py", "start_line": 523, "code": "def test_legend_stackplot():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 10, 10)\n    y1 = 1.0 * x\n    y2 = 2.0 * x + 1\n    y3 = 3.0 * x + 2\n    ax.stackplot(x, y1, y2, y3, labels=['y1', 'y2', 'y3'])\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 70)\n    ax.legend(loc='best')", "documentation": "    \"\"\"Test legend for PolyCollection using stackplot.\"\"\""}, {"filename": "lib/matplotlib/tests/test_legend.py", "start_line": 624, "code": "def test_handler_numpoints():\n    fig, ax = plt.subplots()\n    ax.plot(range(5), label='test')\n    ax.legend(numpoints=0.5)", "documentation": "    \"\"\"Test legend handler with numpoints <= 1.\"\"\""}, {"filename": "lib/matplotlib/tests/test_legend.py", "start_line": 632, "code": "def test_text_nohandler_warning():\n    fig, ax = plt.subplots()\n    ax.plot([0], label=\"mock data\")\n    ax.text(x=0, y=0, s=\"text\", label=\"label\")\n    with pytest.warns(UserWarning) as record:\n        ax.legend()\n    assert len(record) == 1\n    f, ax = plt.subplots()\n    ax.pcolormesh(np.random.uniform(0, 1, (10, 10)))\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\")", "documentation": "    \"\"\"Test that Text artists with labels raise a warning\"\"\""}, {"filename": "lib/matplotlib/tests/test_legend.py", "start_line": 649, "code": "def test_empty_bar_chart_with_legend():\n    plt.bar([], [], label='test')\n    plt.legend()\n@image_comparison(['shadow_argument_types.png'], remove_text=True, style='mpl20',\n                  tol=0 if platform.machine() == 'x86_64' else 0.028)", "documentation": "    \"\"\"Test legend when bar chart is empty with a label.\"\"\""}]}
{"repository": "matplotlib/matplotlib", "commit_sha": "dedfe9be48ad82cade86766ef89410844ff09b31", "commit_message": "Merge pull request #30774 from timhoffm/doc-hexbin\n\nDOC: Fix documentation error of hexbin", "commit_date": "2025-11-21T15:59:36+00:00", "author": "Elliott Sales de Andrade", "file": "lib/matplotlib/axes/_axes.py", "patch": "@@ -5420,8 +5420,9 @@ def hexbin(self, x, y, C=None, gridsize=100, bins=None,\n             - If *None*, no binning is applied; the color of each hexagon\n               directly corresponds to its count value.\n             - If 'log', use a logarithmic scale for the colormap.\n-              Internally, :math:`log_{10}(i+1)` is used to determine the\n+              Internally, :math:`log_{10}(i)` is used to determine the\n               hexagon color. This is equivalent to ``norm=LogNorm()``.\n+              Note that 0 counts are thus marked with the \"bad\" color.\n             - If an integer, divide the counts in the specified number\n               of bins, and color the hexagons accordingly.\n             - If a sequence of values, the values of the lower bound of", "before_segments": [{"filename": "lib/matplotlib/axes/_axes.py", "start_line": 48, "code": "def _make_axes_method(func):\n    func.__qualname__ = f\"Axes.{func.__name__}\"\n    return func", "documentation": "    \"\"\"\n    Patch the qualname for functions that are directly added to Axes.\n\n    Some Axes functionality is defined in functions in other submodules.\n    These are simply added as attributes to Axes. As a result, their\n    ``__qualname__`` is e.g. only \"table\" and not \"Axes.table\". This\n    function fixes that.\n\n    Note that the function itself is patched, so that\n    ``matplotlib.table.table.__qualname__` will also show \"Axes.table\".\n    However, since these functions are not intended to be standalone,\n    this is bearable.\n    \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 66, "code": "class _GroupedBarReturn:", "documentation": "    \"\"\"\n    A provisional result object for `.Axes.grouped_bar`.\n\n    This is a placeholder for a future better return type. We try to build in\n    backward compatibility / migration possibilities.\n\n    The only public interfaces are the ``bar_containers`` attribute and the\n    ``remove()`` method.\n    \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 84, "code": "class Axes(_AxesBase):", "documentation": "    \"\"\"\n    An Axes object encapsulates all the elements of an individual (sub-)plot in\n    a figure.\n\n    It contains most of the (sub-)plot elements: `~.axis.Axis`,\n    `~.axis.Tick`, `~.lines.Line2D`, `~.text.Text`, `~.patches.Polygon`, etc.,\n    and sets the coordinate system.\n\n    Like all visible elements in a figure, Axes is an `.Artist` subclass.\n\n    The `Axes` instance supports callbacks through a callbacks attribute which\n    is a `~.cbook.CallbackRegistry` instance.  The events you can connect to\n    are 'xlim_changed' and 'ylim_changed' and the callback will be called with\n    func(*ax*) where *ax* is the `Axes` instance.\n\n    .. note::\n\n        As a user, you do not instantiate Axes directly, but use Axes creation\n        methods instead; e.g. from `.pyplot` or `.Figure`:\n        `~.pyplot.subplots`, `~.pyplot.subplot_mosaic` or `.Figure.add_axes`.\n\n    \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 109, "code": "    def get_title(self, loc=\"center\"):\n        titles = {'left': self._left_title,\n                  'center': self.title,\n                  'right': self._right_title}\n        title = _api.check_getitem(titles, loc=loc.lower())\n        return title.get_text()", "documentation": "        \"\"\"\n        Get an Axes title.\n\n        Get one of the three available Axes titles. The available titles\n        are positioned above the Axes in the center, flush with the left\n        edge, and flush with the right edge.\n\n        Parameters\n        ----------\n        loc : {'center', 'left', 'right'}, str, default: 'center'\n            Which title to return.\n\n        Returns\n        -------\n        str\n            The title text string.\n\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 215, "code": "    def get_legend_handles_labels(self, legend_handler_map=None):\n        handles, labels = mlegend._get_legend_handles_labels(\n            [self], legend_handler_map)\n        return handles, labels\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Return handles and labels for legend\n\n        ``ax.legend()`` is equivalent to ::\n\n          h, l = ax.get_legend_handles_labels()\n          ax.legend(h, l)\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 230, "code": "    def legend(self, *args, **kwargs):\n        handles, labels, kwargs = mlegend._parse_legend_args([self], *args, **kwargs)\n        self.legend_ = mlegend.Legend(self, handles, labels, **kwargs)\n        self.legend_._remove_method = self._remove_legend\n        return self.legend_", "documentation": "        \"\"\"\n        Place a legend on the Axes.\n\n        Call signatures::\n\n            legend()\n            legend(handles, labels)\n            legend(handles=handles)\n            legend(labels)\n\n        The call signatures correspond to the following different ways to use\n        this method:\n\n        **1. Automatic detection of elements to be shown in the legend**\n\n        The elements to be added to the legend are automatically determined,\n        when you do not pass in any extra arguments.\n\n        In this case, the labels are taken from the artist. You can specify\n        them either at artist creation or by calling the\n        :meth:`~.Artist.set_label` method on the artist::\n\n            ax.plot([1, 2, 3], label='Inline label')\n            ax.legend()\n\n        or::\n\n            line, = ax.plot([1, 2, 3])\n            line.set_label('Label via method')\n            ax.legend()\n\n        .. note::\n            Specific artists can be excluded from the automatic legend element\n            selection by using a label starting with an underscore, \"_\".\n            A string starting with an underscore is the default label for all\n            artists, so calling `.Axes.legend` without any arguments and\n            without setting the labels manually will result in a ``UserWarning``\n            and an empty legend being drawn.\n\n\n        **2. Explicitly listing the artists and labels in the legend**\n\n        For full control of which artists have a legend entry, it is possible\n        to pass an iterable of legend artists followed by an iterable of\n        legend labels respectively::\n\n            ax.legend([line1, line2, line3], ['label1', 'label2', 'label3'])\n\n\n        **3. Explicitly listing the artists in the legend**\n\n        This is similar to 2, but the labels are taken from the artists'\n        label properties. Example::\n\n            line1, = ax.plot([1, 2, 3], label='label1')\n            line2, = ax.plot([1, 2, 3], label='label2')\n            ax.legend(handles=[line1, line2])\n\n\n        **4. Labeling existing plot elements**\n\n        .. admonition:: Discouraged\n\n            This call signature is discouraged, because the relation between\n            plot elements and labels is only implicit by their order and can\n            easily be mixed up.\n\n        To make a legend for all artists on an Axes, call this function with\n        an iterable of strings, one for each legend item. For example::\n\n            ax.plot([1, 2, 3])\n            ax.plot([5, 6, 7])\n            ax.legend(['First line', 'Second line'])\n\n\n        Parameters\n        ----------\n        handles : list of (`.Artist` or tuple of `.Artist`), optional\n            A list of Artists (lines, patches) to be added to the legend.\n            Use this together with *labels*, if you need full control on what\n            is shown in the legend and the automatic mechanism described above\n            is not sufficient.\n\n            The length of handles and labels should be the same in this\n            case. If they are not, they are truncated to the smaller length.\n\n            If an entry contains a tuple, then the legend handler for all Artists in the\n            tuple will be placed alongside a single label.\n\n        labels : list of str, optional\n            A list of labels to show next to the artists.\n            Use this together with *handles*, if you need full control on what\n            is shown in the legend and the automatic mechanism described above\n            is not sufficient.\n\n        Returns\n        -------\n        `~matplotlib.legend.Legend`\n\n        Other Parameters\n        ----------------\n        %(_legend_kw_axes)s\n\n        See Also\n        --------\n        .Figure.legend\n\n        Notes\n        -----\n        Some artists are not supported by this function.  See\n        :ref:`legend_guide` for details.\n\n        Examples\n        --------\n        .. plot:: gallery/text_labels_and_annotations/legend.py\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 355, "code": "    def inset_axes(self, bounds, *, transform=None, zorder=5, **kwargs):\n        if transform is None:\n            transform = self.transAxes\n        kwargs.setdefault('label', 'inset_axes')\n        inset_locator = _TransformedBoundsLocator(bounds, transform)\n        bounds = inset_locator(self, None).bounds\n        fig = self.get_figure(root=False)\n        projection_class, pkw = fig._process_projection_requirements(**kwargs)\n        inset_ax = projection_class(fig, bounds, zorder=zorder, **pkw)\n        inset_ax.set_axes_locator(inset_locator)\n        self.add_child_axes(inset_ax)", "documentation": "        \"\"\"\n        Add a child inset Axes to this existing Axes.\n\n\n        Parameters\n        ----------\n        bounds : [x0, y0, width, height]\n            Lower-left corner of inset Axes, and its width and height.\n\n        transform : `.Transform`\n            Defaults to `!ax.transAxes`, i.e. the units of *rect* are in\n            Axes-relative coordinates.\n\n        projection : {None, 'aitoff', 'hammer', 'lambert', 'mollweide', \\\n'polar', 'rectilinear', str}, optional\n            The projection type of the inset `~.axes.Axes`. *str* is the name\n            of a custom projection, see `~matplotlib.projections`. The default\n            None results in a 'rectilinear' projection.\n\n        polar : bool, default: False\n            If True, equivalent to projection='polar'.\n\n        axes_class : subclass type of `~.axes.Axes`, optional\n            The `.axes.Axes` subclass that is instantiated.  This parameter\n            is incompatible with *projection* and *polar*.  See\n            :ref:`axisartist_users-guide-index` for examples.\n\n        zorder : number\n            Defaults to 5 (same as `.Axes.legend`).  Adjust higher or lower\n            to change whether it is above or below data plotted on the\n            parent Axes.\n\n        **kwargs\n            Other keyword arguments are passed on to the inset Axes class.\n\n        Returns\n        -------\n        ax\n            The created `~.axes.Axes` instance.\n\n        Examples\n        --------\n        This example makes two inset Axes, the first is in Axes-relative\n        coordinates, and the second in data-coordinates::\n\n            fig, ax = plt.subplots()\n            ax.plot(range(10))\n            axin1 = ax.inset_axes([0.8, 0.1, 0.15, 0.15])\n            axin2 = ax.inset_axes(\n                    [5, 7, 2.3, 2.3], transform=ax.transData)\n\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 510, "code": "    def indicate_inset_zoom(self, inset_ax, **kwargs):\n        return self.indicate_inset(None, inset_ax, **kwargs)\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Add an inset indicator rectangle to the Axes based on the axis\n        limits for an *inset_ax* and draw connectors between *inset_ax*\n        and the rectangle.\n\n        Warnings\n        --------\n        This method is experimental as of 3.0, and the API may change.\n\n        Parameters\n        ----------\n        inset_ax : `.Axes`\n            Inset Axes to draw connecting lines to.  Two lines are\n            drawn connecting the indicator box to the inset Axes on corners\n            chosen so as to not overlap with the indicator box.\n\n        **kwargs\n            Other keyword arguments are passed on to `.Axes.indicate_inset`\n\n        Returns\n        -------\n        inset_indicator : `.inset.InsetIndicator`\n            An artist which contains\n\n            inset_indicator.rectangle : `.Rectangle`\n                The indicator frame.\n\n            inset_indicator.connectors : 4-tuple of `.patches.ConnectionPatch`\n                The four connector lines connecting to (lower_left, upper_left,\n                lower_right upper_right) corners of *inset_ax*. Two lines are\n                set with visibility to *False*,  but the user can set the\n                visibility to True if the automatic choice is not deemed correct.\n\n            .. versionchanged:: 3.10\n                Previously the rectangle and connectors tuple were returned.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 551, "code": "    def secondary_xaxis(self, location, functions=None, *, transform=None, **kwargs):\n        if not (location in ['top', 'bottom'] or isinstance(location, Real)):\n            raise ValueError('secondary_xaxis location must be either '\n                             'a float or \"top\"/\"bottom\"')\n        secondary_ax = SecondaryAxis(self, 'x', location, functions,\n                                     transform, **kwargs)\n        self.add_child_axes(secondary_ax)\n        return secondary_ax\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Add a second x-axis to this `~.axes.Axes`.\n\n        For example if we want to have a second scale for the data plotted on\n        the xaxis.\n\n        %(_secax_docstring)s\n\n        Examples\n        --------\n        The main axis shows frequency, and the secondary axis shows period.\n\n        .. plot::\n\n            fig, ax = plt.subplots()\n            ax.loglog(range(1, 360, 5), range(1, 360, 5))\n            ax.set_xlabel('frequency [Hz]')\n\n            def invert(x):\n                # 1/x with special treatment of x == 0\n                x = np.array(x).astype(float)\n                near_zero = np.isclose(x, 0)\n                x[near_zero] = np.inf\n                x[~near_zero] = 1 / x[~near_zero]\n                return x\n\n            # the inverse of 1/x is itself\n            secax = ax.secondary_xaxis('top', functions=(invert, invert))\n            secax.set_xlabel('Period [s]')\n            plt.show()\n\n        To add a secondary axis relative to your data, you can pass a transform\n        to the new axis.\n\n        .. plot::\n\n            fig, ax = plt.subplots()\n            ax.plot(range(0, 5), range(-1, 4))\n\n            # Pass 'ax.transData' as a transform to place the axis\n            # relative to your data at y=0\n            secax = ax.secondary_xaxis(0, transform=ax.transData)\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 605, "code": "    def secondary_yaxis(self, location, functions=None, *, transform=None, **kwargs):\n        if not (location in ['left', 'right'] or isinstance(location, Real)):\n            raise ValueError('secondary_yaxis location must be either '\n                             'a float or \"left\"/\"right\"')\n        secondary_ax = SecondaryAxis(self, 'y', location, functions,\n                                     transform, **kwargs)\n        self.add_child_axes(secondary_ax)\n        return secondary_ax\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Add a second y-axis to this `~.axes.Axes`.\n\n        For example if we want to have a second scale for the data plotted on\n        the yaxis.\n\n        %(_secax_docstring)s\n\n        Examples\n        --------\n        Add a secondary Axes that converts from radians to degrees\n\n        .. plot::\n\n            fig, ax = plt.subplots()\n            ax.plot(range(1, 360, 5), range(1, 360, 5))\n            ax.set_ylabel('degrees')\n            secax = ax.secondary_yaxis('right', functions=(np.deg2rad,\n                                                           np.rad2deg))\n            secax.set_ylabel('radians')\n\n        To add a secondary axis relative to your data, you can pass a transform\n        to the new axis.\n\n        .. plot::\n\n            fig, ax = plt.subplots()\n            ax.plot(range(0, 5), range(-1, 4))\n\n            # Pass 'ax.transData' as a transform to place the axis\n            # relative to your data at x=3\n            secax = ax.secondary_yaxis(3, transform=ax.transData)\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 649, "code": "    def text(self, x, y, s, fontdict=None, **kwargs):\n        effective_kwargs = {\n            'verticalalignment': 'baseline',\n            'horizontalalignment': 'left',\n            'transform': self.transData,\n            'clip_on': False,\n            **(fontdict if fontdict is not None else {}),\n            **kwargs,\n        }\n        t = mtext.Text(x, y, text=s, **effective_kwargs)\n        if t.get_clip_path() is None:", "documentation": "        \"\"\"\n        Add text to the Axes.\n\n        Add the text *s* to the Axes at location *x*, *y* in data coordinates,\n        with a default ``horizontalalignment`` on the ``left`` and\n        ``verticalalignment`` at the ``baseline``. See\n        :doc:`/gallery/text_labels_and_annotations/text_alignment`.\n\n        Parameters\n        ----------\n        x, y : float\n            The position to place the text. By default, this is in data\n            coordinates. The coordinate system can be changed using the\n            *transform* parameter.\n\n        s : str\n            The text.\n\n        fontdict : dict, default: None\n\n            .. admonition:: Discouraged\n\n               The use of *fontdict* is discouraged. Parameters should be passed as\n               individual keyword arguments or using dictionary-unpacking\n               ``text(..., **fontdict)``.\n\n            A dictionary to override the default text properties. If fontdict\n            is None, the defaults are determined by `.rcParams`.\n\n        Returns\n        -------\n        `.Text`\n            The created `.Text` instance.\n\n        Other Parameters\n        ----------------\n        **kwargs : `~matplotlib.text.Text` properties.\n            Other miscellaneous text parameters.\n\n            %(Text:kwdoc)s\n\n        Examples\n        --------\n        Individual keyword arguments can be used to override any given\n        parameter::\n\n            >>> text(x, y, s, fontsize=12)\n\n        The default transform specifies that text is in data coords,\n        alternatively, you can specify text in axis coords ((0, 0) is\n        lower-left and (1, 1) is upper-right).  The example below places\n        text in the center of the Axes::\n\n            >>> text(0.5, 0.5, 'matplotlib', horizontalalignment='center',\n            ...      verticalalignment='center', transform=ax.transAxes)\n\n        You can put a rectangular box around the text instance (e.g., to\n        set a background color) by using the keyword *bbox*.  *bbox* is\n        a dictionary of `~matplotlib.patches.Rectangle`\n        properties.  For example::\n\n            >>> text(x, y, s, bbox=dict(facecolor='red', alpha=0.5))\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 744, "code": "    def axhline(self, y=0, xmin=0, xmax=1, **kwargs):\n        self._check_no_units([xmin, xmax], ['xmin', 'xmax'])\n        if \"transform\" in kwargs:\n            raise ValueError(\"'transform' is not allowed as a keyword \"\n                             \"argument; axhline generates its own transform.\")\n        ymin, ymax = self.get_ybound()\n        yy, = self._process_unit_info([(\"y\", y)], kwargs)\n        scaley = (yy < ymin) or (yy > ymax)\n        trans = self.get_yaxis_transform(which='grid')\n        l = mlines.Line2D([xmin, xmax], [y, y], transform=trans, **kwargs)\n        self.add_line(l)", "documentation": "        \"\"\"\n        Add a horizontal line spanning the whole or fraction of the Axes.\n\n        Note: If you want to set x-limits in data coordinates, use\n        `~.Axes.hlines` instead.\n\n        Parameters\n        ----------\n        y : float, default: 0\n            y position in :ref:`data coordinates <coordinate-systems>`.\n\n        xmin : float, default: 0\n            The start x-position in :ref:`axes coordinates <coordinate-systems>`.\n            Should be between 0 and 1, 0 being the far left of the plot,\n            1 the far right of the plot.\n\n        xmax : float, default: 1\n            The end x-position in :ref:`axes coordinates <coordinate-systems>`.\n            Should be between 0 and 1, 0 being the far left of the plot,\n            1 the far right of the plot.\n\n        Returns\n        -------\n        `~matplotlib.lines.Line2D`\n            A `.Line2D` specified via two points ``(xmin, y)``, ``(xmax, y)``.\n            Its transform is set such that *x* is in\n            :ref:`axes coordinates <coordinate-systems>` and *y* is in\n            :ref:`data coordinates <coordinate-systems>`.\n\n            This is still a generic line and the horizontal character is only\n            realized through using identical *y* values for both points. Thus,\n            if you want to change the *y* value later, you have to provide two\n            values ``line.set_ydata([3, 3])``.\n\n        Other Parameters\n        ----------------\n        **kwargs\n            Valid keyword arguments are `.Line2D` properties, except for\n            'transform':\n\n            %(Line2D:kwdoc)s\n\n        See Also\n        --------\n        hlines : Add horizontal lines in data coordinates.\n        axhspan : Add a horizontal span (rectangle) across the axis.\n        axline : Add a line with an arbitrary slope.\n\n        Examples\n        --------\n        * draw a thick red hline at 'y' = 0 that spans the xrange::\n\n            >>> axhline(linewidth=4, color='r')\n\n        * draw a default hline at 'y' = 1 that spans the xrange::\n\n            >>> axhline(y=1)\n\n        * draw a default hline at 'y' = .5 that spans the middle half of\n          the xrange::\n\n            >>> axhline(y=.5, xmin=0.25, xmax=0.75)\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 827, "code": "    def axvline(self, x=0, ymin=0, ymax=1, **kwargs):\n        self._check_no_units([ymin, ymax], ['ymin', 'ymax'])\n        if \"transform\" in kwargs:\n            raise ValueError(\"'transform' is not allowed as a keyword \"\n                             \"argument; axvline generates its own transform.\")\n        xmin, xmax = self.get_xbound()\n        xx, = self._process_unit_info([(\"x\", x)], kwargs)\n        scalex = (xx < xmin) or (xx > xmax)\n        trans = self.get_xaxis_transform(which='grid')\n        l = mlines.Line2D([x, x], [ymin, ymax], transform=trans, **kwargs)\n        self.add_line(l)", "documentation": "        \"\"\"\n        Add a vertical line spanning the whole or fraction of the Axes.\n\n        Note: If you want to set y-limits in data coordinates, use\n        `~.Axes.vlines` instead.\n\n        Parameters\n        ----------\n        x : float, default: 0\n            x position in :ref:`data coordinates <coordinate-systems>`.\n\n        ymin : float, default: 0\n            The start y-position in :ref:`axes coordinates <coordinate-systems>`.\n            Should be between 0 and 1, 0 being the bottom of the plot, 1 the\n            top of the plot.\n\n        ymax : float, default: 1\n            The end y-position in :ref:`axes coordinates <coordinate-systems>`.\n            Should be between 0 and 1, 0 being the bottom of the plot, 1 the\n            top of the plot.\n\n        Returns\n        -------\n        `~matplotlib.lines.Line2D`\n            A `.Line2D` specified via two points ``(x, ymin)``, ``(x, ymax)``.\n            Its transform is set such that *x* is in\n            :ref:`data coordinates <coordinate-systems>` and *y* is in\n            :ref:`axes coordinates <coordinate-systems>`.\n\n            This is still a generic line and the vertical character is only\n            realized through using identical *x* values for both points. Thus,\n            if you want to change the *x* value later, you have to provide two\n            values ``line.set_xdata([3, 3])``.\n\n        Other Parameters\n        ----------------\n        **kwargs\n            Valid keyword arguments are `.Line2D` properties, except for\n            'transform':\n\n            %(Line2D:kwdoc)s\n\n        See Also\n        --------\n        vlines : Add vertical lines in data coordinates.\n        axvspan : Add a vertical span (rectangle) across the axis.\n        axline : Add a line with an arbitrary slope.\n\n        Examples\n        --------\n        * draw a thick red vline at *x* = 0 that spans the yrange::\n\n            >>> axvline(linewidth=4, color='r')\n\n        * draw a default vline at *x* = 1 that spans the yrange::\n\n            >>> axvline(x=1)\n\n        * draw a default vline at *x* = .5 that spans the middle half of\n          the yrange::\n\n            >>> axvline(x=.5, ymin=0.25, ymax=0.75)\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 918, "code": "    def axline(self, xy1, xy2=None, *, slope=None, **kwargs):\n        if slope is not None and (self.get_xscale() != 'linear' or\n                                  self.get_yscale() != 'linear'):\n            raise TypeError(\"'slope' cannot be used with non-linear scales\")\n        datalim = [xy1] if xy2 is None else [xy1, xy2]\n        if \"transform\" in kwargs:\n            datalim = []\n        line = mlines.AxLine(xy1, xy2, slope, **kwargs)\n        self._set_artist_props(line)\n        if line.get_clip_path() is None:\n            line.set_clip_path(self.patch)", "documentation": "        \"\"\"\n        Add an infinitely long straight line.\n\n        The line can be defined either by two points *xy1* and *xy2*, or\n        by one point *xy1* and a *slope*.\n\n        This draws a straight line \"on the screen\", regardless of the x and y\n        scales, and is thus also suitable for drawing exponential decays in\n        semilog plots, power laws in loglog plots, etc. However, *slope*\n        should only be used with linear scales; It has no clear meaning for\n        all other scales, and thus the behavior is undefined. Please specify\n        the line using the points *xy1*, *xy2* for non-linear scales.\n\n        The *transform* keyword argument only applies to the points *xy1*,\n        *xy2*. The *slope* (if given) is always in data coordinates. This can\n        be used e.g. with ``ax.transAxes`` for drawing grid lines with a fixed\n        slope.\n\n        Parameters\n        ----------\n        xy1, xy2 : (float, float)\n            Points for the line to pass through.\n            Either *xy2* or *slope* has to be given.\n        slope : float, optional\n            The slope of the line. Either *xy2* or *slope* has to be given.\n\n        Returns\n        -------\n        `.AxLine`\n\n        Other Parameters\n        ----------------\n        **kwargs\n            Valid kwargs are `.Line2D` properties\n\n            %(Line2D:kwdoc)s\n\n        See Also\n        --------\n        axhline : for horizontal lines\n        axvline : for vertical lines\n\n        Examples\n        --------\n        Draw a thick red line passing through (0, 0) and (1, 1)::\n\n            >>> axline((0, 0), (1, 1), linewidth=4, color='r')\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 992, "code": "    def axhspan(self, ymin, ymax, xmin=0, xmax=1, **kwargs):\n        self._check_no_units([xmin, xmax], ['xmin', 'xmax'])\n        (ymin, ymax), = self._process_unit_info([(\"y\", [ymin, ymax])], kwargs)\n        p = mpatches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, **kwargs)\n        p.set_transform(self.get_yaxis_transform(which=\"grid\"))\n        ix = self.dataLim.intervalx.copy()\n        mx = self.dataLim.minposx\n        self.add_patch(p)\n        self.dataLim.intervalx = ix\n        self.dataLim.minposx = mx\n        p.get_path()._interpolation_steps = mpl.axis.GRIDLINE_INTERPOLATION_STEPS", "documentation": "        \"\"\"\n        Add a horizontal span (rectangle) across the Axes.\n\n        The rectangle spans from *ymin* to *ymax* vertically, and, by default,\n        the whole x-axis horizontally.  The x-span can be set using *xmin*\n        (default: 0) and *xmax* (default: 1) which are in axis units; e.g.\n        ``xmin = 0.5`` always refers to the middle of the x-axis regardless of\n        the limits set by `~.Axes.set_xlim`.\n\n        Parameters\n        ----------\n        ymin : float\n            Lower y-coordinate of the span, in data units.\n        ymax : float\n            Upper y-coordinate of the span, in data units.\n        xmin : float, default: 0\n            Lower x-coordinate of the span, in x-axis (0-1) units.\n        xmax : float, default: 1\n            Upper x-coordinate of the span, in x-axis (0-1) units.\n\n        Returns\n        -------\n        `~matplotlib.patches.Rectangle`\n            Horizontal span (rectangle) from (xmin, ymin) to (xmax, ymax).\n\n        Other Parameters\n        ----------------\n        **kwargs : `~matplotlib.patches.Rectangle` properties\n\n        %(Rectangle:kwdoc)s\n\n        See Also\n        --------\n        axvspan : Add a vertical span across the Axes.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 1047, "code": "    def axvspan(self, xmin, xmax, ymin=0, ymax=1, **kwargs):\n        self._check_no_units([ymin, ymax], ['ymin', 'ymax'])\n        (xmin, xmax), = self._process_unit_info([(\"x\", [xmin, xmax])], kwargs)\n        p = mpatches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, **kwargs)\n        p.set_transform(self.get_xaxis_transform(which=\"grid\"))\n        iy = self.dataLim.intervaly.copy()\n        my = self.dataLim.minposy\n        self.add_patch(p)\n        self.dataLim.intervaly = iy\n        self.dataLim.minposy = my\n        p.get_path()._interpolation_steps = mpl.axis.GRIDLINE_INTERPOLATION_STEPS", "documentation": "        \"\"\"\n        Add a vertical span (rectangle) across the Axes.\n\n        The rectangle spans from *xmin* to *xmax* horizontally, and, by\n        default, the whole y-axis vertically.  The y-span can be set using\n        *ymin* (default: 0) and *ymax* (default: 1) which are in axis units;\n        e.g. ``ymin = 0.5`` always refers to the middle of the y-axis\n        regardless of the limits set by `~.Axes.set_ylim`.\n\n        Parameters\n        ----------\n        xmin : float\n            Lower x-coordinate of the span, in data units.\n        xmax : float\n            Upper x-coordinate of the span, in data units.\n        ymin : float, default: 0\n            Lower y-coordinate of the span, in y-axis units (0-1).\n        ymax : float, default: 1\n            Upper y-coordinate of the span, in y-axis units (0-1).\n\n        Returns\n        -------\n        `~matplotlib.patches.Rectangle`\n            Vertical span (rectangle) from (xmin, ymin) to (xmax, ymax).\n\n        Other Parameters\n        ----------------\n        **kwargs : `~matplotlib.patches.Rectangle` properties\n\n        %(Rectangle:kwdoc)s\n\n        See Also\n        --------\n        axhspan : Add a horizontal span across the Axes.\n\n        Examples\n        --------\n        Draw a vertical, green, translucent rectangle from x = 1.25 to\n        x = 1.55 that spans the yrange of the Axes.\n\n        >>> axvspan(1.25, 1.55, facecolor='g', alpha=0.5)\n\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 1544, "code": "    def plot(self, *args, scalex=True, scaley=True, data=None, **kwargs):\n        kwargs = cbook.normalize_kwargs(kwargs, mlines.Line2D)\n        lines = [*self._get_lines(self, *args, data=data, **kwargs)]\n        for line in lines:\n            self.add_line(line)\n        if scalex:\n            self._request_autoscale_view(\"x\")\n        if scaley:\n            self._request_autoscale_view(\"y\")\n        return lines\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Plot y versus x as lines and/or markers.\n\n        Call signatures::\n\n            plot([x], y, [fmt], *, data=None, **kwargs)\n            plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\n        The coordinates of the points or line nodes are given by *x*, *y*.\n\n        The optional parameter *fmt* is a convenient way for defining basic\n        formatting like color, marker and linestyle. It's a shortcut string\n        notation described in the *Notes* section below.\n\n        >>> plot(x, y)        # plot x and y using default line style and color\n        >>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n        >>> plot(y)           # plot y using x as index array 0..N-1\n        >>> plot(y, 'r+')     # ditto, but with red plusses\n\n        You can use `.Line2D` properties as keyword arguments for more\n        control on the appearance. Line properties and *fmt* can be mixed.\n        The following two calls yield identical results:\n\n        >>> plot(x, y, 'go--', linewidth=2, markersize=12)\n        >>> plot(x, y, color='green', marker='o', linestyle='dashed',\n        ...      linewidth=2, markersize=12)\n\n        When conflicting with *fmt*, keyword arguments take precedence.\n\n\n        **Plotting labelled data**\n\n        There's a convenient way for plotting objects with labelled data (i.e.\n        data that can be accessed by index ``obj['y']``). Instead of giving\n        the data in *x* and *y*, you can provide the object in the *data*\n        parameter and just give the labels for *x* and *y*::\n\n        >>> plot('xlabel', 'ylabel', data=obj)\n\n        All indexable objects are supported. This could e.g. be a `dict`, a\n        `pandas.DataFrame` or a structured numpy array.\n\n\n        **Plotting multiple sets of data**\n\n        There are various ways to plot multiple sets of data.\n\n        - The most straight forward way is just to call `plot` multiple times.\n          Example:\n\n          >>> plot(x1, y1, 'bo')\n          >>> plot(x2, y2, 'go')\n\n        - If *x* and/or *y* are 2D arrays, a separate data set will be drawn\n          for every column. If both *x* and *y* are 2D, they must have the\n          same shape. If only one of them is 2D with shape (N, m) the other\n          must have length N and will be used for every data set m.\n\n          Example:\n\n          >>> x = [1, 2, 3]\n          >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n          >>> plot(x, y)\n\n          is equivalent to:\n\n          >>> for col in range(y.shape[1]):\n          ...     plot(x, y[:, col])\n\n        - The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n          groups::\n\n          >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n\n          In this case, any additional keyword argument applies to all\n          datasets. Also, this syntax cannot be combined with the *data*\n          parameter.\n\n        By default, each line is assigned a different style specified by a\n        'style cycle'. The *fmt* and line property parameters are only\n        necessary if you want explicit deviations from these defaults.\n        Alternatively, you can also change the style cycle using\n        :rc:`axes.prop_cycle`.\n\n\n        Parameters\n        ----------\n        x, y : array-like or float\n            The horizontal / vertical coordinates of the data points.\n            *x* values are optional and default to ``range(len(y))``.\n\n            Commonly, these parameters are 1D arrays.\n\n            They can also be scalars, or two-dimensional (in that case, the\n            columns represent separate data sets).\n\n            These arguments cannot be passed as keywords.\n\n        fmt : str, optional\n            A format string, e.g. 'ro' for red circles. See the *Notes*\n            section for a full description of the format strings.\n\n            Format strings are just an abbreviation for quickly setting\n            basic line properties. All of these and more can also be\n            controlled by keyword arguments.\n\n            This argument cannot be passed as keyword.\n\n        data : indexable object, optional\n            An object with labelled data. If given, provide the label names to\n            plot in *x* and *y*.\n\n            .. note::\n                Technically there's a slight ambiguity in calls where the\n                second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n                could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n                the former interpretation is chosen, but a warning is issued.\n                You may suppress the warning by adding an empty format string\n                ``plot('n', 'o', '', data=obj)``.\n\n        Returns\n        -------\n        list of `.Line2D`\n            A list of lines representing the plotted data.\n\n        Other Parameters\n        ----------------\n        scalex, scaley : bool, default: True\n            These parameters determine if the view limits are adapted to the\n            data limits. The values are passed on to\n            `~.axes.Axes.autoscale_view`.\n\n        **kwargs : `~matplotlib.lines.Line2D` properties, optional\n            *kwargs* are used to specify properties like a line label (for\n            auto legends), linewidth, antialiasing, marker face color.\n            Example::\n\n            >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n            >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n\n            If you specify multiple lines with one plot call, the kwargs apply\n            to all those lines. In case the label object is iterable, each\n            element is used as labels for each set of data.\n\n            Here is a list of available `.Line2D` properties:\n\n            %(Line2D:kwdoc)s\n\n        See Also\n        --------\n        scatter : XY scatter plot with markers of varying size and/or color (\n            sometimes also called bubble chart).\n\n        Notes\n        -----\n        **Format Strings**\n\n        A format string consists of a part for color, marker and line::\n\n            fmt = '[marker][line][color]'\n\n        Each of them is optional. If not provided, the value from the style\n        cycle is used. Exception: If ``line`` is given, but no ``marker``,\n        the data will be a line without markers.\n\n        Other combinations such as ``[color][marker][line]`` are also\n        supported, but note that their parsing may be ambiguous.\n\n        **Markers**\n\n        =============   ===============================\n        character       description\n        =============   ===============================\n        ``'.'``         point marker\n        ``','``         pixel marker\n        ``'o'``         circle marker\n        ``'v'``         triangle_down marker\n        ``'^'``         triangle_up marker\n        ``'<'``         triangle_left marker\n        ``'>'``         triangle_right marker\n        ``'1'``         tri_down marker\n        ``'2'``         tri_up marker\n        ``'3'``         tri_left marker\n        ``'4'``         tri_right marker\n        ``'8'``         octagon marker\n        ``'s'``         square marker\n        ``'p'``         pentagon marker\n        ``'P'``         plus (filled) marker\n        ``'*'``         star marker\n        ``'h'``         hexagon1 marker\n        ``'H'``         hexagon2 marker\n        ``'+'``         plus marker\n        ``'x'``         x marker\n        ``'X'``         x (filled) marker\n        ``'D'``         diamond marker\n        ``'d'``         thin_diamond marker\n        ``'|'``         vline marker\n        ``'_'``         hline marker\n        =============   ===============================\n\n        **Line Styles**\n\n        =============    ===============================\n        character        description\n        =============    ===============================\n        ``'-'``          solid line style\n        ``'--'``         dashed line style\n        ``'-.'``         dash-dot line style\n        ``':'``          dotted line style\n        =============    ===============================\n\n        Example format strings::\n\n            'b'    # blue markers with default shape\n            'or'   # red circles\n            '-g'   # green solid line\n            '--'   # dashed line with default color\n            '^k:'  # black triangle_up markers connected by a dotted line\n\n        **Colors**\n\n        The supported color abbreviations are the single letter codes\n\n        =============    ===============================\n        character        color\n        =============    ===============================\n        ``'b'``          blue\n        ``'g'``          green\n        ``'r'``          red\n        ``'c'``          cyan\n        ``'m'``          magenta\n        ``'y'``          yellow\n        ``'k'``          black\n        ``'w'``          white\n        =============    ===============================\n\n        and the ``'CN'`` colors that index into the default property cycle.\n\n        If the color is the only part of the format string, you can\n        additionally use any  `matplotlib.colors` spec, e.g. full names\n        (``'green'``) or hex strings (``'#008000'``).\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 1799, "code": "    def loglog(self, *args, **kwargs):\n        dx = {k: v for k, v in kwargs.items()\n              if k in ['base', 'subs', 'nonpositive',\n                       'basex', 'subsx', 'nonposx']}\n        self.set_xscale('log', **dx)\n        dy = {k: v for k, v in kwargs.items()\n              if k in ['base', 'subs', 'nonpositive',\n                       'basey', 'subsy', 'nonposy']}\n        self.set_yscale('log', **dy)\n        return self.plot(\n            *args, **{k: v for k, v in kwargs.items() if k not in {*dx, *dy}})", "documentation": "        \"\"\"\n        Make a plot with log scaling on both the x- and y-axis.\n\n        Call signatures::\n\n            loglog([x], y, [fmt], data=None, **kwargs)\n            loglog([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\n        This is just a thin wrapper around `.plot` which additionally changes\n        both the x-axis and the y-axis to log scaling. All the concepts and\n        parameters of plot can be used here as well.\n\n        The additional parameters *base*, *subs* and *nonpositive* control the\n        x/y-axis properties. They are just forwarded to `.Axes.set_xscale` and\n        `.Axes.set_yscale`. To use different properties on the x-axis and the\n        y-axis, use e.g.\n        ``ax.set_xscale(\"log\", base=10); ax.set_yscale(\"log\", base=2)``.\n\n        Parameters\n        ----------\n        base : float, default: 10\n            Base of the logarithm.\n\n        subs : sequence, optional\n            The location of the minor ticks. If *None*, reasonable locations\n            are automatically chosen depending on the number of decades in the\n            plot. See `.Axes.set_xscale`/`.Axes.set_yscale` for details.\n\n        nonpositive : {'mask', 'clip'}, default: 'clip'\n            Non-positive values can be masked as invalid, or clipped to a very\n            small positive number.\n\n        **kwargs\n            All parameters supported by `.plot`.\n\n        Returns\n        -------\n        list of `.Line2D`\n            Objects representing the plotted data.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 1853, "code": "    def semilogx(self, *args, **kwargs):\n        d = {k: v for k, v in kwargs.items()\n             if k in ['base', 'subs', 'nonpositive',\n                      'basex', 'subsx', 'nonposx']}\n        self.set_xscale('log', **d)\n        return self.plot(\n            *args, **{k: v for k, v in kwargs.items() if k not in d})\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Make a plot with log scaling on the x-axis.\n\n        Call signatures::\n\n            semilogx([x], y, [fmt], data=None, **kwargs)\n            semilogx([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\n        This is just a thin wrapper around `.plot` which additionally changes\n        the x-axis to log scaling. All the concepts and parameters of plot can\n        be used here as well.\n\n        The additional parameters *base*, *subs*, and *nonpositive* control the\n        x-axis properties. They are just forwarded to `.Axes.set_xscale`.\n\n        Parameters\n        ----------\n        base : float, default: 10\n            Base of the x logarithm.\n\n        subs : array-like, optional\n            The location of the minor xticks. If *None*, reasonable locations\n            are automatically chosen depending on the number of decades in the\n            plot. See `.Axes.set_xscale` for details.\n\n        nonpositive : {'mask', 'clip'}, default: 'clip'\n            Non-positive values in x can be masked as invalid, or clipped to a\n            very small positive number.\n\n        **kwargs\n            All parameters supported by `.plot`.\n\n        Returns\n        -------\n        list of `.Line2D`\n            Objects representing the plotted data.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 1900, "code": "    def semilogy(self, *args, **kwargs):\n        d = {k: v for k, v in kwargs.items()\n             if k in ['base', 'subs', 'nonpositive',\n                      'basey', 'subsy', 'nonposy']}\n        self.set_yscale('log', **d)\n        return self.plot(\n            *args, **{k: v for k, v in kwargs.items() if k not in d})\n    @_preprocess_data(replace_names=[\"x\"], label_namer=\"x\")", "documentation": "        \"\"\"\n        Make a plot with log scaling on the y-axis.\n\n        Call signatures::\n\n            semilogy([x], y, [fmt], data=None, **kwargs)\n            semilogy([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\n        This is just a thin wrapper around `.plot` which additionally changes\n        the y-axis to log scaling. All the concepts and parameters of plot can\n        be used here as well.\n\n        The additional parameters *base*, *subs*, and *nonpositive* control the\n        y-axis properties. They are just forwarded to `.Axes.set_yscale`.\n\n        Parameters\n        ----------\n        base : float, default: 10\n            Base of the y logarithm.\n\n        subs : array-like, optional\n            The location of the minor yticks. If *None*, reasonable locations\n            are automatically chosen depending on the number of decades in the\n            plot. See `.Axes.set_yscale` for details.\n\n        nonpositive : {'mask', 'clip'}, default: 'clip'\n            Non-positive values in y can be masked as invalid, or clipped to a\n            very small positive number.\n\n        **kwargs\n            All parameters supported by `.plot`.\n\n        Returns\n        -------\n        list of `.Line2D`\n            Objects representing the plotted data.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 1946, "code": "    def acorr(self, x, **kwargs):\n        return self.xcorr(x, x, **kwargs)\n    @_api.make_keyword_only(\"3.10\", \"normed\")\n    @_preprocess_data(replace_names=[\"x\", \"y\"], label_namer=\"y\")", "documentation": "        \"\"\"\n        Plot the autocorrelation of *x*.\n\n        Parameters\n        ----------\n        x : array-like\n            Not run through Matplotlib's unit conversion, so this should\n            be a unit-less array.\n\n        detrend : callable, default: `.mlab.detrend_none` (no detrending)\n            A detrending function applied to *x*.  It must have the\n            signature ::\n\n                detrend(x: np.ndarray) -> np.ndarray\n\n        normed : bool, default: True\n            If ``True``, input vectors are normalised to unit length.\n\n        usevlines : bool, default: True\n            Determines the plot style.\n\n            If ``True``, vertical lines are plotted from 0 to the acorr value\n            using `.Axes.vlines`. Additionally, a horizontal line is plotted\n            at y=0 using `.Axes.axhline`.\n\n            If ``False``, markers are plotted at the acorr values using\n            `.Axes.plot`.\n\n        maxlags : int, default: 10\n            Number of lags to show. If ``None``, will return all\n            ``2 * len(x) - 1`` lags.\n\n        Returns\n        -------\n        lags : array (length ``2*maxlags+1``)\n            The lag vector.\n        c : array  (length ``2*maxlags+1``)\n            The auto correlation vector.\n        line : `.LineCollection` or `.Line2D`\n            `.Artist` added to the Axes of the correlation:\n\n            - `.LineCollection` if *usevlines* is True.\n            - `.Line2D` if *usevlines* is False.\n        b : `~matplotlib.lines.Line2D` or None\n            Horizontal line at 0 if *usevlines* is True\n            None *usevlines* is False.\n\n        Other Parameters\n        ----------------\n        linestyle : `~matplotlib.lines.Line2D` property, optional\n            The linestyle for plotting the data points.\n            Only used if *usevlines* is ``False``.\n\n        marker : str, default: 'o'\n            The marker for plotting the data points.\n            Only used if *usevlines* is ``False``.\n\n        data : indexable object, optional\n            DATA_PARAMETER_PLACEHOLDER\n\n        **kwargs\n            Additional parameters are passed to `.Axes.vlines` and\n            `.Axes.axhline` if *usevlines* is ``True``; otherwise they are\n            passed to `.Axes.plot`.\n\n        Notes\n        -----\n        The cross correlation is performed with `numpy.correlate` with\n        ``mode = \"full\"``.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 2134, "code": "    def step(self, x, y, *args, where='pre', data=None, **kwargs):\n        _api.check_in_list(('pre', 'post', 'mid'), where=where)\n        kwargs['drawstyle'] = 'steps-' + where\n        return self.plot(x, y, *args, data=data, **kwargs)\n    @staticmethod", "documentation": "        \"\"\"\n        Make a step plot.\n\n        Call signatures::\n\n            step(x, y, [fmt], *, data=None, where='pre', **kwargs)\n            step(x, y, [fmt], x2, y2, [fmt2], ..., *, where='pre', **kwargs)\n\n        This is just a thin wrapper around `.plot` which changes some\n        formatting options. Most of the concepts and parameters of plot can be\n        used here as well.\n\n        .. note::\n\n            This method uses a standard plot with a step drawstyle: The *x*\n            values are the reference positions and steps extend left/right/both\n            directions depending on *where*.\n\n            For the common case where you know the values and edges of the\n            steps, use `~.Axes.stairs` instead.\n\n        Parameters\n        ----------\n        x : array-like\n            1D sequence of x positions. It is assumed, but not checked, that\n            it is uniformly increasing.\n\n        y : array-like\n            1D sequence of y levels.\n\n        fmt : str, optional\n            A format string, e.g. 'g' for a green line. See `.plot` for a more\n            detailed description.\n\n            Note: While full format strings are accepted, it is recommended to\n            only specify the color. Line styles are currently ignored (use\n            the keyword argument *linestyle* instead). Markers are accepted\n            and plotted on the given positions, however, this is a rarely\n            needed feature for step plots.\n\n        where : {'pre', 'post', 'mid'}, default: 'pre'\n            Define where the steps should be placed:\n\n            - 'pre': The y value is continued constantly to the left from\n              every *x* position, i.e. the interval ``(x[i-1], x[i]]`` has the\n              value ``y[i]``.\n            - 'post': The y value is continued constantly to the right from\n              every *x* position, i.e. the interval ``[x[i], x[i+1])`` has the\n              value ``y[i]``.\n            - 'mid': Steps occur half-way between the *x* positions.\n\n        data : indexable object, optional\n            An object with labelled data. If given, provide the label names to\n            plot in *x* and *y*.\n\n        **kwargs\n            Additional parameters are the same as those for `.plot`.\n\n        Returns\n        -------\n        list of `.Line2D`\n            Objects representing the plotted data.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 2203, "code": "    def _convert_dx(dx, x0, xconv, convert):\n        assert type(xconv) is np.ndarray\n        if xconv.size == 0:\n            return convert(dx)\n        try:\n            try:\n                x0 = cbook._safe_first_finite(x0)\n            except (TypeError, IndexError, KeyError):\n                pass\n            try:\n                x = cbook._safe_first_finite(xconv)", "documentation": "        \"\"\"\n        Small helper to do logic of width conversion flexibly.\n\n        *dx* and *x0* have units, but *xconv* has already been converted\n        to unitless (and is an ndarray).  This allows the *dx* to have units\n        that are different from *x0*, but are still accepted by the\n        ``__add__`` operator of *x0*.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 2254, "code": "    def _parse_bar_color_args(self, kwargs):\n        color = kwargs.pop('color', None)\n        facecolor = kwargs.pop('facecolor', color)\n        edgecolor = kwargs.pop('edgecolor', None)\n        facecolor = (facecolor if facecolor is not None\n                     else self._get_patches_for_fill.get_next_color())\n        try:\n            facecolor = mcolors.to_rgba_array(facecolor)\n        except ValueError as err:\n            raise ValueError(\n                \"'facecolor' or 'color' argument must be a valid color or \"", "documentation": "        \"\"\"\n        Helper function to process color-related arguments of `.Axes.bar`.\n\n        Argument precedence for facecolors:\n\n        - kwargs['facecolor']\n        - kwargs['color']\n        - 'Result of ``self._get_patches_for_fill.get_next_color``\n\n        Argument precedence for edgecolors:\n\n        - kwargs['edgecolor']\n        - None\n\n        Parameters\n        ----------\n        self : Axes\n\n        kwargs : dict\n            Additional kwargs. If these keys exist, we pop and process them:\n            'facecolor', 'edgecolor', 'color'\n            Note: The dict is modified by this function.\n\n\n        Returns\n        -------\n        facecolor\n            The facecolor. One or more colors as (N, 4) rgba array.\n        edgecolor\n            The edgecolor. Not normalized; may be any valid color spec or None.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 2968, "code": "    def broken_barh(self, xranges, yrange, align=\"bottom\", **kwargs):\n        xdata = cbook._safe_first_finite(xranges) if len(xranges) else None\n        ydata = cbook._safe_first_finite(yrange) if len(yrange) else None\n        self._process_unit_info(\n            [(\"x\", xdata), (\"y\", ydata)], kwargs, convert=False)\n        vertices = []\n        y0, dy = yrange\n        _api.check_in_list(['bottom', 'center', 'top'], align=align)\n        if align == \"bottom\":\n            y0, y1 = self.convert_yunits((y0, y0 + dy))\n        elif align == \"center\":", "documentation": "        \"\"\"\n        Plot a horizontal sequence of rectangles.\n\n        A rectangle is drawn for each element of *xranges*. All rectangles\n        have the same vertical position and size defined by *yrange*.\n\n        Parameters\n        ----------\n        xranges : sequence of tuples (*xmin*, *xwidth*)\n            The x-positions and extents of the rectangles. For each tuple\n            (*xmin*, *xwidth*) a rectangle is drawn from *xmin* to *xmin* +\n            *xwidth*.\n        yrange : (*ypos*, *yheight*)\n            The y-position and extent for all the rectangles.\n        align : {\"bottom\", \"center\", \"top\"}, default: 'bottom'\n            The alignment of the yrange with respect to the y-position. One of:\n\n            - \"bottom\": Resulting y-range [ypos, ypos + yheight]\n            - \"center\": Resulting y-range [ypos - yheight/2, ypos + yheight/2]\n            - \"top\": Resulting y-range [ypos - yheight, ypos]\n\n            .. versionadded:: 3.11\n\n        Returns\n        -------\n        `~.collections.PolyCollection`\n\n        Other Parameters\n        ----------------\n        data : indexable object, optional\n            DATA_PARAMETER_PLACEHOLDER\n        **kwargs : `.PolyCollection` properties\n\n            Each *kwarg* can be either a single argument applying to all\n            rectangles, e.g.::\n\n                facecolors='black'\n\n            or a sequence of arguments over which is cycled, e.g.::\n\n                facecolors=('black', 'blue')\n\n            would create interleaving black and blue rectangles.\n\n            Supported keywords:\n\n            %(PolyCollection:kwdoc)s\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 3769, "code": "    def _errorevery_to_mask(x, errorevery):\n        if isinstance(errorevery, Integral):\n            errorevery = (0, errorevery)\n        if isinstance(errorevery, tuple):\n            if (len(errorevery) == 2 and\n                    isinstance(errorevery[0], Integral) and\n                    isinstance(errorevery[1], Integral)):\n                errorevery = slice(errorevery[0], None, errorevery[1])\n            else:\n                raise ValueError(\n                    f'{errorevery=!r} is a not a tuple of two integers')", "documentation": "        \"\"\"\n        Normalize `errorbar`'s *errorevery* to be a boolean mask for data *x*.\n\n        This function is split out to be usable both by 2D and 3D errorbars.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 3959, "code": "        def _upcast_err(err):\n            if (\n                    np.iterable(err) and\n                    len(err) > 0 and\n                    isinstance(cbook._safe_first_finite(err), np.ndarray)\n            ):\n                atype = type(cbook._safe_first_finite(err))\n                if atype is np.ndarray:\n                    return np.asarray(err, dtype=object)\n                return atype(err)\n            return np.asarray(err, dtype=object)", "documentation": "            \"\"\"\n            Safely handle tuple of containers that carry units.\n\n            This function covers the case where the input to the xerr/yerr is a\n            length 2 tuple of equal length ndarray-subclasses that carry the\n            unit information in the container.\n\n            If we have a tuple of nested numpy array (subclasses), we defer\n            coercing the units to be consistent to the underlying unit\n            library (and implicitly the broadcasting).\n\n            Otherwise, fallback to casting to an object array.\n            \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 5762, "code": "    def arrow(self, x, y, dx, dy, **kwargs):\n        x = self.convert_xunits(x)\n        y = self.convert_yunits(y)\n        dx = self.convert_xunits(dx)\n        dy = self.convert_yunits(dy)\n        a = mpatches.FancyArrow(x, y, dx, dy, **kwargs)\n        self.add_patch(a)\n        self._request_autoscale_view()\n        return a\n    @_docstring.copy(mquiver.QuiverKey.__init__)", "documentation": "        \"\"\"\n        [*Discouraged*] Add an arrow to the Axes.\n\n        This draws an arrow from ``(x, y)`` to ``(x+dx, y+dy)``.\n\n        .. admonition:: Discouraged\n\n            The use of this method is discouraged because it is not guaranteed\n            that the arrow renders reasonably. For example, the resulting arrow\n            is affected by the Axes aspect ratio and limits, which may distort\n            the arrow.\n\n            Consider using `~.Axes.annotate` without a text instead, e.g. ::\n\n                ax.annotate(\"\", xytext=(0, 0), xy=(0.5, 0.5),\n                            arrowprops=dict(arrowstyle=\"->\"))\n\n        Parameters\n        ----------\n        %(FancyArrow)s\n\n        Returns\n        -------\n        `.FancyArrow`\n            The created `.FancyArrow` object.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 5818, "code": "    def quiver(self, *args, **kwargs):\n        args = self._quiver_units(args, kwargs)\n        q = mquiver.Quiver(self, *args, **kwargs)\n        self.add_collection(q)\n        return q\n    @_preprocess_data()\n    @_docstring.interpd", "documentation": "        \"\"\"%(quiver_doc)s\"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 5829, "code": "    def barbs(self, *args, **kwargs):\n        args = self._quiver_units(args, kwargs)\n        b = mquiver.Barbs(self, *args, **kwargs)\n        self.add_collection(b)\n        return b", "documentation": "        \"\"\"%(barbs_doc)s\"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 5839, "code": "    def fill(self, *args, data=None, **kwargs):\n        kwargs = cbook.normalize_kwargs(kwargs, mlines.Line2D)\n        patches = [*self._get_patches_for_fill(self, *args, data=data, **kwargs)]\n        for poly in patches:\n            self.add_patch(poly)\n        self._request_autoscale_view()\n        return patches", "documentation": "        \"\"\"\n        Plot filled polygons.\n\n        Parameters\n        ----------\n        *args : sequence of x, y, [color]\n            Each polygon is defined by the lists of *x* and *y* positions of\n            its nodes, optionally followed by a *color* specifier. See\n            :mod:`matplotlib.colors` for supported color specifiers. The\n            standard color cycle is used for polygons without a color\n            specifier.\n\n            You can plot multiple polygons by providing multiple *x*, *y*,\n            *[color]* groups.\n\n            For example, each of the following is legal::\n\n                ax.fill(x, y)                    # a polygon with default color\n                ax.fill(x, y, \"b\")               # a blue polygon\n                ax.fill(x, y, x2, y2)            # two polygons\n                ax.fill(x, y, \"b\", x2, y2, \"r\")  # a blue and a red polygon\n\n        data : indexable object, optional\n            An object with labelled data. If given, provide the label names to\n            plot in *x* and *y*, e.g.::\n\n                ax.fill(\"time\", \"signal\",\n                        data={\"time\": [0, 1, 2], \"signal\": [0, 1, 0]})\n\n        Returns\n        -------\n        list of `~matplotlib.patches.Polygon`\n\n        Other Parameters\n        ----------------\n        **kwargs : `~matplotlib.patches.Polygon` properties\n\n        Notes\n        -----\n        Use :meth:`fill_between` if you would like to fill the region between\n        two curves.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 5996, "code": "    def _fill_between_process_units(self, ind_dir, dep_dir, ind, dep1, dep2, **kwargs):\n        return map(np.ma.masked_invalid, self._process_unit_info(\n            [(ind_dir, ind), (dep_dir, dep1), (dep_dir, dep2)], kwargs))", "documentation": "        \"\"\"Handle united data, such as dates.\"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 6801, "code": "    def _update_pcolor_lims(self, collection, coords):\n        t = collection._transform\n        if (not isinstance(t, mtransforms.Transform) and\n                hasattr(t, '_as_mpl_transform')):\n            t = t._as_mpl_transform(self.axes)\n        if t and any(t.contains_branch_separately(self.transData)):\n            trans_to_data = t - self.transData\n            coords = trans_to_data.transform(coords)\n        self.add_collection(collection, autolim=False)\n        minx, miny = np.min(coords, axis=0)\n        maxx, maxy = np.max(coords, axis=0)", "documentation": "        \"\"\"\n        Common code for updating lims in pcolor() and pcolormesh() methods.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 7028, "code": "    def contour(self, *args, **kwargs):\n        kwargs['filled'] = False\n        contours = mcontour.QuadContourSet(self, *args, **kwargs)\n        self._request_autoscale_view()\n        return contours\n    @_preprocess_data()\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Plot contour lines.\n\n        Call signature::\n\n            contour([X, Y,] Z, /, [levels], **kwargs)\n\n        The arguments *X*, *Y*, *Z* are positional-only.\n        %(contour_doc)s\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 7046, "code": "    def contourf(self, *args, **kwargs):\n        kwargs['filled'] = True\n        contours = mcontour.QuadContourSet(self, *args, **kwargs)\n        self._request_autoscale_view()\n        return contours", "documentation": "        \"\"\"\n        Plot filled contours.\n\n        Call signature::\n\n            contourf([X, Y,] Z, /, [levels], **kwargs)\n\n        The arguments *X*, *Y*, *Z* are positional-only.\n        %(contour_doc)s\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 7062, "code": "    def clabel(self, CS, levels=None, **kwargs):\n        return CS.clabel(levels, **kwargs)\n    @_api.make_keyword_only(\"3.10\", \"range\")\n    @_preprocess_data(replace_names=[\"x\", 'weights'], label_namer=\"x\")", "documentation": "        \"\"\"\n        Label a contour plot.\n\n        Adds labels to line contours in given `.ContourSet`.\n\n        Parameters\n        ----------\n        CS : `.ContourSet` instance\n            Line contours to label.\n\n        levels : array-like, optional\n            A list of level values, that should be labeled. The list must be\n            a subset of ``CS.levels``. If not given, all levels are labeled.\n\n        **kwargs\n            All other parameters are documented in `~.ContourLabeler.clabel`.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 8708, "code": "    def matshow(self, Z, **kwargs):\n        Z = np.asanyarray(Z)\n        kw = {'origin': 'upper',\n              'interpolation': 'nearest',\n              'aspect': 'equal',          # (already the imshow default)\n              **kwargs}\n        im = self.imshow(Z, **kw)\n        self.title.set_y(1.05)\n        self.xaxis.tick_top()\n        self.xaxis.set_ticks_position('both')\n        self.xaxis.set_major_locator(", "documentation": "        \"\"\"\n        Plot the values of a 2D matrix or array as color-coded image.\n\n        The matrix will be shown the way it would be printed, with the first\n        row at the top.  Row and column numbering is zero-based.\n\n        Parameters\n        ----------\n        Z : (M, N) array-like\n            The matrix to be displayed.\n\n        Returns\n        -------\n        `~matplotlib.image.AxesImage`\n\n        Other Parameters\n        ----------------\n        **kwargs : `~matplotlib.axes.Axes.imshow` arguments\n\n        See Also\n        --------\n        imshow : More general function to plot data on a 2D regular raster.\n\n        Notes\n        -----\n        This is just a convenience function wrapping `.imshow` to set useful\n        defaults for displaying a matrix. In particular:\n\n        - Set ``origin='upper'``.\n        - Set ``interpolation='nearest'``.\n        - Set ``aspect='equal'``.\n        - Ticks are placed to the left and above.\n        - Ticks are formatted to show integer indices.\n\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 9183, "code": "    def _get_aspect_ratio(self):\n        figure_size = self.get_figure().get_size_inches()\n        ll, ur = self.get_position() * figure_size\n        width, height = ur - ll\n        return height / (width * self.get_data_ratio())", "documentation": "        \"\"\"\n        Convenience method to calculate the aspect ratio of the Axes in\n        the display coordinate system.\n        \"\"\""}], "after_segments": [{"filename": "lib/matplotlib/axes/_axes.py", "start_line": 48, "code": "def _make_axes_method(func):\n    func.__qualname__ = f\"Axes.{func.__name__}\"\n    return func", "documentation": "    \"\"\"\n    Patch the qualname for functions that are directly added to Axes.\n\n    Some Axes functionality is defined in functions in other submodules.\n    These are simply added as attributes to Axes. As a result, their\n    ``__qualname__`` is e.g. only \"table\" and not \"Axes.table\". This\n    function fixes that.\n\n    Note that the function itself is patched, so that\n    ``matplotlib.table.table.__qualname__` will also show \"Axes.table\".\n    However, since these functions are not intended to be standalone,\n    this is bearable.\n    \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 66, "code": "class _GroupedBarReturn:", "documentation": "    \"\"\"\n    A provisional result object for `.Axes.grouped_bar`.\n\n    This is a placeholder for a future better return type. We try to build in\n    backward compatibility / migration possibilities.\n\n    The only public interfaces are the ``bar_containers`` attribute and the\n    ``remove()`` method.\n    \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 84, "code": "class Axes(_AxesBase):", "documentation": "    \"\"\"\n    An Axes object encapsulates all the elements of an individual (sub-)plot in\n    a figure.\n\n    It contains most of the (sub-)plot elements: `~.axis.Axis`,\n    `~.axis.Tick`, `~.lines.Line2D`, `~.text.Text`, `~.patches.Polygon`, etc.,\n    and sets the coordinate system.\n\n    Like all visible elements in a figure, Axes is an `.Artist` subclass.\n\n    The `Axes` instance supports callbacks through a callbacks attribute which\n    is a `~.cbook.CallbackRegistry` instance.  The events you can connect to\n    are 'xlim_changed' and 'ylim_changed' and the callback will be called with\n    func(*ax*) where *ax* is the `Axes` instance.\n\n    .. note::\n\n        As a user, you do not instantiate Axes directly, but use Axes creation\n        methods instead; e.g. from `.pyplot` or `.Figure`:\n        `~.pyplot.subplots`, `~.pyplot.subplot_mosaic` or `.Figure.add_axes`.\n\n    \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 109, "code": "    def get_title(self, loc=\"center\"):\n        titles = {'left': self._left_title,\n                  'center': self.title,\n                  'right': self._right_title}\n        title = _api.check_getitem(titles, loc=loc.lower())\n        return title.get_text()", "documentation": "        \"\"\"\n        Get an Axes title.\n\n        Get one of the three available Axes titles. The available titles\n        are positioned above the Axes in the center, flush with the left\n        edge, and flush with the right edge.\n\n        Parameters\n        ----------\n        loc : {'center', 'left', 'right'}, str, default: 'center'\n            Which title to return.\n\n        Returns\n        -------\n        str\n            The title text string.\n\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 215, "code": "    def get_legend_handles_labels(self, legend_handler_map=None):\n        handles, labels = mlegend._get_legend_handles_labels(\n            [self], legend_handler_map)\n        return handles, labels\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Return handles and labels for legend\n\n        ``ax.legend()`` is equivalent to ::\n\n          h, l = ax.get_legend_handles_labels()\n          ax.legend(h, l)\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 230, "code": "    def legend(self, *args, **kwargs):\n        handles, labels, kwargs = mlegend._parse_legend_args([self], *args, **kwargs)\n        self.legend_ = mlegend.Legend(self, handles, labels, **kwargs)\n        self.legend_._remove_method = self._remove_legend\n        return self.legend_", "documentation": "        \"\"\"\n        Place a legend on the Axes.\n\n        Call signatures::\n\n            legend()\n            legend(handles, labels)\n            legend(handles=handles)\n            legend(labels)\n\n        The call signatures correspond to the following different ways to use\n        this method:\n\n        **1. Automatic detection of elements to be shown in the legend**\n\n        The elements to be added to the legend are automatically determined,\n        when you do not pass in any extra arguments.\n\n        In this case, the labels are taken from the artist. You can specify\n        them either at artist creation or by calling the\n        :meth:`~.Artist.set_label` method on the artist::\n\n            ax.plot([1, 2, 3], label='Inline label')\n            ax.legend()\n\n        or::\n\n            line, = ax.plot([1, 2, 3])\n            line.set_label('Label via method')\n            ax.legend()\n\n        .. note::\n            Specific artists can be excluded from the automatic legend element\n            selection by using a label starting with an underscore, \"_\".\n            A string starting with an underscore is the default label for all\n            artists, so calling `.Axes.legend` without any arguments and\n            without setting the labels manually will result in a ``UserWarning``\n            and an empty legend being drawn.\n\n\n        **2. Explicitly listing the artists and labels in the legend**\n\n        For full control of which artists have a legend entry, it is possible\n        to pass an iterable of legend artists followed by an iterable of\n        legend labels respectively::\n\n            ax.legend([line1, line2, line3], ['label1', 'label2', 'label3'])\n\n\n        **3. Explicitly listing the artists in the legend**\n\n        This is similar to 2, but the labels are taken from the artists'\n        label properties. Example::\n\n            line1, = ax.plot([1, 2, 3], label='label1')\n            line2, = ax.plot([1, 2, 3], label='label2')\n            ax.legend(handles=[line1, line2])\n\n\n        **4. Labeling existing plot elements**\n\n        .. admonition:: Discouraged\n\n            This call signature is discouraged, because the relation between\n            plot elements and labels is only implicit by their order and can\n            easily be mixed up.\n\n        To make a legend for all artists on an Axes, call this function with\n        an iterable of strings, one for each legend item. For example::\n\n            ax.plot([1, 2, 3])\n            ax.plot([5, 6, 7])\n            ax.legend(['First line', 'Second line'])\n\n\n        Parameters\n        ----------\n        handles : list of (`.Artist` or tuple of `.Artist`), optional\n            A list of Artists (lines, patches) to be added to the legend.\n            Use this together with *labels*, if you need full control on what\n            is shown in the legend and the automatic mechanism described above\n            is not sufficient.\n\n            The length of handles and labels should be the same in this\n            case. If they are not, they are truncated to the smaller length.\n\n            If an entry contains a tuple, then the legend handler for all Artists in the\n            tuple will be placed alongside a single label.\n\n        labels : list of str, optional\n            A list of labels to show next to the artists.\n            Use this together with *handles*, if you need full control on what\n            is shown in the legend and the automatic mechanism described above\n            is not sufficient.\n\n        Returns\n        -------\n        `~matplotlib.legend.Legend`\n\n        Other Parameters\n        ----------------\n        %(_legend_kw_axes)s\n\n        See Also\n        --------\n        .Figure.legend\n\n        Notes\n        -----\n        Some artists are not supported by this function.  See\n        :ref:`legend_guide` for details.\n\n        Examples\n        --------\n        .. plot:: gallery/text_labels_and_annotations/legend.py\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 355, "code": "    def inset_axes(self, bounds, *, transform=None, zorder=5, **kwargs):\n        if transform is None:\n            transform = self.transAxes\n        kwargs.setdefault('label', 'inset_axes')\n        inset_locator = _TransformedBoundsLocator(bounds, transform)\n        bounds = inset_locator(self, None).bounds\n        fig = self.get_figure(root=False)\n        projection_class, pkw = fig._process_projection_requirements(**kwargs)\n        inset_ax = projection_class(fig, bounds, zorder=zorder, **pkw)\n        inset_ax.set_axes_locator(inset_locator)\n        self.add_child_axes(inset_ax)", "documentation": "        \"\"\"\n        Add a child inset Axes to this existing Axes.\n\n\n        Parameters\n        ----------\n        bounds : [x0, y0, width, height]\n            Lower-left corner of inset Axes, and its width and height.\n\n        transform : `.Transform`\n            Defaults to `!ax.transAxes`, i.e. the units of *rect* are in\n            Axes-relative coordinates.\n\n        projection : {None, 'aitoff', 'hammer', 'lambert', 'mollweide', \\\n'polar', 'rectilinear', str}, optional\n            The projection type of the inset `~.axes.Axes`. *str* is the name\n            of a custom projection, see `~matplotlib.projections`. The default\n            None results in a 'rectilinear' projection.\n\n        polar : bool, default: False\n            If True, equivalent to projection='polar'.\n\n        axes_class : subclass type of `~.axes.Axes`, optional\n            The `.axes.Axes` subclass that is instantiated.  This parameter\n            is incompatible with *projection* and *polar*.  See\n            :ref:`axisartist_users-guide-index` for examples.\n\n        zorder : number\n            Defaults to 5 (same as `.Axes.legend`).  Adjust higher or lower\n            to change whether it is above or below data plotted on the\n            parent Axes.\n\n        **kwargs\n            Other keyword arguments are passed on to the inset Axes class.\n\n        Returns\n        -------\n        ax\n            The created `~.axes.Axes` instance.\n\n        Examples\n        --------\n        This example makes two inset Axes, the first is in Axes-relative\n        coordinates, and the second in data-coordinates::\n\n            fig, ax = plt.subplots()\n            ax.plot(range(10))\n            axin1 = ax.inset_axes([0.8, 0.1, 0.15, 0.15])\n            axin2 = ax.inset_axes(\n                    [5, 7, 2.3, 2.3], transform=ax.transData)\n\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 510, "code": "    def indicate_inset_zoom(self, inset_ax, **kwargs):\n        return self.indicate_inset(None, inset_ax, **kwargs)\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Add an inset indicator rectangle to the Axes based on the axis\n        limits for an *inset_ax* and draw connectors between *inset_ax*\n        and the rectangle.\n\n        Warnings\n        --------\n        This method is experimental as of 3.0, and the API may change.\n\n        Parameters\n        ----------\n        inset_ax : `.Axes`\n            Inset Axes to draw connecting lines to.  Two lines are\n            drawn connecting the indicator box to the inset Axes on corners\n            chosen so as to not overlap with the indicator box.\n\n        **kwargs\n            Other keyword arguments are passed on to `.Axes.indicate_inset`\n\n        Returns\n        -------\n        inset_indicator : `.inset.InsetIndicator`\n            An artist which contains\n\n            inset_indicator.rectangle : `.Rectangle`\n                The indicator frame.\n\n            inset_indicator.connectors : 4-tuple of `.patches.ConnectionPatch`\n                The four connector lines connecting to (lower_left, upper_left,\n                lower_right upper_right) corners of *inset_ax*. Two lines are\n                set with visibility to *False*,  but the user can set the\n                visibility to True if the automatic choice is not deemed correct.\n\n            .. versionchanged:: 3.10\n                Previously the rectangle and connectors tuple were returned.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 551, "code": "    def secondary_xaxis(self, location, functions=None, *, transform=None, **kwargs):\n        if not (location in ['top', 'bottom'] or isinstance(location, Real)):\n            raise ValueError('secondary_xaxis location must be either '\n                             'a float or \"top\"/\"bottom\"')\n        secondary_ax = SecondaryAxis(self, 'x', location, functions,\n                                     transform, **kwargs)\n        self.add_child_axes(secondary_ax)\n        return secondary_ax\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Add a second x-axis to this `~.axes.Axes`.\n\n        For example if we want to have a second scale for the data plotted on\n        the xaxis.\n\n        %(_secax_docstring)s\n\n        Examples\n        --------\n        The main axis shows frequency, and the secondary axis shows period.\n\n        .. plot::\n\n            fig, ax = plt.subplots()\n            ax.loglog(range(1, 360, 5), range(1, 360, 5))\n            ax.set_xlabel('frequency [Hz]')\n\n            def invert(x):\n                # 1/x with special treatment of x == 0\n                x = np.array(x).astype(float)\n                near_zero = np.isclose(x, 0)\n                x[near_zero] = np.inf\n                x[~near_zero] = 1 / x[~near_zero]\n                return x\n\n            # the inverse of 1/x is itself\n            secax = ax.secondary_xaxis('top', functions=(invert, invert))\n            secax.set_xlabel('Period [s]')\n            plt.show()\n\n        To add a secondary axis relative to your data, you can pass a transform\n        to the new axis.\n\n        .. plot::\n\n            fig, ax = plt.subplots()\n            ax.plot(range(0, 5), range(-1, 4))\n\n            # Pass 'ax.transData' as a transform to place the axis\n            # relative to your data at y=0\n            secax = ax.secondary_xaxis(0, transform=ax.transData)\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 605, "code": "    def secondary_yaxis(self, location, functions=None, *, transform=None, **kwargs):\n        if not (location in ['left', 'right'] or isinstance(location, Real)):\n            raise ValueError('secondary_yaxis location must be either '\n                             'a float or \"left\"/\"right\"')\n        secondary_ax = SecondaryAxis(self, 'y', location, functions,\n                                     transform, **kwargs)\n        self.add_child_axes(secondary_ax)\n        return secondary_ax\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Add a second y-axis to this `~.axes.Axes`.\n\n        For example if we want to have a second scale for the data plotted on\n        the yaxis.\n\n        %(_secax_docstring)s\n\n        Examples\n        --------\n        Add a secondary Axes that converts from radians to degrees\n\n        .. plot::\n\n            fig, ax = plt.subplots()\n            ax.plot(range(1, 360, 5), range(1, 360, 5))\n            ax.set_ylabel('degrees')\n            secax = ax.secondary_yaxis('right', functions=(np.deg2rad,\n                                                           np.rad2deg))\n            secax.set_ylabel('radians')\n\n        To add a secondary axis relative to your data, you can pass a transform\n        to the new axis.\n\n        .. plot::\n\n            fig, ax = plt.subplots()\n            ax.plot(range(0, 5), range(-1, 4))\n\n            # Pass 'ax.transData' as a transform to place the axis\n            # relative to your data at x=3\n            secax = ax.secondary_yaxis(3, transform=ax.transData)\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 649, "code": "    def text(self, x, y, s, fontdict=None, **kwargs):\n        effective_kwargs = {\n            'verticalalignment': 'baseline',\n            'horizontalalignment': 'left',\n            'transform': self.transData,\n            'clip_on': False,\n            **(fontdict if fontdict is not None else {}),\n            **kwargs,\n        }\n        t = mtext.Text(x, y, text=s, **effective_kwargs)\n        if t.get_clip_path() is None:", "documentation": "        \"\"\"\n        Add text to the Axes.\n\n        Add the text *s* to the Axes at location *x*, *y* in data coordinates,\n        with a default ``horizontalalignment`` on the ``left`` and\n        ``verticalalignment`` at the ``baseline``. See\n        :doc:`/gallery/text_labels_and_annotations/text_alignment`.\n\n        Parameters\n        ----------\n        x, y : float\n            The position to place the text. By default, this is in data\n            coordinates. The coordinate system can be changed using the\n            *transform* parameter.\n\n        s : str\n            The text.\n\n        fontdict : dict, default: None\n\n            .. admonition:: Discouraged\n\n               The use of *fontdict* is discouraged. Parameters should be passed as\n               individual keyword arguments or using dictionary-unpacking\n               ``text(..., **fontdict)``.\n\n            A dictionary to override the default text properties. If fontdict\n            is None, the defaults are determined by `.rcParams`.\n\n        Returns\n        -------\n        `.Text`\n            The created `.Text` instance.\n\n        Other Parameters\n        ----------------\n        **kwargs : `~matplotlib.text.Text` properties.\n            Other miscellaneous text parameters.\n\n            %(Text:kwdoc)s\n\n        Examples\n        --------\n        Individual keyword arguments can be used to override any given\n        parameter::\n\n            >>> text(x, y, s, fontsize=12)\n\n        The default transform specifies that text is in data coords,\n        alternatively, you can specify text in axis coords ((0, 0) is\n        lower-left and (1, 1) is upper-right).  The example below places\n        text in the center of the Axes::\n\n            >>> text(0.5, 0.5, 'matplotlib', horizontalalignment='center',\n            ...      verticalalignment='center', transform=ax.transAxes)\n\n        You can put a rectangular box around the text instance (e.g., to\n        set a background color) by using the keyword *bbox*.  *bbox* is\n        a dictionary of `~matplotlib.patches.Rectangle`\n        properties.  For example::\n\n            >>> text(x, y, s, bbox=dict(facecolor='red', alpha=0.5))\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 744, "code": "    def axhline(self, y=0, xmin=0, xmax=1, **kwargs):\n        self._check_no_units([xmin, xmax], ['xmin', 'xmax'])\n        if \"transform\" in kwargs:\n            raise ValueError(\"'transform' is not allowed as a keyword \"\n                             \"argument; axhline generates its own transform.\")\n        ymin, ymax = self.get_ybound()\n        yy, = self._process_unit_info([(\"y\", y)], kwargs)\n        scaley = (yy < ymin) or (yy > ymax)\n        trans = self.get_yaxis_transform(which='grid')\n        l = mlines.Line2D([xmin, xmax], [y, y], transform=trans, **kwargs)\n        self.add_line(l)", "documentation": "        \"\"\"\n        Add a horizontal line spanning the whole or fraction of the Axes.\n\n        Note: If you want to set x-limits in data coordinates, use\n        `~.Axes.hlines` instead.\n\n        Parameters\n        ----------\n        y : float, default: 0\n            y position in :ref:`data coordinates <coordinate-systems>`.\n\n        xmin : float, default: 0\n            The start x-position in :ref:`axes coordinates <coordinate-systems>`.\n            Should be between 0 and 1, 0 being the far left of the plot,\n            1 the far right of the plot.\n\n        xmax : float, default: 1\n            The end x-position in :ref:`axes coordinates <coordinate-systems>`.\n            Should be between 0 and 1, 0 being the far left of the plot,\n            1 the far right of the plot.\n\n        Returns\n        -------\n        `~matplotlib.lines.Line2D`\n            A `.Line2D` specified via two points ``(xmin, y)``, ``(xmax, y)``.\n            Its transform is set such that *x* is in\n            :ref:`axes coordinates <coordinate-systems>` and *y* is in\n            :ref:`data coordinates <coordinate-systems>`.\n\n            This is still a generic line and the horizontal character is only\n            realized through using identical *y* values for both points. Thus,\n            if you want to change the *y* value later, you have to provide two\n            values ``line.set_ydata([3, 3])``.\n\n        Other Parameters\n        ----------------\n        **kwargs\n            Valid keyword arguments are `.Line2D` properties, except for\n            'transform':\n\n            %(Line2D:kwdoc)s\n\n        See Also\n        --------\n        hlines : Add horizontal lines in data coordinates.\n        axhspan : Add a horizontal span (rectangle) across the axis.\n        axline : Add a line with an arbitrary slope.\n\n        Examples\n        --------\n        * draw a thick red hline at 'y' = 0 that spans the xrange::\n\n            >>> axhline(linewidth=4, color='r')\n\n        * draw a default hline at 'y' = 1 that spans the xrange::\n\n            >>> axhline(y=1)\n\n        * draw a default hline at 'y' = .5 that spans the middle half of\n          the xrange::\n\n            >>> axhline(y=.5, xmin=0.25, xmax=0.75)\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 827, "code": "    def axvline(self, x=0, ymin=0, ymax=1, **kwargs):\n        self._check_no_units([ymin, ymax], ['ymin', 'ymax'])\n        if \"transform\" in kwargs:\n            raise ValueError(\"'transform' is not allowed as a keyword \"\n                             \"argument; axvline generates its own transform.\")\n        xmin, xmax = self.get_xbound()\n        xx, = self._process_unit_info([(\"x\", x)], kwargs)\n        scalex = (xx < xmin) or (xx > xmax)\n        trans = self.get_xaxis_transform(which='grid')\n        l = mlines.Line2D([x, x], [ymin, ymax], transform=trans, **kwargs)\n        self.add_line(l)", "documentation": "        \"\"\"\n        Add a vertical line spanning the whole or fraction of the Axes.\n\n        Note: If you want to set y-limits in data coordinates, use\n        `~.Axes.vlines` instead.\n\n        Parameters\n        ----------\n        x : float, default: 0\n            x position in :ref:`data coordinates <coordinate-systems>`.\n\n        ymin : float, default: 0\n            The start y-position in :ref:`axes coordinates <coordinate-systems>`.\n            Should be between 0 and 1, 0 being the bottom of the plot, 1 the\n            top of the plot.\n\n        ymax : float, default: 1\n            The end y-position in :ref:`axes coordinates <coordinate-systems>`.\n            Should be between 0 and 1, 0 being the bottom of the plot, 1 the\n            top of the plot.\n\n        Returns\n        -------\n        `~matplotlib.lines.Line2D`\n            A `.Line2D` specified via two points ``(x, ymin)``, ``(x, ymax)``.\n            Its transform is set such that *x* is in\n            :ref:`data coordinates <coordinate-systems>` and *y* is in\n            :ref:`axes coordinates <coordinate-systems>`.\n\n            This is still a generic line and the vertical character is only\n            realized through using identical *x* values for both points. Thus,\n            if you want to change the *x* value later, you have to provide two\n            values ``line.set_xdata([3, 3])``.\n\n        Other Parameters\n        ----------------\n        **kwargs\n            Valid keyword arguments are `.Line2D` properties, except for\n            'transform':\n\n            %(Line2D:kwdoc)s\n\n        See Also\n        --------\n        vlines : Add vertical lines in data coordinates.\n        axvspan : Add a vertical span (rectangle) across the axis.\n        axline : Add a line with an arbitrary slope.\n\n        Examples\n        --------\n        * draw a thick red vline at *x* = 0 that spans the yrange::\n\n            >>> axvline(linewidth=4, color='r')\n\n        * draw a default vline at *x* = 1 that spans the yrange::\n\n            >>> axvline(x=1)\n\n        * draw a default vline at *x* = .5 that spans the middle half of\n          the yrange::\n\n            >>> axvline(x=.5, ymin=0.25, ymax=0.75)\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 918, "code": "    def axline(self, xy1, xy2=None, *, slope=None, **kwargs):\n        if slope is not None and (self.get_xscale() != 'linear' or\n                                  self.get_yscale() != 'linear'):\n            raise TypeError(\"'slope' cannot be used with non-linear scales\")\n        datalim = [xy1] if xy2 is None else [xy1, xy2]\n        if \"transform\" in kwargs:\n            datalim = []\n        line = mlines.AxLine(xy1, xy2, slope, **kwargs)\n        self._set_artist_props(line)\n        if line.get_clip_path() is None:\n            line.set_clip_path(self.patch)", "documentation": "        \"\"\"\n        Add an infinitely long straight line.\n\n        The line can be defined either by two points *xy1* and *xy2*, or\n        by one point *xy1* and a *slope*.\n\n        This draws a straight line \"on the screen\", regardless of the x and y\n        scales, and is thus also suitable for drawing exponential decays in\n        semilog plots, power laws in loglog plots, etc. However, *slope*\n        should only be used with linear scales; It has no clear meaning for\n        all other scales, and thus the behavior is undefined. Please specify\n        the line using the points *xy1*, *xy2* for non-linear scales.\n\n        The *transform* keyword argument only applies to the points *xy1*,\n        *xy2*. The *slope* (if given) is always in data coordinates. This can\n        be used e.g. with ``ax.transAxes`` for drawing grid lines with a fixed\n        slope.\n\n        Parameters\n        ----------\n        xy1, xy2 : (float, float)\n            Points for the line to pass through.\n            Either *xy2* or *slope* has to be given.\n        slope : float, optional\n            The slope of the line. Either *xy2* or *slope* has to be given.\n\n        Returns\n        -------\n        `.AxLine`\n\n        Other Parameters\n        ----------------\n        **kwargs\n            Valid kwargs are `.Line2D` properties\n\n            %(Line2D:kwdoc)s\n\n        See Also\n        --------\n        axhline : for horizontal lines\n        axvline : for vertical lines\n\n        Examples\n        --------\n        Draw a thick red line passing through (0, 0) and (1, 1)::\n\n            >>> axline((0, 0), (1, 1), linewidth=4, color='r')\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 992, "code": "    def axhspan(self, ymin, ymax, xmin=0, xmax=1, **kwargs):\n        self._check_no_units([xmin, xmax], ['xmin', 'xmax'])\n        (ymin, ymax), = self._process_unit_info([(\"y\", [ymin, ymax])], kwargs)\n        p = mpatches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, **kwargs)\n        p.set_transform(self.get_yaxis_transform(which=\"grid\"))\n        ix = self.dataLim.intervalx.copy()\n        mx = self.dataLim.minposx\n        self.add_patch(p)\n        self.dataLim.intervalx = ix\n        self.dataLim.minposx = mx\n        p.get_path()._interpolation_steps = mpl.axis.GRIDLINE_INTERPOLATION_STEPS", "documentation": "        \"\"\"\n        Add a horizontal span (rectangle) across the Axes.\n\n        The rectangle spans from *ymin* to *ymax* vertically, and, by default,\n        the whole x-axis horizontally.  The x-span can be set using *xmin*\n        (default: 0) and *xmax* (default: 1) which are in axis units; e.g.\n        ``xmin = 0.5`` always refers to the middle of the x-axis regardless of\n        the limits set by `~.Axes.set_xlim`.\n\n        Parameters\n        ----------\n        ymin : float\n            Lower y-coordinate of the span, in data units.\n        ymax : float\n            Upper y-coordinate of the span, in data units.\n        xmin : float, default: 0\n            Lower x-coordinate of the span, in x-axis (0-1) units.\n        xmax : float, default: 1\n            Upper x-coordinate of the span, in x-axis (0-1) units.\n\n        Returns\n        -------\n        `~matplotlib.patches.Rectangle`\n            Horizontal span (rectangle) from (xmin, ymin) to (xmax, ymax).\n\n        Other Parameters\n        ----------------\n        **kwargs : `~matplotlib.patches.Rectangle` properties\n\n        %(Rectangle:kwdoc)s\n\n        See Also\n        --------\n        axvspan : Add a vertical span across the Axes.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 1047, "code": "    def axvspan(self, xmin, xmax, ymin=0, ymax=1, **kwargs):\n        self._check_no_units([ymin, ymax], ['ymin', 'ymax'])\n        (xmin, xmax), = self._process_unit_info([(\"x\", [xmin, xmax])], kwargs)\n        p = mpatches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, **kwargs)\n        p.set_transform(self.get_xaxis_transform(which=\"grid\"))\n        iy = self.dataLim.intervaly.copy()\n        my = self.dataLim.minposy\n        self.add_patch(p)\n        self.dataLim.intervaly = iy\n        self.dataLim.minposy = my\n        p.get_path()._interpolation_steps = mpl.axis.GRIDLINE_INTERPOLATION_STEPS", "documentation": "        \"\"\"\n        Add a vertical span (rectangle) across the Axes.\n\n        The rectangle spans from *xmin* to *xmax* horizontally, and, by\n        default, the whole y-axis vertically.  The y-span can be set using\n        *ymin* (default: 0) and *ymax* (default: 1) which are in axis units;\n        e.g. ``ymin = 0.5`` always refers to the middle of the y-axis\n        regardless of the limits set by `~.Axes.set_ylim`.\n\n        Parameters\n        ----------\n        xmin : float\n            Lower x-coordinate of the span, in data units.\n        xmax : float\n            Upper x-coordinate of the span, in data units.\n        ymin : float, default: 0\n            Lower y-coordinate of the span, in y-axis units (0-1).\n        ymax : float, default: 1\n            Upper y-coordinate of the span, in y-axis units (0-1).\n\n        Returns\n        -------\n        `~matplotlib.patches.Rectangle`\n            Vertical span (rectangle) from (xmin, ymin) to (xmax, ymax).\n\n        Other Parameters\n        ----------------\n        **kwargs : `~matplotlib.patches.Rectangle` properties\n\n        %(Rectangle:kwdoc)s\n\n        See Also\n        --------\n        axhspan : Add a horizontal span across the Axes.\n\n        Examples\n        --------\n        Draw a vertical, green, translucent rectangle from x = 1.25 to\n        x = 1.55 that spans the yrange of the Axes.\n\n        >>> axvspan(1.25, 1.55, facecolor='g', alpha=0.5)\n\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 1544, "code": "    def plot(self, *args, scalex=True, scaley=True, data=None, **kwargs):\n        kwargs = cbook.normalize_kwargs(kwargs, mlines.Line2D)\n        lines = [*self._get_lines(self, *args, data=data, **kwargs)]\n        for line in lines:\n            self.add_line(line)\n        if scalex:\n            self._request_autoscale_view(\"x\")\n        if scaley:\n            self._request_autoscale_view(\"y\")\n        return lines\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Plot y versus x as lines and/or markers.\n\n        Call signatures::\n\n            plot([x], y, [fmt], *, data=None, **kwargs)\n            plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\n        The coordinates of the points or line nodes are given by *x*, *y*.\n\n        The optional parameter *fmt* is a convenient way for defining basic\n        formatting like color, marker and linestyle. It's a shortcut string\n        notation described in the *Notes* section below.\n\n        >>> plot(x, y)        # plot x and y using default line style and color\n        >>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n        >>> plot(y)           # plot y using x as index array 0..N-1\n        >>> plot(y, 'r+')     # ditto, but with red plusses\n\n        You can use `.Line2D` properties as keyword arguments for more\n        control on the appearance. Line properties and *fmt* can be mixed.\n        The following two calls yield identical results:\n\n        >>> plot(x, y, 'go--', linewidth=2, markersize=12)\n        >>> plot(x, y, color='green', marker='o', linestyle='dashed',\n        ...      linewidth=2, markersize=12)\n\n        When conflicting with *fmt*, keyword arguments take precedence.\n\n\n        **Plotting labelled data**\n\n        There's a convenient way for plotting objects with labelled data (i.e.\n        data that can be accessed by index ``obj['y']``). Instead of giving\n        the data in *x* and *y*, you can provide the object in the *data*\n        parameter and just give the labels for *x* and *y*::\n\n        >>> plot('xlabel', 'ylabel', data=obj)\n\n        All indexable objects are supported. This could e.g. be a `dict`, a\n        `pandas.DataFrame` or a structured numpy array.\n\n\n        **Plotting multiple sets of data**\n\n        There are various ways to plot multiple sets of data.\n\n        - The most straight forward way is just to call `plot` multiple times.\n          Example:\n\n          >>> plot(x1, y1, 'bo')\n          >>> plot(x2, y2, 'go')\n\n        - If *x* and/or *y* are 2D arrays, a separate data set will be drawn\n          for every column. If both *x* and *y* are 2D, they must have the\n          same shape. If only one of them is 2D with shape (N, m) the other\n          must have length N and will be used for every data set m.\n\n          Example:\n\n          >>> x = [1, 2, 3]\n          >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n          >>> plot(x, y)\n\n          is equivalent to:\n\n          >>> for col in range(y.shape[1]):\n          ...     plot(x, y[:, col])\n\n        - The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n          groups::\n\n          >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n\n          In this case, any additional keyword argument applies to all\n          datasets. Also, this syntax cannot be combined with the *data*\n          parameter.\n\n        By default, each line is assigned a different style specified by a\n        'style cycle'. The *fmt* and line property parameters are only\n        necessary if you want explicit deviations from these defaults.\n        Alternatively, you can also change the style cycle using\n        :rc:`axes.prop_cycle`.\n\n\n        Parameters\n        ----------\n        x, y : array-like or float\n            The horizontal / vertical coordinates of the data points.\n            *x* values are optional and default to ``range(len(y))``.\n\n            Commonly, these parameters are 1D arrays.\n\n            They can also be scalars, or two-dimensional (in that case, the\n            columns represent separate data sets).\n\n            These arguments cannot be passed as keywords.\n\n        fmt : str, optional\n            A format string, e.g. 'ro' for red circles. See the *Notes*\n            section for a full description of the format strings.\n\n            Format strings are just an abbreviation for quickly setting\n            basic line properties. All of these and more can also be\n            controlled by keyword arguments.\n\n            This argument cannot be passed as keyword.\n\n        data : indexable object, optional\n            An object with labelled data. If given, provide the label names to\n            plot in *x* and *y*.\n\n            .. note::\n                Technically there's a slight ambiguity in calls where the\n                second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n                could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n                the former interpretation is chosen, but a warning is issued.\n                You may suppress the warning by adding an empty format string\n                ``plot('n', 'o', '', data=obj)``.\n\n        Returns\n        -------\n        list of `.Line2D`\n            A list of lines representing the plotted data.\n\n        Other Parameters\n        ----------------\n        scalex, scaley : bool, default: True\n            These parameters determine if the view limits are adapted to the\n            data limits. The values are passed on to\n            `~.axes.Axes.autoscale_view`.\n\n        **kwargs : `~matplotlib.lines.Line2D` properties, optional\n            *kwargs* are used to specify properties like a line label (for\n            auto legends), linewidth, antialiasing, marker face color.\n            Example::\n\n            >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n            >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n\n            If you specify multiple lines with one plot call, the kwargs apply\n            to all those lines. In case the label object is iterable, each\n            element is used as labels for each set of data.\n\n            Here is a list of available `.Line2D` properties:\n\n            %(Line2D:kwdoc)s\n\n        See Also\n        --------\n        scatter : XY scatter plot with markers of varying size and/or color (\n            sometimes also called bubble chart).\n\n        Notes\n        -----\n        **Format Strings**\n\n        A format string consists of a part for color, marker and line::\n\n            fmt = '[marker][line][color]'\n\n        Each of them is optional. If not provided, the value from the style\n        cycle is used. Exception: If ``line`` is given, but no ``marker``,\n        the data will be a line without markers.\n\n        Other combinations such as ``[color][marker][line]`` are also\n        supported, but note that their parsing may be ambiguous.\n\n        **Markers**\n\n        =============   ===============================\n        character       description\n        =============   ===============================\n        ``'.'``         point marker\n        ``','``         pixel marker\n        ``'o'``         circle marker\n        ``'v'``         triangle_down marker\n        ``'^'``         triangle_up marker\n        ``'<'``         triangle_left marker\n        ``'>'``         triangle_right marker\n        ``'1'``         tri_down marker\n        ``'2'``         tri_up marker\n        ``'3'``         tri_left marker\n        ``'4'``         tri_right marker\n        ``'8'``         octagon marker\n        ``'s'``         square marker\n        ``'p'``         pentagon marker\n        ``'P'``         plus (filled) marker\n        ``'*'``         star marker\n        ``'h'``         hexagon1 marker\n        ``'H'``         hexagon2 marker\n        ``'+'``         plus marker\n        ``'x'``         x marker\n        ``'X'``         x (filled) marker\n        ``'D'``         diamond marker\n        ``'d'``         thin_diamond marker\n        ``'|'``         vline marker\n        ``'_'``         hline marker\n        =============   ===============================\n\n        **Line Styles**\n\n        =============    ===============================\n        character        description\n        =============    ===============================\n        ``'-'``          solid line style\n        ``'--'``         dashed line style\n        ``'-.'``         dash-dot line style\n        ``':'``          dotted line style\n        =============    ===============================\n\n        Example format strings::\n\n            'b'    # blue markers with default shape\n            'or'   # red circles\n            '-g'   # green solid line\n            '--'   # dashed line with default color\n            '^k:'  # black triangle_up markers connected by a dotted line\n\n        **Colors**\n\n        The supported color abbreviations are the single letter codes\n\n        =============    ===============================\n        character        color\n        =============    ===============================\n        ``'b'``          blue\n        ``'g'``          green\n        ``'r'``          red\n        ``'c'``          cyan\n        ``'m'``          magenta\n        ``'y'``          yellow\n        ``'k'``          black\n        ``'w'``          white\n        =============    ===============================\n\n        and the ``'CN'`` colors that index into the default property cycle.\n\n        If the color is the only part of the format string, you can\n        additionally use any  `matplotlib.colors` spec, e.g. full names\n        (``'green'``) or hex strings (``'#008000'``).\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 1799, "code": "    def loglog(self, *args, **kwargs):\n        dx = {k: v for k, v in kwargs.items()\n              if k in ['base', 'subs', 'nonpositive',\n                       'basex', 'subsx', 'nonposx']}\n        self.set_xscale('log', **dx)\n        dy = {k: v for k, v in kwargs.items()\n              if k in ['base', 'subs', 'nonpositive',\n                       'basey', 'subsy', 'nonposy']}\n        self.set_yscale('log', **dy)\n        return self.plot(\n            *args, **{k: v for k, v in kwargs.items() if k not in {*dx, *dy}})", "documentation": "        \"\"\"\n        Make a plot with log scaling on both the x- and y-axis.\n\n        Call signatures::\n\n            loglog([x], y, [fmt], data=None, **kwargs)\n            loglog([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\n        This is just a thin wrapper around `.plot` which additionally changes\n        both the x-axis and the y-axis to log scaling. All the concepts and\n        parameters of plot can be used here as well.\n\n        The additional parameters *base*, *subs* and *nonpositive* control the\n        x/y-axis properties. They are just forwarded to `.Axes.set_xscale` and\n        `.Axes.set_yscale`. To use different properties on the x-axis and the\n        y-axis, use e.g.\n        ``ax.set_xscale(\"log\", base=10); ax.set_yscale(\"log\", base=2)``.\n\n        Parameters\n        ----------\n        base : float, default: 10\n            Base of the logarithm.\n\n        subs : sequence, optional\n            The location of the minor ticks. If *None*, reasonable locations\n            are automatically chosen depending on the number of decades in the\n            plot. See `.Axes.set_xscale`/`.Axes.set_yscale` for details.\n\n        nonpositive : {'mask', 'clip'}, default: 'clip'\n            Non-positive values can be masked as invalid, or clipped to a very\n            small positive number.\n\n        **kwargs\n            All parameters supported by `.plot`.\n\n        Returns\n        -------\n        list of `.Line2D`\n            Objects representing the plotted data.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 1853, "code": "    def semilogx(self, *args, **kwargs):\n        d = {k: v for k, v in kwargs.items()\n             if k in ['base', 'subs', 'nonpositive',\n                      'basex', 'subsx', 'nonposx']}\n        self.set_xscale('log', **d)\n        return self.plot(\n            *args, **{k: v for k, v in kwargs.items() if k not in d})\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Make a plot with log scaling on the x-axis.\n\n        Call signatures::\n\n            semilogx([x], y, [fmt], data=None, **kwargs)\n            semilogx([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\n        This is just a thin wrapper around `.plot` which additionally changes\n        the x-axis to log scaling. All the concepts and parameters of plot can\n        be used here as well.\n\n        The additional parameters *base*, *subs*, and *nonpositive* control the\n        x-axis properties. They are just forwarded to `.Axes.set_xscale`.\n\n        Parameters\n        ----------\n        base : float, default: 10\n            Base of the x logarithm.\n\n        subs : array-like, optional\n            The location of the minor xticks. If *None*, reasonable locations\n            are automatically chosen depending on the number of decades in the\n            plot. See `.Axes.set_xscale` for details.\n\n        nonpositive : {'mask', 'clip'}, default: 'clip'\n            Non-positive values in x can be masked as invalid, or clipped to a\n            very small positive number.\n\n        **kwargs\n            All parameters supported by `.plot`.\n\n        Returns\n        -------\n        list of `.Line2D`\n            Objects representing the plotted data.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 1900, "code": "    def semilogy(self, *args, **kwargs):\n        d = {k: v for k, v in kwargs.items()\n             if k in ['base', 'subs', 'nonpositive',\n                      'basey', 'subsy', 'nonposy']}\n        self.set_yscale('log', **d)\n        return self.plot(\n            *args, **{k: v for k, v in kwargs.items() if k not in d})\n    @_preprocess_data(replace_names=[\"x\"], label_namer=\"x\")", "documentation": "        \"\"\"\n        Make a plot with log scaling on the y-axis.\n\n        Call signatures::\n\n            semilogy([x], y, [fmt], data=None, **kwargs)\n            semilogy([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\n        This is just a thin wrapper around `.plot` which additionally changes\n        the y-axis to log scaling. All the concepts and parameters of plot can\n        be used here as well.\n\n        The additional parameters *base*, *subs*, and *nonpositive* control the\n        y-axis properties. They are just forwarded to `.Axes.set_yscale`.\n\n        Parameters\n        ----------\n        base : float, default: 10\n            Base of the y logarithm.\n\n        subs : array-like, optional\n            The location of the minor yticks. If *None*, reasonable locations\n            are automatically chosen depending on the number of decades in the\n            plot. See `.Axes.set_yscale` for details.\n\n        nonpositive : {'mask', 'clip'}, default: 'clip'\n            Non-positive values in y can be masked as invalid, or clipped to a\n            very small positive number.\n\n        **kwargs\n            All parameters supported by `.plot`.\n\n        Returns\n        -------\n        list of `.Line2D`\n            Objects representing the plotted data.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 1946, "code": "    def acorr(self, x, **kwargs):\n        return self.xcorr(x, x, **kwargs)\n    @_api.make_keyword_only(\"3.10\", \"normed\")\n    @_preprocess_data(replace_names=[\"x\", \"y\"], label_namer=\"y\")", "documentation": "        \"\"\"\n        Plot the autocorrelation of *x*.\n\n        Parameters\n        ----------\n        x : array-like\n            Not run through Matplotlib's unit conversion, so this should\n            be a unit-less array.\n\n        detrend : callable, default: `.mlab.detrend_none` (no detrending)\n            A detrending function applied to *x*.  It must have the\n            signature ::\n\n                detrend(x: np.ndarray) -> np.ndarray\n\n        normed : bool, default: True\n            If ``True``, input vectors are normalised to unit length.\n\n        usevlines : bool, default: True\n            Determines the plot style.\n\n            If ``True``, vertical lines are plotted from 0 to the acorr value\n            using `.Axes.vlines`. Additionally, a horizontal line is plotted\n            at y=0 using `.Axes.axhline`.\n\n            If ``False``, markers are plotted at the acorr values using\n            `.Axes.plot`.\n\n        maxlags : int, default: 10\n            Number of lags to show. If ``None``, will return all\n            ``2 * len(x) - 1`` lags.\n\n        Returns\n        -------\n        lags : array (length ``2*maxlags+1``)\n            The lag vector.\n        c : array  (length ``2*maxlags+1``)\n            The auto correlation vector.\n        line : `.LineCollection` or `.Line2D`\n            `.Artist` added to the Axes of the correlation:\n\n            - `.LineCollection` if *usevlines* is True.\n            - `.Line2D` if *usevlines* is False.\n        b : `~matplotlib.lines.Line2D` or None\n            Horizontal line at 0 if *usevlines* is True\n            None *usevlines* is False.\n\n        Other Parameters\n        ----------------\n        linestyle : `~matplotlib.lines.Line2D` property, optional\n            The linestyle for plotting the data points.\n            Only used if *usevlines* is ``False``.\n\n        marker : str, default: 'o'\n            The marker for plotting the data points.\n            Only used if *usevlines* is ``False``.\n\n        data : indexable object, optional\n            DATA_PARAMETER_PLACEHOLDER\n\n        **kwargs\n            Additional parameters are passed to `.Axes.vlines` and\n            `.Axes.axhline` if *usevlines* is ``True``; otherwise they are\n            passed to `.Axes.plot`.\n\n        Notes\n        -----\n        The cross correlation is performed with `numpy.correlate` with\n        ``mode = \"full\"``.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 2134, "code": "    def step(self, x, y, *args, where='pre', data=None, **kwargs):\n        _api.check_in_list(('pre', 'post', 'mid'), where=where)\n        kwargs['drawstyle'] = 'steps-' + where\n        return self.plot(x, y, *args, data=data, **kwargs)\n    @staticmethod", "documentation": "        \"\"\"\n        Make a step plot.\n\n        Call signatures::\n\n            step(x, y, [fmt], *, data=None, where='pre', **kwargs)\n            step(x, y, [fmt], x2, y2, [fmt2], ..., *, where='pre', **kwargs)\n\n        This is just a thin wrapper around `.plot` which changes some\n        formatting options. Most of the concepts and parameters of plot can be\n        used here as well.\n\n        .. note::\n\n            This method uses a standard plot with a step drawstyle: The *x*\n            values are the reference positions and steps extend left/right/both\n            directions depending on *where*.\n\n            For the common case where you know the values and edges of the\n            steps, use `~.Axes.stairs` instead.\n\n        Parameters\n        ----------\n        x : array-like\n            1D sequence of x positions. It is assumed, but not checked, that\n            it is uniformly increasing.\n\n        y : array-like\n            1D sequence of y levels.\n\n        fmt : str, optional\n            A format string, e.g. 'g' for a green line. See `.plot` for a more\n            detailed description.\n\n            Note: While full format strings are accepted, it is recommended to\n            only specify the color. Line styles are currently ignored (use\n            the keyword argument *linestyle* instead). Markers are accepted\n            and plotted on the given positions, however, this is a rarely\n            needed feature for step plots.\n\n        where : {'pre', 'post', 'mid'}, default: 'pre'\n            Define where the steps should be placed:\n\n            - 'pre': The y value is continued constantly to the left from\n              every *x* position, i.e. the interval ``(x[i-1], x[i]]`` has the\n              value ``y[i]``.\n            - 'post': The y value is continued constantly to the right from\n              every *x* position, i.e. the interval ``[x[i], x[i+1])`` has the\n              value ``y[i]``.\n            - 'mid': Steps occur half-way between the *x* positions.\n\n        data : indexable object, optional\n            An object with labelled data. If given, provide the label names to\n            plot in *x* and *y*.\n\n        **kwargs\n            Additional parameters are the same as those for `.plot`.\n\n        Returns\n        -------\n        list of `.Line2D`\n            Objects representing the plotted data.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 2203, "code": "    def _convert_dx(dx, x0, xconv, convert):\n        assert type(xconv) is np.ndarray\n        if xconv.size == 0:\n            return convert(dx)\n        try:\n            try:\n                x0 = cbook._safe_first_finite(x0)\n            except (TypeError, IndexError, KeyError):\n                pass\n            try:\n                x = cbook._safe_first_finite(xconv)", "documentation": "        \"\"\"\n        Small helper to do logic of width conversion flexibly.\n\n        *dx* and *x0* have units, but *xconv* has already been converted\n        to unitless (and is an ndarray).  This allows the *dx* to have units\n        that are different from *x0*, but are still accepted by the\n        ``__add__`` operator of *x0*.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 2254, "code": "    def _parse_bar_color_args(self, kwargs):\n        color = kwargs.pop('color', None)\n        facecolor = kwargs.pop('facecolor', color)\n        edgecolor = kwargs.pop('edgecolor', None)\n        facecolor = (facecolor if facecolor is not None\n                     else self._get_patches_for_fill.get_next_color())\n        try:\n            facecolor = mcolors.to_rgba_array(facecolor)\n        except ValueError as err:\n            raise ValueError(\n                \"'facecolor' or 'color' argument must be a valid color or \"", "documentation": "        \"\"\"\n        Helper function to process color-related arguments of `.Axes.bar`.\n\n        Argument precedence for facecolors:\n\n        - kwargs['facecolor']\n        - kwargs['color']\n        - 'Result of ``self._get_patches_for_fill.get_next_color``\n\n        Argument precedence for edgecolors:\n\n        - kwargs['edgecolor']\n        - None\n\n        Parameters\n        ----------\n        self : Axes\n\n        kwargs : dict\n            Additional kwargs. If these keys exist, we pop and process them:\n            'facecolor', 'edgecolor', 'color'\n            Note: The dict is modified by this function.\n\n\n        Returns\n        -------\n        facecolor\n            The facecolor. One or more colors as (N, 4) rgba array.\n        edgecolor\n            The edgecolor. Not normalized; may be any valid color spec or None.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 2968, "code": "    def broken_barh(self, xranges, yrange, align=\"bottom\", **kwargs):\n        xdata = cbook._safe_first_finite(xranges) if len(xranges) else None\n        ydata = cbook._safe_first_finite(yrange) if len(yrange) else None\n        self._process_unit_info(\n            [(\"x\", xdata), (\"y\", ydata)], kwargs, convert=False)\n        vertices = []\n        y0, dy = yrange\n        _api.check_in_list(['bottom', 'center', 'top'], align=align)\n        if align == \"bottom\":\n            y0, y1 = self.convert_yunits((y0, y0 + dy))\n        elif align == \"center\":", "documentation": "        \"\"\"\n        Plot a horizontal sequence of rectangles.\n\n        A rectangle is drawn for each element of *xranges*. All rectangles\n        have the same vertical position and size defined by *yrange*.\n\n        Parameters\n        ----------\n        xranges : sequence of tuples (*xmin*, *xwidth*)\n            The x-positions and extents of the rectangles. For each tuple\n            (*xmin*, *xwidth*) a rectangle is drawn from *xmin* to *xmin* +\n            *xwidth*.\n        yrange : (*ypos*, *yheight*)\n            The y-position and extent for all the rectangles.\n        align : {\"bottom\", \"center\", \"top\"}, default: 'bottom'\n            The alignment of the yrange with respect to the y-position. One of:\n\n            - \"bottom\": Resulting y-range [ypos, ypos + yheight]\n            - \"center\": Resulting y-range [ypos - yheight/2, ypos + yheight/2]\n            - \"top\": Resulting y-range [ypos - yheight, ypos]\n\n            .. versionadded:: 3.11\n\n        Returns\n        -------\n        `~.collections.PolyCollection`\n\n        Other Parameters\n        ----------------\n        data : indexable object, optional\n            DATA_PARAMETER_PLACEHOLDER\n        **kwargs : `.PolyCollection` properties\n\n            Each *kwarg* can be either a single argument applying to all\n            rectangles, e.g.::\n\n                facecolors='black'\n\n            or a sequence of arguments over which is cycled, e.g.::\n\n                facecolors=('black', 'blue')\n\n            would create interleaving black and blue rectangles.\n\n            Supported keywords:\n\n            %(PolyCollection:kwdoc)s\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 3769, "code": "    def _errorevery_to_mask(x, errorevery):\n        if isinstance(errorevery, Integral):\n            errorevery = (0, errorevery)\n        if isinstance(errorevery, tuple):\n            if (len(errorevery) == 2 and\n                    isinstance(errorevery[0], Integral) and\n                    isinstance(errorevery[1], Integral)):\n                errorevery = slice(errorevery[0], None, errorevery[1])\n            else:\n                raise ValueError(\n                    f'{errorevery=!r} is a not a tuple of two integers')", "documentation": "        \"\"\"\n        Normalize `errorbar`'s *errorevery* to be a boolean mask for data *x*.\n\n        This function is split out to be usable both by 2D and 3D errorbars.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 3959, "code": "        def _upcast_err(err):\n            if (\n                    np.iterable(err) and\n                    len(err) > 0 and\n                    isinstance(cbook._safe_first_finite(err), np.ndarray)\n            ):\n                atype = type(cbook._safe_first_finite(err))\n                if atype is np.ndarray:\n                    return np.asarray(err, dtype=object)\n                return atype(err)\n            return np.asarray(err, dtype=object)", "documentation": "            \"\"\"\n            Safely handle tuple of containers that carry units.\n\n            This function covers the case where the input to the xerr/yerr is a\n            length 2 tuple of equal length ndarray-subclasses that carry the\n            unit information in the container.\n\n            If we have a tuple of nested numpy array (subclasses), we defer\n            coercing the units to be consistent to the underlying unit\n            library (and implicitly the broadcasting).\n\n            Otherwise, fallback to casting to an object array.\n            \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 5763, "code": "    def arrow(self, x, y, dx, dy, **kwargs):\n        x = self.convert_xunits(x)\n        y = self.convert_yunits(y)\n        dx = self.convert_xunits(dx)\n        dy = self.convert_yunits(dy)\n        a = mpatches.FancyArrow(x, y, dx, dy, **kwargs)\n        self.add_patch(a)\n        self._request_autoscale_view()\n        return a\n    @_docstring.copy(mquiver.QuiverKey.__init__)", "documentation": "        \"\"\"\n        [*Discouraged*] Add an arrow to the Axes.\n\n        This draws an arrow from ``(x, y)`` to ``(x+dx, y+dy)``.\n\n        .. admonition:: Discouraged\n\n            The use of this method is discouraged because it is not guaranteed\n            that the arrow renders reasonably. For example, the resulting arrow\n            is affected by the Axes aspect ratio and limits, which may distort\n            the arrow.\n\n            Consider using `~.Axes.annotate` without a text instead, e.g. ::\n\n                ax.annotate(\"\", xytext=(0, 0), xy=(0.5, 0.5),\n                            arrowprops=dict(arrowstyle=\"->\"))\n\n        Parameters\n        ----------\n        %(FancyArrow)s\n\n        Returns\n        -------\n        `.FancyArrow`\n            The created `.FancyArrow` object.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 5819, "code": "    def quiver(self, *args, **kwargs):\n        args = self._quiver_units(args, kwargs)\n        q = mquiver.Quiver(self, *args, **kwargs)\n        self.add_collection(q)\n        return q\n    @_preprocess_data()\n    @_docstring.interpd", "documentation": "        \"\"\"%(quiver_doc)s\"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 5830, "code": "    def barbs(self, *args, **kwargs):\n        args = self._quiver_units(args, kwargs)\n        b = mquiver.Barbs(self, *args, **kwargs)\n        self.add_collection(b)\n        return b", "documentation": "        \"\"\"%(barbs_doc)s\"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 5840, "code": "    def fill(self, *args, data=None, **kwargs):\n        kwargs = cbook.normalize_kwargs(kwargs, mlines.Line2D)\n        patches = [*self._get_patches_for_fill(self, *args, data=data, **kwargs)]\n        for poly in patches:\n            self.add_patch(poly)\n        self._request_autoscale_view()\n        return patches", "documentation": "        \"\"\"\n        Plot filled polygons.\n\n        Parameters\n        ----------\n        *args : sequence of x, y, [color]\n            Each polygon is defined by the lists of *x* and *y* positions of\n            its nodes, optionally followed by a *color* specifier. See\n            :mod:`matplotlib.colors` for supported color specifiers. The\n            standard color cycle is used for polygons without a color\n            specifier.\n\n            You can plot multiple polygons by providing multiple *x*, *y*,\n            *[color]* groups.\n\n            For example, each of the following is legal::\n\n                ax.fill(x, y)                    # a polygon with default color\n                ax.fill(x, y, \"b\")               # a blue polygon\n                ax.fill(x, y, x2, y2)            # two polygons\n                ax.fill(x, y, \"b\", x2, y2, \"r\")  # a blue and a red polygon\n\n        data : indexable object, optional\n            An object with labelled data. If given, provide the label names to\n            plot in *x* and *y*, e.g.::\n\n                ax.fill(\"time\", \"signal\",\n                        data={\"time\": [0, 1, 2], \"signal\": [0, 1, 0]})\n\n        Returns\n        -------\n        list of `~matplotlib.patches.Polygon`\n\n        Other Parameters\n        ----------------\n        **kwargs : `~matplotlib.patches.Polygon` properties\n\n        Notes\n        -----\n        Use :meth:`fill_between` if you would like to fill the region between\n        two curves.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 5997, "code": "    def _fill_between_process_units(self, ind_dir, dep_dir, ind, dep1, dep2, **kwargs):\n        return map(np.ma.masked_invalid, self._process_unit_info(\n            [(ind_dir, ind), (dep_dir, dep1), (dep_dir, dep2)], kwargs))", "documentation": "        \"\"\"Handle united data, such as dates.\"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 6802, "code": "    def _update_pcolor_lims(self, collection, coords):\n        t = collection._transform\n        if (not isinstance(t, mtransforms.Transform) and\n                hasattr(t, '_as_mpl_transform')):\n            t = t._as_mpl_transform(self.axes)\n        if t and any(t.contains_branch_separately(self.transData)):\n            trans_to_data = t - self.transData\n            coords = trans_to_data.transform(coords)\n        self.add_collection(collection, autolim=False)\n        minx, miny = np.min(coords, axis=0)\n        maxx, maxy = np.max(coords, axis=0)", "documentation": "        \"\"\"\n        Common code for updating lims in pcolor() and pcolormesh() methods.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 7029, "code": "    def contour(self, *args, **kwargs):\n        kwargs['filled'] = False\n        contours = mcontour.QuadContourSet(self, *args, **kwargs)\n        self._request_autoscale_view()\n        return contours\n    @_preprocess_data()\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Plot contour lines.\n\n        Call signature::\n\n            contour([X, Y,] Z, /, [levels], **kwargs)\n\n        The arguments *X*, *Y*, *Z* are positional-only.\n        %(contour_doc)s\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 7047, "code": "    def contourf(self, *args, **kwargs):\n        kwargs['filled'] = True\n        contours = mcontour.QuadContourSet(self, *args, **kwargs)\n        self._request_autoscale_view()\n        return contours", "documentation": "        \"\"\"\n        Plot filled contours.\n\n        Call signature::\n\n            contourf([X, Y,] Z, /, [levels], **kwargs)\n\n        The arguments *X*, *Y*, *Z* are positional-only.\n        %(contour_doc)s\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 7063, "code": "    def clabel(self, CS, levels=None, **kwargs):\n        return CS.clabel(levels, **kwargs)\n    @_api.make_keyword_only(\"3.10\", \"range\")\n    @_preprocess_data(replace_names=[\"x\", 'weights'], label_namer=\"x\")", "documentation": "        \"\"\"\n        Label a contour plot.\n\n        Adds labels to line contours in given `.ContourSet`.\n\n        Parameters\n        ----------\n        CS : `.ContourSet` instance\n            Line contours to label.\n\n        levels : array-like, optional\n            A list of level values, that should be labeled. The list must be\n            a subset of ``CS.levels``. If not given, all levels are labeled.\n\n        **kwargs\n            All other parameters are documented in `~.ContourLabeler.clabel`.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 8709, "code": "    def matshow(self, Z, **kwargs):\n        Z = np.asanyarray(Z)\n        kw = {'origin': 'upper',\n              'interpolation': 'nearest',\n              'aspect': 'equal',          # (already the imshow default)\n              **kwargs}\n        im = self.imshow(Z, **kw)\n        self.title.set_y(1.05)\n        self.xaxis.tick_top()\n        self.xaxis.set_ticks_position('both')\n        self.xaxis.set_major_locator(", "documentation": "        \"\"\"\n        Plot the values of a 2D matrix or array as color-coded image.\n\n        The matrix will be shown the way it would be printed, with the first\n        row at the top.  Row and column numbering is zero-based.\n\n        Parameters\n        ----------\n        Z : (M, N) array-like\n            The matrix to be displayed.\n\n        Returns\n        -------\n        `~matplotlib.image.AxesImage`\n\n        Other Parameters\n        ----------------\n        **kwargs : `~matplotlib.axes.Axes.imshow` arguments\n\n        See Also\n        --------\n        imshow : More general function to plot data on a 2D regular raster.\n\n        Notes\n        -----\n        This is just a convenience function wrapping `.imshow` to set useful\n        defaults for displaying a matrix. In particular:\n\n        - Set ``origin='upper'``.\n        - Set ``interpolation='nearest'``.\n        - Set ``aspect='equal'``.\n        - Ticks are placed to the left and above.\n        - Ticks are formatted to show integer indices.\n\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 9184, "code": "    def _get_aspect_ratio(self):\n        figure_size = self.get_figure().get_size_inches()\n        ll, ur = self.get_position() * figure_size\n        width, height = ur - ll\n        return height / (width * self.get_data_ratio())", "documentation": "        \"\"\"\n        Convenience method to calculate the aspect ratio of the Axes in\n        the display coordinate system.\n        \"\"\""}]}
{"repository": "matplotlib/matplotlib", "commit_sha": "00b862f0b1c450ffd8c9ba0a98b02a62376a80e2", "commit_message": "DOC: Fix documentation error of hexbin\n\nCloses #30764.", "commit_date": "2025-11-21T11:06:11+00:00", "author": "Tim Hoffmann", "file": "lib/matplotlib/axes/_axes.py", "patch": "@@ -5420,8 +5420,9 @@ def hexbin(self, x, y, C=None, gridsize=100, bins=None,\n             - If *None*, no binning is applied; the color of each hexagon\n               directly corresponds to its count value.\n             - If 'log', use a logarithmic scale for the colormap.\n-              Internally, :math:`log_{10}(i+1)` is used to determine the\n+              Internally, :math:`log_{10}(i)` is used to determine the\n               hexagon color. This is equivalent to ``norm=LogNorm()``.\n+              Note that 0 counts are thus marked with the \"bad\" color.\n             - If an integer, divide the counts in the specified number\n               of bins, and color the hexagons accordingly.\n             - If a sequence of values, the values of the lower bound of", "before_segments": [{"filename": "lib/matplotlib/axes/_axes.py", "start_line": 48, "code": "def _make_axes_method(func):\n    func.__qualname__ = f\"Axes.{func.__name__}\"\n    return func", "documentation": "    \"\"\"\n    Patch the qualname for functions that are directly added to Axes.\n\n    Some Axes functionality is defined in functions in other submodules.\n    These are simply added as attributes to Axes. As a result, their\n    ``__qualname__`` is e.g. only \"table\" and not \"Axes.table\". This\n    function fixes that.\n\n    Note that the function itself is patched, so that\n    ``matplotlib.table.table.__qualname__` will also show \"Axes.table\".\n    However, since these functions are not intended to be standalone,\n    this is bearable.\n    \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 66, "code": "class _GroupedBarReturn:", "documentation": "    \"\"\"\n    A provisional result object for `.Axes.grouped_bar`.\n\n    This is a placeholder for a future better return type. We try to build in\n    backward compatibility / migration possibilities.\n\n    The only public interfaces are the ``bar_containers`` attribute and the\n    ``remove()`` method.\n    \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 84, "code": "class Axes(_AxesBase):", "documentation": "    \"\"\"\n    An Axes object encapsulates all the elements of an individual (sub-)plot in\n    a figure.\n\n    It contains most of the (sub-)plot elements: `~.axis.Axis`,\n    `~.axis.Tick`, `~.lines.Line2D`, `~.text.Text`, `~.patches.Polygon`, etc.,\n    and sets the coordinate system.\n\n    Like all visible elements in a figure, Axes is an `.Artist` subclass.\n\n    The `Axes` instance supports callbacks through a callbacks attribute which\n    is a `~.cbook.CallbackRegistry` instance.  The events you can connect to\n    are 'xlim_changed' and 'ylim_changed' and the callback will be called with\n    func(*ax*) where *ax* is the `Axes` instance.\n\n    .. note::\n\n        As a user, you do not instantiate Axes directly, but use Axes creation\n        methods instead; e.g. from `.pyplot` or `.Figure`:\n        `~.pyplot.subplots`, `~.pyplot.subplot_mosaic` or `.Figure.add_axes`.\n\n    \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 109, "code": "    def get_title(self, loc=\"center\"):\n        titles = {'left': self._left_title,\n                  'center': self.title,\n                  'right': self._right_title}\n        title = _api.check_getitem(titles, loc=loc.lower())\n        return title.get_text()", "documentation": "        \"\"\"\n        Get an Axes title.\n\n        Get one of the three available Axes titles. The available titles\n        are positioned above the Axes in the center, flush with the left\n        edge, and flush with the right edge.\n\n        Parameters\n        ----------\n        loc : {'center', 'left', 'right'}, str, default: 'center'\n            Which title to return.\n\n        Returns\n        -------\n        str\n            The title text string.\n\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 215, "code": "    def get_legend_handles_labels(self, legend_handler_map=None):\n        handles, labels = mlegend._get_legend_handles_labels(\n            [self], legend_handler_map)\n        return handles, labels\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Return handles and labels for legend\n\n        ``ax.legend()`` is equivalent to ::\n\n          h, l = ax.get_legend_handles_labels()\n          ax.legend(h, l)\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 230, "code": "    def legend(self, *args, **kwargs):\n        handles, labels, kwargs = mlegend._parse_legend_args([self], *args, **kwargs)\n        self.legend_ = mlegend.Legend(self, handles, labels, **kwargs)\n        self.legend_._remove_method = self._remove_legend\n        return self.legend_", "documentation": "        \"\"\"\n        Place a legend on the Axes.\n\n        Call signatures::\n\n            legend()\n            legend(handles, labels)\n            legend(handles=handles)\n            legend(labels)\n\n        The call signatures correspond to the following different ways to use\n        this method:\n\n        **1. Automatic detection of elements to be shown in the legend**\n\n        The elements to be added to the legend are automatically determined,\n        when you do not pass in any extra arguments.\n\n        In this case, the labels are taken from the artist. You can specify\n        them either at artist creation or by calling the\n        :meth:`~.Artist.set_label` method on the artist::\n\n            ax.plot([1, 2, 3], label='Inline label')\n            ax.legend()\n\n        or::\n\n            line, = ax.plot([1, 2, 3])\n            line.set_label('Label via method')\n            ax.legend()\n\n        .. note::\n            Specific artists can be excluded from the automatic legend element\n            selection by using a label starting with an underscore, \"_\".\n            A string starting with an underscore is the default label for all\n            artists, so calling `.Axes.legend` without any arguments and\n            without setting the labels manually will result in a ``UserWarning``\n            and an empty legend being drawn.\n\n\n        **2. Explicitly listing the artists and labels in the legend**\n\n        For full control of which artists have a legend entry, it is possible\n        to pass an iterable of legend artists followed by an iterable of\n        legend labels respectively::\n\n            ax.legend([line1, line2, line3], ['label1', 'label2', 'label3'])\n\n\n        **3. Explicitly listing the artists in the legend**\n\n        This is similar to 2, but the labels are taken from the artists'\n        label properties. Example::\n\n            line1, = ax.plot([1, 2, 3], label='label1')\n            line2, = ax.plot([1, 2, 3], label='label2')\n            ax.legend(handles=[line1, line2])\n\n\n        **4. Labeling existing plot elements**\n\n        .. admonition:: Discouraged\n\n            This call signature is discouraged, because the relation between\n            plot elements and labels is only implicit by their order and can\n            easily be mixed up.\n\n        To make a legend for all artists on an Axes, call this function with\n        an iterable of strings, one for each legend item. For example::\n\n            ax.plot([1, 2, 3])\n            ax.plot([5, 6, 7])\n            ax.legend(['First line', 'Second line'])\n\n\n        Parameters\n        ----------\n        handles : list of (`.Artist` or tuple of `.Artist`), optional\n            A list of Artists (lines, patches) to be added to the legend.\n            Use this together with *labels*, if you need full control on what\n            is shown in the legend and the automatic mechanism described above\n            is not sufficient.\n\n            The length of handles and labels should be the same in this\n            case. If they are not, they are truncated to the smaller length.\n\n            If an entry contains a tuple, then the legend handler for all Artists in the\n            tuple will be placed alongside a single label.\n\n        labels : list of str, optional\n            A list of labels to show next to the artists.\n            Use this together with *handles*, if you need full control on what\n            is shown in the legend and the automatic mechanism described above\n            is not sufficient.\n\n        Returns\n        -------\n        `~matplotlib.legend.Legend`\n\n        Other Parameters\n        ----------------\n        %(_legend_kw_axes)s\n\n        See Also\n        --------\n        .Figure.legend\n\n        Notes\n        -----\n        Some artists are not supported by this function.  See\n        :ref:`legend_guide` for details.\n\n        Examples\n        --------\n        .. plot:: gallery/text_labels_and_annotations/legend.py\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 355, "code": "    def inset_axes(self, bounds, *, transform=None, zorder=5, **kwargs):\n        if transform is None:\n            transform = self.transAxes\n        kwargs.setdefault('label', 'inset_axes')\n        inset_locator = _TransformedBoundsLocator(bounds, transform)\n        bounds = inset_locator(self, None).bounds\n        fig = self.get_figure(root=False)\n        projection_class, pkw = fig._process_projection_requirements(**kwargs)\n        inset_ax = projection_class(fig, bounds, zorder=zorder, **pkw)\n        inset_ax.set_axes_locator(inset_locator)\n        self.add_child_axes(inset_ax)", "documentation": "        \"\"\"\n        Add a child inset Axes to this existing Axes.\n\n\n        Parameters\n        ----------\n        bounds : [x0, y0, width, height]\n            Lower-left corner of inset Axes, and its width and height.\n\n        transform : `.Transform`\n            Defaults to `!ax.transAxes`, i.e. the units of *rect* are in\n            Axes-relative coordinates.\n\n        projection : {None, 'aitoff', 'hammer', 'lambert', 'mollweide', \\\n'polar', 'rectilinear', str}, optional\n            The projection type of the inset `~.axes.Axes`. *str* is the name\n            of a custom projection, see `~matplotlib.projections`. The default\n            None results in a 'rectilinear' projection.\n\n        polar : bool, default: False\n            If True, equivalent to projection='polar'.\n\n        axes_class : subclass type of `~.axes.Axes`, optional\n            The `.axes.Axes` subclass that is instantiated.  This parameter\n            is incompatible with *projection* and *polar*.  See\n            :ref:`axisartist_users-guide-index` for examples.\n\n        zorder : number\n            Defaults to 5 (same as `.Axes.legend`).  Adjust higher or lower\n            to change whether it is above or below data plotted on the\n            parent Axes.\n\n        **kwargs\n            Other keyword arguments are passed on to the inset Axes class.\n\n        Returns\n        -------\n        ax\n            The created `~.axes.Axes` instance.\n\n        Examples\n        --------\n        This example makes two inset Axes, the first is in Axes-relative\n        coordinates, and the second in data-coordinates::\n\n            fig, ax = plt.subplots()\n            ax.plot(range(10))\n            axin1 = ax.inset_axes([0.8, 0.1, 0.15, 0.15])\n            axin2 = ax.inset_axes(\n                    [5, 7, 2.3, 2.3], transform=ax.transData)\n\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 510, "code": "    def indicate_inset_zoom(self, inset_ax, **kwargs):\n        return self.indicate_inset(None, inset_ax, **kwargs)\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Add an inset indicator rectangle to the Axes based on the axis\n        limits for an *inset_ax* and draw connectors between *inset_ax*\n        and the rectangle.\n\n        Warnings\n        --------\n        This method is experimental as of 3.0, and the API may change.\n\n        Parameters\n        ----------\n        inset_ax : `.Axes`\n            Inset Axes to draw connecting lines to.  Two lines are\n            drawn connecting the indicator box to the inset Axes on corners\n            chosen so as to not overlap with the indicator box.\n\n        **kwargs\n            Other keyword arguments are passed on to `.Axes.indicate_inset`\n\n        Returns\n        -------\n        inset_indicator : `.inset.InsetIndicator`\n            An artist which contains\n\n            inset_indicator.rectangle : `.Rectangle`\n                The indicator frame.\n\n            inset_indicator.connectors : 4-tuple of `.patches.ConnectionPatch`\n                The four connector lines connecting to (lower_left, upper_left,\n                lower_right upper_right) corners of *inset_ax*. Two lines are\n                set with visibility to *False*,  but the user can set the\n                visibility to True if the automatic choice is not deemed correct.\n\n            .. versionchanged:: 3.10\n                Previously the rectangle and connectors tuple were returned.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 551, "code": "    def secondary_xaxis(self, location, functions=None, *, transform=None, **kwargs):\n        if not (location in ['top', 'bottom'] or isinstance(location, Real)):\n            raise ValueError('secondary_xaxis location must be either '\n                             'a float or \"top\"/\"bottom\"')\n        secondary_ax = SecondaryAxis(self, 'x', location, functions,\n                                     transform, **kwargs)\n        self.add_child_axes(secondary_ax)\n        return secondary_ax\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Add a second x-axis to this `~.axes.Axes`.\n\n        For example if we want to have a second scale for the data plotted on\n        the xaxis.\n\n        %(_secax_docstring)s\n\n        Examples\n        --------\n        The main axis shows frequency, and the secondary axis shows period.\n\n        .. plot::\n\n            fig, ax = plt.subplots()\n            ax.loglog(range(1, 360, 5), range(1, 360, 5))\n            ax.set_xlabel('frequency [Hz]')\n\n            def invert(x):\n                # 1/x with special treatment of x == 0\n                x = np.array(x).astype(float)\n                near_zero = np.isclose(x, 0)\n                x[near_zero] = np.inf\n                x[~near_zero] = 1 / x[~near_zero]\n                return x\n\n            # the inverse of 1/x is itself\n            secax = ax.secondary_xaxis('top', functions=(invert, invert))\n            secax.set_xlabel('Period [s]')\n            plt.show()\n\n        To add a secondary axis relative to your data, you can pass a transform\n        to the new axis.\n\n        .. plot::\n\n            fig, ax = plt.subplots()\n            ax.plot(range(0, 5), range(-1, 4))\n\n            # Pass 'ax.transData' as a transform to place the axis\n            # relative to your data at y=0\n            secax = ax.secondary_xaxis(0, transform=ax.transData)\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 605, "code": "    def secondary_yaxis(self, location, functions=None, *, transform=None, **kwargs):\n        if not (location in ['left', 'right'] or isinstance(location, Real)):\n            raise ValueError('secondary_yaxis location must be either '\n                             'a float or \"left\"/\"right\"')\n        secondary_ax = SecondaryAxis(self, 'y', location, functions,\n                                     transform, **kwargs)\n        self.add_child_axes(secondary_ax)\n        return secondary_ax\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Add a second y-axis to this `~.axes.Axes`.\n\n        For example if we want to have a second scale for the data plotted on\n        the yaxis.\n\n        %(_secax_docstring)s\n\n        Examples\n        --------\n        Add a secondary Axes that converts from radians to degrees\n\n        .. plot::\n\n            fig, ax = plt.subplots()\n            ax.plot(range(1, 360, 5), range(1, 360, 5))\n            ax.set_ylabel('degrees')\n            secax = ax.secondary_yaxis('right', functions=(np.deg2rad,\n                                                           np.rad2deg))\n            secax.set_ylabel('radians')\n\n        To add a secondary axis relative to your data, you can pass a transform\n        to the new axis.\n\n        .. plot::\n\n            fig, ax = plt.subplots()\n            ax.plot(range(0, 5), range(-1, 4))\n\n            # Pass 'ax.transData' as a transform to place the axis\n            # relative to your data at x=3\n            secax = ax.secondary_yaxis(3, transform=ax.transData)\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 649, "code": "    def text(self, x, y, s, fontdict=None, **kwargs):\n        effective_kwargs = {\n            'verticalalignment': 'baseline',\n            'horizontalalignment': 'left',\n            'transform': self.transData,\n            'clip_on': False,\n            **(fontdict if fontdict is not None else {}),\n            **kwargs,\n        }\n        t = mtext.Text(x, y, text=s, **effective_kwargs)\n        if t.get_clip_path() is None:", "documentation": "        \"\"\"\n        Add text to the Axes.\n\n        Add the text *s* to the Axes at location *x*, *y* in data coordinates,\n        with a default ``horizontalalignment`` on the ``left`` and\n        ``verticalalignment`` at the ``baseline``. See\n        :doc:`/gallery/text_labels_and_annotations/text_alignment`.\n\n        Parameters\n        ----------\n        x, y : float\n            The position to place the text. By default, this is in data\n            coordinates. The coordinate system can be changed using the\n            *transform* parameter.\n\n        s : str\n            The text.\n\n        fontdict : dict, default: None\n\n            .. admonition:: Discouraged\n\n               The use of *fontdict* is discouraged. Parameters should be passed as\n               individual keyword arguments or using dictionary-unpacking\n               ``text(..., **fontdict)``.\n\n            A dictionary to override the default text properties. If fontdict\n            is None, the defaults are determined by `.rcParams`.\n\n        Returns\n        -------\n        `.Text`\n            The created `.Text` instance.\n\n        Other Parameters\n        ----------------\n        **kwargs : `~matplotlib.text.Text` properties.\n            Other miscellaneous text parameters.\n\n            %(Text:kwdoc)s\n\n        Examples\n        --------\n        Individual keyword arguments can be used to override any given\n        parameter::\n\n            >>> text(x, y, s, fontsize=12)\n\n        The default transform specifies that text is in data coords,\n        alternatively, you can specify text in axis coords ((0, 0) is\n        lower-left and (1, 1) is upper-right).  The example below places\n        text in the center of the Axes::\n\n            >>> text(0.5, 0.5, 'matplotlib', horizontalalignment='center',\n            ...      verticalalignment='center', transform=ax.transAxes)\n\n        You can put a rectangular box around the text instance (e.g., to\n        set a background color) by using the keyword *bbox*.  *bbox* is\n        a dictionary of `~matplotlib.patches.Rectangle`\n        properties.  For example::\n\n            >>> text(x, y, s, bbox=dict(facecolor='red', alpha=0.5))\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 744, "code": "    def axhline(self, y=0, xmin=0, xmax=1, **kwargs):\n        self._check_no_units([xmin, xmax], ['xmin', 'xmax'])\n        if \"transform\" in kwargs:\n            raise ValueError(\"'transform' is not allowed as a keyword \"\n                             \"argument; axhline generates its own transform.\")\n        ymin, ymax = self.get_ybound()\n        yy, = self._process_unit_info([(\"y\", y)], kwargs)\n        scaley = (yy < ymin) or (yy > ymax)\n        trans = self.get_yaxis_transform(which='grid')\n        l = mlines.Line2D([xmin, xmax], [y, y], transform=trans, **kwargs)\n        self.add_line(l)", "documentation": "        \"\"\"\n        Add a horizontal line spanning the whole or fraction of the Axes.\n\n        Note: If you want to set x-limits in data coordinates, use\n        `~.Axes.hlines` instead.\n\n        Parameters\n        ----------\n        y : float, default: 0\n            y position in :ref:`data coordinates <coordinate-systems>`.\n\n        xmin : float, default: 0\n            The start x-position in :ref:`axes coordinates <coordinate-systems>`.\n            Should be between 0 and 1, 0 being the far left of the plot,\n            1 the far right of the plot.\n\n        xmax : float, default: 1\n            The end x-position in :ref:`axes coordinates <coordinate-systems>`.\n            Should be between 0 and 1, 0 being the far left of the plot,\n            1 the far right of the plot.\n\n        Returns\n        -------\n        `~matplotlib.lines.Line2D`\n            A `.Line2D` specified via two points ``(xmin, y)``, ``(xmax, y)``.\n            Its transform is set such that *x* is in\n            :ref:`axes coordinates <coordinate-systems>` and *y* is in\n            :ref:`data coordinates <coordinate-systems>`.\n\n            This is still a generic line and the horizontal character is only\n            realized through using identical *y* values for both points. Thus,\n            if you want to change the *y* value later, you have to provide two\n            values ``line.set_ydata([3, 3])``.\n\n        Other Parameters\n        ----------------\n        **kwargs\n            Valid keyword arguments are `.Line2D` properties, except for\n            'transform':\n\n            %(Line2D:kwdoc)s\n\n        See Also\n        --------\n        hlines : Add horizontal lines in data coordinates.\n        axhspan : Add a horizontal span (rectangle) across the axis.\n        axline : Add a line with an arbitrary slope.\n\n        Examples\n        --------\n        * draw a thick red hline at 'y' = 0 that spans the xrange::\n\n            >>> axhline(linewidth=4, color='r')\n\n        * draw a default hline at 'y' = 1 that spans the xrange::\n\n            >>> axhline(y=1)\n\n        * draw a default hline at 'y' = .5 that spans the middle half of\n          the xrange::\n\n            >>> axhline(y=.5, xmin=0.25, xmax=0.75)\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 827, "code": "    def axvline(self, x=0, ymin=0, ymax=1, **kwargs):\n        self._check_no_units([ymin, ymax], ['ymin', 'ymax'])\n        if \"transform\" in kwargs:\n            raise ValueError(\"'transform' is not allowed as a keyword \"\n                             \"argument; axvline generates its own transform.\")\n        xmin, xmax = self.get_xbound()\n        xx, = self._process_unit_info([(\"x\", x)], kwargs)\n        scalex = (xx < xmin) or (xx > xmax)\n        trans = self.get_xaxis_transform(which='grid')\n        l = mlines.Line2D([x, x], [ymin, ymax], transform=trans, **kwargs)\n        self.add_line(l)", "documentation": "        \"\"\"\n        Add a vertical line spanning the whole or fraction of the Axes.\n\n        Note: If you want to set y-limits in data coordinates, use\n        `~.Axes.vlines` instead.\n\n        Parameters\n        ----------\n        x : float, default: 0\n            x position in :ref:`data coordinates <coordinate-systems>`.\n\n        ymin : float, default: 0\n            The start y-position in :ref:`axes coordinates <coordinate-systems>`.\n            Should be between 0 and 1, 0 being the bottom of the plot, 1 the\n            top of the plot.\n\n        ymax : float, default: 1\n            The end y-position in :ref:`axes coordinates <coordinate-systems>`.\n            Should be between 0 and 1, 0 being the bottom of the plot, 1 the\n            top of the plot.\n\n        Returns\n        -------\n        `~matplotlib.lines.Line2D`\n            A `.Line2D` specified via two points ``(x, ymin)``, ``(x, ymax)``.\n            Its transform is set such that *x* is in\n            :ref:`data coordinates <coordinate-systems>` and *y* is in\n            :ref:`axes coordinates <coordinate-systems>`.\n\n            This is still a generic line and the vertical character is only\n            realized through using identical *x* values for both points. Thus,\n            if you want to change the *x* value later, you have to provide two\n            values ``line.set_xdata([3, 3])``.\n\n        Other Parameters\n        ----------------\n        **kwargs\n            Valid keyword arguments are `.Line2D` properties, except for\n            'transform':\n\n            %(Line2D:kwdoc)s\n\n        See Also\n        --------\n        vlines : Add vertical lines in data coordinates.\n        axvspan : Add a vertical span (rectangle) across the axis.\n        axline : Add a line with an arbitrary slope.\n\n        Examples\n        --------\n        * draw a thick red vline at *x* = 0 that spans the yrange::\n\n            >>> axvline(linewidth=4, color='r')\n\n        * draw a default vline at *x* = 1 that spans the yrange::\n\n            >>> axvline(x=1)\n\n        * draw a default vline at *x* = .5 that spans the middle half of\n          the yrange::\n\n            >>> axvline(x=.5, ymin=0.25, ymax=0.75)\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 918, "code": "    def axline(self, xy1, xy2=None, *, slope=None, **kwargs):\n        if slope is not None and (self.get_xscale() != 'linear' or\n                                  self.get_yscale() != 'linear'):\n            raise TypeError(\"'slope' cannot be used with non-linear scales\")\n        datalim = [xy1] if xy2 is None else [xy1, xy2]\n        if \"transform\" in kwargs:\n            datalim = []\n        line = mlines.AxLine(xy1, xy2, slope, **kwargs)\n        self._set_artist_props(line)\n        if line.get_clip_path() is None:\n            line.set_clip_path(self.patch)", "documentation": "        \"\"\"\n        Add an infinitely long straight line.\n\n        The line can be defined either by two points *xy1* and *xy2*, or\n        by one point *xy1* and a *slope*.\n\n        This draws a straight line \"on the screen\", regardless of the x and y\n        scales, and is thus also suitable for drawing exponential decays in\n        semilog plots, power laws in loglog plots, etc. However, *slope*\n        should only be used with linear scales; It has no clear meaning for\n        all other scales, and thus the behavior is undefined. Please specify\n        the line using the points *xy1*, *xy2* for non-linear scales.\n\n        The *transform* keyword argument only applies to the points *xy1*,\n        *xy2*. The *slope* (if given) is always in data coordinates. This can\n        be used e.g. with ``ax.transAxes`` for drawing grid lines with a fixed\n        slope.\n\n        Parameters\n        ----------\n        xy1, xy2 : (float, float)\n            Points for the line to pass through.\n            Either *xy2* or *slope* has to be given.\n        slope : float, optional\n            The slope of the line. Either *xy2* or *slope* has to be given.\n\n        Returns\n        -------\n        `.AxLine`\n\n        Other Parameters\n        ----------------\n        **kwargs\n            Valid kwargs are `.Line2D` properties\n\n            %(Line2D:kwdoc)s\n\n        See Also\n        --------\n        axhline : for horizontal lines\n        axvline : for vertical lines\n\n        Examples\n        --------\n        Draw a thick red line passing through (0, 0) and (1, 1)::\n\n            >>> axline((0, 0), (1, 1), linewidth=4, color='r')\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 992, "code": "    def axhspan(self, ymin, ymax, xmin=0, xmax=1, **kwargs):\n        self._check_no_units([xmin, xmax], ['xmin', 'xmax'])\n        (ymin, ymax), = self._process_unit_info([(\"y\", [ymin, ymax])], kwargs)\n        p = mpatches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, **kwargs)\n        p.set_transform(self.get_yaxis_transform(which=\"grid\"))\n        ix = self.dataLim.intervalx.copy()\n        mx = self.dataLim.minposx\n        self.add_patch(p)\n        self.dataLim.intervalx = ix\n        self.dataLim.minposx = mx\n        p.get_path()._interpolation_steps = mpl.axis.GRIDLINE_INTERPOLATION_STEPS", "documentation": "        \"\"\"\n        Add a horizontal span (rectangle) across the Axes.\n\n        The rectangle spans from *ymin* to *ymax* vertically, and, by default,\n        the whole x-axis horizontally.  The x-span can be set using *xmin*\n        (default: 0) and *xmax* (default: 1) which are in axis units; e.g.\n        ``xmin = 0.5`` always refers to the middle of the x-axis regardless of\n        the limits set by `~.Axes.set_xlim`.\n\n        Parameters\n        ----------\n        ymin : float\n            Lower y-coordinate of the span, in data units.\n        ymax : float\n            Upper y-coordinate of the span, in data units.\n        xmin : float, default: 0\n            Lower x-coordinate of the span, in x-axis (0-1) units.\n        xmax : float, default: 1\n            Upper x-coordinate of the span, in x-axis (0-1) units.\n\n        Returns\n        -------\n        `~matplotlib.patches.Rectangle`\n            Horizontal span (rectangle) from (xmin, ymin) to (xmax, ymax).\n\n        Other Parameters\n        ----------------\n        **kwargs : `~matplotlib.patches.Rectangle` properties\n\n        %(Rectangle:kwdoc)s\n\n        See Also\n        --------\n        axvspan : Add a vertical span across the Axes.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 1047, "code": "    def axvspan(self, xmin, xmax, ymin=0, ymax=1, **kwargs):\n        self._check_no_units([ymin, ymax], ['ymin', 'ymax'])\n        (xmin, xmax), = self._process_unit_info([(\"x\", [xmin, xmax])], kwargs)\n        p = mpatches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, **kwargs)\n        p.set_transform(self.get_xaxis_transform(which=\"grid\"))\n        iy = self.dataLim.intervaly.copy()\n        my = self.dataLim.minposy\n        self.add_patch(p)\n        self.dataLim.intervaly = iy\n        self.dataLim.minposy = my\n        p.get_path()._interpolation_steps = mpl.axis.GRIDLINE_INTERPOLATION_STEPS", "documentation": "        \"\"\"\n        Add a vertical span (rectangle) across the Axes.\n\n        The rectangle spans from *xmin* to *xmax* horizontally, and, by\n        default, the whole y-axis vertically.  The y-span can be set using\n        *ymin* (default: 0) and *ymax* (default: 1) which are in axis units;\n        e.g. ``ymin = 0.5`` always refers to the middle of the y-axis\n        regardless of the limits set by `~.Axes.set_ylim`.\n\n        Parameters\n        ----------\n        xmin : float\n            Lower x-coordinate of the span, in data units.\n        xmax : float\n            Upper x-coordinate of the span, in data units.\n        ymin : float, default: 0\n            Lower y-coordinate of the span, in y-axis units (0-1).\n        ymax : float, default: 1\n            Upper y-coordinate of the span, in y-axis units (0-1).\n\n        Returns\n        -------\n        `~matplotlib.patches.Rectangle`\n            Vertical span (rectangle) from (xmin, ymin) to (xmax, ymax).\n\n        Other Parameters\n        ----------------\n        **kwargs : `~matplotlib.patches.Rectangle` properties\n\n        %(Rectangle:kwdoc)s\n\n        See Also\n        --------\n        axhspan : Add a horizontal span across the Axes.\n\n        Examples\n        --------\n        Draw a vertical, green, translucent rectangle from x = 1.25 to\n        x = 1.55 that spans the yrange of the Axes.\n\n        >>> axvspan(1.25, 1.55, facecolor='g', alpha=0.5)\n\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 1544, "code": "    def plot(self, *args, scalex=True, scaley=True, data=None, **kwargs):\n        kwargs = cbook.normalize_kwargs(kwargs, mlines.Line2D)\n        lines = [*self._get_lines(self, *args, data=data, **kwargs)]\n        for line in lines:\n            self.add_line(line)\n        if scalex:\n            self._request_autoscale_view(\"x\")\n        if scaley:\n            self._request_autoscale_view(\"y\")\n        return lines\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Plot y versus x as lines and/or markers.\n\n        Call signatures::\n\n            plot([x], y, [fmt], *, data=None, **kwargs)\n            plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\n        The coordinates of the points or line nodes are given by *x*, *y*.\n\n        The optional parameter *fmt* is a convenient way for defining basic\n        formatting like color, marker and linestyle. It's a shortcut string\n        notation described in the *Notes* section below.\n\n        >>> plot(x, y)        # plot x and y using default line style and color\n        >>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n        >>> plot(y)           # plot y using x as index array 0..N-1\n        >>> plot(y, 'r+')     # ditto, but with red plusses\n\n        You can use `.Line2D` properties as keyword arguments for more\n        control on the appearance. Line properties and *fmt* can be mixed.\n        The following two calls yield identical results:\n\n        >>> plot(x, y, 'go--', linewidth=2, markersize=12)\n        >>> plot(x, y, color='green', marker='o', linestyle='dashed',\n        ...      linewidth=2, markersize=12)\n\n        When conflicting with *fmt*, keyword arguments take precedence.\n\n\n        **Plotting labelled data**\n\n        There's a convenient way for plotting objects with labelled data (i.e.\n        data that can be accessed by index ``obj['y']``). Instead of giving\n        the data in *x* and *y*, you can provide the object in the *data*\n        parameter and just give the labels for *x* and *y*::\n\n        >>> plot('xlabel', 'ylabel', data=obj)\n\n        All indexable objects are supported. This could e.g. be a `dict`, a\n        `pandas.DataFrame` or a structured numpy array.\n\n\n        **Plotting multiple sets of data**\n\n        There are various ways to plot multiple sets of data.\n\n        - The most straight forward way is just to call `plot` multiple times.\n          Example:\n\n          >>> plot(x1, y1, 'bo')\n          >>> plot(x2, y2, 'go')\n\n        - If *x* and/or *y* are 2D arrays, a separate data set will be drawn\n          for every column. If both *x* and *y* are 2D, they must have the\n          same shape. If only one of them is 2D with shape (N, m) the other\n          must have length N and will be used for every data set m.\n\n          Example:\n\n          >>> x = [1, 2, 3]\n          >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n          >>> plot(x, y)\n\n          is equivalent to:\n\n          >>> for col in range(y.shape[1]):\n          ...     plot(x, y[:, col])\n\n        - The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n          groups::\n\n          >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n\n          In this case, any additional keyword argument applies to all\n          datasets. Also, this syntax cannot be combined with the *data*\n          parameter.\n\n        By default, each line is assigned a different style specified by a\n        'style cycle'. The *fmt* and line property parameters are only\n        necessary if you want explicit deviations from these defaults.\n        Alternatively, you can also change the style cycle using\n        :rc:`axes.prop_cycle`.\n\n\n        Parameters\n        ----------\n        x, y : array-like or float\n            The horizontal / vertical coordinates of the data points.\n            *x* values are optional and default to ``range(len(y))``.\n\n            Commonly, these parameters are 1D arrays.\n\n            They can also be scalars, or two-dimensional (in that case, the\n            columns represent separate data sets).\n\n            These arguments cannot be passed as keywords.\n\n        fmt : str, optional\n            A format string, e.g. 'ro' for red circles. See the *Notes*\n            section for a full description of the format strings.\n\n            Format strings are just an abbreviation for quickly setting\n            basic line properties. All of these and more can also be\n            controlled by keyword arguments.\n\n            This argument cannot be passed as keyword.\n\n        data : indexable object, optional\n            An object with labelled data. If given, provide the label names to\n            plot in *x* and *y*.\n\n            .. note::\n                Technically there's a slight ambiguity in calls where the\n                second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n                could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n                the former interpretation is chosen, but a warning is issued.\n                You may suppress the warning by adding an empty format string\n                ``plot('n', 'o', '', data=obj)``.\n\n        Returns\n        -------\n        list of `.Line2D`\n            A list of lines representing the plotted data.\n\n        Other Parameters\n        ----------------\n        scalex, scaley : bool, default: True\n            These parameters determine if the view limits are adapted to the\n            data limits. The values are passed on to\n            `~.axes.Axes.autoscale_view`.\n\n        **kwargs : `~matplotlib.lines.Line2D` properties, optional\n            *kwargs* are used to specify properties like a line label (for\n            auto legends), linewidth, antialiasing, marker face color.\n            Example::\n\n            >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n            >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n\n            If you specify multiple lines with one plot call, the kwargs apply\n            to all those lines. In case the label object is iterable, each\n            element is used as labels for each set of data.\n\n            Here is a list of available `.Line2D` properties:\n\n            %(Line2D:kwdoc)s\n\n        See Also\n        --------\n        scatter : XY scatter plot with markers of varying size and/or color (\n            sometimes also called bubble chart).\n\n        Notes\n        -----\n        **Format Strings**\n\n        A format string consists of a part for color, marker and line::\n\n            fmt = '[marker][line][color]'\n\n        Each of them is optional. If not provided, the value from the style\n        cycle is used. Exception: If ``line`` is given, but no ``marker``,\n        the data will be a line without markers.\n\n        Other combinations such as ``[color][marker][line]`` are also\n        supported, but note that their parsing may be ambiguous.\n\n        **Markers**\n\n        =============   ===============================\n        character       description\n        =============   ===============================\n        ``'.'``         point marker\n        ``','``         pixel marker\n        ``'o'``         circle marker\n        ``'v'``         triangle_down marker\n        ``'^'``         triangle_up marker\n        ``'<'``         triangle_left marker\n        ``'>'``         triangle_right marker\n        ``'1'``         tri_down marker\n        ``'2'``         tri_up marker\n        ``'3'``         tri_left marker\n        ``'4'``         tri_right marker\n        ``'8'``         octagon marker\n        ``'s'``         square marker\n        ``'p'``         pentagon marker\n        ``'P'``         plus (filled) marker\n        ``'*'``         star marker\n        ``'h'``         hexagon1 marker\n        ``'H'``         hexagon2 marker\n        ``'+'``         plus marker\n        ``'x'``         x marker\n        ``'X'``         x (filled) marker\n        ``'D'``         diamond marker\n        ``'d'``         thin_diamond marker\n        ``'|'``         vline marker\n        ``'_'``         hline marker\n        =============   ===============================\n\n        **Line Styles**\n\n        =============    ===============================\n        character        description\n        =============    ===============================\n        ``'-'``          solid line style\n        ``'--'``         dashed line style\n        ``'-.'``         dash-dot line style\n        ``':'``          dotted line style\n        =============    ===============================\n\n        Example format strings::\n\n            'b'    # blue markers with default shape\n            'or'   # red circles\n            '-g'   # green solid line\n            '--'   # dashed line with default color\n            '^k:'  # black triangle_up markers connected by a dotted line\n\n        **Colors**\n\n        The supported color abbreviations are the single letter codes\n\n        =============    ===============================\n        character        color\n        =============    ===============================\n        ``'b'``          blue\n        ``'g'``          green\n        ``'r'``          red\n        ``'c'``          cyan\n        ``'m'``          magenta\n        ``'y'``          yellow\n        ``'k'``          black\n        ``'w'``          white\n        =============    ===============================\n\n        and the ``'CN'`` colors that index into the default property cycle.\n\n        If the color is the only part of the format string, you can\n        additionally use any  `matplotlib.colors` spec, e.g. full names\n        (``'green'``) or hex strings (``'#008000'``).\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 1799, "code": "    def loglog(self, *args, **kwargs):\n        dx = {k: v for k, v in kwargs.items()\n              if k in ['base', 'subs', 'nonpositive',\n                       'basex', 'subsx', 'nonposx']}\n        self.set_xscale('log', **dx)\n        dy = {k: v for k, v in kwargs.items()\n              if k in ['base', 'subs', 'nonpositive',\n                       'basey', 'subsy', 'nonposy']}\n        self.set_yscale('log', **dy)\n        return self.plot(\n            *args, **{k: v for k, v in kwargs.items() if k not in {*dx, *dy}})", "documentation": "        \"\"\"\n        Make a plot with log scaling on both the x- and y-axis.\n\n        Call signatures::\n\n            loglog([x], y, [fmt], data=None, **kwargs)\n            loglog([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\n        This is just a thin wrapper around `.plot` which additionally changes\n        both the x-axis and the y-axis to log scaling. All the concepts and\n        parameters of plot can be used here as well.\n\n        The additional parameters *base*, *subs* and *nonpositive* control the\n        x/y-axis properties. They are just forwarded to `.Axes.set_xscale` and\n        `.Axes.set_yscale`. To use different properties on the x-axis and the\n        y-axis, use e.g.\n        ``ax.set_xscale(\"log\", base=10); ax.set_yscale(\"log\", base=2)``.\n\n        Parameters\n        ----------\n        base : float, default: 10\n            Base of the logarithm.\n\n        subs : sequence, optional\n            The location of the minor ticks. If *None*, reasonable locations\n            are automatically chosen depending on the number of decades in the\n            plot. See `.Axes.set_xscale`/`.Axes.set_yscale` for details.\n\n        nonpositive : {'mask', 'clip'}, default: 'clip'\n            Non-positive values can be masked as invalid, or clipped to a very\n            small positive number.\n\n        **kwargs\n            All parameters supported by `.plot`.\n\n        Returns\n        -------\n        list of `.Line2D`\n            Objects representing the plotted data.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 1853, "code": "    def semilogx(self, *args, **kwargs):\n        d = {k: v for k, v in kwargs.items()\n             if k in ['base', 'subs', 'nonpositive',\n                      'basex', 'subsx', 'nonposx']}\n        self.set_xscale('log', **d)\n        return self.plot(\n            *args, **{k: v for k, v in kwargs.items() if k not in d})\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Make a plot with log scaling on the x-axis.\n\n        Call signatures::\n\n            semilogx([x], y, [fmt], data=None, **kwargs)\n            semilogx([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\n        This is just a thin wrapper around `.plot` which additionally changes\n        the x-axis to log scaling. All the concepts and parameters of plot can\n        be used here as well.\n\n        The additional parameters *base*, *subs*, and *nonpositive* control the\n        x-axis properties. They are just forwarded to `.Axes.set_xscale`.\n\n        Parameters\n        ----------\n        base : float, default: 10\n            Base of the x logarithm.\n\n        subs : array-like, optional\n            The location of the minor xticks. If *None*, reasonable locations\n            are automatically chosen depending on the number of decades in the\n            plot. See `.Axes.set_xscale` for details.\n\n        nonpositive : {'mask', 'clip'}, default: 'clip'\n            Non-positive values in x can be masked as invalid, or clipped to a\n            very small positive number.\n\n        **kwargs\n            All parameters supported by `.plot`.\n\n        Returns\n        -------\n        list of `.Line2D`\n            Objects representing the plotted data.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 1900, "code": "    def semilogy(self, *args, **kwargs):\n        d = {k: v for k, v in kwargs.items()\n             if k in ['base', 'subs', 'nonpositive',\n                      'basey', 'subsy', 'nonposy']}\n        self.set_yscale('log', **d)\n        return self.plot(\n            *args, **{k: v for k, v in kwargs.items() if k not in d})\n    @_preprocess_data(replace_names=[\"x\"], label_namer=\"x\")", "documentation": "        \"\"\"\n        Make a plot with log scaling on the y-axis.\n\n        Call signatures::\n\n            semilogy([x], y, [fmt], data=None, **kwargs)\n            semilogy([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\n        This is just a thin wrapper around `.plot` which additionally changes\n        the y-axis to log scaling. All the concepts and parameters of plot can\n        be used here as well.\n\n        The additional parameters *base*, *subs*, and *nonpositive* control the\n        y-axis properties. They are just forwarded to `.Axes.set_yscale`.\n\n        Parameters\n        ----------\n        base : float, default: 10\n            Base of the y logarithm.\n\n        subs : array-like, optional\n            The location of the minor yticks. If *None*, reasonable locations\n            are automatically chosen depending on the number of decades in the\n            plot. See `.Axes.set_yscale` for details.\n\n        nonpositive : {'mask', 'clip'}, default: 'clip'\n            Non-positive values in y can be masked as invalid, or clipped to a\n            very small positive number.\n\n        **kwargs\n            All parameters supported by `.plot`.\n\n        Returns\n        -------\n        list of `.Line2D`\n            Objects representing the plotted data.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 1946, "code": "    def acorr(self, x, **kwargs):\n        return self.xcorr(x, x, **kwargs)\n    @_api.make_keyword_only(\"3.10\", \"normed\")\n    @_preprocess_data(replace_names=[\"x\", \"y\"], label_namer=\"y\")", "documentation": "        \"\"\"\n        Plot the autocorrelation of *x*.\n\n        Parameters\n        ----------\n        x : array-like\n            Not run through Matplotlib's unit conversion, so this should\n            be a unit-less array.\n\n        detrend : callable, default: `.mlab.detrend_none` (no detrending)\n            A detrending function applied to *x*.  It must have the\n            signature ::\n\n                detrend(x: np.ndarray) -> np.ndarray\n\n        normed : bool, default: True\n            If ``True``, input vectors are normalised to unit length.\n\n        usevlines : bool, default: True\n            Determines the plot style.\n\n            If ``True``, vertical lines are plotted from 0 to the acorr value\n            using `.Axes.vlines`. Additionally, a horizontal line is plotted\n            at y=0 using `.Axes.axhline`.\n\n            If ``False``, markers are plotted at the acorr values using\n            `.Axes.plot`.\n\n        maxlags : int, default: 10\n            Number of lags to show. If ``None``, will return all\n            ``2 * len(x) - 1`` lags.\n\n        Returns\n        -------\n        lags : array (length ``2*maxlags+1``)\n            The lag vector.\n        c : array  (length ``2*maxlags+1``)\n            The auto correlation vector.\n        line : `.LineCollection` or `.Line2D`\n            `.Artist` added to the Axes of the correlation:\n\n            - `.LineCollection` if *usevlines* is True.\n            - `.Line2D` if *usevlines* is False.\n        b : `~matplotlib.lines.Line2D` or None\n            Horizontal line at 0 if *usevlines* is True\n            None *usevlines* is False.\n\n        Other Parameters\n        ----------------\n        linestyle : `~matplotlib.lines.Line2D` property, optional\n            The linestyle for plotting the data points.\n            Only used if *usevlines* is ``False``.\n\n        marker : str, default: 'o'\n            The marker for plotting the data points.\n            Only used if *usevlines* is ``False``.\n\n        data : indexable object, optional\n            DATA_PARAMETER_PLACEHOLDER\n\n        **kwargs\n            Additional parameters are passed to `.Axes.vlines` and\n            `.Axes.axhline` if *usevlines* is ``True``; otherwise they are\n            passed to `.Axes.plot`.\n\n        Notes\n        -----\n        The cross correlation is performed with `numpy.correlate` with\n        ``mode = \"full\"``.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 2134, "code": "    def step(self, x, y, *args, where='pre', data=None, **kwargs):\n        _api.check_in_list(('pre', 'post', 'mid'), where=where)\n        kwargs['drawstyle'] = 'steps-' + where\n        return self.plot(x, y, *args, data=data, **kwargs)\n    @staticmethod", "documentation": "        \"\"\"\n        Make a step plot.\n\n        Call signatures::\n\n            step(x, y, [fmt], *, data=None, where='pre', **kwargs)\n            step(x, y, [fmt], x2, y2, [fmt2], ..., *, where='pre', **kwargs)\n\n        This is just a thin wrapper around `.plot` which changes some\n        formatting options. Most of the concepts and parameters of plot can be\n        used here as well.\n\n        .. note::\n\n            This method uses a standard plot with a step drawstyle: The *x*\n            values are the reference positions and steps extend left/right/both\n            directions depending on *where*.\n\n            For the common case where you know the values and edges of the\n            steps, use `~.Axes.stairs` instead.\n\n        Parameters\n        ----------\n        x : array-like\n            1D sequence of x positions. It is assumed, but not checked, that\n            it is uniformly increasing.\n\n        y : array-like\n            1D sequence of y levels.\n\n        fmt : str, optional\n            A format string, e.g. 'g' for a green line. See `.plot` for a more\n            detailed description.\n\n            Note: While full format strings are accepted, it is recommended to\n            only specify the color. Line styles are currently ignored (use\n            the keyword argument *linestyle* instead). Markers are accepted\n            and plotted on the given positions, however, this is a rarely\n            needed feature for step plots.\n\n        where : {'pre', 'post', 'mid'}, default: 'pre'\n            Define where the steps should be placed:\n\n            - 'pre': The y value is continued constantly to the left from\n              every *x* position, i.e. the interval ``(x[i-1], x[i]]`` has the\n              value ``y[i]``.\n            - 'post': The y value is continued constantly to the right from\n              every *x* position, i.e. the interval ``[x[i], x[i+1])`` has the\n              value ``y[i]``.\n            - 'mid': Steps occur half-way between the *x* positions.\n\n        data : indexable object, optional\n            An object with labelled data. If given, provide the label names to\n            plot in *x* and *y*.\n\n        **kwargs\n            Additional parameters are the same as those for `.plot`.\n\n        Returns\n        -------\n        list of `.Line2D`\n            Objects representing the plotted data.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 2203, "code": "    def _convert_dx(dx, x0, xconv, convert):\n        assert type(xconv) is np.ndarray\n        if xconv.size == 0:\n            return convert(dx)\n        try:\n            try:\n                x0 = cbook._safe_first_finite(x0)\n            except (TypeError, IndexError, KeyError):\n                pass\n            try:\n                x = cbook._safe_first_finite(xconv)", "documentation": "        \"\"\"\n        Small helper to do logic of width conversion flexibly.\n\n        *dx* and *x0* have units, but *xconv* has already been converted\n        to unitless (and is an ndarray).  This allows the *dx* to have units\n        that are different from *x0*, but are still accepted by the\n        ``__add__`` operator of *x0*.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 2254, "code": "    def _parse_bar_color_args(self, kwargs):\n        color = kwargs.pop('color', None)\n        facecolor = kwargs.pop('facecolor', color)\n        edgecolor = kwargs.pop('edgecolor', None)\n        facecolor = (facecolor if facecolor is not None\n                     else self._get_patches_for_fill.get_next_color())\n        try:\n            facecolor = mcolors.to_rgba_array(facecolor)\n        except ValueError as err:\n            raise ValueError(\n                \"'facecolor' or 'color' argument must be a valid color or \"", "documentation": "        \"\"\"\n        Helper function to process color-related arguments of `.Axes.bar`.\n\n        Argument precedence for facecolors:\n\n        - kwargs['facecolor']\n        - kwargs['color']\n        - 'Result of ``self._get_patches_for_fill.get_next_color``\n\n        Argument precedence for edgecolors:\n\n        - kwargs['edgecolor']\n        - None\n\n        Parameters\n        ----------\n        self : Axes\n\n        kwargs : dict\n            Additional kwargs. If these keys exist, we pop and process them:\n            'facecolor', 'edgecolor', 'color'\n            Note: The dict is modified by this function.\n\n\n        Returns\n        -------\n        facecolor\n            The facecolor. One or more colors as (N, 4) rgba array.\n        edgecolor\n            The edgecolor. Not normalized; may be any valid color spec or None.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 2968, "code": "    def broken_barh(self, xranges, yrange, align=\"bottom\", **kwargs):\n        xdata = cbook._safe_first_finite(xranges) if len(xranges) else None\n        ydata = cbook._safe_first_finite(yrange) if len(yrange) else None\n        self._process_unit_info(\n            [(\"x\", xdata), (\"y\", ydata)], kwargs, convert=False)\n        vertices = []\n        y0, dy = yrange\n        _api.check_in_list(['bottom', 'center', 'top'], align=align)\n        if align == \"bottom\":\n            y0, y1 = self.convert_yunits((y0, y0 + dy))\n        elif align == \"center\":", "documentation": "        \"\"\"\n        Plot a horizontal sequence of rectangles.\n\n        A rectangle is drawn for each element of *xranges*. All rectangles\n        have the same vertical position and size defined by *yrange*.\n\n        Parameters\n        ----------\n        xranges : sequence of tuples (*xmin*, *xwidth*)\n            The x-positions and extents of the rectangles. For each tuple\n            (*xmin*, *xwidth*) a rectangle is drawn from *xmin* to *xmin* +\n            *xwidth*.\n        yrange : (*ypos*, *yheight*)\n            The y-position and extent for all the rectangles.\n        align : {\"bottom\", \"center\", \"top\"}, default: 'bottom'\n            The alignment of the yrange with respect to the y-position. One of:\n\n            - \"bottom\": Resulting y-range [ypos, ypos + yheight]\n            - \"center\": Resulting y-range [ypos - yheight/2, ypos + yheight/2]\n            - \"top\": Resulting y-range [ypos - yheight, ypos]\n\n            .. versionadded:: 3.11\n\n        Returns\n        -------\n        `~.collections.PolyCollection`\n\n        Other Parameters\n        ----------------\n        data : indexable object, optional\n            DATA_PARAMETER_PLACEHOLDER\n        **kwargs : `.PolyCollection` properties\n\n            Each *kwarg* can be either a single argument applying to all\n            rectangles, e.g.::\n\n                facecolors='black'\n\n            or a sequence of arguments over which is cycled, e.g.::\n\n                facecolors=('black', 'blue')\n\n            would create interleaving black and blue rectangles.\n\n            Supported keywords:\n\n            %(PolyCollection:kwdoc)s\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 3769, "code": "    def _errorevery_to_mask(x, errorevery):\n        if isinstance(errorevery, Integral):\n            errorevery = (0, errorevery)\n        if isinstance(errorevery, tuple):\n            if (len(errorevery) == 2 and\n                    isinstance(errorevery[0], Integral) and\n                    isinstance(errorevery[1], Integral)):\n                errorevery = slice(errorevery[0], None, errorevery[1])\n            else:\n                raise ValueError(\n                    f'{errorevery=!r} is a not a tuple of two integers')", "documentation": "        \"\"\"\n        Normalize `errorbar`'s *errorevery* to be a boolean mask for data *x*.\n\n        This function is split out to be usable both by 2D and 3D errorbars.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 3959, "code": "        def _upcast_err(err):\n            if (\n                    np.iterable(err) and\n                    len(err) > 0 and\n                    isinstance(cbook._safe_first_finite(err), np.ndarray)\n            ):\n                atype = type(cbook._safe_first_finite(err))\n                if atype is np.ndarray:\n                    return np.asarray(err, dtype=object)\n                return atype(err)\n            return np.asarray(err, dtype=object)", "documentation": "            \"\"\"\n            Safely handle tuple of containers that carry units.\n\n            This function covers the case where the input to the xerr/yerr is a\n            length 2 tuple of equal length ndarray-subclasses that carry the\n            unit information in the container.\n\n            If we have a tuple of nested numpy array (subclasses), we defer\n            coercing the units to be consistent to the underlying unit\n            library (and implicitly the broadcasting).\n\n            Otherwise, fallback to casting to an object array.\n            \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 5762, "code": "    def arrow(self, x, y, dx, dy, **kwargs):\n        x = self.convert_xunits(x)\n        y = self.convert_yunits(y)\n        dx = self.convert_xunits(dx)\n        dy = self.convert_yunits(dy)\n        a = mpatches.FancyArrow(x, y, dx, dy, **kwargs)\n        self.add_patch(a)\n        self._request_autoscale_view()\n        return a\n    @_docstring.copy(mquiver.QuiverKey.__init__)", "documentation": "        \"\"\"\n        [*Discouraged*] Add an arrow to the Axes.\n\n        This draws an arrow from ``(x, y)`` to ``(x+dx, y+dy)``.\n\n        .. admonition:: Discouraged\n\n            The use of this method is discouraged because it is not guaranteed\n            that the arrow renders reasonably. For example, the resulting arrow\n            is affected by the Axes aspect ratio and limits, which may distort\n            the arrow.\n\n            Consider using `~.Axes.annotate` without a text instead, e.g. ::\n\n                ax.annotate(\"\", xytext=(0, 0), xy=(0.5, 0.5),\n                            arrowprops=dict(arrowstyle=\"->\"))\n\n        Parameters\n        ----------\n        %(FancyArrow)s\n\n        Returns\n        -------\n        `.FancyArrow`\n            The created `.FancyArrow` object.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 5818, "code": "    def quiver(self, *args, **kwargs):\n        args = self._quiver_units(args, kwargs)\n        q = mquiver.Quiver(self, *args, **kwargs)\n        self.add_collection(q)\n        return q\n    @_preprocess_data()\n    @_docstring.interpd", "documentation": "        \"\"\"%(quiver_doc)s\"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 5829, "code": "    def barbs(self, *args, **kwargs):\n        args = self._quiver_units(args, kwargs)\n        b = mquiver.Barbs(self, *args, **kwargs)\n        self.add_collection(b)\n        return b", "documentation": "        \"\"\"%(barbs_doc)s\"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 5839, "code": "    def fill(self, *args, data=None, **kwargs):\n        kwargs = cbook.normalize_kwargs(kwargs, mlines.Line2D)\n        patches = [*self._get_patches_for_fill(self, *args, data=data, **kwargs)]\n        for poly in patches:\n            self.add_patch(poly)\n        self._request_autoscale_view()\n        return patches", "documentation": "        \"\"\"\n        Plot filled polygons.\n\n        Parameters\n        ----------\n        *args : sequence of x, y, [color]\n            Each polygon is defined by the lists of *x* and *y* positions of\n            its nodes, optionally followed by a *color* specifier. See\n            :mod:`matplotlib.colors` for supported color specifiers. The\n            standard color cycle is used for polygons without a color\n            specifier.\n\n            You can plot multiple polygons by providing multiple *x*, *y*,\n            *[color]* groups.\n\n            For example, each of the following is legal::\n\n                ax.fill(x, y)                    # a polygon with default color\n                ax.fill(x, y, \"b\")               # a blue polygon\n                ax.fill(x, y, x2, y2)            # two polygons\n                ax.fill(x, y, \"b\", x2, y2, \"r\")  # a blue and a red polygon\n\n        data : indexable object, optional\n            An object with labelled data. If given, provide the label names to\n            plot in *x* and *y*, e.g.::\n\n                ax.fill(\"time\", \"signal\",\n                        data={\"time\": [0, 1, 2], \"signal\": [0, 1, 0]})\n\n        Returns\n        -------\n        list of `~matplotlib.patches.Polygon`\n\n        Other Parameters\n        ----------------\n        **kwargs : `~matplotlib.patches.Polygon` properties\n\n        Notes\n        -----\n        Use :meth:`fill_between` if you would like to fill the region between\n        two curves.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 5996, "code": "    def _fill_between_process_units(self, ind_dir, dep_dir, ind, dep1, dep2, **kwargs):\n        return map(np.ma.masked_invalid, self._process_unit_info(\n            [(ind_dir, ind), (dep_dir, dep1), (dep_dir, dep2)], kwargs))", "documentation": "        \"\"\"Handle united data, such as dates.\"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 6801, "code": "    def _update_pcolor_lims(self, collection, coords):\n        t = collection._transform\n        if (not isinstance(t, mtransforms.Transform) and\n                hasattr(t, '_as_mpl_transform')):\n            t = t._as_mpl_transform(self.axes)\n        if t and any(t.contains_branch_separately(self.transData)):\n            trans_to_data = t - self.transData\n            coords = trans_to_data.transform(coords)\n        self.add_collection(collection, autolim=False)\n        minx, miny = np.min(coords, axis=0)\n        maxx, maxy = np.max(coords, axis=0)", "documentation": "        \"\"\"\n        Common code for updating lims in pcolor() and pcolormesh() methods.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 7028, "code": "    def contour(self, *args, **kwargs):\n        kwargs['filled'] = False\n        contours = mcontour.QuadContourSet(self, *args, **kwargs)\n        self._request_autoscale_view()\n        return contours\n    @_preprocess_data()\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Plot contour lines.\n\n        Call signature::\n\n            contour([X, Y,] Z, /, [levels], **kwargs)\n\n        The arguments *X*, *Y*, *Z* are positional-only.\n        %(contour_doc)s\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 7046, "code": "    def contourf(self, *args, **kwargs):\n        kwargs['filled'] = True\n        contours = mcontour.QuadContourSet(self, *args, **kwargs)\n        self._request_autoscale_view()\n        return contours", "documentation": "        \"\"\"\n        Plot filled contours.\n\n        Call signature::\n\n            contourf([X, Y,] Z, /, [levels], **kwargs)\n\n        The arguments *X*, *Y*, *Z* are positional-only.\n        %(contour_doc)s\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 7062, "code": "    def clabel(self, CS, levels=None, **kwargs):\n        return CS.clabel(levels, **kwargs)\n    @_api.make_keyword_only(\"3.10\", \"range\")\n    @_preprocess_data(replace_names=[\"x\", 'weights'], label_namer=\"x\")", "documentation": "        \"\"\"\n        Label a contour plot.\n\n        Adds labels to line contours in given `.ContourSet`.\n\n        Parameters\n        ----------\n        CS : `.ContourSet` instance\n            Line contours to label.\n\n        levels : array-like, optional\n            A list of level values, that should be labeled. The list must be\n            a subset of ``CS.levels``. If not given, all levels are labeled.\n\n        **kwargs\n            All other parameters are documented in `~.ContourLabeler.clabel`.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 8708, "code": "    def matshow(self, Z, **kwargs):\n        Z = np.asanyarray(Z)\n        kw = {'origin': 'upper',\n              'interpolation': 'nearest',\n              'aspect': 'equal',          # (already the imshow default)\n              **kwargs}\n        im = self.imshow(Z, **kw)\n        self.title.set_y(1.05)\n        self.xaxis.tick_top()\n        self.xaxis.set_ticks_position('both')\n        self.xaxis.set_major_locator(", "documentation": "        \"\"\"\n        Plot the values of a 2D matrix or array as color-coded image.\n\n        The matrix will be shown the way it would be printed, with the first\n        row at the top.  Row and column numbering is zero-based.\n\n        Parameters\n        ----------\n        Z : (M, N) array-like\n            The matrix to be displayed.\n\n        Returns\n        -------\n        `~matplotlib.image.AxesImage`\n\n        Other Parameters\n        ----------------\n        **kwargs : `~matplotlib.axes.Axes.imshow` arguments\n\n        See Also\n        --------\n        imshow : More general function to plot data on a 2D regular raster.\n\n        Notes\n        -----\n        This is just a convenience function wrapping `.imshow` to set useful\n        defaults for displaying a matrix. In particular:\n\n        - Set ``origin='upper'``.\n        - Set ``interpolation='nearest'``.\n        - Set ``aspect='equal'``.\n        - Ticks are placed to the left and above.\n        - Ticks are formatted to show integer indices.\n\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 9183, "code": "    def _get_aspect_ratio(self):\n        figure_size = self.get_figure().get_size_inches()\n        ll, ur = self.get_position() * figure_size\n        width, height = ur - ll\n        return height / (width * self.get_data_ratio())", "documentation": "        \"\"\"\n        Convenience method to calculate the aspect ratio of the Axes in\n        the display coordinate system.\n        \"\"\""}], "after_segments": [{"filename": "lib/matplotlib/axes/_axes.py", "start_line": 48, "code": "def _make_axes_method(func):\n    func.__qualname__ = f\"Axes.{func.__name__}\"\n    return func", "documentation": "    \"\"\"\n    Patch the qualname for functions that are directly added to Axes.\n\n    Some Axes functionality is defined in functions in other submodules.\n    These are simply added as attributes to Axes. As a result, their\n    ``__qualname__`` is e.g. only \"table\" and not \"Axes.table\". This\n    function fixes that.\n\n    Note that the function itself is patched, so that\n    ``matplotlib.table.table.__qualname__` will also show \"Axes.table\".\n    However, since these functions are not intended to be standalone,\n    this is bearable.\n    \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 66, "code": "class _GroupedBarReturn:", "documentation": "    \"\"\"\n    A provisional result object for `.Axes.grouped_bar`.\n\n    This is a placeholder for a future better return type. We try to build in\n    backward compatibility / migration possibilities.\n\n    The only public interfaces are the ``bar_containers`` attribute and the\n    ``remove()`` method.\n    \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 84, "code": "class Axes(_AxesBase):", "documentation": "    \"\"\"\n    An Axes object encapsulates all the elements of an individual (sub-)plot in\n    a figure.\n\n    It contains most of the (sub-)plot elements: `~.axis.Axis`,\n    `~.axis.Tick`, `~.lines.Line2D`, `~.text.Text`, `~.patches.Polygon`, etc.,\n    and sets the coordinate system.\n\n    Like all visible elements in a figure, Axes is an `.Artist` subclass.\n\n    The `Axes` instance supports callbacks through a callbacks attribute which\n    is a `~.cbook.CallbackRegistry` instance.  The events you can connect to\n    are 'xlim_changed' and 'ylim_changed' and the callback will be called with\n    func(*ax*) where *ax* is the `Axes` instance.\n\n    .. note::\n\n        As a user, you do not instantiate Axes directly, but use Axes creation\n        methods instead; e.g. from `.pyplot` or `.Figure`:\n        `~.pyplot.subplots`, `~.pyplot.subplot_mosaic` or `.Figure.add_axes`.\n\n    \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 109, "code": "    def get_title(self, loc=\"center\"):\n        titles = {'left': self._left_title,\n                  'center': self.title,\n                  'right': self._right_title}\n        title = _api.check_getitem(titles, loc=loc.lower())\n        return title.get_text()", "documentation": "        \"\"\"\n        Get an Axes title.\n\n        Get one of the three available Axes titles. The available titles\n        are positioned above the Axes in the center, flush with the left\n        edge, and flush with the right edge.\n\n        Parameters\n        ----------\n        loc : {'center', 'left', 'right'}, str, default: 'center'\n            Which title to return.\n\n        Returns\n        -------\n        str\n            The title text string.\n\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 215, "code": "    def get_legend_handles_labels(self, legend_handler_map=None):\n        handles, labels = mlegend._get_legend_handles_labels(\n            [self], legend_handler_map)\n        return handles, labels\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Return handles and labels for legend\n\n        ``ax.legend()`` is equivalent to ::\n\n          h, l = ax.get_legend_handles_labels()\n          ax.legend(h, l)\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 230, "code": "    def legend(self, *args, **kwargs):\n        handles, labels, kwargs = mlegend._parse_legend_args([self], *args, **kwargs)\n        self.legend_ = mlegend.Legend(self, handles, labels, **kwargs)\n        self.legend_._remove_method = self._remove_legend\n        return self.legend_", "documentation": "        \"\"\"\n        Place a legend on the Axes.\n\n        Call signatures::\n\n            legend()\n            legend(handles, labels)\n            legend(handles=handles)\n            legend(labels)\n\n        The call signatures correspond to the following different ways to use\n        this method:\n\n        **1. Automatic detection of elements to be shown in the legend**\n\n        The elements to be added to the legend are automatically determined,\n        when you do not pass in any extra arguments.\n\n        In this case, the labels are taken from the artist. You can specify\n        them either at artist creation or by calling the\n        :meth:`~.Artist.set_label` method on the artist::\n\n            ax.plot([1, 2, 3], label='Inline label')\n            ax.legend()\n\n        or::\n\n            line, = ax.plot([1, 2, 3])\n            line.set_label('Label via method')\n            ax.legend()\n\n        .. note::\n            Specific artists can be excluded from the automatic legend element\n            selection by using a label starting with an underscore, \"_\".\n            A string starting with an underscore is the default label for all\n            artists, so calling `.Axes.legend` without any arguments and\n            without setting the labels manually will result in a ``UserWarning``\n            and an empty legend being drawn.\n\n\n        **2. Explicitly listing the artists and labels in the legend**\n\n        For full control of which artists have a legend entry, it is possible\n        to pass an iterable of legend artists followed by an iterable of\n        legend labels respectively::\n\n            ax.legend([line1, line2, line3], ['label1', 'label2', 'label3'])\n\n\n        **3. Explicitly listing the artists in the legend**\n\n        This is similar to 2, but the labels are taken from the artists'\n        label properties. Example::\n\n            line1, = ax.plot([1, 2, 3], label='label1')\n            line2, = ax.plot([1, 2, 3], label='label2')\n            ax.legend(handles=[line1, line2])\n\n\n        **4. Labeling existing plot elements**\n\n        .. admonition:: Discouraged\n\n            This call signature is discouraged, because the relation between\n            plot elements and labels is only implicit by their order and can\n            easily be mixed up.\n\n        To make a legend for all artists on an Axes, call this function with\n        an iterable of strings, one for each legend item. For example::\n\n            ax.plot([1, 2, 3])\n            ax.plot([5, 6, 7])\n            ax.legend(['First line', 'Second line'])\n\n\n        Parameters\n        ----------\n        handles : list of (`.Artist` or tuple of `.Artist`), optional\n            A list of Artists (lines, patches) to be added to the legend.\n            Use this together with *labels*, if you need full control on what\n            is shown in the legend and the automatic mechanism described above\n            is not sufficient.\n\n            The length of handles and labels should be the same in this\n            case. If they are not, they are truncated to the smaller length.\n\n            If an entry contains a tuple, then the legend handler for all Artists in the\n            tuple will be placed alongside a single label.\n\n        labels : list of str, optional\n            A list of labels to show next to the artists.\n            Use this together with *handles*, if you need full control on what\n            is shown in the legend and the automatic mechanism described above\n            is not sufficient.\n\n        Returns\n        -------\n        `~matplotlib.legend.Legend`\n\n        Other Parameters\n        ----------------\n        %(_legend_kw_axes)s\n\n        See Also\n        --------\n        .Figure.legend\n\n        Notes\n        -----\n        Some artists are not supported by this function.  See\n        :ref:`legend_guide` for details.\n\n        Examples\n        --------\n        .. plot:: gallery/text_labels_and_annotations/legend.py\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 355, "code": "    def inset_axes(self, bounds, *, transform=None, zorder=5, **kwargs):\n        if transform is None:\n            transform = self.transAxes\n        kwargs.setdefault('label', 'inset_axes')\n        inset_locator = _TransformedBoundsLocator(bounds, transform)\n        bounds = inset_locator(self, None).bounds\n        fig = self.get_figure(root=False)\n        projection_class, pkw = fig._process_projection_requirements(**kwargs)\n        inset_ax = projection_class(fig, bounds, zorder=zorder, **pkw)\n        inset_ax.set_axes_locator(inset_locator)\n        self.add_child_axes(inset_ax)", "documentation": "        \"\"\"\n        Add a child inset Axes to this existing Axes.\n\n\n        Parameters\n        ----------\n        bounds : [x0, y0, width, height]\n            Lower-left corner of inset Axes, and its width and height.\n\n        transform : `.Transform`\n            Defaults to `!ax.transAxes`, i.e. the units of *rect* are in\n            Axes-relative coordinates.\n\n        projection : {None, 'aitoff', 'hammer', 'lambert', 'mollweide', \\\n'polar', 'rectilinear', str}, optional\n            The projection type of the inset `~.axes.Axes`. *str* is the name\n            of a custom projection, see `~matplotlib.projections`. The default\n            None results in a 'rectilinear' projection.\n\n        polar : bool, default: False\n            If True, equivalent to projection='polar'.\n\n        axes_class : subclass type of `~.axes.Axes`, optional\n            The `.axes.Axes` subclass that is instantiated.  This parameter\n            is incompatible with *projection* and *polar*.  See\n            :ref:`axisartist_users-guide-index` for examples.\n\n        zorder : number\n            Defaults to 5 (same as `.Axes.legend`).  Adjust higher or lower\n            to change whether it is above or below data plotted on the\n            parent Axes.\n\n        **kwargs\n            Other keyword arguments are passed on to the inset Axes class.\n\n        Returns\n        -------\n        ax\n            The created `~.axes.Axes` instance.\n\n        Examples\n        --------\n        This example makes two inset Axes, the first is in Axes-relative\n        coordinates, and the second in data-coordinates::\n\n            fig, ax = plt.subplots()\n            ax.plot(range(10))\n            axin1 = ax.inset_axes([0.8, 0.1, 0.15, 0.15])\n            axin2 = ax.inset_axes(\n                    [5, 7, 2.3, 2.3], transform=ax.transData)\n\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 510, "code": "    def indicate_inset_zoom(self, inset_ax, **kwargs):\n        return self.indicate_inset(None, inset_ax, **kwargs)\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Add an inset indicator rectangle to the Axes based on the axis\n        limits for an *inset_ax* and draw connectors between *inset_ax*\n        and the rectangle.\n\n        Warnings\n        --------\n        This method is experimental as of 3.0, and the API may change.\n\n        Parameters\n        ----------\n        inset_ax : `.Axes`\n            Inset Axes to draw connecting lines to.  Two lines are\n            drawn connecting the indicator box to the inset Axes on corners\n            chosen so as to not overlap with the indicator box.\n\n        **kwargs\n            Other keyword arguments are passed on to `.Axes.indicate_inset`\n\n        Returns\n        -------\n        inset_indicator : `.inset.InsetIndicator`\n            An artist which contains\n\n            inset_indicator.rectangle : `.Rectangle`\n                The indicator frame.\n\n            inset_indicator.connectors : 4-tuple of `.patches.ConnectionPatch`\n                The four connector lines connecting to (lower_left, upper_left,\n                lower_right upper_right) corners of *inset_ax*. Two lines are\n                set with visibility to *False*,  but the user can set the\n                visibility to True if the automatic choice is not deemed correct.\n\n            .. versionchanged:: 3.10\n                Previously the rectangle and connectors tuple were returned.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 551, "code": "    def secondary_xaxis(self, location, functions=None, *, transform=None, **kwargs):\n        if not (location in ['top', 'bottom'] or isinstance(location, Real)):\n            raise ValueError('secondary_xaxis location must be either '\n                             'a float or \"top\"/\"bottom\"')\n        secondary_ax = SecondaryAxis(self, 'x', location, functions,\n                                     transform, **kwargs)\n        self.add_child_axes(secondary_ax)\n        return secondary_ax\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Add a second x-axis to this `~.axes.Axes`.\n\n        For example if we want to have a second scale for the data plotted on\n        the xaxis.\n\n        %(_secax_docstring)s\n\n        Examples\n        --------\n        The main axis shows frequency, and the secondary axis shows period.\n\n        .. plot::\n\n            fig, ax = plt.subplots()\n            ax.loglog(range(1, 360, 5), range(1, 360, 5))\n            ax.set_xlabel('frequency [Hz]')\n\n            def invert(x):\n                # 1/x with special treatment of x == 0\n                x = np.array(x).astype(float)\n                near_zero = np.isclose(x, 0)\n                x[near_zero] = np.inf\n                x[~near_zero] = 1 / x[~near_zero]\n                return x\n\n            # the inverse of 1/x is itself\n            secax = ax.secondary_xaxis('top', functions=(invert, invert))\n            secax.set_xlabel('Period [s]')\n            plt.show()\n\n        To add a secondary axis relative to your data, you can pass a transform\n        to the new axis.\n\n        .. plot::\n\n            fig, ax = plt.subplots()\n            ax.plot(range(0, 5), range(-1, 4))\n\n            # Pass 'ax.transData' as a transform to place the axis\n            # relative to your data at y=0\n            secax = ax.secondary_xaxis(0, transform=ax.transData)\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 605, "code": "    def secondary_yaxis(self, location, functions=None, *, transform=None, **kwargs):\n        if not (location in ['left', 'right'] or isinstance(location, Real)):\n            raise ValueError('secondary_yaxis location must be either '\n                             'a float or \"left\"/\"right\"')\n        secondary_ax = SecondaryAxis(self, 'y', location, functions,\n                                     transform, **kwargs)\n        self.add_child_axes(secondary_ax)\n        return secondary_ax\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Add a second y-axis to this `~.axes.Axes`.\n\n        For example if we want to have a second scale for the data plotted on\n        the yaxis.\n\n        %(_secax_docstring)s\n\n        Examples\n        --------\n        Add a secondary Axes that converts from radians to degrees\n\n        .. plot::\n\n            fig, ax = plt.subplots()\n            ax.plot(range(1, 360, 5), range(1, 360, 5))\n            ax.set_ylabel('degrees')\n            secax = ax.secondary_yaxis('right', functions=(np.deg2rad,\n                                                           np.rad2deg))\n            secax.set_ylabel('radians')\n\n        To add a secondary axis relative to your data, you can pass a transform\n        to the new axis.\n\n        .. plot::\n\n            fig, ax = plt.subplots()\n            ax.plot(range(0, 5), range(-1, 4))\n\n            # Pass 'ax.transData' as a transform to place the axis\n            # relative to your data at x=3\n            secax = ax.secondary_yaxis(3, transform=ax.transData)\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 649, "code": "    def text(self, x, y, s, fontdict=None, **kwargs):\n        effective_kwargs = {\n            'verticalalignment': 'baseline',\n            'horizontalalignment': 'left',\n            'transform': self.transData,\n            'clip_on': False,\n            **(fontdict if fontdict is not None else {}),\n            **kwargs,\n        }\n        t = mtext.Text(x, y, text=s, **effective_kwargs)\n        if t.get_clip_path() is None:", "documentation": "        \"\"\"\n        Add text to the Axes.\n\n        Add the text *s* to the Axes at location *x*, *y* in data coordinates,\n        with a default ``horizontalalignment`` on the ``left`` and\n        ``verticalalignment`` at the ``baseline``. See\n        :doc:`/gallery/text_labels_and_annotations/text_alignment`.\n\n        Parameters\n        ----------\n        x, y : float\n            The position to place the text. By default, this is in data\n            coordinates. The coordinate system can be changed using the\n            *transform* parameter.\n\n        s : str\n            The text.\n\n        fontdict : dict, default: None\n\n            .. admonition:: Discouraged\n\n               The use of *fontdict* is discouraged. Parameters should be passed as\n               individual keyword arguments or using dictionary-unpacking\n               ``text(..., **fontdict)``.\n\n            A dictionary to override the default text properties. If fontdict\n            is None, the defaults are determined by `.rcParams`.\n\n        Returns\n        -------\n        `.Text`\n            The created `.Text` instance.\n\n        Other Parameters\n        ----------------\n        **kwargs : `~matplotlib.text.Text` properties.\n            Other miscellaneous text parameters.\n\n            %(Text:kwdoc)s\n\n        Examples\n        --------\n        Individual keyword arguments can be used to override any given\n        parameter::\n\n            >>> text(x, y, s, fontsize=12)\n\n        The default transform specifies that text is in data coords,\n        alternatively, you can specify text in axis coords ((0, 0) is\n        lower-left and (1, 1) is upper-right).  The example below places\n        text in the center of the Axes::\n\n            >>> text(0.5, 0.5, 'matplotlib', horizontalalignment='center',\n            ...      verticalalignment='center', transform=ax.transAxes)\n\n        You can put a rectangular box around the text instance (e.g., to\n        set a background color) by using the keyword *bbox*.  *bbox* is\n        a dictionary of `~matplotlib.patches.Rectangle`\n        properties.  For example::\n\n            >>> text(x, y, s, bbox=dict(facecolor='red', alpha=0.5))\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 744, "code": "    def axhline(self, y=0, xmin=0, xmax=1, **kwargs):\n        self._check_no_units([xmin, xmax], ['xmin', 'xmax'])\n        if \"transform\" in kwargs:\n            raise ValueError(\"'transform' is not allowed as a keyword \"\n                             \"argument; axhline generates its own transform.\")\n        ymin, ymax = self.get_ybound()\n        yy, = self._process_unit_info([(\"y\", y)], kwargs)\n        scaley = (yy < ymin) or (yy > ymax)\n        trans = self.get_yaxis_transform(which='grid')\n        l = mlines.Line2D([xmin, xmax], [y, y], transform=trans, **kwargs)\n        self.add_line(l)", "documentation": "        \"\"\"\n        Add a horizontal line spanning the whole or fraction of the Axes.\n\n        Note: If you want to set x-limits in data coordinates, use\n        `~.Axes.hlines` instead.\n\n        Parameters\n        ----------\n        y : float, default: 0\n            y position in :ref:`data coordinates <coordinate-systems>`.\n\n        xmin : float, default: 0\n            The start x-position in :ref:`axes coordinates <coordinate-systems>`.\n            Should be between 0 and 1, 0 being the far left of the plot,\n            1 the far right of the plot.\n\n        xmax : float, default: 1\n            The end x-position in :ref:`axes coordinates <coordinate-systems>`.\n            Should be between 0 and 1, 0 being the far left of the plot,\n            1 the far right of the plot.\n\n        Returns\n        -------\n        `~matplotlib.lines.Line2D`\n            A `.Line2D` specified via two points ``(xmin, y)``, ``(xmax, y)``.\n            Its transform is set such that *x* is in\n            :ref:`axes coordinates <coordinate-systems>` and *y* is in\n            :ref:`data coordinates <coordinate-systems>`.\n\n            This is still a generic line and the horizontal character is only\n            realized through using identical *y* values for both points. Thus,\n            if you want to change the *y* value later, you have to provide two\n            values ``line.set_ydata([3, 3])``.\n\n        Other Parameters\n        ----------------\n        **kwargs\n            Valid keyword arguments are `.Line2D` properties, except for\n            'transform':\n\n            %(Line2D:kwdoc)s\n\n        See Also\n        --------\n        hlines : Add horizontal lines in data coordinates.\n        axhspan : Add a horizontal span (rectangle) across the axis.\n        axline : Add a line with an arbitrary slope.\n\n        Examples\n        --------\n        * draw a thick red hline at 'y' = 0 that spans the xrange::\n\n            >>> axhline(linewidth=4, color='r')\n\n        * draw a default hline at 'y' = 1 that spans the xrange::\n\n            >>> axhline(y=1)\n\n        * draw a default hline at 'y' = .5 that spans the middle half of\n          the xrange::\n\n            >>> axhline(y=.5, xmin=0.25, xmax=0.75)\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 827, "code": "    def axvline(self, x=0, ymin=0, ymax=1, **kwargs):\n        self._check_no_units([ymin, ymax], ['ymin', 'ymax'])\n        if \"transform\" in kwargs:\n            raise ValueError(\"'transform' is not allowed as a keyword \"\n                             \"argument; axvline generates its own transform.\")\n        xmin, xmax = self.get_xbound()\n        xx, = self._process_unit_info([(\"x\", x)], kwargs)\n        scalex = (xx < xmin) or (xx > xmax)\n        trans = self.get_xaxis_transform(which='grid')\n        l = mlines.Line2D([x, x], [ymin, ymax], transform=trans, **kwargs)\n        self.add_line(l)", "documentation": "        \"\"\"\n        Add a vertical line spanning the whole or fraction of the Axes.\n\n        Note: If you want to set y-limits in data coordinates, use\n        `~.Axes.vlines` instead.\n\n        Parameters\n        ----------\n        x : float, default: 0\n            x position in :ref:`data coordinates <coordinate-systems>`.\n\n        ymin : float, default: 0\n            The start y-position in :ref:`axes coordinates <coordinate-systems>`.\n            Should be between 0 and 1, 0 being the bottom of the plot, 1 the\n            top of the plot.\n\n        ymax : float, default: 1\n            The end y-position in :ref:`axes coordinates <coordinate-systems>`.\n            Should be between 0 and 1, 0 being the bottom of the plot, 1 the\n            top of the plot.\n\n        Returns\n        -------\n        `~matplotlib.lines.Line2D`\n            A `.Line2D` specified via two points ``(x, ymin)``, ``(x, ymax)``.\n            Its transform is set such that *x* is in\n            :ref:`data coordinates <coordinate-systems>` and *y* is in\n            :ref:`axes coordinates <coordinate-systems>`.\n\n            This is still a generic line and the vertical character is only\n            realized through using identical *x* values for both points. Thus,\n            if you want to change the *x* value later, you have to provide two\n            values ``line.set_xdata([3, 3])``.\n\n        Other Parameters\n        ----------------\n        **kwargs\n            Valid keyword arguments are `.Line2D` properties, except for\n            'transform':\n\n            %(Line2D:kwdoc)s\n\n        See Also\n        --------\n        vlines : Add vertical lines in data coordinates.\n        axvspan : Add a vertical span (rectangle) across the axis.\n        axline : Add a line with an arbitrary slope.\n\n        Examples\n        --------\n        * draw a thick red vline at *x* = 0 that spans the yrange::\n\n            >>> axvline(linewidth=4, color='r')\n\n        * draw a default vline at *x* = 1 that spans the yrange::\n\n            >>> axvline(x=1)\n\n        * draw a default vline at *x* = .5 that spans the middle half of\n          the yrange::\n\n            >>> axvline(x=.5, ymin=0.25, ymax=0.75)\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 918, "code": "    def axline(self, xy1, xy2=None, *, slope=None, **kwargs):\n        if slope is not None and (self.get_xscale() != 'linear' or\n                                  self.get_yscale() != 'linear'):\n            raise TypeError(\"'slope' cannot be used with non-linear scales\")\n        datalim = [xy1] if xy2 is None else [xy1, xy2]\n        if \"transform\" in kwargs:\n            datalim = []\n        line = mlines.AxLine(xy1, xy2, slope, **kwargs)\n        self._set_artist_props(line)\n        if line.get_clip_path() is None:\n            line.set_clip_path(self.patch)", "documentation": "        \"\"\"\n        Add an infinitely long straight line.\n\n        The line can be defined either by two points *xy1* and *xy2*, or\n        by one point *xy1* and a *slope*.\n\n        This draws a straight line \"on the screen\", regardless of the x and y\n        scales, and is thus also suitable for drawing exponential decays in\n        semilog plots, power laws in loglog plots, etc. However, *slope*\n        should only be used with linear scales; It has no clear meaning for\n        all other scales, and thus the behavior is undefined. Please specify\n        the line using the points *xy1*, *xy2* for non-linear scales.\n\n        The *transform* keyword argument only applies to the points *xy1*,\n        *xy2*. The *slope* (if given) is always in data coordinates. This can\n        be used e.g. with ``ax.transAxes`` for drawing grid lines with a fixed\n        slope.\n\n        Parameters\n        ----------\n        xy1, xy2 : (float, float)\n            Points for the line to pass through.\n            Either *xy2* or *slope* has to be given.\n        slope : float, optional\n            The slope of the line. Either *xy2* or *slope* has to be given.\n\n        Returns\n        -------\n        `.AxLine`\n\n        Other Parameters\n        ----------------\n        **kwargs\n            Valid kwargs are `.Line2D` properties\n\n            %(Line2D:kwdoc)s\n\n        See Also\n        --------\n        axhline : for horizontal lines\n        axvline : for vertical lines\n\n        Examples\n        --------\n        Draw a thick red line passing through (0, 0) and (1, 1)::\n\n            >>> axline((0, 0), (1, 1), linewidth=4, color='r')\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 992, "code": "    def axhspan(self, ymin, ymax, xmin=0, xmax=1, **kwargs):\n        self._check_no_units([xmin, xmax], ['xmin', 'xmax'])\n        (ymin, ymax), = self._process_unit_info([(\"y\", [ymin, ymax])], kwargs)\n        p = mpatches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, **kwargs)\n        p.set_transform(self.get_yaxis_transform(which=\"grid\"))\n        ix = self.dataLim.intervalx.copy()\n        mx = self.dataLim.minposx\n        self.add_patch(p)\n        self.dataLim.intervalx = ix\n        self.dataLim.minposx = mx\n        p.get_path()._interpolation_steps = mpl.axis.GRIDLINE_INTERPOLATION_STEPS", "documentation": "        \"\"\"\n        Add a horizontal span (rectangle) across the Axes.\n\n        The rectangle spans from *ymin* to *ymax* vertically, and, by default,\n        the whole x-axis horizontally.  The x-span can be set using *xmin*\n        (default: 0) and *xmax* (default: 1) which are in axis units; e.g.\n        ``xmin = 0.5`` always refers to the middle of the x-axis regardless of\n        the limits set by `~.Axes.set_xlim`.\n\n        Parameters\n        ----------\n        ymin : float\n            Lower y-coordinate of the span, in data units.\n        ymax : float\n            Upper y-coordinate of the span, in data units.\n        xmin : float, default: 0\n            Lower x-coordinate of the span, in x-axis (0-1) units.\n        xmax : float, default: 1\n            Upper x-coordinate of the span, in x-axis (0-1) units.\n\n        Returns\n        -------\n        `~matplotlib.patches.Rectangle`\n            Horizontal span (rectangle) from (xmin, ymin) to (xmax, ymax).\n\n        Other Parameters\n        ----------------\n        **kwargs : `~matplotlib.patches.Rectangle` properties\n\n        %(Rectangle:kwdoc)s\n\n        See Also\n        --------\n        axvspan : Add a vertical span across the Axes.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 1047, "code": "    def axvspan(self, xmin, xmax, ymin=0, ymax=1, **kwargs):\n        self._check_no_units([ymin, ymax], ['ymin', 'ymax'])\n        (xmin, xmax), = self._process_unit_info([(\"x\", [xmin, xmax])], kwargs)\n        p = mpatches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, **kwargs)\n        p.set_transform(self.get_xaxis_transform(which=\"grid\"))\n        iy = self.dataLim.intervaly.copy()\n        my = self.dataLim.minposy\n        self.add_patch(p)\n        self.dataLim.intervaly = iy\n        self.dataLim.minposy = my\n        p.get_path()._interpolation_steps = mpl.axis.GRIDLINE_INTERPOLATION_STEPS", "documentation": "        \"\"\"\n        Add a vertical span (rectangle) across the Axes.\n\n        The rectangle spans from *xmin* to *xmax* horizontally, and, by\n        default, the whole y-axis vertically.  The y-span can be set using\n        *ymin* (default: 0) and *ymax* (default: 1) which are in axis units;\n        e.g. ``ymin = 0.5`` always refers to the middle of the y-axis\n        regardless of the limits set by `~.Axes.set_ylim`.\n\n        Parameters\n        ----------\n        xmin : float\n            Lower x-coordinate of the span, in data units.\n        xmax : float\n            Upper x-coordinate of the span, in data units.\n        ymin : float, default: 0\n            Lower y-coordinate of the span, in y-axis units (0-1).\n        ymax : float, default: 1\n            Upper y-coordinate of the span, in y-axis units (0-1).\n\n        Returns\n        -------\n        `~matplotlib.patches.Rectangle`\n            Vertical span (rectangle) from (xmin, ymin) to (xmax, ymax).\n\n        Other Parameters\n        ----------------\n        **kwargs : `~matplotlib.patches.Rectangle` properties\n\n        %(Rectangle:kwdoc)s\n\n        See Also\n        --------\n        axhspan : Add a horizontal span across the Axes.\n\n        Examples\n        --------\n        Draw a vertical, green, translucent rectangle from x = 1.25 to\n        x = 1.55 that spans the yrange of the Axes.\n\n        >>> axvspan(1.25, 1.55, facecolor='g', alpha=0.5)\n\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 1544, "code": "    def plot(self, *args, scalex=True, scaley=True, data=None, **kwargs):\n        kwargs = cbook.normalize_kwargs(kwargs, mlines.Line2D)\n        lines = [*self._get_lines(self, *args, data=data, **kwargs)]\n        for line in lines:\n            self.add_line(line)\n        if scalex:\n            self._request_autoscale_view(\"x\")\n        if scaley:\n            self._request_autoscale_view(\"y\")\n        return lines\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Plot y versus x as lines and/or markers.\n\n        Call signatures::\n\n            plot([x], y, [fmt], *, data=None, **kwargs)\n            plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\n        The coordinates of the points or line nodes are given by *x*, *y*.\n\n        The optional parameter *fmt* is a convenient way for defining basic\n        formatting like color, marker and linestyle. It's a shortcut string\n        notation described in the *Notes* section below.\n\n        >>> plot(x, y)        # plot x and y using default line style and color\n        >>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n        >>> plot(y)           # plot y using x as index array 0..N-1\n        >>> plot(y, 'r+')     # ditto, but with red plusses\n\n        You can use `.Line2D` properties as keyword arguments for more\n        control on the appearance. Line properties and *fmt* can be mixed.\n        The following two calls yield identical results:\n\n        >>> plot(x, y, 'go--', linewidth=2, markersize=12)\n        >>> plot(x, y, color='green', marker='o', linestyle='dashed',\n        ...      linewidth=2, markersize=12)\n\n        When conflicting with *fmt*, keyword arguments take precedence.\n\n\n        **Plotting labelled data**\n\n        There's a convenient way for plotting objects with labelled data (i.e.\n        data that can be accessed by index ``obj['y']``). Instead of giving\n        the data in *x* and *y*, you can provide the object in the *data*\n        parameter and just give the labels for *x* and *y*::\n\n        >>> plot('xlabel', 'ylabel', data=obj)\n\n        All indexable objects are supported. This could e.g. be a `dict`, a\n        `pandas.DataFrame` or a structured numpy array.\n\n\n        **Plotting multiple sets of data**\n\n        There are various ways to plot multiple sets of data.\n\n        - The most straight forward way is just to call `plot` multiple times.\n          Example:\n\n          >>> plot(x1, y1, 'bo')\n          >>> plot(x2, y2, 'go')\n\n        - If *x* and/or *y* are 2D arrays, a separate data set will be drawn\n          for every column. If both *x* and *y* are 2D, they must have the\n          same shape. If only one of them is 2D with shape (N, m) the other\n          must have length N and will be used for every data set m.\n\n          Example:\n\n          >>> x = [1, 2, 3]\n          >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n          >>> plot(x, y)\n\n          is equivalent to:\n\n          >>> for col in range(y.shape[1]):\n          ...     plot(x, y[:, col])\n\n        - The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n          groups::\n\n          >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n\n          In this case, any additional keyword argument applies to all\n          datasets. Also, this syntax cannot be combined with the *data*\n          parameter.\n\n        By default, each line is assigned a different style specified by a\n        'style cycle'. The *fmt* and line property parameters are only\n        necessary if you want explicit deviations from these defaults.\n        Alternatively, you can also change the style cycle using\n        :rc:`axes.prop_cycle`.\n\n\n        Parameters\n        ----------\n        x, y : array-like or float\n            The horizontal / vertical coordinates of the data points.\n            *x* values are optional and default to ``range(len(y))``.\n\n            Commonly, these parameters are 1D arrays.\n\n            They can also be scalars, or two-dimensional (in that case, the\n            columns represent separate data sets).\n\n            These arguments cannot be passed as keywords.\n\n        fmt : str, optional\n            A format string, e.g. 'ro' for red circles. See the *Notes*\n            section for a full description of the format strings.\n\n            Format strings are just an abbreviation for quickly setting\n            basic line properties. All of these and more can also be\n            controlled by keyword arguments.\n\n            This argument cannot be passed as keyword.\n\n        data : indexable object, optional\n            An object with labelled data. If given, provide the label names to\n            plot in *x* and *y*.\n\n            .. note::\n                Technically there's a slight ambiguity in calls where the\n                second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n                could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n                the former interpretation is chosen, but a warning is issued.\n                You may suppress the warning by adding an empty format string\n                ``plot('n', 'o', '', data=obj)``.\n\n        Returns\n        -------\n        list of `.Line2D`\n            A list of lines representing the plotted data.\n\n        Other Parameters\n        ----------------\n        scalex, scaley : bool, default: True\n            These parameters determine if the view limits are adapted to the\n            data limits. The values are passed on to\n            `~.axes.Axes.autoscale_view`.\n\n        **kwargs : `~matplotlib.lines.Line2D` properties, optional\n            *kwargs* are used to specify properties like a line label (for\n            auto legends), linewidth, antialiasing, marker face color.\n            Example::\n\n            >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n            >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n\n            If you specify multiple lines with one plot call, the kwargs apply\n            to all those lines. In case the label object is iterable, each\n            element is used as labels for each set of data.\n\n            Here is a list of available `.Line2D` properties:\n\n            %(Line2D:kwdoc)s\n\n        See Also\n        --------\n        scatter : XY scatter plot with markers of varying size and/or color (\n            sometimes also called bubble chart).\n\n        Notes\n        -----\n        **Format Strings**\n\n        A format string consists of a part for color, marker and line::\n\n            fmt = '[marker][line][color]'\n\n        Each of them is optional. If not provided, the value from the style\n        cycle is used. Exception: If ``line`` is given, but no ``marker``,\n        the data will be a line without markers.\n\n        Other combinations such as ``[color][marker][line]`` are also\n        supported, but note that their parsing may be ambiguous.\n\n        **Markers**\n\n        =============   ===============================\n        character       description\n        =============   ===============================\n        ``'.'``         point marker\n        ``','``         pixel marker\n        ``'o'``         circle marker\n        ``'v'``         triangle_down marker\n        ``'^'``         triangle_up marker\n        ``'<'``         triangle_left marker\n        ``'>'``         triangle_right marker\n        ``'1'``         tri_down marker\n        ``'2'``         tri_up marker\n        ``'3'``         tri_left marker\n        ``'4'``         tri_right marker\n        ``'8'``         octagon marker\n        ``'s'``         square marker\n        ``'p'``         pentagon marker\n        ``'P'``         plus (filled) marker\n        ``'*'``         star marker\n        ``'h'``         hexagon1 marker\n        ``'H'``         hexagon2 marker\n        ``'+'``         plus marker\n        ``'x'``         x marker\n        ``'X'``         x (filled) marker\n        ``'D'``         diamond marker\n        ``'d'``         thin_diamond marker\n        ``'|'``         vline marker\n        ``'_'``         hline marker\n        =============   ===============================\n\n        **Line Styles**\n\n        =============    ===============================\n        character        description\n        =============    ===============================\n        ``'-'``          solid line style\n        ``'--'``         dashed line style\n        ``'-.'``         dash-dot line style\n        ``':'``          dotted line style\n        =============    ===============================\n\n        Example format strings::\n\n            'b'    # blue markers with default shape\n            'or'   # red circles\n            '-g'   # green solid line\n            '--'   # dashed line with default color\n            '^k:'  # black triangle_up markers connected by a dotted line\n\n        **Colors**\n\n        The supported color abbreviations are the single letter codes\n\n        =============    ===============================\n        character        color\n        =============    ===============================\n        ``'b'``          blue\n        ``'g'``          green\n        ``'r'``          red\n        ``'c'``          cyan\n        ``'m'``          magenta\n        ``'y'``          yellow\n        ``'k'``          black\n        ``'w'``          white\n        =============    ===============================\n\n        and the ``'CN'`` colors that index into the default property cycle.\n\n        If the color is the only part of the format string, you can\n        additionally use any  `matplotlib.colors` spec, e.g. full names\n        (``'green'``) or hex strings (``'#008000'``).\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 1799, "code": "    def loglog(self, *args, **kwargs):\n        dx = {k: v for k, v in kwargs.items()\n              if k in ['base', 'subs', 'nonpositive',\n                       'basex', 'subsx', 'nonposx']}\n        self.set_xscale('log', **dx)\n        dy = {k: v for k, v in kwargs.items()\n              if k in ['base', 'subs', 'nonpositive',\n                       'basey', 'subsy', 'nonposy']}\n        self.set_yscale('log', **dy)\n        return self.plot(\n            *args, **{k: v for k, v in kwargs.items() if k not in {*dx, *dy}})", "documentation": "        \"\"\"\n        Make a plot with log scaling on both the x- and y-axis.\n\n        Call signatures::\n\n            loglog([x], y, [fmt], data=None, **kwargs)\n            loglog([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\n        This is just a thin wrapper around `.plot` which additionally changes\n        both the x-axis and the y-axis to log scaling. All the concepts and\n        parameters of plot can be used here as well.\n\n        The additional parameters *base*, *subs* and *nonpositive* control the\n        x/y-axis properties. They are just forwarded to `.Axes.set_xscale` and\n        `.Axes.set_yscale`. To use different properties on the x-axis and the\n        y-axis, use e.g.\n        ``ax.set_xscale(\"log\", base=10); ax.set_yscale(\"log\", base=2)``.\n\n        Parameters\n        ----------\n        base : float, default: 10\n            Base of the logarithm.\n\n        subs : sequence, optional\n            The location of the minor ticks. If *None*, reasonable locations\n            are automatically chosen depending on the number of decades in the\n            plot. See `.Axes.set_xscale`/`.Axes.set_yscale` for details.\n\n        nonpositive : {'mask', 'clip'}, default: 'clip'\n            Non-positive values can be masked as invalid, or clipped to a very\n            small positive number.\n\n        **kwargs\n            All parameters supported by `.plot`.\n\n        Returns\n        -------\n        list of `.Line2D`\n            Objects representing the plotted data.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 1853, "code": "    def semilogx(self, *args, **kwargs):\n        d = {k: v for k, v in kwargs.items()\n             if k in ['base', 'subs', 'nonpositive',\n                      'basex', 'subsx', 'nonposx']}\n        self.set_xscale('log', **d)\n        return self.plot(\n            *args, **{k: v for k, v in kwargs.items() if k not in d})\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Make a plot with log scaling on the x-axis.\n\n        Call signatures::\n\n            semilogx([x], y, [fmt], data=None, **kwargs)\n            semilogx([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\n        This is just a thin wrapper around `.plot` which additionally changes\n        the x-axis to log scaling. All the concepts and parameters of plot can\n        be used here as well.\n\n        The additional parameters *base*, *subs*, and *nonpositive* control the\n        x-axis properties. They are just forwarded to `.Axes.set_xscale`.\n\n        Parameters\n        ----------\n        base : float, default: 10\n            Base of the x logarithm.\n\n        subs : array-like, optional\n            The location of the minor xticks. If *None*, reasonable locations\n            are automatically chosen depending on the number of decades in the\n            plot. See `.Axes.set_xscale` for details.\n\n        nonpositive : {'mask', 'clip'}, default: 'clip'\n            Non-positive values in x can be masked as invalid, or clipped to a\n            very small positive number.\n\n        **kwargs\n            All parameters supported by `.plot`.\n\n        Returns\n        -------\n        list of `.Line2D`\n            Objects representing the plotted data.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 1900, "code": "    def semilogy(self, *args, **kwargs):\n        d = {k: v for k, v in kwargs.items()\n             if k in ['base', 'subs', 'nonpositive',\n                      'basey', 'subsy', 'nonposy']}\n        self.set_yscale('log', **d)\n        return self.plot(\n            *args, **{k: v for k, v in kwargs.items() if k not in d})\n    @_preprocess_data(replace_names=[\"x\"], label_namer=\"x\")", "documentation": "        \"\"\"\n        Make a plot with log scaling on the y-axis.\n\n        Call signatures::\n\n            semilogy([x], y, [fmt], data=None, **kwargs)\n            semilogy([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\n        This is just a thin wrapper around `.plot` which additionally changes\n        the y-axis to log scaling. All the concepts and parameters of plot can\n        be used here as well.\n\n        The additional parameters *base*, *subs*, and *nonpositive* control the\n        y-axis properties. They are just forwarded to `.Axes.set_yscale`.\n\n        Parameters\n        ----------\n        base : float, default: 10\n            Base of the y logarithm.\n\n        subs : array-like, optional\n            The location of the minor yticks. If *None*, reasonable locations\n            are automatically chosen depending on the number of decades in the\n            plot. See `.Axes.set_yscale` for details.\n\n        nonpositive : {'mask', 'clip'}, default: 'clip'\n            Non-positive values in y can be masked as invalid, or clipped to a\n            very small positive number.\n\n        **kwargs\n            All parameters supported by `.plot`.\n\n        Returns\n        -------\n        list of `.Line2D`\n            Objects representing the plotted data.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 1946, "code": "    def acorr(self, x, **kwargs):\n        return self.xcorr(x, x, **kwargs)\n    @_api.make_keyword_only(\"3.10\", \"normed\")\n    @_preprocess_data(replace_names=[\"x\", \"y\"], label_namer=\"y\")", "documentation": "        \"\"\"\n        Plot the autocorrelation of *x*.\n\n        Parameters\n        ----------\n        x : array-like\n            Not run through Matplotlib's unit conversion, so this should\n            be a unit-less array.\n\n        detrend : callable, default: `.mlab.detrend_none` (no detrending)\n            A detrending function applied to *x*.  It must have the\n            signature ::\n\n                detrend(x: np.ndarray) -> np.ndarray\n\n        normed : bool, default: True\n            If ``True``, input vectors are normalised to unit length.\n\n        usevlines : bool, default: True\n            Determines the plot style.\n\n            If ``True``, vertical lines are plotted from 0 to the acorr value\n            using `.Axes.vlines`. Additionally, a horizontal line is plotted\n            at y=0 using `.Axes.axhline`.\n\n            If ``False``, markers are plotted at the acorr values using\n            `.Axes.plot`.\n\n        maxlags : int, default: 10\n            Number of lags to show. If ``None``, will return all\n            ``2 * len(x) - 1`` lags.\n\n        Returns\n        -------\n        lags : array (length ``2*maxlags+1``)\n            The lag vector.\n        c : array  (length ``2*maxlags+1``)\n            The auto correlation vector.\n        line : `.LineCollection` or `.Line2D`\n            `.Artist` added to the Axes of the correlation:\n\n            - `.LineCollection` if *usevlines* is True.\n            - `.Line2D` if *usevlines* is False.\n        b : `~matplotlib.lines.Line2D` or None\n            Horizontal line at 0 if *usevlines* is True\n            None *usevlines* is False.\n\n        Other Parameters\n        ----------------\n        linestyle : `~matplotlib.lines.Line2D` property, optional\n            The linestyle for plotting the data points.\n            Only used if *usevlines* is ``False``.\n\n        marker : str, default: 'o'\n            The marker for plotting the data points.\n            Only used if *usevlines* is ``False``.\n\n        data : indexable object, optional\n            DATA_PARAMETER_PLACEHOLDER\n\n        **kwargs\n            Additional parameters are passed to `.Axes.vlines` and\n            `.Axes.axhline` if *usevlines* is ``True``; otherwise they are\n            passed to `.Axes.plot`.\n\n        Notes\n        -----\n        The cross correlation is performed with `numpy.correlate` with\n        ``mode = \"full\"``.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 2134, "code": "    def step(self, x, y, *args, where='pre', data=None, **kwargs):\n        _api.check_in_list(('pre', 'post', 'mid'), where=where)\n        kwargs['drawstyle'] = 'steps-' + where\n        return self.plot(x, y, *args, data=data, **kwargs)\n    @staticmethod", "documentation": "        \"\"\"\n        Make a step plot.\n\n        Call signatures::\n\n            step(x, y, [fmt], *, data=None, where='pre', **kwargs)\n            step(x, y, [fmt], x2, y2, [fmt2], ..., *, where='pre', **kwargs)\n\n        This is just a thin wrapper around `.plot` which changes some\n        formatting options. Most of the concepts and parameters of plot can be\n        used here as well.\n\n        .. note::\n\n            This method uses a standard plot with a step drawstyle: The *x*\n            values are the reference positions and steps extend left/right/both\n            directions depending on *where*.\n\n            For the common case where you know the values and edges of the\n            steps, use `~.Axes.stairs` instead.\n\n        Parameters\n        ----------\n        x : array-like\n            1D sequence of x positions. It is assumed, but not checked, that\n            it is uniformly increasing.\n\n        y : array-like\n            1D sequence of y levels.\n\n        fmt : str, optional\n            A format string, e.g. 'g' for a green line. See `.plot` for a more\n            detailed description.\n\n            Note: While full format strings are accepted, it is recommended to\n            only specify the color. Line styles are currently ignored (use\n            the keyword argument *linestyle* instead). Markers are accepted\n            and plotted on the given positions, however, this is a rarely\n            needed feature for step plots.\n\n        where : {'pre', 'post', 'mid'}, default: 'pre'\n            Define where the steps should be placed:\n\n            - 'pre': The y value is continued constantly to the left from\n              every *x* position, i.e. the interval ``(x[i-1], x[i]]`` has the\n              value ``y[i]``.\n            - 'post': The y value is continued constantly to the right from\n              every *x* position, i.e. the interval ``[x[i], x[i+1])`` has the\n              value ``y[i]``.\n            - 'mid': Steps occur half-way between the *x* positions.\n\n        data : indexable object, optional\n            An object with labelled data. If given, provide the label names to\n            plot in *x* and *y*.\n\n        **kwargs\n            Additional parameters are the same as those for `.plot`.\n\n        Returns\n        -------\n        list of `.Line2D`\n            Objects representing the plotted data.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 2203, "code": "    def _convert_dx(dx, x0, xconv, convert):\n        assert type(xconv) is np.ndarray\n        if xconv.size == 0:\n            return convert(dx)\n        try:\n            try:\n                x0 = cbook._safe_first_finite(x0)\n            except (TypeError, IndexError, KeyError):\n                pass\n            try:\n                x = cbook._safe_first_finite(xconv)", "documentation": "        \"\"\"\n        Small helper to do logic of width conversion flexibly.\n\n        *dx* and *x0* have units, but *xconv* has already been converted\n        to unitless (and is an ndarray).  This allows the *dx* to have units\n        that are different from *x0*, but are still accepted by the\n        ``__add__`` operator of *x0*.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 2254, "code": "    def _parse_bar_color_args(self, kwargs):\n        color = kwargs.pop('color', None)\n        facecolor = kwargs.pop('facecolor', color)\n        edgecolor = kwargs.pop('edgecolor', None)\n        facecolor = (facecolor if facecolor is not None\n                     else self._get_patches_for_fill.get_next_color())\n        try:\n            facecolor = mcolors.to_rgba_array(facecolor)\n        except ValueError as err:\n            raise ValueError(\n                \"'facecolor' or 'color' argument must be a valid color or \"", "documentation": "        \"\"\"\n        Helper function to process color-related arguments of `.Axes.bar`.\n\n        Argument precedence for facecolors:\n\n        - kwargs['facecolor']\n        - kwargs['color']\n        - 'Result of ``self._get_patches_for_fill.get_next_color``\n\n        Argument precedence for edgecolors:\n\n        - kwargs['edgecolor']\n        - None\n\n        Parameters\n        ----------\n        self : Axes\n\n        kwargs : dict\n            Additional kwargs. If these keys exist, we pop and process them:\n            'facecolor', 'edgecolor', 'color'\n            Note: The dict is modified by this function.\n\n\n        Returns\n        -------\n        facecolor\n            The facecolor. One or more colors as (N, 4) rgba array.\n        edgecolor\n            The edgecolor. Not normalized; may be any valid color spec or None.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 2968, "code": "    def broken_barh(self, xranges, yrange, align=\"bottom\", **kwargs):\n        xdata = cbook._safe_first_finite(xranges) if len(xranges) else None\n        ydata = cbook._safe_first_finite(yrange) if len(yrange) else None\n        self._process_unit_info(\n            [(\"x\", xdata), (\"y\", ydata)], kwargs, convert=False)\n        vertices = []\n        y0, dy = yrange\n        _api.check_in_list(['bottom', 'center', 'top'], align=align)\n        if align == \"bottom\":\n            y0, y1 = self.convert_yunits((y0, y0 + dy))\n        elif align == \"center\":", "documentation": "        \"\"\"\n        Plot a horizontal sequence of rectangles.\n\n        A rectangle is drawn for each element of *xranges*. All rectangles\n        have the same vertical position and size defined by *yrange*.\n\n        Parameters\n        ----------\n        xranges : sequence of tuples (*xmin*, *xwidth*)\n            The x-positions and extents of the rectangles. For each tuple\n            (*xmin*, *xwidth*) a rectangle is drawn from *xmin* to *xmin* +\n            *xwidth*.\n        yrange : (*ypos*, *yheight*)\n            The y-position and extent for all the rectangles.\n        align : {\"bottom\", \"center\", \"top\"}, default: 'bottom'\n            The alignment of the yrange with respect to the y-position. One of:\n\n            - \"bottom\": Resulting y-range [ypos, ypos + yheight]\n            - \"center\": Resulting y-range [ypos - yheight/2, ypos + yheight/2]\n            - \"top\": Resulting y-range [ypos - yheight, ypos]\n\n            .. versionadded:: 3.11\n\n        Returns\n        -------\n        `~.collections.PolyCollection`\n\n        Other Parameters\n        ----------------\n        data : indexable object, optional\n            DATA_PARAMETER_PLACEHOLDER\n        **kwargs : `.PolyCollection` properties\n\n            Each *kwarg* can be either a single argument applying to all\n            rectangles, e.g.::\n\n                facecolors='black'\n\n            or a sequence of arguments over which is cycled, e.g.::\n\n                facecolors=('black', 'blue')\n\n            would create interleaving black and blue rectangles.\n\n            Supported keywords:\n\n            %(PolyCollection:kwdoc)s\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 3769, "code": "    def _errorevery_to_mask(x, errorevery):\n        if isinstance(errorevery, Integral):\n            errorevery = (0, errorevery)\n        if isinstance(errorevery, tuple):\n            if (len(errorevery) == 2 and\n                    isinstance(errorevery[0], Integral) and\n                    isinstance(errorevery[1], Integral)):\n                errorevery = slice(errorevery[0], None, errorevery[1])\n            else:\n                raise ValueError(\n                    f'{errorevery=!r} is a not a tuple of two integers')", "documentation": "        \"\"\"\n        Normalize `errorbar`'s *errorevery* to be a boolean mask for data *x*.\n\n        This function is split out to be usable both by 2D and 3D errorbars.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 3959, "code": "        def _upcast_err(err):\n            if (\n                    np.iterable(err) and\n                    len(err) > 0 and\n                    isinstance(cbook._safe_first_finite(err), np.ndarray)\n            ):\n                atype = type(cbook._safe_first_finite(err))\n                if atype is np.ndarray:\n                    return np.asarray(err, dtype=object)\n                return atype(err)\n            return np.asarray(err, dtype=object)", "documentation": "            \"\"\"\n            Safely handle tuple of containers that carry units.\n\n            This function covers the case where the input to the xerr/yerr is a\n            length 2 tuple of equal length ndarray-subclasses that carry the\n            unit information in the container.\n\n            If we have a tuple of nested numpy array (subclasses), we defer\n            coercing the units to be consistent to the underlying unit\n            library (and implicitly the broadcasting).\n\n            Otherwise, fallback to casting to an object array.\n            \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 5763, "code": "    def arrow(self, x, y, dx, dy, **kwargs):\n        x = self.convert_xunits(x)\n        y = self.convert_yunits(y)\n        dx = self.convert_xunits(dx)\n        dy = self.convert_yunits(dy)\n        a = mpatches.FancyArrow(x, y, dx, dy, **kwargs)\n        self.add_patch(a)\n        self._request_autoscale_view()\n        return a\n    @_docstring.copy(mquiver.QuiverKey.__init__)", "documentation": "        \"\"\"\n        [*Discouraged*] Add an arrow to the Axes.\n\n        This draws an arrow from ``(x, y)`` to ``(x+dx, y+dy)``.\n\n        .. admonition:: Discouraged\n\n            The use of this method is discouraged because it is not guaranteed\n            that the arrow renders reasonably. For example, the resulting arrow\n            is affected by the Axes aspect ratio and limits, which may distort\n            the arrow.\n\n            Consider using `~.Axes.annotate` without a text instead, e.g. ::\n\n                ax.annotate(\"\", xytext=(0, 0), xy=(0.5, 0.5),\n                            arrowprops=dict(arrowstyle=\"->\"))\n\n        Parameters\n        ----------\n        %(FancyArrow)s\n\n        Returns\n        -------\n        `.FancyArrow`\n            The created `.FancyArrow` object.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 5819, "code": "    def quiver(self, *args, **kwargs):\n        args = self._quiver_units(args, kwargs)\n        q = mquiver.Quiver(self, *args, **kwargs)\n        self.add_collection(q)\n        return q\n    @_preprocess_data()\n    @_docstring.interpd", "documentation": "        \"\"\"%(quiver_doc)s\"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 5830, "code": "    def barbs(self, *args, **kwargs):\n        args = self._quiver_units(args, kwargs)\n        b = mquiver.Barbs(self, *args, **kwargs)\n        self.add_collection(b)\n        return b", "documentation": "        \"\"\"%(barbs_doc)s\"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 5840, "code": "    def fill(self, *args, data=None, **kwargs):\n        kwargs = cbook.normalize_kwargs(kwargs, mlines.Line2D)\n        patches = [*self._get_patches_for_fill(self, *args, data=data, **kwargs)]\n        for poly in patches:\n            self.add_patch(poly)\n        self._request_autoscale_view()\n        return patches", "documentation": "        \"\"\"\n        Plot filled polygons.\n\n        Parameters\n        ----------\n        *args : sequence of x, y, [color]\n            Each polygon is defined by the lists of *x* and *y* positions of\n            its nodes, optionally followed by a *color* specifier. See\n            :mod:`matplotlib.colors` for supported color specifiers. The\n            standard color cycle is used for polygons without a color\n            specifier.\n\n            You can plot multiple polygons by providing multiple *x*, *y*,\n            *[color]* groups.\n\n            For example, each of the following is legal::\n\n                ax.fill(x, y)                    # a polygon with default color\n                ax.fill(x, y, \"b\")               # a blue polygon\n                ax.fill(x, y, x2, y2)            # two polygons\n                ax.fill(x, y, \"b\", x2, y2, \"r\")  # a blue and a red polygon\n\n        data : indexable object, optional\n            An object with labelled data. If given, provide the label names to\n            plot in *x* and *y*, e.g.::\n\n                ax.fill(\"time\", \"signal\",\n                        data={\"time\": [0, 1, 2], \"signal\": [0, 1, 0]})\n\n        Returns\n        -------\n        list of `~matplotlib.patches.Polygon`\n\n        Other Parameters\n        ----------------\n        **kwargs : `~matplotlib.patches.Polygon` properties\n\n        Notes\n        -----\n        Use :meth:`fill_between` if you would like to fill the region between\n        two curves.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 5997, "code": "    def _fill_between_process_units(self, ind_dir, dep_dir, ind, dep1, dep2, **kwargs):\n        return map(np.ma.masked_invalid, self._process_unit_info(\n            [(ind_dir, ind), (dep_dir, dep1), (dep_dir, dep2)], kwargs))", "documentation": "        \"\"\"Handle united data, such as dates.\"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 6802, "code": "    def _update_pcolor_lims(self, collection, coords):\n        t = collection._transform\n        if (not isinstance(t, mtransforms.Transform) and\n                hasattr(t, '_as_mpl_transform')):\n            t = t._as_mpl_transform(self.axes)\n        if t and any(t.contains_branch_separately(self.transData)):\n            trans_to_data = t - self.transData\n            coords = trans_to_data.transform(coords)\n        self.add_collection(collection, autolim=False)\n        minx, miny = np.min(coords, axis=0)\n        maxx, maxy = np.max(coords, axis=0)", "documentation": "        \"\"\"\n        Common code for updating lims in pcolor() and pcolormesh() methods.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 7029, "code": "    def contour(self, *args, **kwargs):\n        kwargs['filled'] = False\n        contours = mcontour.QuadContourSet(self, *args, **kwargs)\n        self._request_autoscale_view()\n        return contours\n    @_preprocess_data()\n    @_docstring.interpd", "documentation": "        \"\"\"\n        Plot contour lines.\n\n        Call signature::\n\n            contour([X, Y,] Z, /, [levels], **kwargs)\n\n        The arguments *X*, *Y*, *Z* are positional-only.\n        %(contour_doc)s\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 7047, "code": "    def contourf(self, *args, **kwargs):\n        kwargs['filled'] = True\n        contours = mcontour.QuadContourSet(self, *args, **kwargs)\n        self._request_autoscale_view()\n        return contours", "documentation": "        \"\"\"\n        Plot filled contours.\n\n        Call signature::\n\n            contourf([X, Y,] Z, /, [levels], **kwargs)\n\n        The arguments *X*, *Y*, *Z* are positional-only.\n        %(contour_doc)s\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 7063, "code": "    def clabel(self, CS, levels=None, **kwargs):\n        return CS.clabel(levels, **kwargs)\n    @_api.make_keyword_only(\"3.10\", \"range\")\n    @_preprocess_data(replace_names=[\"x\", 'weights'], label_namer=\"x\")", "documentation": "        \"\"\"\n        Label a contour plot.\n\n        Adds labels to line contours in given `.ContourSet`.\n\n        Parameters\n        ----------\n        CS : `.ContourSet` instance\n            Line contours to label.\n\n        levels : array-like, optional\n            A list of level values, that should be labeled. The list must be\n            a subset of ``CS.levels``. If not given, all levels are labeled.\n\n        **kwargs\n            All other parameters are documented in `~.ContourLabeler.clabel`.\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 8709, "code": "    def matshow(self, Z, **kwargs):\n        Z = np.asanyarray(Z)\n        kw = {'origin': 'upper',\n              'interpolation': 'nearest',\n              'aspect': 'equal',          # (already the imshow default)\n              **kwargs}\n        im = self.imshow(Z, **kw)\n        self.title.set_y(1.05)\n        self.xaxis.tick_top()\n        self.xaxis.set_ticks_position('both')\n        self.xaxis.set_major_locator(", "documentation": "        \"\"\"\n        Plot the values of a 2D matrix or array as color-coded image.\n\n        The matrix will be shown the way it would be printed, with the first\n        row at the top.  Row and column numbering is zero-based.\n\n        Parameters\n        ----------\n        Z : (M, N) array-like\n            The matrix to be displayed.\n\n        Returns\n        -------\n        `~matplotlib.image.AxesImage`\n\n        Other Parameters\n        ----------------\n        **kwargs : `~matplotlib.axes.Axes.imshow` arguments\n\n        See Also\n        --------\n        imshow : More general function to plot data on a 2D regular raster.\n\n        Notes\n        -----\n        This is just a convenience function wrapping `.imshow` to set useful\n        defaults for displaying a matrix. In particular:\n\n        - Set ``origin='upper'``.\n        - Set ``interpolation='nearest'``.\n        - Set ``aspect='equal'``.\n        - Ticks are placed to the left and above.\n        - Ticks are formatted to show integer indices.\n\n        \"\"\""}, {"filename": "lib/matplotlib/axes/_axes.py", "start_line": 9184, "code": "    def _get_aspect_ratio(self):\n        figure_size = self.get_figure().get_size_inches()\n        ll, ur = self.get_position() * figure_size\n        width, height = ur - ll\n        return height / (width * self.get_data_ratio())", "documentation": "        \"\"\"\n        Convenience method to calculate the aspect ratio of the Axes in\n        the display coordinate system.\n        \"\"\""}]}
{"repository": "matplotlib/matplotlib", "commit_sha": "bd81e73c44f4b4ce1682ba71bf5bd480850892fd", "commit_message": "Fix comment casing in test_sankey.py [ci docs]", "commit_date": "2025-10-21T22:33:56+00:00", "author": "Christine P. Chai", "file": "lib/matplotlib/tests/test_sankey.py", "patch": "@@ -6,7 +6,7 @@\n \n \n def test_sankey():\n-    # Let's just create a sankey instance and check the code runs\n+    # let's just create a sankey instance and check the code runs\n     sankey = Sankey()\n     sankey.add()\n ", "before_segments": [], "after_segments": []}
{"repository": "scikit-learn/scikit-learn", "commit_sha": "664ec17228e8edd8682b9307d81ca707c697bdff", "commit_message": "DOC Fix comment in `_median` (#32616)", "commit_date": "2025-10-31T05:18:56+00:00", "author": "Lucy Liu", "file": "sklearn/utils/_array_api.py", "patch": "@@ -691,7 +691,7 @@ def _median(x, axis=None, keepdims=False, xp=None):\n     # in most array libraries, and all that we support (as of May 2025).\n     # TODO: consider simplifying this code to use scipy instead once the oldest\n     # supported SciPy version provides `scipy.stats.quantile` with native array API\n-    # support (likely scipy 1.6 at the time of writing). Proper benchmarking of\n+    # support (likely scipy 1.16 at the time of writing). Proper benchmarking of\n     # either option with popular array namespaces is required to evaluate the\n     # impact of this choice.\n     xp, _, device = get_namespace_and_device(x, xp=xp)", "before_segments": [{"filename": "sklearn/utils/_array_api.py", "start_line": 26, "code": "def yield_namespaces(include_numpy_namespaces=True):\n    for array_namespace in [\n        \"numpy\",\n        \"array_api_strict\",\n        \"cupy\",\n        \"torch\",\n    ]:\n        if not include_numpy_namespaces and array_namespace in _NUMPY_NAMESPACE_NAMES:\n            continue\n        yield array_namespace", "documentation": "    \"\"\"Yield supported namespace.\n\n    This is meant to be used for testing purposes only.\n\n    Parameters\n    ----------\n    include_numpy_namespaces : bool, default=True\n        If True, also yield numpy namespaces.\n\n    Returns\n    -------\n    array_namespace : str\n        The name of the Array API namespace.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 57, "code": "def yield_namespace_device_dtype_combinations(include_numpy_namespaces=True):\n    for array_namespace in yield_namespaces(\n        include_numpy_namespaces=include_numpy_namespaces\n    ):\n        if array_namespace == \"torch\":\n            for device, dtype in itertools.product(\n                (\"cpu\", \"cuda\", \"xpu\"), (\"float64\", \"float32\")\n            ):\n                yield array_namespace, device, dtype\n            yield array_namespace, \"mps\", \"float32\"\n        elif array_namespace == \"array_api_strict\":", "documentation": "    \"\"\"Yield supported namespace, device, dtype tuples for testing.\n\n    Use this to test that an estimator works with all combinations.\n    Use in conjunction with `ids=_get_namespace_device_dtype_ids` to give\n    clearer pytest parametrization ID names.\n\n    Parameters\n    ----------\n    include_numpy_namespaces : bool, default=True\n        If True, also yield numpy namespaces.\n\n    Returns\n    -------\n    array_namespace : str\n        The name of the Array API namespace.\n\n    device : str\n        The name of the device on which to allocate the arrays. Can be None to\n        indicate that the default value should be used.\n\n    dtype_name : str\n        The name of the data type to use for arrays. Can be None to indicate\n        that the default value should be used.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 108, "code": "def _get_namespace_device_dtype_ids(param):\n    try:\n        import array_api_strict\n    except ImportError:\n        return None\n    else:\n        if param == array_api_strict.Device(\"CPU_DEVICE\"):\n            return \"CPU_DEVICE\"\n        if param == array_api_strict.Device(\"device1\"):\n            return \"device1\"\n        if param == array_api_strict.Device(\"device2\"):", "documentation": "    \"\"\"Get pytest parametrization IDs for `yield_namespace_device_dtype_combinations`\"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 125, "code": "def _check_array_api_dispatch(array_api_dispatch):\n    if not array_api_dispatch:\n        return\n    scipy_version = parse_version(scipy.__version__)\n    min_scipy_version = \"1.14.0\"\n    if scipy_version < parse_version(min_scipy_version):\n        raise ImportError(\n            f\"SciPy must be {min_scipy_version} or newer\"\n            \" (found {scipy.__version__}) to dispatch array using\"\n            \" the array API specification\"\n        )", "documentation": "    \"\"\"Checks that array API support is functional.\n\n    In particular scipy needs to be recent enough and the environment variable\n    needs to be set: SCIPY_ARRAY_API=1.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 152, "code": "def _single_array_device(array):\n    if (\n        isinstance(array, (numpy.ndarray, numpy.generic))\n        or not hasattr(array, \"device\")\n        or not get_config()[\"array_api_dispatch\"]\n    ):\n        return None\n    else:\n        return array.device", "documentation": "    \"\"\"Hardware device where the array data resides on.\"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 169, "code": "def device(*array_list, remove_none=True, remove_types=(str,)):\n    array_list = _remove_non_arrays(\n        *array_list, remove_none=remove_none, remove_types=remove_types\n    )\n    if not array_list:\n        return None\n    device_ = _single_array_device(array_list[0])\n    for array in array_list[1:]:\n        device_other = _single_array_device(array)\n        if device_ != device_other:\n            raise ValueError(", "documentation": "    \"\"\"Hardware device where the array data resides on.\n\n    If the hardware device is not the same for all arrays, an error is raised.\n\n    Parameters\n    ----------\n    *array_list : arrays\n        List of array instances from NumPy or an array API compatible library.\n\n    remove_none : bool, default=True\n        Whether to ignore None objects passed in array_list.\n\n    remove_types : tuple or list, default=(str,)\n        Types to ignore in array_list.\n\n    Returns\n    -------\n    out : device\n        `device` object (see the \"Device Support\" section of the array API spec).\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 213, "code": "def size(x):\n    return math.prod(x.shape)", "documentation": "    \"\"\"Return the total number of elements of x.\n\n    Parameters\n    ----------\n    x : array\n        Array instance from NumPy or an array API compatible library.\n\n    Returns\n    -------\n    out : int\n        Total number of elements.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 229, "code": "def _is_numpy_namespace(xp):\n    return xp.__name__ in _NUMPY_NAMESPACE_NAMES", "documentation": "    \"\"\"Return True if xp is backed by NumPy.\"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 245, "code": "def supported_float_dtypes(xp, device=None):\n    dtypes_dict = xp.__array_namespace_info__().dtypes(\n        kind=\"real floating\", device=device\n    )\n    valid_float_dtypes = []\n    for dtype_key in (\"float64\", \"float32\"):\n        if dtype_key in dtypes_dict:\n            valid_float_dtypes.append(dtypes_dict[dtype_key])\n    if hasattr(xp, \"float16\"):\n        valid_float_dtypes.append(xp.float16)\n    return tuple(valid_float_dtypes)", "documentation": "    \"\"\"Supported floating point types for the namespace.\n\n    Parameters\n    ----------\n    xp : module\n        Array namespace to inspect.\n\n    device : str or device instance from xp, default=None\n        Device to use for dtype selection. If ``None``, then a default device\n        is assumed.\n\n    Returns\n    -------\n    supported_dtypes : tuple\n        Tuple of real floating data types supported by the provided array namespace,\n        ordered from the highest precision to lowest.\n\n    See Also\n    --------\n    max_precision_float_dtype : Maximum float dtype for a namespace/device pair.\n\n    Notes\n    -----\n    `float16` is not officially part of the Array API spec at the\n    time of writing but scikit-learn estimators and functions can choose\n    to accept it when xp.float16 is defined.\n\n    Additionally, some devices available within a namespace may not support\n    all floating-point types that the namespace provides.\n\n    https://data-apis.org/array-api/latest/API_specification/data_types.html\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 292, "code": "def ensure_common_namespace_device(reference, *arrays):\n    xp, is_array_api = get_namespace(reference)\n    if is_array_api:\n        device_ = device(reference)\n        return [\n            xp.asarray(a, device=device_) if a is not None else None for a in arrays\n        ]\n    else:\n        return arrays", "documentation": "    \"\"\"Ensure that all arrays use the same namespace and device as reference.\n\n    If necessary the arrays are moved to the same namespace and device as\n    the reference array.\n\n    Parameters\n    ----------\n    reference : array\n        Reference array.\n\n    *arrays : array\n        Arrays to check.\n\n    Returns\n    -------\n    arrays : list\n        Arrays with the same namespace and device as reference.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 323, "code": "def _remove_non_arrays(*arrays, remove_none=True, remove_types=(str,)):\n    filtered_arrays = []\n    remove_types = tuple(remove_types)\n    for array in arrays:\n        if remove_none and array is None:\n            continue\n        if isinstance(array, remove_types):\n            continue\n        if sp.issparse(array):\n            continue\n        filtered_arrays.append(array)", "documentation": "    \"\"\"Filter arrays to exclude None and/or specific types.\n\n    Sparse arrays are always filtered out.\n\n    Parameters\n    ----------\n    *arrays : array objects\n        Array objects.\n\n    remove_none : bool, default=True\n        Whether to ignore None objects passed in arrays.\n\n    remove_types : tuple or list, default=(str,)\n        Types to ignore in the arrays.\n\n    Returns\n    -------\n    filtered_arrays : list\n        List of arrays filtered as requested. An empty list is returned if no input\n        passes the filters.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 359, "code": "def get_namespace(*arrays, remove_none=True, remove_types=(str,), xp=None):\n    array_api_dispatch = get_config()[\"array_api_dispatch\"]\n    if not array_api_dispatch:\n        if xp is not None:\n            return xp, False\n        else:\n            return np_compat, False\n    if xp is not None:\n        return xp, True\n    arrays = _remove_non_arrays(\n        *arrays,", "documentation": "    \"\"\"Get namespace of arrays.\n\n    Introspect `arrays` arguments and return their common Array API compatible\n    namespace object, if any.\n\n    Note that sparse arrays are filtered by default.\n\n    See: https://numpy.org/neps/nep-0047-array-api-standard.html\n\n    If `arrays` are regular numpy arrays, `array_api_compat.numpy` is returned instead.\n\n    Namespace support is not enabled by default. To enabled it call:\n\n      sklearn.set_config(array_api_dispatch=True)\n\n    or:\n\n      with sklearn.config_context(array_api_dispatch=True):\n          # your code here\n\n    Otherwise `array_api_compat.numpy` is\n    always returned irrespective of the fact that arrays implement the\n    `__array_namespace__` protocol or not.\n\n    Note that if no arrays pass the set filters, ``_NUMPY_API_WRAPPER_INSTANCE, False``\n    is returned.\n\n    Parameters\n    ----------\n    *arrays : array objects\n        Array objects.\n\n    remove_none : bool, default=True\n        Whether to ignore None objects passed in arrays.\n\n    remove_types : tuple or list, default=(str,)\n        Types to ignore in the arrays.\n\n    xp : module, default=None\n        Precomputed array namespace module. When passed, typically from a caller\n        that has already performed inspection of its own inputs, skips array\n        namespace inspection.\n\n    Returns\n    -------\n    namespace : module\n        Namespace shared by array objects. If any of the `arrays` are not arrays,\n        the namespace defaults to the NumPy namespace.\n\n    is_array_api_compliant : bool\n        True if the arrays are containers that implement the array API spec (see\n        https://data-apis.org/array-api/latest/index.html).\n        Always False when array_api_dispatch=False.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 502, "code": "def _validate_diagonal_args(array, value, xp):\n    if array.ndim != 2:\n        raise ValueError(\n            f\"`array` should be 2D. Got array with shape {tuple(array.shape)}\"\n        )\n    value = xp.asarray(value, dtype=array.dtype, device=device(array))\n    if value.ndim not in [0, 1]:\n        raise ValueError(\n            \"`value` needs to be a scalar or a 1D array, \"\n            f\"got a {value.ndim}D array instead.\"\n        )", "documentation": "    \"\"\"Validate arguments to `_fill_diagonal`/`_add_to_diagonal`.\"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 525, "code": "def _fill_diagonal(array, value, xp):\n    value, min_rows_columns = _validate_diagonal_args(array, value, xp)\n    if _is_numpy_namespace(xp):\n        xp.fill_diagonal(array, value, wrap=False)\n    else:\n        if value.ndim == 0:\n            for i in range(min_rows_columns):\n                array[i, i] = value\n        else:\n            for i in range(min_rows_columns):\n                array[i, i] = value[i]", "documentation": "    \"\"\"Minimal implementation of `numpy.fill_diagonal`.\n\n    `wrap` is not supported (i.e. always False). `value` should be a scalar or\n    1D of greater or equal length as the diagonal (i.e., `value` is never repeated\n    when shorter).\n\n    Note `array` is altered in place.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 551, "code": "def _add_to_diagonal(array, value, xp):\n    value, min_rows_columns = _validate_diagonal_args(array, value, xp)\n    if _is_numpy_namespace(xp):\n        step = array.shape[1] + 1\n        end = array.shape[1] * array.shape[1]\n        array.flat[:end:step] += value\n        return\n    value = xp.linalg.diagonal(array) + value\n    for i in range(min_rows_columns):\n        array[i, i] = value[i]", "documentation": "    \"\"\"Add `value` to diagonal of `array`.\n\n    Related to `fill_diagonal`. `value` should be a scalar or\n    1D of greater or equal length as the diagonal (i.e., `value` is never repeated\n    when shorter).\n\n    Note `array` is altered in place.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 586, "code": "def _max_precision_float_dtype(xp, device):\n    if _is_xp_namespace(xp, \"torch\") and str(device).startswith(\n        \"mps\"\n    ):  # pragma: no cover\n        return xp.float32\n    return xp.float64", "documentation": "    \"\"\"Return the float dtype with the highest precision supported by the device.\"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 597, "code": "def _find_matching_floating_dtype(*arrays, xp):\n    dtyped_arrays = [xp.asarray(a) for a in arrays if hasattr(a, \"dtype\")]\n    floating_dtypes = [\n        a.dtype for a in dtyped_arrays if xp.isdtype(a.dtype, \"real floating\")\n    ]\n    if floating_dtypes:\n        return xp.result_type(*floating_dtypes)\n    return xp.asarray(0.0).dtype", "documentation": "    \"\"\"Find a suitable floating point dtype when computing with arrays.\n\n    If any of the arrays are floating point, return the dtype with the highest\n    precision by following official type promotion rules:\n\n    https://data-apis.org/array-api/latest/API_specification/type_promotion.html\n\n    If there are no floating point input arrays (all integral inputs for\n    instance), return the default floating point dtype for the namespace.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 622, "code": "def _average(a, axis=None, weights=None, normalize=True, xp=None):\n    xp, _, device_ = get_namespace_and_device(a, weights, xp=xp)\n    if _is_numpy_namespace(xp):\n        if normalize:\n            return xp.asarray(numpy.average(a, axis=axis, weights=weights))\n        elif axis is None and weights is not None:\n            return xp.asarray(numpy.dot(a, weights))\n    a = xp.asarray(a, device=device_)\n    if weights is not None:\n        weights = xp.asarray(weights, device=device_)\n    if weights is not None and a.shape != weights.shape:", "documentation": "    \"\"\"Partial port of np.average to support the Array API.\n\n    It does a best effort at mimicking the return dtype rule described at\n    https://numpy.org/doc/stable/reference/generated/numpy.average.html but\n    only for the common cases needed in scikit-learn.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 819, "code": "def _ravel(array, xp=None):\n    xp, _ = get_namespace(array, xp=xp)\n    if _is_numpy_namespace(xp):\n        array = numpy.asarray(array)\n        return xp.asarray(numpy.ravel(array, order=\"C\"))\n    return xp.reshape(array, shape=(-1,))", "documentation": "    \"\"\"Array API compliant version of np.ravel.\n\n    For non numpy namespaces, it just returns a flattened array, that might\n    be or not be a copy.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 833, "code": "def _convert_to_numpy(array, xp):\n    if _is_xp_namespace(xp, \"torch\"):\n        return array.cpu().numpy()\n    elif _is_xp_namespace(xp, \"cupy\"):  # pragma: nocover\n        return array.get()\n    elif _is_xp_namespace(xp, \"array_api_strict\"):\n        return numpy.asarray(xp.asarray(array, device=xp.Device(\"CPU_DEVICE\")))\n    return numpy.asarray(array)", "documentation": "    \"\"\"Convert X into a NumPy ndarray on the CPU.\"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 845, "code": "def _estimator_with_converted_arrays(estimator, converter):\n    from sklearn.base import clone\n    new_estimator = clone(estimator)\n    for key, attribute in vars(estimator).items():\n        if hasattr(attribute, \"__dlpack__\") or isinstance(attribute, numpy.ndarray):\n            attribute = converter(attribute)\n        setattr(new_estimator, key, attribute)\n    return new_estimator", "documentation": "    \"\"\"Create new estimator which converting all attributes that are arrays.\n\n    The converter is called on all NumPy arrays and arrays that support the\n    `DLPack interface <https://dmlc.github.io/dlpack/latest/>`__.\n\n    Parameters\n    ----------\n    estimator : Estimator\n        Estimator to convert\n\n    converter : callable\n        Callable that takes an array attribute and returns the converted array.\n\n    Returns\n    -------\n    new_estimator : Estimator\n        Convert estimator\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 874, "code": "def _atol_for_type(dtype_or_dtype_name):\n    if dtype_or_dtype_name is None:\n        dtype_or_dtype_name = numpy.float64\n    return numpy.finfo(dtype_or_dtype_name).eps * 1000", "documentation": "    \"\"\"Return the absolute tolerance for a given numpy dtype.\"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 884, "code": "def indexing_dtype(xp):\n    return xp.asarray(0).dtype", "documentation": "    \"\"\"Return a platform-specific integer dtype suitable for indexing.\n\n    On 32-bit platforms, this will typically return int32 and int64 otherwise.\n\n    Note: using dtype is recommended for indexing transient array\n    datastructures. For long-lived arrays, such as the fitted attributes of\n    estimators, it is instead recommended to use platform-independent int32 if\n    we do not expect to index more 2B elements. Using fixed dtypes simplifies\n    the handling of serialized models, e.g. to deploy a model fit on a 64-bit\n    platform to a target 32-bit platform such as WASM/pyodide.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 908, "code": "def _isin(element, test_elements, xp, assume_unique=False, invert=False):\n    if _is_numpy_namespace(xp):\n        return xp.asarray(\n            numpy.isin(\n                element=element,\n                test_elements=test_elements,\n                assume_unique=assume_unique,\n                invert=invert,\n            )\n        )\n    original_element_shape = element.shape", "documentation": "    \"\"\"Calculates ``element in test_elements``, broadcasting over `element`\n    only.\n\n    Returns a boolean array of the same shape as `element` that is True\n    where an element of `element` is in `test_elements` and False otherwise.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 942, "code": "def _in1d(ar1, ar2, xp, assume_unique=False, invert=False):\n    xp, _ = get_namespace(ar1, ar2, xp=xp)\n    if ar2.shape[0] < 10 * ar1.shape[0] ** 0.145:\n        if invert:\n            mask = xp.ones(ar1.shape[0], dtype=xp.bool, device=device(ar1))\n            for a in ar2:\n                mask &= ar1 != a\n        else:\n            mask = xp.zeros(ar1.shape[0], dtype=xp.bool, device=device(ar1))\n            for a in ar2:\n                mask |= ar1 == a", "documentation": "    \"\"\"Checks whether each element of an array is also present in a\n    second array.\n\n    Returns a boolean array the same length as `ar1` that is True\n    where an element of `ar1` is in `ar2` and False otherwise.\n\n    This function has been adapted using the original implementation\n    present in numpy:\n    https://github.com/numpy/numpy/blob/v1.26.0/numpy/lib/arraysetops.py#L524-L758\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 991, "code": "def _count_nonzero(X, axis=None, sample_weight=None, xp=None, device=None):\n    from sklearn.utils.sparsefuncs import count_nonzero\n    xp, _ = get_namespace(X, sample_weight, xp=xp)\n    if _is_numpy_namespace(xp) and sp.issparse(X):\n        return count_nonzero(X, axis=axis, sample_weight=sample_weight)\n    assert X.ndim == 2\n    weights = xp.ones_like(X, device=device)\n    if sample_weight is not None:\n        sample_weight = xp.asarray(sample_weight, device=device)\n        sample_weight = xp.reshape(sample_weight, (sample_weight.shape[0], 1))\n        weights = xp.astype(weights, sample_weight.dtype) * sample_weight", "documentation": "    \"\"\"A variant of `sklearn.utils.sparsefuncs.count_nonzero` for the Array API.\n\n    If the array `X` is sparse, and we are using the numpy namespace then we\n    simply call the original function. This function only supports 2D arrays.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 1096, "code": "def _half_multinomial_loss(y, pred, sample_weight=None, xp=None):\n    xp, _, device_ = get_namespace_and_device(y, pred, sample_weight)\n    log_sum_exp = _logsumexp(pred, axis=1, xp=xp)\n    y = xp.asarray(y, dtype=xp.int64, device=device_)\n    class_margins = xp.arange(y.shape[0], device=device_) * pred.shape[1]\n    label_predictions = xp.take(_ravel(pred), y + class_margins)\n    return float(\n        _average(log_sum_exp - label_predictions, weights=sample_weight, xp=xp)\n    )", "documentation": "    \"\"\"A version of the multinomial loss that is compatible with the array API\"\"\""}], "after_segments": [{"filename": "sklearn/utils/_array_api.py", "start_line": 26, "code": "def yield_namespaces(include_numpy_namespaces=True):\n    for array_namespace in [\n        \"numpy\",\n        \"array_api_strict\",\n        \"cupy\",\n        \"torch\",\n    ]:\n        if not include_numpy_namespaces and array_namespace in _NUMPY_NAMESPACE_NAMES:\n            continue\n        yield array_namespace", "documentation": "    \"\"\"Yield supported namespace.\n\n    This is meant to be used for testing purposes only.\n\n    Parameters\n    ----------\n    include_numpy_namespaces : bool, default=True\n        If True, also yield numpy namespaces.\n\n    Returns\n    -------\n    array_namespace : str\n        The name of the Array API namespace.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 57, "code": "def yield_namespace_device_dtype_combinations(include_numpy_namespaces=True):\n    for array_namespace in yield_namespaces(\n        include_numpy_namespaces=include_numpy_namespaces\n    ):\n        if array_namespace == \"torch\":\n            for device, dtype in itertools.product(\n                (\"cpu\", \"cuda\", \"xpu\"), (\"float64\", \"float32\")\n            ):\n                yield array_namespace, device, dtype\n            yield array_namespace, \"mps\", \"float32\"\n        elif array_namespace == \"array_api_strict\":", "documentation": "    \"\"\"Yield supported namespace, device, dtype tuples for testing.\n\n    Use this to test that an estimator works with all combinations.\n    Use in conjunction with `ids=_get_namespace_device_dtype_ids` to give\n    clearer pytest parametrization ID names.\n\n    Parameters\n    ----------\n    include_numpy_namespaces : bool, default=True\n        If True, also yield numpy namespaces.\n\n    Returns\n    -------\n    array_namespace : str\n        The name of the Array API namespace.\n\n    device : str\n        The name of the device on which to allocate the arrays. Can be None to\n        indicate that the default value should be used.\n\n    dtype_name : str\n        The name of the data type to use for arrays. Can be None to indicate\n        that the default value should be used.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 108, "code": "def _get_namespace_device_dtype_ids(param):\n    try:\n        import array_api_strict\n    except ImportError:\n        return None\n    else:\n        if param == array_api_strict.Device(\"CPU_DEVICE\"):\n            return \"CPU_DEVICE\"\n        if param == array_api_strict.Device(\"device1\"):\n            return \"device1\"\n        if param == array_api_strict.Device(\"device2\"):", "documentation": "    \"\"\"Get pytest parametrization IDs for `yield_namespace_device_dtype_combinations`\"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 125, "code": "def _check_array_api_dispatch(array_api_dispatch):\n    if not array_api_dispatch:\n        return\n    scipy_version = parse_version(scipy.__version__)\n    min_scipy_version = \"1.14.0\"\n    if scipy_version < parse_version(min_scipy_version):\n        raise ImportError(\n            f\"SciPy must be {min_scipy_version} or newer\"\n            \" (found {scipy.__version__}) to dispatch array using\"\n            \" the array API specification\"\n        )", "documentation": "    \"\"\"Checks that array API support is functional.\n\n    In particular scipy needs to be recent enough and the environment variable\n    needs to be set: SCIPY_ARRAY_API=1.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 152, "code": "def _single_array_device(array):\n    if (\n        isinstance(array, (numpy.ndarray, numpy.generic))\n        or not hasattr(array, \"device\")\n        or not get_config()[\"array_api_dispatch\"]\n    ):\n        return None\n    else:\n        return array.device", "documentation": "    \"\"\"Hardware device where the array data resides on.\"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 169, "code": "def device(*array_list, remove_none=True, remove_types=(str,)):\n    array_list = _remove_non_arrays(\n        *array_list, remove_none=remove_none, remove_types=remove_types\n    )\n    if not array_list:\n        return None\n    device_ = _single_array_device(array_list[0])\n    for array in array_list[1:]:\n        device_other = _single_array_device(array)\n        if device_ != device_other:\n            raise ValueError(", "documentation": "    \"\"\"Hardware device where the array data resides on.\n\n    If the hardware device is not the same for all arrays, an error is raised.\n\n    Parameters\n    ----------\n    *array_list : arrays\n        List of array instances from NumPy or an array API compatible library.\n\n    remove_none : bool, default=True\n        Whether to ignore None objects passed in array_list.\n\n    remove_types : tuple or list, default=(str,)\n        Types to ignore in array_list.\n\n    Returns\n    -------\n    out : device\n        `device` object (see the \"Device Support\" section of the array API spec).\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 213, "code": "def size(x):\n    return math.prod(x.shape)", "documentation": "    \"\"\"Return the total number of elements of x.\n\n    Parameters\n    ----------\n    x : array\n        Array instance from NumPy or an array API compatible library.\n\n    Returns\n    -------\n    out : int\n        Total number of elements.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 229, "code": "def _is_numpy_namespace(xp):\n    return xp.__name__ in _NUMPY_NAMESPACE_NAMES", "documentation": "    \"\"\"Return True if xp is backed by NumPy.\"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 245, "code": "def supported_float_dtypes(xp, device=None):\n    dtypes_dict = xp.__array_namespace_info__().dtypes(\n        kind=\"real floating\", device=device\n    )\n    valid_float_dtypes = []\n    for dtype_key in (\"float64\", \"float32\"):\n        if dtype_key in dtypes_dict:\n            valid_float_dtypes.append(dtypes_dict[dtype_key])\n    if hasattr(xp, \"float16\"):\n        valid_float_dtypes.append(xp.float16)\n    return tuple(valid_float_dtypes)", "documentation": "    \"\"\"Supported floating point types for the namespace.\n\n    Parameters\n    ----------\n    xp : module\n        Array namespace to inspect.\n\n    device : str or device instance from xp, default=None\n        Device to use for dtype selection. If ``None``, then a default device\n        is assumed.\n\n    Returns\n    -------\n    supported_dtypes : tuple\n        Tuple of real floating data types supported by the provided array namespace,\n        ordered from the highest precision to lowest.\n\n    See Also\n    --------\n    max_precision_float_dtype : Maximum float dtype for a namespace/device pair.\n\n    Notes\n    -----\n    `float16` is not officially part of the Array API spec at the\n    time of writing but scikit-learn estimators and functions can choose\n    to accept it when xp.float16 is defined.\n\n    Additionally, some devices available within a namespace may not support\n    all floating-point types that the namespace provides.\n\n    https://data-apis.org/array-api/latest/API_specification/data_types.html\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 292, "code": "def ensure_common_namespace_device(reference, *arrays):\n    xp, is_array_api = get_namespace(reference)\n    if is_array_api:\n        device_ = device(reference)\n        return [\n            xp.asarray(a, device=device_) if a is not None else None for a in arrays\n        ]\n    else:\n        return arrays", "documentation": "    \"\"\"Ensure that all arrays use the same namespace and device as reference.\n\n    If necessary the arrays are moved to the same namespace and device as\n    the reference array.\n\n    Parameters\n    ----------\n    reference : array\n        Reference array.\n\n    *arrays : array\n        Arrays to check.\n\n    Returns\n    -------\n    arrays : list\n        Arrays with the same namespace and device as reference.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 323, "code": "def _remove_non_arrays(*arrays, remove_none=True, remove_types=(str,)):\n    filtered_arrays = []\n    remove_types = tuple(remove_types)\n    for array in arrays:\n        if remove_none and array is None:\n            continue\n        if isinstance(array, remove_types):\n            continue\n        if sp.issparse(array):\n            continue\n        filtered_arrays.append(array)", "documentation": "    \"\"\"Filter arrays to exclude None and/or specific types.\n\n    Sparse arrays are always filtered out.\n\n    Parameters\n    ----------\n    *arrays : array objects\n        Array objects.\n\n    remove_none : bool, default=True\n        Whether to ignore None objects passed in arrays.\n\n    remove_types : tuple or list, default=(str,)\n        Types to ignore in the arrays.\n\n    Returns\n    -------\n    filtered_arrays : list\n        List of arrays filtered as requested. An empty list is returned if no input\n        passes the filters.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 359, "code": "def get_namespace(*arrays, remove_none=True, remove_types=(str,), xp=None):\n    array_api_dispatch = get_config()[\"array_api_dispatch\"]\n    if not array_api_dispatch:\n        if xp is not None:\n            return xp, False\n        else:\n            return np_compat, False\n    if xp is not None:\n        return xp, True\n    arrays = _remove_non_arrays(\n        *arrays,", "documentation": "    \"\"\"Get namespace of arrays.\n\n    Introspect `arrays` arguments and return their common Array API compatible\n    namespace object, if any.\n\n    Note that sparse arrays are filtered by default.\n\n    See: https://numpy.org/neps/nep-0047-array-api-standard.html\n\n    If `arrays` are regular numpy arrays, `array_api_compat.numpy` is returned instead.\n\n    Namespace support is not enabled by default. To enabled it call:\n\n      sklearn.set_config(array_api_dispatch=True)\n\n    or:\n\n      with sklearn.config_context(array_api_dispatch=True):\n          # your code here\n\n    Otherwise `array_api_compat.numpy` is\n    always returned irrespective of the fact that arrays implement the\n    `__array_namespace__` protocol or not.\n\n    Note that if no arrays pass the set filters, ``_NUMPY_API_WRAPPER_INSTANCE, False``\n    is returned.\n\n    Parameters\n    ----------\n    *arrays : array objects\n        Array objects.\n\n    remove_none : bool, default=True\n        Whether to ignore None objects passed in arrays.\n\n    remove_types : tuple or list, default=(str,)\n        Types to ignore in the arrays.\n\n    xp : module, default=None\n        Precomputed array namespace module. When passed, typically from a caller\n        that has already performed inspection of its own inputs, skips array\n        namespace inspection.\n\n    Returns\n    -------\n    namespace : module\n        Namespace shared by array objects. If any of the `arrays` are not arrays,\n        the namespace defaults to the NumPy namespace.\n\n    is_array_api_compliant : bool\n        True if the arrays are containers that implement the array API spec (see\n        https://data-apis.org/array-api/latest/index.html).\n        Always False when array_api_dispatch=False.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 502, "code": "def _validate_diagonal_args(array, value, xp):\n    if array.ndim != 2:\n        raise ValueError(\n            f\"`array` should be 2D. Got array with shape {tuple(array.shape)}\"\n        )\n    value = xp.asarray(value, dtype=array.dtype, device=device(array))\n    if value.ndim not in [0, 1]:\n        raise ValueError(\n            \"`value` needs to be a scalar or a 1D array, \"\n            f\"got a {value.ndim}D array instead.\"\n        )", "documentation": "    \"\"\"Validate arguments to `_fill_diagonal`/`_add_to_diagonal`.\"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 525, "code": "def _fill_diagonal(array, value, xp):\n    value, min_rows_columns = _validate_diagonal_args(array, value, xp)\n    if _is_numpy_namespace(xp):\n        xp.fill_diagonal(array, value, wrap=False)\n    else:\n        if value.ndim == 0:\n            for i in range(min_rows_columns):\n                array[i, i] = value\n        else:\n            for i in range(min_rows_columns):\n                array[i, i] = value[i]", "documentation": "    \"\"\"Minimal implementation of `numpy.fill_diagonal`.\n\n    `wrap` is not supported (i.e. always False). `value` should be a scalar or\n    1D of greater or equal length as the diagonal (i.e., `value` is never repeated\n    when shorter).\n\n    Note `array` is altered in place.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 551, "code": "def _add_to_diagonal(array, value, xp):\n    value, min_rows_columns = _validate_diagonal_args(array, value, xp)\n    if _is_numpy_namespace(xp):\n        step = array.shape[1] + 1\n        end = array.shape[1] * array.shape[1]\n        array.flat[:end:step] += value\n        return\n    value = xp.linalg.diagonal(array) + value\n    for i in range(min_rows_columns):\n        array[i, i] = value[i]", "documentation": "    \"\"\"Add `value` to diagonal of `array`.\n\n    Related to `fill_diagonal`. `value` should be a scalar or\n    1D of greater or equal length as the diagonal (i.e., `value` is never repeated\n    when shorter).\n\n    Note `array` is altered in place.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 586, "code": "def _max_precision_float_dtype(xp, device):\n    if _is_xp_namespace(xp, \"torch\") and str(device).startswith(\n        \"mps\"\n    ):  # pragma: no cover\n        return xp.float32\n    return xp.float64", "documentation": "    \"\"\"Return the float dtype with the highest precision supported by the device.\"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 597, "code": "def _find_matching_floating_dtype(*arrays, xp):\n    dtyped_arrays = [xp.asarray(a) for a in arrays if hasattr(a, \"dtype\")]\n    floating_dtypes = [\n        a.dtype for a in dtyped_arrays if xp.isdtype(a.dtype, \"real floating\")\n    ]\n    if floating_dtypes:\n        return xp.result_type(*floating_dtypes)\n    return xp.asarray(0.0).dtype", "documentation": "    \"\"\"Find a suitable floating point dtype when computing with arrays.\n\n    If any of the arrays are floating point, return the dtype with the highest\n    precision by following official type promotion rules:\n\n    https://data-apis.org/array-api/latest/API_specification/type_promotion.html\n\n    If there are no floating point input arrays (all integral inputs for\n    instance), return the default floating point dtype for the namespace.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 622, "code": "def _average(a, axis=None, weights=None, normalize=True, xp=None):\n    xp, _, device_ = get_namespace_and_device(a, weights, xp=xp)\n    if _is_numpy_namespace(xp):\n        if normalize:\n            return xp.asarray(numpy.average(a, axis=axis, weights=weights))\n        elif axis is None and weights is not None:\n            return xp.asarray(numpy.dot(a, weights))\n    a = xp.asarray(a, device=device_)\n    if weights is not None:\n        weights = xp.asarray(weights, device=device_)\n    if weights is not None and a.shape != weights.shape:", "documentation": "    \"\"\"Partial port of np.average to support the Array API.\n\n    It does a best effort at mimicking the return dtype rule described at\n    https://numpy.org/doc/stable/reference/generated/numpy.average.html but\n    only for the common cases needed in scikit-learn.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 819, "code": "def _ravel(array, xp=None):\n    xp, _ = get_namespace(array, xp=xp)\n    if _is_numpy_namespace(xp):\n        array = numpy.asarray(array)\n        return xp.asarray(numpy.ravel(array, order=\"C\"))\n    return xp.reshape(array, shape=(-1,))", "documentation": "    \"\"\"Array API compliant version of np.ravel.\n\n    For non numpy namespaces, it just returns a flattened array, that might\n    be or not be a copy.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 833, "code": "def _convert_to_numpy(array, xp):\n    if _is_xp_namespace(xp, \"torch\"):\n        return array.cpu().numpy()\n    elif _is_xp_namespace(xp, \"cupy\"):  # pragma: nocover\n        return array.get()\n    elif _is_xp_namespace(xp, \"array_api_strict\"):\n        return numpy.asarray(xp.asarray(array, device=xp.Device(\"CPU_DEVICE\")))\n    return numpy.asarray(array)", "documentation": "    \"\"\"Convert X into a NumPy ndarray on the CPU.\"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 845, "code": "def _estimator_with_converted_arrays(estimator, converter):\n    from sklearn.base import clone\n    new_estimator = clone(estimator)\n    for key, attribute in vars(estimator).items():\n        if hasattr(attribute, \"__dlpack__\") or isinstance(attribute, numpy.ndarray):\n            attribute = converter(attribute)\n        setattr(new_estimator, key, attribute)\n    return new_estimator", "documentation": "    \"\"\"Create new estimator which converting all attributes that are arrays.\n\n    The converter is called on all NumPy arrays and arrays that support the\n    `DLPack interface <https://dmlc.github.io/dlpack/latest/>`__.\n\n    Parameters\n    ----------\n    estimator : Estimator\n        Estimator to convert\n\n    converter : callable\n        Callable that takes an array attribute and returns the converted array.\n\n    Returns\n    -------\n    new_estimator : Estimator\n        Convert estimator\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 874, "code": "def _atol_for_type(dtype_or_dtype_name):\n    if dtype_or_dtype_name is None:\n        dtype_or_dtype_name = numpy.float64\n    return numpy.finfo(dtype_or_dtype_name).eps * 1000", "documentation": "    \"\"\"Return the absolute tolerance for a given numpy dtype.\"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 884, "code": "def indexing_dtype(xp):\n    return xp.asarray(0).dtype", "documentation": "    \"\"\"Return a platform-specific integer dtype suitable for indexing.\n\n    On 32-bit platforms, this will typically return int32 and int64 otherwise.\n\n    Note: using dtype is recommended for indexing transient array\n    datastructures. For long-lived arrays, such as the fitted attributes of\n    estimators, it is instead recommended to use platform-independent int32 if\n    we do not expect to index more 2B elements. Using fixed dtypes simplifies\n    the handling of serialized models, e.g. to deploy a model fit on a 64-bit\n    platform to a target 32-bit platform such as WASM/pyodide.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 908, "code": "def _isin(element, test_elements, xp, assume_unique=False, invert=False):\n    if _is_numpy_namespace(xp):\n        return xp.asarray(\n            numpy.isin(\n                element=element,\n                test_elements=test_elements,\n                assume_unique=assume_unique,\n                invert=invert,\n            )\n        )\n    original_element_shape = element.shape", "documentation": "    \"\"\"Calculates ``element in test_elements``, broadcasting over `element`\n    only.\n\n    Returns a boolean array of the same shape as `element` that is True\n    where an element of `element` is in `test_elements` and False otherwise.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 942, "code": "def _in1d(ar1, ar2, xp, assume_unique=False, invert=False):\n    xp, _ = get_namespace(ar1, ar2, xp=xp)\n    if ar2.shape[0] < 10 * ar1.shape[0] ** 0.145:\n        if invert:\n            mask = xp.ones(ar1.shape[0], dtype=xp.bool, device=device(ar1))\n            for a in ar2:\n                mask &= ar1 != a\n        else:\n            mask = xp.zeros(ar1.shape[0], dtype=xp.bool, device=device(ar1))\n            for a in ar2:\n                mask |= ar1 == a", "documentation": "    \"\"\"Checks whether each element of an array is also present in a\n    second array.\n\n    Returns a boolean array the same length as `ar1` that is True\n    where an element of `ar1` is in `ar2` and False otherwise.\n\n    This function has been adapted using the original implementation\n    present in numpy:\n    https://github.com/numpy/numpy/blob/v1.26.0/numpy/lib/arraysetops.py#L524-L758\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 991, "code": "def _count_nonzero(X, axis=None, sample_weight=None, xp=None, device=None):\n    from sklearn.utils.sparsefuncs import count_nonzero\n    xp, _ = get_namespace(X, sample_weight, xp=xp)\n    if _is_numpy_namespace(xp) and sp.issparse(X):\n        return count_nonzero(X, axis=axis, sample_weight=sample_weight)\n    assert X.ndim == 2\n    weights = xp.ones_like(X, device=device)\n    if sample_weight is not None:\n        sample_weight = xp.asarray(sample_weight, device=device)\n        sample_weight = xp.reshape(sample_weight, (sample_weight.shape[0], 1))\n        weights = xp.astype(weights, sample_weight.dtype) * sample_weight", "documentation": "    \"\"\"A variant of `sklearn.utils.sparsefuncs.count_nonzero` for the Array API.\n\n    If the array `X` is sparse, and we are using the numpy namespace then we\n    simply call the original function. This function only supports 2D arrays.\n    \"\"\""}, {"filename": "sklearn/utils/_array_api.py", "start_line": 1096, "code": "def _half_multinomial_loss(y, pred, sample_weight=None, xp=None):\n    xp, _, device_ = get_namespace_and_device(y, pred, sample_weight)\n    log_sum_exp = _logsumexp(pred, axis=1, xp=xp)\n    y = xp.asarray(y, dtype=xp.int64, device=device_)\n    class_margins = xp.arange(y.shape[0], device=device_) * pred.shape[1]\n    label_predictions = xp.take(_ravel(pred), y + class_margins)\n    return float(\n        _average(log_sum_exp - label_predictions, weights=sample_weight, xp=xp)\n    )", "documentation": "    \"\"\"A version of the multinomial loss that is compatible with the array API\"\"\""}]}
{"repository": "scikit-learn/scikit-learn", "commit_sha": "886829ae577ba7a47307e9cfbe6bcc6118296830", "commit_message": "FIX remove useless method overrides to fix docstrings (#32607)", "commit_date": "2025-10-30T05:06:56+00:00", "author": "Olivier Grisel", "file": "sklearn/discriminant_analysis.py", "patch": "@@ -796,13 +796,7 @@ def predict_log_proba(self, X):\n         xp, _ = get_namespace(X)\n         prediction = self.predict_proba(X)\n \n-        info = xp.finfo(prediction.dtype)\n-        if hasattr(info, \"smallest_normal\"):\n-            smallest_normal = info.smallest_normal\n-        else:\n-            # smallest_normal was introduced in NumPy 1.22\n-            smallest_normal = info.tiny\n-\n+        smallest_normal = xp.finfo(prediction.dtype).smallest_normal\n         prediction[prediction == 0.0] += smallest_normal\n         return xp.log(prediction)\n \n@@ -826,7 +820,7 @@ def decision_function(self, X):\n             In the two-class case, the shape is `(n_samples,)`, giving the\n             log likelihood ratio of the positive class.\n         \"\"\"\n-        # Only override for the doc\n+        # Only overrides for the docstring.\n         return super().decision_function(X)\n \n     def __sklearn_tags__(self):\n@@ -1074,55 +1068,5 @@ def decision_function(self, X):\n             In the two-class case, the shape is `(n_samples,)`, giving the\n             log likelihood ratio of the positive class.\n         \"\"\"\n+        # Only overrides for the docstring.\n         return super().decision_function(X)\n-\n-    def predict(self, X):\n-        \"\"\"Perform classification on an array of test vectors X.\n-\n-        The predicted class C for each sample in X is returned.\n-\n-        Parameters\n-        ----------\n-        X : array-like of shape (n_samples, n_features)\n-            Vector to be scored, where `n_samples` is the number of samples and\n-            `n_features` is the number of features.\n-\n-        Returns\n-        -------\n-        C : ndarray of shape (n_samples,)\n-            Estimated probabilities.\n-        \"\"\"\n-        return super().predict(X)\n-\n-    def predict_proba(self, X):\n-        \"\"\"Return posterior probabilities of classification.\n-\n-        Parameters\n-        ----------\n-        X : array-like of shape (n_samples, n_features)\n-            Array of samples/test vectors.\n-\n-        Returns\n-        -------\n-        C : ndarray of shape (n_samples, n_classes)\n-            Posterior probabilities of classification per class.\n-        \"\"\"\n-        # compute the likelihood of the underlying gaussian models\n-        # up to a multiplicative constant.\n-        return super().predict_proba(X)\n-\n-    def predict_log_proba(self, X):\n-        \"\"\"Return log of posterior probabilities of classification.\n-\n-        Parameters\n-        ----------\n-        X : array-like of shape (n_samples, n_features)\n-            Array of samples/test vectors.\n-\n-        Returns\n-        -------\n-        C : ndarray of shape (n_samples, n_classes)\n-            Posterior log-probabilities of classification per class.\n-        \"\"\"\n-        # XXX : can do better to avoid precision overflows\n-        return super().predict_log_proba(X)", "before_segments": [{"filename": "sklearn/discriminant_analysis.py", "start_line": 31, "code": "def _cov(X, shrinkage=None, covariance_estimator=None):\n    if covariance_estimator is None:\n        shrinkage = \"empirical\" if shrinkage is None else shrinkage\n        if isinstance(shrinkage, str):\n            if shrinkage == \"auto\":\n                sc = StandardScaler()  # standardize features\n                X = sc.fit_transform(X)\n                s = ledoit_wolf(X)[0]\n                s = sc.scale_[:, np.newaxis] * s * sc.scale_[np.newaxis, :]\n            elif shrinkage == \"empirical\":\n                s = empirical_covariance(X)", "documentation": "    \"\"\"Estimate covariance matrix (using optional covariance_estimator).\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Input data.\n\n    shrinkage : {'empirical', 'auto'} or float, default=None\n        Shrinkage parameter, possible values:\n          - None or 'empirical': no shrinkage (default).\n          - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.\n          - float between 0 and 1: fixed shrinkage parameter.\n\n        Shrinkage parameter is ignored if  `covariance_estimator`\n        is not None.\n\n    covariance_estimator : estimator, default=None\n        If not None, `covariance_estimator` is used to estimate\n        the covariance matrices instead of relying on the empirical\n        covariance estimator (with potential shrinkage).\n        The object should have a fit method and a ``covariance_`` attribute\n        like the estimators in :mod:`sklearn.covariance``.\n        If None the shrinkage parameter drives the estimate.\n\n        .. versionadded:: 0.24\n\n    Returns\n    -------\n    s : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix.\n    \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 91, "code": "def _class_means(X, y):\n    xp, is_array_api_compliant = get_namespace(X)\n    classes, y = xp.unique_inverse(y)\n    means = xp.zeros((classes.shape[0], X.shape[1]), device=device(X), dtype=X.dtype)\n    if is_array_api_compliant:\n        for i in range(classes.shape[0]):\n            means[i, :] = xp.mean(X[y == i], axis=0)\n    else:\n        cnt = np.bincount(y)\n        np.add.at(means, y, X)\n        means /= cnt[:, None]", "documentation": "    \"\"\"Compute class means.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Input data.\n\n    y : array-like of shape (n_samples,) or (n_samples, n_targets)\n        Target values.\n\n    Returns\n    -------\n    means : array-like of shape (n_classes, n_features)\n        Class means.\n    \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 123, "code": "def _class_cov(X, y, priors, shrinkage=None, covariance_estimator=None):\n    classes = np.unique(y)\n    cov = np.zeros(shape=(X.shape[1], X.shape[1]))\n    for idx, group in enumerate(classes):\n        Xg = X[y == group, :]\n        cov += priors[idx] * np.atleast_2d(_cov(Xg, shrinkage, covariance_estimator))\n    return cov", "documentation": "    \"\"\"Compute weighted within-class covariance matrix.\n\n    The per-class covariance are weighted by the class priors.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Input data.\n\n    y : array-like of shape (n_samples,) or (n_samples, n_targets)\n        Target values.\n\n    priors : array-like of shape (n_classes,)\n        Class priors.\n\n    shrinkage : 'auto' or float, default=None\n        Shrinkage parameter, possible values:\n          - None: no shrinkage (default).\n          - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.\n          - float between 0 and 1: fixed shrinkage parameter.\n\n        Shrinkage parameter is ignored if `covariance_estimator` is not None.\n\n    covariance_estimator : estimator, default=None\n        If not None, `covariance_estimator` is used to estimate\n        the covariance matrices instead of relying the empirical\n        covariance estimator (with potential shrinkage).\n        The object should have a fit method and a ``covariance_`` attribute\n        like the estimators in sklearn.covariance.\n        If None, the shrinkage parameter drives the estimate.\n\n        .. versionadded:: 0.24\n\n    Returns\n    -------\n    cov : array-like of shape (n_features, n_features)\n        Weighted within-class covariance matrix\n    \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 170, "code": "class DiscriminantAnalysisPredictionMixin:", "documentation": "    \"\"\"Mixin class for QuadraticDiscriminantAnalysis and NearestCentroid.\"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 173, "code": "    def decision_function(self, X):\n        y_scores = self._decision_function(X)\n        if len(self.classes_) == 2:\n            return y_scores[:, 1] - y_scores[:, 0]\n        return y_scores", "documentation": "        \"\"\"Apply decision function to an array of samples.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Array of samples (test vectors).\n\n        Returns\n        -------\n        y_scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n            Decision function values related to each class, per sample.\n            In the two-class case, the shape is `(n_samples,)`, giving the\n            log likelihood ratio of the positive class.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 193, "code": "    def predict(self, X):\n        scores = self._decision_function(X)\n        return self.classes_.take(scores.argmax(axis=1))", "documentation": "        \"\"\"Perform classification on an array of vectors `X`.\n\n        Returns the class label for each sample.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Class label for each sample.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 212, "code": "    def predict_proba(self, X):\n        return np.exp(self.predict_log_proba(X))", "documentation": "        \"\"\"Estimate class probabilities.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.\n\n        Returns\n        -------\n        y_proba : ndarray of shape (n_samples, n_classes)\n            Probability estimate of the sample for each class in the\n            model, where classes are ordered as they are in `self.classes_`.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 228, "code": "    def predict_log_proba(self, X):\n        scores = self._decision_function(X)\n        log_likelihood = scores - scores.max(axis=1)[:, np.newaxis]\n        return log_likelihood - np.log(\n            np.exp(log_likelihood).sum(axis=1)[:, np.newaxis]\n        )", "documentation": "        \"\"\"Estimate log class probabilities.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.\n\n        Returns\n        -------\n        y_log_proba : ndarray of shape (n_samples, n_classes)\n            Estimated log probabilities.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 438, "code": "    def _solve_lstsq(self, X, y, shrinkage, covariance_estimator):\n        self.means_ = _class_means(X, y)\n        self.covariance_ = _class_cov(\n            X, y, self.priors_, shrinkage, covariance_estimator\n        )\n        self.coef_ = linalg.lstsq(self.covariance_, self.means_.T)[0].T\n        self.intercept_ = -0.5 * np.diag(np.dot(self.means_, self.coef_.T)) + np.log(\n            self.priors_\n        )", "documentation": "        \"\"\"Least squares solver.\n\n        The least squares solver computes a straightforward solution of the\n        optimal decision rule based directly on the discriminant functions. It\n        can only be used for classification (with any covariance estimator),\n        because\n        estimation of eigenvectors is not performed. Therefore, dimensionality\n        reduction with the transform is not supported.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_classes)\n            Target values.\n\n        shrinkage : 'auto', float or None\n            Shrinkage parameter, possible values:\n              - None: no shrinkage.\n              - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.\n              - float between 0 and 1: fixed shrinkage parameter.\n\n            Shrinkage parameter is ignored if  `covariance_estimator` is\n            not None\n\n        covariance_estimator : estimator, default=None\n            If not None, `covariance_estimator` is used to estimate\n            the covariance matrices instead of relying the empirical\n            covariance estimator (with potential shrinkage).\n            The object should have a fit method and a ``covariance_`` attribute\n            like the estimators in sklearn.covariance.\n            if None the shrinkage parameter drives the estimate.\n\n            .. versionadded:: 0.24\n\n        Notes\n        -----\n        This solver is based on [1]_, section 2.6.2, pp. 39-41.\n\n        References\n        ----------\n        .. [1] R. O. Duda, P. E. Hart, D. G. Stork. Pattern Classification\n           (Second Edition). John Wiley & Sons, Inc., New York, 2001. ISBN\n           0-471-05669-3.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 494, "code": "    def _solve_eigen(self, X, y, shrinkage, covariance_estimator):\n        self.means_ = _class_means(X, y)\n        self.covariance_ = _class_cov(\n            X, y, self.priors_, shrinkage, covariance_estimator\n        )\n        Sw = self.covariance_  # within scatter\n        St = _cov(X, shrinkage, covariance_estimator)  # total scatter\n        Sb = St - Sw  # between scatter\n        evals, evecs = linalg.eigh(Sb, Sw)\n        self.explained_variance_ratio_ = np.sort(evals / np.sum(evals))[::-1][\n            : self._max_components", "documentation": "        \"\"\"Eigenvalue solver.\n\n        The eigenvalue solver computes the optimal solution of the Rayleigh\n        coefficient (basically the ratio of between class scatter to within\n        class scatter). This solver supports both classification and\n        dimensionality reduction (with any covariance estimator).\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target values.\n\n        shrinkage : 'auto', float or None\n            Shrinkage parameter, possible values:\n              - None: no shrinkage.\n              - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.\n              - float between 0 and 1: fixed shrinkage constant.\n\n            Shrinkage parameter is ignored if  `covariance_estimator` i\n            not None\n\n        covariance_estimator : estimator, default=None\n            If not None, `covariance_estimator` is used to estimate\n            the covariance matrices instead of relying the empirical\n            covariance estimator (with potential shrinkage).\n            The object should have a fit method and a ``covariance_`` attribute\n            like the estimators in sklearn.covariance.\n            if None the shrinkage parameter drives the estimate.\n\n            .. versionadded:: 0.24\n\n        Notes\n        -----\n        This solver is based on [1]_, section 3.8.3, pp. 121-124.\n\n        References\n        ----------\n        .. [1] R. O. Duda, P. E. Hart, D. G. Stork. Pattern Classification\n           (Second Edition). John Wiley & Sons, Inc., New York, 2001. ISBN\n           0-471-05669-3.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 560, "code": "    def _solve_svd(self, X, y):\n        xp, is_array_api_compliant = get_namespace(X)\n        if is_array_api_compliant:\n            svd = xp.linalg.svd\n        else:\n            svd = scipy.linalg.svd\n        n_samples, _ = X.shape\n        n_classes = self.classes_.shape[0]\n        self.means_ = _class_means(X, y)\n        if self.store_covariance:\n            self.covariance_ = _class_cov(X, y, self.priors_)", "documentation": "        \"\"\"SVD solver.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target values.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 638, "code": "    def fit(self, X, y):\n        xp, _ = get_namespace(X)\n        X, y = validate_data(\n            self, X, y, ensure_min_samples=2, dtype=[xp.float64, xp.float32]\n        )\n        self.classes_ = unique_labels(y)\n        n_samples, n_features = X.shape\n        n_classes = self.classes_.shape[0]\n        if n_samples == n_classes:\n            raise ValueError(\n                \"The number of samples must be more than the number of classes.\"", "documentation": "        \"\"\"Fit the Linear Discriminant Analysis model.\n\n        .. versionchanged:: 0.19\n            `store_covariance` and `tol` has been moved to main constructor.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 731, "code": "    def transform(self, X):\n        if self.solver == \"lsqr\":\n            raise NotImplementedError(\n                \"transform not implemented for 'lsqr' solver (use 'svd' or 'eigen').\"\n            )\n        check_is_fitted(self)\n        X = validate_data(self, X, reset=False)\n        if self.solver == \"svd\":\n            X_new = (X - self.xbar_) @ self.scalings_\n        elif self.solver == \"eigen\":\n            X_new = X @ self.scalings_", "documentation": "        \"\"\"Project data to maximize class separation.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components) or \\\n            (n_samples, min(rank, n_components))\n            Transformed data. In the case of the 'svd' solver, the shape\n            is (n_samples, min(rank, n_components)).\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 760, "code": "    def predict_proba(self, X):\n        check_is_fitted(self)\n        xp, _ = get_namespace(X)\n        decision = self.decision_function(X)\n        if size(self.classes_) == 2:\n            proba = _expit(decision, xp)\n            return xp.stack([1 - proba, proba], axis=1)\n        else:\n            return softmax(decision)", "documentation": "        \"\"\"Estimate probability.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples, n_classes)\n            Estimated probabilities.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 782, "code": "    def predict_log_proba(self, X):\n        xp, _ = get_namespace(X)\n        prediction = self.predict_proba(X)\n        info = xp.finfo(prediction.dtype)\n        if hasattr(info, \"smallest_normal\"):\n            smallest_normal = info.smallest_normal\n        else:\n            smallest_normal = info.tiny\n        prediction[prediction == 0.0] += smallest_normal\n        return xp.log(prediction)", "documentation": "        \"\"\"Estimate log probability.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples, n_classes)\n            Estimated log probabilities.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 808, "code": "    def decision_function(self, X):\n        return super().decision_function(X)", "documentation": "        \"\"\"Apply decision function to an array of samples.\n\n        The decision function is equal (up to a constant factor) to the\n        log-posterior of the model, i.e. `log p(y = k | x)`. In a binary\n        classification setting this instead corresponds to the difference\n        `log p(y = 1 | x) - log p(y = 0 | x)`. See :ref:`lda_qda_math`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Array of samples (test vectors).\n\n        Returns\n        -------\n        y_scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n            Decision function values related to each class, per sample.\n            In the two-class case, the shape is `(n_samples,)`, giving the\n            log likelihood ratio of the positive class.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 960, "code": "    def fit(self, X, y):\n        X, y = validate_data(self, X, y)\n        check_classification_targets(y)\n        self.classes_, y = np.unique(y, return_inverse=True)\n        n_samples, n_features = X.shape\n        n_classes = len(self.classes_)\n        if n_classes < 2:\n            raise ValueError(\n                \"The number of classes has to be greater than one; got %d class\"\n                % (n_classes)\n            )", "documentation": "        \"\"\"Fit the model according to the given training data and parameters.\n\n        .. versionchanged:: 0.19\n            ``store_covariances`` has been moved to main constructor as\n            ``store_covariance``.\n\n        .. versionchanged:: 0.19\n            ``tol`` has been moved to main constructor.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values (integers).\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 1056, "code": "    def decision_function(self, X):\n        return super().decision_function(X)", "documentation": "        \"\"\"Apply decision function to an array of samples.\n\n        The decision function is equal (up to a constant factor) to the\n        log-posterior of the model, i.e. `log p(y = k | x)`. In a binary\n        classification setting this instead corresponds to the difference\n        `log p(y = 1 | x) - log p(y = 0 | x)`. See :ref:`lda_qda_math`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Array of samples (test vectors).\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples,) or (n_samples, n_classes)\n            Decision function values related to each class, per sample.\n            In the two-class case, the shape is `(n_samples,)`, giving the\n            log likelihood ratio of the positive class.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 1078, "code": "    def predict(self, X):\n        return super().predict(X)", "documentation": "        \"\"\"Perform classification on an array of test vectors X.\n\n        The predicted class C for each sample in X is returned.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Vector to be scored, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples,)\n            Estimated probabilities.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 1096, "code": "    def predict_proba(self, X):\n        return super().predict_proba(X)", "documentation": "        \"\"\"Return posterior probabilities of classification.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Array of samples/test vectors.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples, n_classes)\n            Posterior probabilities of classification per class.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 1113, "code": "    def predict_log_proba(self, X):\n        return super().predict_log_proba(X)", "documentation": "        \"\"\"Return log of posterior probabilities of classification.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Array of samples/test vectors.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples, n_classes)\n            Posterior log-probabilities of classification per class.\n        \"\"\""}], "after_segments": [{"filename": "sklearn/discriminant_analysis.py", "start_line": 31, "code": "def _cov(X, shrinkage=None, covariance_estimator=None):\n    if covariance_estimator is None:\n        shrinkage = \"empirical\" if shrinkage is None else shrinkage\n        if isinstance(shrinkage, str):\n            if shrinkage == \"auto\":\n                sc = StandardScaler()  # standardize features\n                X = sc.fit_transform(X)\n                s = ledoit_wolf(X)[0]\n                s = sc.scale_[:, np.newaxis] * s * sc.scale_[np.newaxis, :]\n            elif shrinkage == \"empirical\":\n                s = empirical_covariance(X)", "documentation": "    \"\"\"Estimate covariance matrix (using optional covariance_estimator).\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Input data.\n\n    shrinkage : {'empirical', 'auto'} or float, default=None\n        Shrinkage parameter, possible values:\n          - None or 'empirical': no shrinkage (default).\n          - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.\n          - float between 0 and 1: fixed shrinkage parameter.\n\n        Shrinkage parameter is ignored if  `covariance_estimator`\n        is not None.\n\n    covariance_estimator : estimator, default=None\n        If not None, `covariance_estimator` is used to estimate\n        the covariance matrices instead of relying on the empirical\n        covariance estimator (with potential shrinkage).\n        The object should have a fit method and a ``covariance_`` attribute\n        like the estimators in :mod:`sklearn.covariance``.\n        If None the shrinkage parameter drives the estimate.\n\n        .. versionadded:: 0.24\n\n    Returns\n    -------\n    s : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix.\n    \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 91, "code": "def _class_means(X, y):\n    xp, is_array_api_compliant = get_namespace(X)\n    classes, y = xp.unique_inverse(y)\n    means = xp.zeros((classes.shape[0], X.shape[1]), device=device(X), dtype=X.dtype)\n    if is_array_api_compliant:\n        for i in range(classes.shape[0]):\n            means[i, :] = xp.mean(X[y == i], axis=0)\n    else:\n        cnt = np.bincount(y)\n        np.add.at(means, y, X)\n        means /= cnt[:, None]", "documentation": "    \"\"\"Compute class means.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Input data.\n\n    y : array-like of shape (n_samples,) or (n_samples, n_targets)\n        Target values.\n\n    Returns\n    -------\n    means : array-like of shape (n_classes, n_features)\n        Class means.\n    \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 123, "code": "def _class_cov(X, y, priors, shrinkage=None, covariance_estimator=None):\n    classes = np.unique(y)\n    cov = np.zeros(shape=(X.shape[1], X.shape[1]))\n    for idx, group in enumerate(classes):\n        Xg = X[y == group, :]\n        cov += priors[idx] * np.atleast_2d(_cov(Xg, shrinkage, covariance_estimator))\n    return cov", "documentation": "    \"\"\"Compute weighted within-class covariance matrix.\n\n    The per-class covariance are weighted by the class priors.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Input data.\n\n    y : array-like of shape (n_samples,) or (n_samples, n_targets)\n        Target values.\n\n    priors : array-like of shape (n_classes,)\n        Class priors.\n\n    shrinkage : 'auto' or float, default=None\n        Shrinkage parameter, possible values:\n          - None: no shrinkage (default).\n          - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.\n          - float between 0 and 1: fixed shrinkage parameter.\n\n        Shrinkage parameter is ignored if `covariance_estimator` is not None.\n\n    covariance_estimator : estimator, default=None\n        If not None, `covariance_estimator` is used to estimate\n        the covariance matrices instead of relying the empirical\n        covariance estimator (with potential shrinkage).\n        The object should have a fit method and a ``covariance_`` attribute\n        like the estimators in sklearn.covariance.\n        If None, the shrinkage parameter drives the estimate.\n\n        .. versionadded:: 0.24\n\n    Returns\n    -------\n    cov : array-like of shape (n_features, n_features)\n        Weighted within-class covariance matrix\n    \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 170, "code": "class DiscriminantAnalysisPredictionMixin:", "documentation": "    \"\"\"Mixin class for QuadraticDiscriminantAnalysis and NearestCentroid.\"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 173, "code": "    def decision_function(self, X):\n        y_scores = self._decision_function(X)\n        if len(self.classes_) == 2:\n            return y_scores[:, 1] - y_scores[:, 0]\n        return y_scores", "documentation": "        \"\"\"Apply decision function to an array of samples.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Array of samples (test vectors).\n\n        Returns\n        -------\n        y_scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n            Decision function values related to each class, per sample.\n            In the two-class case, the shape is `(n_samples,)`, giving the\n            log likelihood ratio of the positive class.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 193, "code": "    def predict(self, X):\n        scores = self._decision_function(X)\n        return self.classes_.take(scores.argmax(axis=1))", "documentation": "        \"\"\"Perform classification on an array of vectors `X`.\n\n        Returns the class label for each sample.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Class label for each sample.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 212, "code": "    def predict_proba(self, X):\n        return np.exp(self.predict_log_proba(X))", "documentation": "        \"\"\"Estimate class probabilities.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.\n\n        Returns\n        -------\n        y_proba : ndarray of shape (n_samples, n_classes)\n            Probability estimate of the sample for each class in the\n            model, where classes are ordered as they are in `self.classes_`.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 228, "code": "    def predict_log_proba(self, X):\n        scores = self._decision_function(X)\n        log_likelihood = scores - scores.max(axis=1)[:, np.newaxis]\n        return log_likelihood - np.log(\n            np.exp(log_likelihood).sum(axis=1)[:, np.newaxis]\n        )", "documentation": "        \"\"\"Estimate log class probabilities.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.\n\n        Returns\n        -------\n        y_log_proba : ndarray of shape (n_samples, n_classes)\n            Estimated log probabilities.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 438, "code": "    def _solve_lstsq(self, X, y, shrinkage, covariance_estimator):\n        self.means_ = _class_means(X, y)\n        self.covariance_ = _class_cov(\n            X, y, self.priors_, shrinkage, covariance_estimator\n        )\n        self.coef_ = linalg.lstsq(self.covariance_, self.means_.T)[0].T\n        self.intercept_ = -0.5 * np.diag(np.dot(self.means_, self.coef_.T)) + np.log(\n            self.priors_\n        )", "documentation": "        \"\"\"Least squares solver.\n\n        The least squares solver computes a straightforward solution of the\n        optimal decision rule based directly on the discriminant functions. It\n        can only be used for classification (with any covariance estimator),\n        because\n        estimation of eigenvectors is not performed. Therefore, dimensionality\n        reduction with the transform is not supported.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_classes)\n            Target values.\n\n        shrinkage : 'auto', float or None\n            Shrinkage parameter, possible values:\n              - None: no shrinkage.\n              - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.\n              - float between 0 and 1: fixed shrinkage parameter.\n\n            Shrinkage parameter is ignored if  `covariance_estimator` is\n            not None\n\n        covariance_estimator : estimator, default=None\n            If not None, `covariance_estimator` is used to estimate\n            the covariance matrices instead of relying the empirical\n            covariance estimator (with potential shrinkage).\n            The object should have a fit method and a ``covariance_`` attribute\n            like the estimators in sklearn.covariance.\n            if None the shrinkage parameter drives the estimate.\n\n            .. versionadded:: 0.24\n\n        Notes\n        -----\n        This solver is based on [1]_, section 2.6.2, pp. 39-41.\n\n        References\n        ----------\n        .. [1] R. O. Duda, P. E. Hart, D. G. Stork. Pattern Classification\n           (Second Edition). John Wiley & Sons, Inc., New York, 2001. ISBN\n           0-471-05669-3.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 494, "code": "    def _solve_eigen(self, X, y, shrinkage, covariance_estimator):\n        self.means_ = _class_means(X, y)\n        self.covariance_ = _class_cov(\n            X, y, self.priors_, shrinkage, covariance_estimator\n        )\n        Sw = self.covariance_  # within scatter\n        St = _cov(X, shrinkage, covariance_estimator)  # total scatter\n        Sb = St - Sw  # between scatter\n        evals, evecs = linalg.eigh(Sb, Sw)\n        self.explained_variance_ratio_ = np.sort(evals / np.sum(evals))[::-1][\n            : self._max_components", "documentation": "        \"\"\"Eigenvalue solver.\n\n        The eigenvalue solver computes the optimal solution of the Rayleigh\n        coefficient (basically the ratio of between class scatter to within\n        class scatter). This solver supports both classification and\n        dimensionality reduction (with any covariance estimator).\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target values.\n\n        shrinkage : 'auto', float or None\n            Shrinkage parameter, possible values:\n              - None: no shrinkage.\n              - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.\n              - float between 0 and 1: fixed shrinkage constant.\n\n            Shrinkage parameter is ignored if  `covariance_estimator` i\n            not None\n\n        covariance_estimator : estimator, default=None\n            If not None, `covariance_estimator` is used to estimate\n            the covariance matrices instead of relying the empirical\n            covariance estimator (with potential shrinkage).\n            The object should have a fit method and a ``covariance_`` attribute\n            like the estimators in sklearn.covariance.\n            if None the shrinkage parameter drives the estimate.\n\n            .. versionadded:: 0.24\n\n        Notes\n        -----\n        This solver is based on [1]_, section 3.8.3, pp. 121-124.\n\n        References\n        ----------\n        .. [1] R. O. Duda, P. E. Hart, D. G. Stork. Pattern Classification\n           (Second Edition). John Wiley & Sons, Inc., New York, 2001. ISBN\n           0-471-05669-3.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 560, "code": "    def _solve_svd(self, X, y):\n        xp, is_array_api_compliant = get_namespace(X)\n        if is_array_api_compliant:\n            svd = xp.linalg.svd\n        else:\n            svd = scipy.linalg.svd\n        n_samples, _ = X.shape\n        n_classes = self.classes_.shape[0]\n        self.means_ = _class_means(X, y)\n        if self.store_covariance:\n            self.covariance_ = _class_cov(X, y, self.priors_)", "documentation": "        \"\"\"SVD solver.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target values.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 638, "code": "    def fit(self, X, y):\n        xp, _ = get_namespace(X)\n        X, y = validate_data(\n            self, X, y, ensure_min_samples=2, dtype=[xp.float64, xp.float32]\n        )\n        self.classes_ = unique_labels(y)\n        n_samples, n_features = X.shape\n        n_classes = self.classes_.shape[0]\n        if n_samples == n_classes:\n            raise ValueError(\n                \"The number of samples must be more than the number of classes.\"", "documentation": "        \"\"\"Fit the Linear Discriminant Analysis model.\n\n        .. versionchanged:: 0.19\n            `store_covariance` and `tol` has been moved to main constructor.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 731, "code": "    def transform(self, X):\n        if self.solver == \"lsqr\":\n            raise NotImplementedError(\n                \"transform not implemented for 'lsqr' solver (use 'svd' or 'eigen').\"\n            )\n        check_is_fitted(self)\n        X = validate_data(self, X, reset=False)\n        if self.solver == \"svd\":\n            X_new = (X - self.xbar_) @ self.scalings_\n        elif self.solver == \"eigen\":\n            X_new = X @ self.scalings_", "documentation": "        \"\"\"Project data to maximize class separation.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components) or \\\n            (n_samples, min(rank, n_components))\n            Transformed data. In the case of the 'svd' solver, the shape\n            is (n_samples, min(rank, n_components)).\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 760, "code": "    def predict_proba(self, X):\n        check_is_fitted(self)\n        xp, _ = get_namespace(X)\n        decision = self.decision_function(X)\n        if size(self.classes_) == 2:\n            proba = _expit(decision, xp)\n            return xp.stack([1 - proba, proba], axis=1)\n        else:\n            return softmax(decision)", "documentation": "        \"\"\"Estimate probability.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples, n_classes)\n            Estimated probabilities.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 782, "code": "    def predict_log_proba(self, X):\n        xp, _ = get_namespace(X)\n        prediction = self.predict_proba(X)\n        smallest_normal = xp.finfo(prediction.dtype).smallest_normal\n        prediction[prediction == 0.0] += smallest_normal\n        return xp.log(prediction)", "documentation": "        \"\"\"Estimate log probability.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples, n_classes)\n            Estimated log probabilities.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 802, "code": "    def decision_function(self, X):\n        return super().decision_function(X)", "documentation": "        \"\"\"Apply decision function to an array of samples.\n\n        The decision function is equal (up to a constant factor) to the\n        log-posterior of the model, i.e. `log p(y = k | x)`. In a binary\n        classification setting this instead corresponds to the difference\n        `log p(y = 1 | x) - log p(y = 0 | x)`. See :ref:`lda_qda_math`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Array of samples (test vectors).\n\n        Returns\n        -------\n        y_scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n            Decision function values related to each class, per sample.\n            In the two-class case, the shape is `(n_samples,)`, giving the\n            log likelihood ratio of the positive class.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 954, "code": "    def fit(self, X, y):\n        X, y = validate_data(self, X, y)\n        check_classification_targets(y)\n        self.classes_, y = np.unique(y, return_inverse=True)\n        n_samples, n_features = X.shape\n        n_classes = len(self.classes_)\n        if n_classes < 2:\n            raise ValueError(\n                \"The number of classes has to be greater than one; got %d class\"\n                % (n_classes)\n            )", "documentation": "        \"\"\"Fit the model according to the given training data and parameters.\n\n        .. versionchanged:: 0.19\n            ``store_covariances`` has been moved to main constructor as\n            ``store_covariance``.\n\n        .. versionchanged:: 0.19\n            ``tol`` has been moved to main constructor.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values (integers).\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\""}, {"filename": "sklearn/discriminant_analysis.py", "start_line": 1050, "code": "    def decision_function(self, X):\n        return super().decision_function(X)", "documentation": "        \"\"\"Apply decision function to an array of samples.\n\n        The decision function is equal (up to a constant factor) to the\n        log-posterior of the model, i.e. `log p(y = k | x)`. In a binary\n        classification setting this instead corresponds to the difference\n        `log p(y = 1 | x) - log p(y = 0 | x)`. See :ref:`lda_qda_math`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Array of samples (test vectors).\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples,) or (n_samples, n_classes)\n            Decision function values related to each class, per sample.\n            In the two-class case, the shape is `(n_samples,)`, giving the\n            log likelihood ratio of the positive class.\n        \"\"\""}]}
{"repository": "scikit-learn/scikit-learn", "commit_sha": "c9a0d2da49b1612fdacb75139f405344716ed35e", "commit_message": "DOC Fix docstring in `RocDisplayCurve` and add `from_cv_results` to see also (#32237)", "commit_date": "2025-09-22T09:16:52+00:00", "author": "Lucy Liu", "file": "sklearn/metrics/_plot/roc_curve.py", "patch": "@@ -61,11 +61,11 @@ class RocCurveDisplay(_BinaryClassifierCurveDisplayMixin):\n         Name for labeling legend entries. The number of legend entries is determined\n         by the `curve_kwargs` passed to `plot`, and is not affected by `name`.\n         To label each curve, provide a list of strings. To avoid labeling\n-        individual curves that have the same appearance, this cannot be used in\n+        individual curves that have the same appearance, a list cannot be used in\n         conjunction with `curve_kwargs` being a dictionary or None. If a\n         string is provided, it will be used to either label the single legend entry\n         or if there are multiple legend entries, label each individual curve with\n-        the same name. If still `None`, no name is shown in the legend.\n+        the same name. If `None`, no name is shown in the legend.\n \n         .. versionadded:: 1.7\n \n@@ -109,6 +109,8 @@ class RocCurveDisplay(_BinaryClassifierCurveDisplayMixin):\n         (ROC) curve given an estimator and some data.\n     RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic\n         (ROC) curve given the true and predicted values.\n+    RocCurveDisplay.from_cv_results : Plot multi-fold ROC curves given\n+        cross-validation results.\n     roc_auc_score : Compute the area under the ROC curve.\n \n     Examples\n@@ -184,7 +186,7 @@ def plot(\n             Name for labeling legend entries. The number of legend entries\n             is determined by `curve_kwargs`, and is not affected by `name`.\n             To label each curve, provide a list of strings. To avoid labeling\n-            individual curves that have the same appearance, this cannot be used in\n+            individual curves that have the same appearance, a list cannot be used in\n             conjunction with `curve_kwargs` being a dictionary or None. If a\n             string is provided, it will be used to either label the single legend entry\n             or if there are multiple legend entries, label each individual curve with\n@@ -406,6 +408,8 @@ def from_estimator(\n         roc_curve : Compute Receiver operating characteristic (ROC) curve.\n         RocCurveDisplay.from_predictions : ROC Curve visualization given the\n             probabilities of scores of a classifier.\n+        RocCurveDisplay.from_cv_results : Plot multi-fold ROC curves given\n+            cross-validation results.\n         roc_auc_score : Compute the area under the ROC curve.\n \n         Examples\n@@ -557,6 +561,8 @@ def from_predictions(\n         roc_curve : Compute Receiver operating characteristic (ROC) curve.\n         RocCurveDisplay.from_estimator : ROC Curve visualization given an\n             estimator and some data.\n+        RocCurveDisplay.from_cv_results : Plot multi-fold ROC curves given\n+            cross-validation results.\n         roc_auc_score : Compute the area under the ROC curve.\n \n         Examples\n@@ -669,7 +675,7 @@ def from_cv_results(\n             Name for labeling legend entries. The number of legend entries\n             is determined by `curve_kwargs`, and is not affected by `name`.\n             To label each curve, provide a list of strings. To avoid labeling\n-            individual curves that have the same appearance, this cannot be used in\n+            individual curves that have the same appearance, a list cannot be used in\n             conjunction with `curve_kwargs` being a dictionary or None. If a\n             string is provided, it will be used to either label the single legend entry\n             or if there are multiple legend entries, label each individual curve with\n@@ -702,8 +708,8 @@ def from_cv_results(\n         See Also\n         --------\n         roc_curve : Compute Receiver operating characteristic (ROC) curve.\n-            RocCurveDisplay.from_estimator : ROC Curve visualization given an\n-            estimator and some data.\n+        RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic\n+            (ROC) curve given an estimator and some data.\n         RocCurveDisplay.from_predictions : ROC Curve visualization given the\n             probabilities of scores of a classifier.\n         roc_auc_score : Compute the area under the ROC curve.", "before_segments": [{"filename": "sklearn/metrics/_plot/roc_curve.py", "start_line": 20, "code": "class RocCurveDisplay(_BinaryClassifierCurveDisplayMixin):", "documentation": "    \"\"\"ROC Curve visualization.\n\n    It is recommended to use\n    :func:`~sklearn.metrics.RocCurveDisplay.from_estimator` or\n    :func:`~sklearn.metrics.RocCurveDisplay.from_predictions` or\n    :func:`~sklearn.metrics.RocCurveDisplay.from_cv_results` to create\n    a :class:`~sklearn.metrics.RocCurveDisplay`. All parameters are\n    stored as attributes.\n\n    For general information regarding `scikit-learn` visualization tools, see\n    the :ref:`Visualization Guide <visualizations>`.\n    For guidance on interpreting these plots, refer to the :ref:`Model\n    Evaluation Guide <roc_metrics>`.\n\n    Parameters\n    ----------\n    fpr : ndarray or list of ndarrays\n        False positive rates. Each ndarray should contain values for a single curve.\n        If plotting multiple curves, list should be of same length as `tpr`.\n\n        .. versionchanged:: 1.7\n            Now accepts a list for plotting multiple curves.\n\n    tpr : ndarray or list of ndarrays\n        True positive rates. Each ndarray should contain values for a single curve.\n        If plotting multiple curves, list should be of same length as `fpr`.\n\n        .. versionchanged:: 1.7\n            Now accepts a list for plotting multiple curves.\n\n    roc_auc : float or list of floats, default=None\n        Area under ROC curve, used for labeling each curve in the legend.\n        If plotting multiple curves, should be a list of the same length as `fpr`\n        and `tpr`. If `None`, ROC AUC scores are not shown in the legend.\n\n        .. versionchanged:: 1.7\n            Now accepts a list for plotting multiple curves.\n\n    name : str or list of str, default=None\n        Name for labeling legend entries. The number of legend entries is determined\n        by the `curve_kwargs` passed to `plot`, and is not affected by `name`.\n        To label each curve, provide a list of strings. To avoid labeling\n        individual curves that have the same appearance, this cannot be used in\n        conjunction with `curve_kwargs` being a dictionary or None. If a\n        string is provided, it will be used to either label the single legend entry\n        or if there are multiple legend entries, label each individual curve with\n        the same name. If still `None`, no name is shown in the legend.\n\n        .. versionadded:: 1.7\n\n    pos_label : int, float, bool or str, default=None\n        The class considered the positive class when ROC AUC metrics computed.\n        If not `None`, this value is displayed in the x- and y-axes labels.\n\n        .. versionadded:: 0.24\n\n    estimator_name : str, default=None\n        Name of estimator. If None, the estimator name is not shown.\n\n        .. deprecated:: 1.7\n            `estimator_name` is deprecated and will be removed in 1.9. Use `name`\n            instead.\n\n    Attributes\n    ----------\n    line_ : matplotlib Artist or list of matplotlib Artists\n        ROC Curves.\n\n        .. versionchanged:: 1.7\n            This attribute can now be a list of Artists, for when multiple curves\n            are plotted.\n\n    chance_level_ : matplotlib Artist or None\n        The chance level line. It is `None` if the chance level is not plotted.\n\n        .. versionadded:: 1.3\n\n    ax_ : matplotlib Axes\n        Axes with ROC Curve.\n\n    figure_ : matplotlib Figure\n        Figure containing the curve.\n\n    See Also\n    --------\n    roc_curve : Compute Receiver operating characteristic (ROC) curve.\n    RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic\n        (ROC) curve given an estimator and some data.\n    RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic\n        (ROC) curve given the true and predicted values.\n    roc_auc_score : Compute the area under the ROC curve.\n\n    Examples\n    --------\n    >>> import matplotlib.pyplot as plt\n    >>> import numpy as np\n    >>> from sklearn import metrics\n    >>> y_true = np.array([0, 0, 1, 1])\n    >>> y_score = np.array([0.1, 0.4, 0.35, 0.8])\n    >>> fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score)\n    >>> roc_auc = metrics.auc(fpr, tpr)\n    >>> display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n    ...                                   name='example estimator')\n    >>> display.plot()\n    <...>\n    >>> plt.show()\n    \"\"\""}], "after_segments": [{"filename": "sklearn/metrics/_plot/roc_curve.py", "start_line": 20, "code": "class RocCurveDisplay(_BinaryClassifierCurveDisplayMixin):", "documentation": "    \"\"\"ROC Curve visualization.\n\n    It is recommended to use\n    :func:`~sklearn.metrics.RocCurveDisplay.from_estimator` or\n    :func:`~sklearn.metrics.RocCurveDisplay.from_predictions` or\n    :func:`~sklearn.metrics.RocCurveDisplay.from_cv_results` to create\n    a :class:`~sklearn.metrics.RocCurveDisplay`. All parameters are\n    stored as attributes.\n\n    For general information regarding `scikit-learn` visualization tools, see\n    the :ref:`Visualization Guide <visualizations>`.\n    For guidance on interpreting these plots, refer to the :ref:`Model\n    Evaluation Guide <roc_metrics>`.\n\n    Parameters\n    ----------\n    fpr : ndarray or list of ndarrays\n        False positive rates. Each ndarray should contain values for a single curve.\n        If plotting multiple curves, list should be of same length as `tpr`.\n\n        .. versionchanged:: 1.7\n            Now accepts a list for plotting multiple curves.\n\n    tpr : ndarray or list of ndarrays\n        True positive rates. Each ndarray should contain values for a single curve.\n        If plotting multiple curves, list should be of same length as `fpr`.\n\n        .. versionchanged:: 1.7\n            Now accepts a list for plotting multiple curves.\n\n    roc_auc : float or list of floats, default=None\n        Area under ROC curve, used for labeling each curve in the legend.\n        If plotting multiple curves, should be a list of the same length as `fpr`\n        and `tpr`. If `None`, ROC AUC scores are not shown in the legend.\n\n        .. versionchanged:: 1.7\n            Now accepts a list for plotting multiple curves.\n\n    name : str or list of str, default=None\n        Name for labeling legend entries. The number of legend entries is determined\n        by the `curve_kwargs` passed to `plot`, and is not affected by `name`.\n        To label each curve, provide a list of strings. To avoid labeling\n        individual curves that have the same appearance, a list cannot be used in\n        conjunction with `curve_kwargs` being a dictionary or None. If a\n        string is provided, it will be used to either label the single legend entry\n        or if there are multiple legend entries, label each individual curve with\n        the same name. If `None`, no name is shown in the legend.\n\n        .. versionadded:: 1.7\n\n    pos_label : int, float, bool or str, default=None\n        The class considered the positive class when ROC AUC metrics computed.\n        If not `None`, this value is displayed in the x- and y-axes labels.\n\n        .. versionadded:: 0.24\n\n    estimator_name : str, default=None\n        Name of estimator. If None, the estimator name is not shown.\n\n        .. deprecated:: 1.7\n            `estimator_name` is deprecated and will be removed in 1.9. Use `name`\n            instead.\n\n    Attributes\n    ----------\n    line_ : matplotlib Artist or list of matplotlib Artists\n        ROC Curves.\n\n        .. versionchanged:: 1.7\n            This attribute can now be a list of Artists, for when multiple curves\n            are plotted.\n\n    chance_level_ : matplotlib Artist or None\n        The chance level line. It is `None` if the chance level is not plotted.\n\n        .. versionadded:: 1.3\n\n    ax_ : matplotlib Axes\n        Axes with ROC Curve.\n\n    figure_ : matplotlib Figure\n        Figure containing the curve.\n\n    See Also\n    --------\n    roc_curve : Compute Receiver operating characteristic (ROC) curve.\n    RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic\n        (ROC) curve given an estimator and some data.\n    RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic\n        (ROC) curve given the true and predicted values.\n    RocCurveDisplay.from_cv_results : Plot multi-fold ROC curves given\n        cross-validation results.\n    roc_auc_score : Compute the area under the ROC curve.\n\n    Examples\n    --------\n    >>> import matplotlib.pyplot as plt\n    >>> import numpy as np\n    >>> from sklearn import metrics\n    >>> y_true = np.array([0, 0, 1, 1])\n    >>> y_score = np.array([0.1, 0.4, 0.35, 0.8])\n    >>> fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score)\n    >>> roc_auc = metrics.auc(fpr, tpr)\n    >>> display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n    ...                                   name='example estimator')\n    >>> display.plot()\n    <...>\n    >>> plt.show()\n    \"\"\""}]}
{"repository": "scikit-learn/scikit-learn", "commit_sha": "c9a0d2da49b1612fdacb75139f405344716ed35e", "commit_message": "DOC Fix docstring in `RocDisplayCurve` and add `from_cv_results` to see also (#32237)", "commit_date": "2025-09-22T09:16:52+00:00", "author": "Lucy Liu", "file": "sklearn/metrics/_ranking.py", "patch": "@@ -1145,6 +1145,8 @@ def roc_curve(\n         (ROC) curve given an estimator and some data.\n     RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic\n         (ROC) curve given the true and predicted values.\n+    RocCurveDisplay.from_cv_results : Plot multi-fold ROC curves given\n+        cross-validation results.\n     det_curve: Compute error rates for different probability thresholds.\n     roc_auc_score : Compute the area under the ROC curve.\n ", "before_segments": [{"filename": "sklearn/metrics/_ranking.py", "start_line": 46, "code": "def auc(x, y):\n    check_consistent_length(x, y)\n    x = column_or_1d(x)\n    y = column_or_1d(y)\n    if x.shape[0] < 2:\n        raise ValueError(\n            \"At least 2 points are needed to compute area under curve, but x.shape = %s\"\n            % x.shape\n        )\n    direction = 1\n    dx = np.diff(x)", "documentation": "    \"\"\"Compute Area Under the Curve (AUC) using the trapezoidal rule.\n\n    This is a general function, given points on a curve.  For computing the\n    area under the ROC-curve, see :func:`roc_auc_score`.  For an alternative\n    way to summarize a precision-recall curve, see\n    :func:`average_precision_score`.\n\n    Parameters\n    ----------\n    x : array-like of shape (n,)\n        X coordinates. These must be either monotonic increasing or monotonic\n        decreasing.\n    y : array-like of shape (n,)\n        Y coordinates.\n\n    Returns\n    -------\n    auc : float\n        Area Under the Curve.\n\n    See Also\n    --------\n    roc_auc_score : Compute the area under the ROC curve.\n    average_precision_score : Compute average precision from prediction scores.\n    precision_recall_curve : Compute precision-recall pairs for different\n        probability thresholds.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn import metrics\n    >>> y_true = np.array([1, 1, 2, 2])\n    >>> y_score = np.array([0.1, 0.4, 0.35, 0.8])\n    >>> fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score, pos_label=2)\n    >>> metrics.auc(fpr, tpr)\n    0.75\n    \"\"\""}, {"filename": "sklearn/metrics/_ranking.py", "start_line": 424, "code": "def _binary_roc_auc_score(y_true, y_score, sample_weight=None, max_fpr=None):\n    if len(np.unique(y_true)) != 2:\n        warnings.warn(\n            (\n                \"Only one class is present in y_true. ROC AUC score \"\n                \"is not defined in that case.\"\n            ),\n            UndefinedMetricWarning,\n        )\n        return np.nan\n    fpr, tpr, _ = roc_curve(y_true, y_score, sample_weight=sample_weight)", "documentation": "    \"\"\"Binary roc auc score.\"\"\""}, {"filename": "sklearn/metrics/_ranking.py", "start_line": 829, "code": "def _binary_clf_curve(y_true, y_score, pos_label=None, sample_weight=None):\n    y_type = type_of_target(y_true, input_name=\"y_true\")\n    if not (y_type == \"binary\" or (y_type == \"multiclass\" and pos_label is not None)):\n        raise ValueError(\"{0} format is not supported\".format(y_type))\n    xp, _, device = get_namespace_and_device(y_true, y_score, sample_weight)\n    check_consistent_length(y_true, y_score, sample_weight)\n    y_true = column_or_1d(y_true)\n    y_score = column_or_1d(y_score)\n    assert_all_finite(y_true)\n    assert_all_finite(y_score)\n    if sample_weight is not None:", "documentation": "    \"\"\"Calculate true and false positives per binary classification threshold.\n\n    Parameters\n    ----------\n    y_true : ndarray of shape (n_samples,)\n        True targets of binary classification.\n\n    y_score : ndarray of shape (n_samples,)\n        Estimated probabilities or output of a decision function.\n\n    pos_label : int, float, bool or str, default=None\n        The label of the positive class.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights.\n\n    Returns\n    -------\n    fps : ndarray of shape (n_thresholds,)\n        A count of false positives, at index i being the number of negative\n        samples assigned a score >= thresholds[i]. The total number of\n        negative samples is equal to fps[-1] (thus true negatives are given by\n        fps[-1] - fps).\n\n    tps : ndarray of shape (n_thresholds,)\n        An increasing count of true positives, at index i being the number\n        of positive samples assigned a score >= thresholds[i]. The total\n        number of positive samples is equal to tps[-1] (thus false negatives\n        are given by tps[-1] - tps).\n\n    thresholds : ndarray of shape (n_thresholds,)\n        Decreasing score values.\n    \"\"\""}, {"filename": "sklearn/metrics/_ranking.py", "start_line": 1243, "code": "def label_ranking_average_precision_score(y_true, y_score, *, sample_weight=None):\n    check_consistent_length(y_true, y_score, sample_weight)\n    y_true = check_array(y_true, ensure_2d=False, accept_sparse=\"csr\")\n    y_score = check_array(y_score, ensure_2d=False)\n    if y_true.shape != y_score.shape:\n        raise ValueError(\"y_true and y_score have different shape\")\n    y_type = type_of_target(y_true, input_name=\"y_true\")\n    if y_type != \"multilabel-indicator\" and not (\n        y_type == \"binary\" and y_true.ndim == 2\n    ):\n        raise ValueError(\"{0} format is not supported\".format(y_type))", "documentation": "    \"\"\"Compute ranking-based average precision.\n\n    Label ranking average precision (LRAP) is the average over each ground\n    truth label assigned to each sample, of the ratio of true vs. total\n    labels with lower score.\n\n    This metric is used in multilabel ranking problem, where the goal\n    is to give better rank to the labels associated to each sample.\n\n    The obtained score is always strictly greater than 0 and\n    the best value is 1.\n\n    Read more in the :ref:`User Guide <label_ranking_average_precision>`.\n\n    Parameters\n    ----------\n    y_true : {array-like, sparse matrix} of shape (n_samples, n_labels)\n        True binary labels in binary indicator format.\n\n    y_score : array-like of shape (n_samples, n_labels)\n        Target scores, can either be probability estimates of the positive\n        class, confidence values, or non-thresholded measure of decisions\n        (as returned by \"decision_function\" on some classifiers).\n        For :term:`decision_function` scores, values greater than or equal to\n        zero should indicate the positive class.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights.\n\n        .. versionadded:: 0.20\n\n    Returns\n    -------\n    score : float\n        Ranking-based average precision score.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.metrics import label_ranking_average_precision_score\n    >>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n    >>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n    >>> label_ranking_average_precision_score(y_true, y_score)\n    0.416\n    \"\"\""}, {"filename": "sklearn/metrics/_ranking.py", "start_line": 1344, "code": "def coverage_error(y_true, y_score, *, sample_weight=None):\n    y_true = check_array(y_true, ensure_2d=True)\n    y_score = check_array(y_score, ensure_2d=True)\n    check_consistent_length(y_true, y_score, sample_weight)\n    y_type = type_of_target(y_true, input_name=\"y_true\")\n    if y_type != \"multilabel-indicator\":\n        raise ValueError(\"{0} format is not supported\".format(y_type))\n    if y_true.shape != y_score.shape:\n        raise ValueError(\"y_true and y_score have different shape\")\n    y_score_mask = np.ma.masked_array(y_score, mask=np.logical_not(y_true))\n    y_min_relevant = y_score_mask.min(axis=1).reshape((-1, 1))", "documentation": "    \"\"\"Coverage error measure.\n\n    Compute how far we need to go through the ranked scores to cover all\n    true labels. The best value is equal to the average number\n    of labels in ``y_true`` per sample.\n\n    Ties in ``y_scores`` are broken by giving maximal rank that would have\n    been assigned to all tied values.\n\n    Note: Our implementation's score is 1 greater than the one given in\n    Tsoumakas et al., 2010. This extends it to handle the degenerate case\n    in which an instance has 0 true labels.\n\n    Read more in the :ref:`User Guide <coverage_error>`.\n\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples, n_labels)\n        True binary labels in binary indicator format.\n\n    y_score : array-like of shape (n_samples, n_labels)\n        Target scores, can either be probability estimates of the positive\n        class, confidence values, or non-thresholded measure of decisions\n        (as returned by \"decision_function\" on some classifiers).\n        For :term:`decision_function` scores, values greater than or equal to\n        zero should indicate the positive class.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights.\n\n    Returns\n    -------\n    coverage_error : float\n        The coverage error.\n\n    References\n    ----------\n    .. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n           Mining multi-label data. In Data mining and knowledge discovery\n           handbook (pp. 667-685). Springer US.\n\n    Examples\n    --------\n    >>> from sklearn.metrics import coverage_error\n    >>> y_true = [[1, 0, 0], [0, 1, 1]]\n    >>> y_score = [[1, 0, 0], [0, 1, 1]]\n    >>> coverage_error(y_true, y_score)\n    1.5\n    \"\"\""}, {"filename": "sklearn/metrics/_ranking.py", "start_line": 1421, "code": "def label_ranking_loss(y_true, y_score, *, sample_weight=None):\n    y_true = check_array(y_true, ensure_2d=False, accept_sparse=\"csr\")\n    y_score = check_array(y_score, ensure_2d=False)\n    check_consistent_length(y_true, y_score, sample_weight)\n    y_type = type_of_target(y_true, input_name=\"y_true\")\n    if y_type not in (\"multilabel-indicator\",):\n        raise ValueError(\"{0} format is not supported\".format(y_type))\n    if y_true.shape != y_score.shape:\n        raise ValueError(\"y_true and y_score have different shape\")\n    n_samples, n_labels = y_true.shape\n    y_true = csr_matrix(y_true)", "documentation": "    \"\"\"Compute Ranking loss measure.\n\n    Compute the average number of label pairs that are incorrectly ordered\n    given y_score weighted by the size of the label set and the number of\n    labels not in the label set.\n\n    This is similar to the error set size, but weighted by the number of\n    relevant and irrelevant labels. The best performance is achieved with\n    a ranking loss of zero.\n\n    Read more in the :ref:`User Guide <label_ranking_loss>`.\n\n    .. versionadded:: 0.17\n       A function *label_ranking_loss*\n\n    Parameters\n    ----------\n    y_true : {array-like, sparse matrix} of shape (n_samples, n_labels)\n        True binary labels in binary indicator format.\n\n    y_score : array-like of shape (n_samples, n_labels)\n        Target scores, can either be probability estimates of the positive\n        class, confidence values, or non-thresholded measure of decisions\n        (as returned by \"decision_function\" on some classifiers).\n        For :term:`decision_function` scores, values greater than or equal to\n        zero should indicate the positive class.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights.\n\n    Returns\n    -------\n    loss : float\n        Average number of label pairs that are incorrectly ordered given\n        y_score weighted by the size of the label set and the number of labels not\n        in the label set.\n\n    References\n    ----------\n    .. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n           Mining multi-label data. In Data mining and knowledge discovery\n           handbook (pp. 667-685). Springer US.\n\n    Examples\n    --------\n    >>> from sklearn.metrics import label_ranking_loss\n    >>> y_true = [[1, 0, 0], [0, 0, 1]]\n    >>> y_score = [[0.75, 0.5, 1], [1, 0.2, 0.1]]\n    >>> label_ranking_loss(y_true, y_score)\n    0.75\n    \"\"\""}, {"filename": "sklearn/metrics/_ranking.py", "start_line": 1515, "code": "def _dcg_sample_scores(y_true, y_score, k=None, log_base=2, ignore_ties=False):\n    discount = 1 / (np.log(np.arange(y_true.shape[1]) + 2) / np.log(log_base))\n    if k is not None:\n        discount[k:] = 0\n    if ignore_ties:\n        ranking = np.argsort(y_score)[:, ::-1]\n        ranked = y_true[np.arange(ranking.shape[0])[:, np.newaxis], ranking]\n        cumulative_gains = discount.dot(ranked.T)\n    else:\n        discount_cumsum = np.cumsum(discount)\n        cumulative_gains = [", "documentation": "    \"\"\"Compute Discounted Cumulative Gain.\n\n    Sum the true scores ranked in the order induced by the predicted scores,\n    after applying a logarithmic discount.\n\n    This ranking metric yields a high value if true labels are ranked high by\n    ``y_score``.\n\n    Parameters\n    ----------\n    y_true : ndarray of shape (n_samples, n_labels)\n        True targets of multilabel classification, or true scores of entities\n        to be ranked.\n\n    y_score : ndarray of shape (n_samples, n_labels)\n        Target scores, can either be probability estimates, confidence values,\n        or non-thresholded measure of decisions (as returned by\n        \"decision_function\" on some classifiers).\n\n    k : int, default=None\n        Only consider the highest k scores in the ranking. If `None`, use all\n        outputs.\n\n    log_base : float, default=2\n        Base of the logarithm used for the discount. A low value means a\n        sharper discount (top results are more important).\n\n    ignore_ties : bool, default=False\n        Assume that there are no ties in y_score (which is likely to be the\n        case if y_score is continuous) for efficiency gains.\n\n    Returns\n    -------\n    discounted_cumulative_gain : ndarray of shape (n_samples,)\n        The DCG score for each sample.\n\n    See Also\n    --------\n    ndcg_score : The Discounted Cumulative Gain divided by the Ideal Discounted\n        Cumulative Gain (the DCG obtained for a perfect ranking), in order to\n        have a score between 0 and 1.\n    \"\"\""}, {"filename": "sklearn/metrics/_ranking.py", "start_line": 1575, "code": "def _tie_averaged_dcg(y_true, y_score, discount_cumsum):\n    _, inv, counts = np.unique(-y_score, return_inverse=True, return_counts=True)\n    ranked = np.zeros(len(counts))\n    np.add.at(ranked, inv, y_true)\n    ranked /= counts\n    groups = np.cumsum(counts) - 1\n    discount_sums = np.empty(len(counts))\n    discount_sums[0] = discount_cumsum[groups[0]]\n    discount_sums[1:] = np.diff(discount_cumsum[groups])\n    return (ranked * discount_sums).sum()", "documentation": "    \"\"\"\n    Compute DCG by averaging over possible permutations of ties.\n\n    The gain (`y_true`) of an index falling inside a tied group (in the order\n    induced by `y_score`) is replaced by the average gain within this group.\n    The discounted gain for a tied group is then the average `y_true` within\n    this group times the sum of discounts of the corresponding ranks.\n\n    This amounts to averaging scores for all possible orderings of the tied\n    groups.\n\n    (note in the case of dcg@k the discount is 0 after index k)\n\n    Parameters\n    ----------\n    y_true : ndarray\n        The true relevance scores.\n\n    y_score : ndarray\n        Predicted scores.\n\n    discount_cumsum : ndarray\n        Precomputed cumulative sum of the discounts.\n\n    Returns\n    -------\n    discounted_cumulative_gain : float\n        The discounted cumulative gain.\n\n    References\n    ----------\n    McSherry, F., & Najork, M. (2008, March). Computing information retrieval\n    performance measures efficiently in the presence of tied scores. In\n    European conference on information retrieval (pp. 414-421). Springer,\n    Berlin, Heidelberg.\n    \"\"\""}, {"filename": "sklearn/metrics/_ranking.py", "start_line": 1758, "code": "def _ndcg_sample_scores(y_true, y_score, k=None, ignore_ties=False):\n    gain = _dcg_sample_scores(y_true, y_score, k, ignore_ties=ignore_ties)\n    normalizing_gain = _dcg_sample_scores(y_true, y_true, k, ignore_ties=True)\n    all_irrelevant = normalizing_gain == 0\n    gain[all_irrelevant] = 0\n    gain[~all_irrelevant] /= normalizing_gain[~all_irrelevant]\n    return gain\n@validate_params(\n    {\n        \"y_true\": [\"array-like\"],\n        \"y_score\": [\"array-like\"],", "documentation": "    \"\"\"Compute Normalized Discounted Cumulative Gain.\n\n    Sum the true scores ranked in the order induced by the predicted scores,\n    after applying a logarithmic discount. Then divide by the best possible\n    score (Ideal DCG, obtained for a perfect ranking) to obtain a score between\n    0 and 1.\n\n    This ranking metric yields a high value if true labels are ranked high by\n    ``y_score``.\n\n    Parameters\n    ----------\n    y_true : ndarray of shape (n_samples, n_labels)\n        True targets of multilabel classification, or true scores of entities\n        to be ranked.\n\n    y_score : ndarray of shape (n_samples, n_labels)\n        Target scores, can either be probability estimates, confidence values,\n        or non-thresholded measure of decisions (as returned by\n        \"decision_function\" on some classifiers).\n\n    k : int, default=None\n        Only consider the highest k scores in the ranking. If None, use all\n        outputs.\n\n    ignore_ties : bool, default=False\n        Assume that there are no ties in y_score (which is likely to be the\n        case if y_score is continuous) for efficiency gains.\n\n    Returns\n    -------\n    normalized_discounted_cumulative_gain : ndarray of shape (n_samples,)\n        The NDCG score for each sample (float in [0., 1.]).\n\n    See Also\n    --------\n    dcg_score : Discounted Cumulative Gain (not normalized).\n\n    \"\"\""}, {"filename": "sklearn/metrics/_ranking.py", "start_line": 1819, "code": "def ndcg_score(y_true, y_score, *, k=None, sample_weight=None, ignore_ties=False):\n    y_true = check_array(y_true, ensure_2d=False)\n    y_score = check_array(y_score, ensure_2d=False)\n    check_consistent_length(y_true, y_score, sample_weight)\n    if y_true.min() < 0:\n        raise ValueError(\"ndcg_score should not be used on negative y_true values.\")\n    if y_true.ndim > 1 and y_true.shape[1] <= 1:\n        raise ValueError(\n            \"Computing NDCG is only meaningful when there is more than 1 document. \"\n            f\"Got {y_true.shape[1]} instead.\"\n        )", "documentation": "    \"\"\"Compute Normalized Discounted Cumulative Gain.\n\n    Sum the true scores ranked in the order induced by the predicted scores,\n    after applying a logarithmic discount. Then divide by the best possible\n    score (Ideal DCG, obtained for a perfect ranking) to obtain a score between\n    0 and 1.\n\n    This ranking metric returns a high value if true labels are ranked high by\n    ``y_score``.\n\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples, n_labels)\n        True targets of multilabel classification, or true scores of entities\n        to be ranked. Negative values in `y_true` may result in an output\n        that is not between 0 and 1.\n\n    y_score : array-like of shape (n_samples, n_labels)\n        Target scores, can either be probability estimates, confidence values,\n        or non-thresholded measure of decisions (as returned by\n        \"decision_function\" on some classifiers).\n\n    k : int, default=None\n        Only consider the highest k scores in the ranking. If `None`, use all\n        outputs.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights. If `None`, all samples are given the same weight.\n\n    ignore_ties : bool, default=False\n        Assume that there are no ties in y_score (which is likely to be the\n        case if y_score is continuous) for efficiency gains.\n\n    Returns\n    -------\n    normalized_discounted_cumulative_gain : float in [0., 1.]\n        The averaged NDCG scores for all samples.\n\n    See Also\n    --------\n    dcg_score : Discounted Cumulative Gain (not normalized).\n\n    References\n    ----------\n    `Wikipedia entry for Discounted Cumulative Gain\n    <https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_\n\n    Jarvelin, K., & Kekalainen, J. (2002).\n    Cumulated gain-based evaluation of IR techniques. ACM Transactions on\n    Information Systems (TOIS), 20(4), 422-446.\n\n    Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).\n    A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th\n    Annual Conference on Learning Theory (COLT 2013)\n\n    McSherry, F., & Najork, M. (2008, March). Computing information retrieval\n    performance measures efficiently in the presence of tied scores. In\n    European conference on information retrieval (pp. 414-421). Springer,\n    Berlin, Heidelberg.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.metrics import ndcg_score\n    >>> # we have ground-truth relevance of some answers to a query:\n    >>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])\n    >>> # we predict some scores (relevance) for the answers\n    >>> scores = np.asarray([[.1, .2, .3, 4, 70]])\n    >>> ndcg_score(true_relevance, scores)\n    0.69\n    >>> scores = np.asarray([[.05, 1.1, 1., .5, .0]])\n    >>> ndcg_score(true_relevance, scores)\n    0.49\n    >>> # we can set k to truncate the sum; only top k answers contribute.\n    >>> ndcg_score(true_relevance, scores, k=4)\n    0.35\n    >>> # the normalization takes k into account so a perfect answer\n    >>> # would still get 1.0\n    >>> ndcg_score(true_relevance, true_relevance, k=4)\n    1.0...\n    >>> # now we have some ties in our prediction\n    >>> scores = np.asarray([[1, 0, 0, 0, 1]])\n    >>> # by default ties are averaged, so here we get the average (normalized)\n    >>> # true relevance of our top predictions: (10 / 10 + 5 / 10) / 2 = .75\n    >>> ndcg_score(true_relevance, scores, k=1)\n    0.75\n    >>> # we can choose to ignore ties for faster results, but only\n    >>> # if we know there aren't ties in our scores, otherwise we get\n    >>> # wrong results:\n    >>> ndcg_score(true_relevance,\n    ...           scores, k=1, ignore_ties=True)\n    0.5...\n    \"\"\""}], "after_segments": [{"filename": "sklearn/metrics/_ranking.py", "start_line": 46, "code": "def auc(x, y):\n    check_consistent_length(x, y)\n    x = column_or_1d(x)\n    y = column_or_1d(y)\n    if x.shape[0] < 2:\n        raise ValueError(\n            \"At least 2 points are needed to compute area under curve, but x.shape = %s\"\n            % x.shape\n        )\n    direction = 1\n    dx = np.diff(x)", "documentation": "    \"\"\"Compute Area Under the Curve (AUC) using the trapezoidal rule.\n\n    This is a general function, given points on a curve.  For computing the\n    area under the ROC-curve, see :func:`roc_auc_score`.  For an alternative\n    way to summarize a precision-recall curve, see\n    :func:`average_precision_score`.\n\n    Parameters\n    ----------\n    x : array-like of shape (n,)\n        X coordinates. These must be either monotonic increasing or monotonic\n        decreasing.\n    y : array-like of shape (n,)\n        Y coordinates.\n\n    Returns\n    -------\n    auc : float\n        Area Under the Curve.\n\n    See Also\n    --------\n    roc_auc_score : Compute the area under the ROC curve.\n    average_precision_score : Compute average precision from prediction scores.\n    precision_recall_curve : Compute precision-recall pairs for different\n        probability thresholds.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn import metrics\n    >>> y_true = np.array([1, 1, 2, 2])\n    >>> y_score = np.array([0.1, 0.4, 0.35, 0.8])\n    >>> fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score, pos_label=2)\n    >>> metrics.auc(fpr, tpr)\n    0.75\n    \"\"\""}, {"filename": "sklearn/metrics/_ranking.py", "start_line": 424, "code": "def _binary_roc_auc_score(y_true, y_score, sample_weight=None, max_fpr=None):\n    if len(np.unique(y_true)) != 2:\n        warnings.warn(\n            (\n                \"Only one class is present in y_true. ROC AUC score \"\n                \"is not defined in that case.\"\n            ),\n            UndefinedMetricWarning,\n        )\n        return np.nan\n    fpr, tpr, _ = roc_curve(y_true, y_score, sample_weight=sample_weight)", "documentation": "    \"\"\"Binary roc auc score.\"\"\""}, {"filename": "sklearn/metrics/_ranking.py", "start_line": 829, "code": "def _binary_clf_curve(y_true, y_score, pos_label=None, sample_weight=None):\n    y_type = type_of_target(y_true, input_name=\"y_true\")\n    if not (y_type == \"binary\" or (y_type == \"multiclass\" and pos_label is not None)):\n        raise ValueError(\"{0} format is not supported\".format(y_type))\n    xp, _, device = get_namespace_and_device(y_true, y_score, sample_weight)\n    check_consistent_length(y_true, y_score, sample_weight)\n    y_true = column_or_1d(y_true)\n    y_score = column_or_1d(y_score)\n    assert_all_finite(y_true)\n    assert_all_finite(y_score)\n    if sample_weight is not None:", "documentation": "    \"\"\"Calculate true and false positives per binary classification threshold.\n\n    Parameters\n    ----------\n    y_true : ndarray of shape (n_samples,)\n        True targets of binary classification.\n\n    y_score : ndarray of shape (n_samples,)\n        Estimated probabilities or output of a decision function.\n\n    pos_label : int, float, bool or str, default=None\n        The label of the positive class.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights.\n\n    Returns\n    -------\n    fps : ndarray of shape (n_thresholds,)\n        A count of false positives, at index i being the number of negative\n        samples assigned a score >= thresholds[i]. The total number of\n        negative samples is equal to fps[-1] (thus true negatives are given by\n        fps[-1] - fps).\n\n    tps : ndarray of shape (n_thresholds,)\n        An increasing count of true positives, at index i being the number\n        of positive samples assigned a score >= thresholds[i]. The total\n        number of positive samples is equal to tps[-1] (thus false negatives\n        are given by tps[-1] - tps).\n\n    thresholds : ndarray of shape (n_thresholds,)\n        Decreasing score values.\n    \"\"\""}, {"filename": "sklearn/metrics/_ranking.py", "start_line": 1245, "code": "def label_ranking_average_precision_score(y_true, y_score, *, sample_weight=None):\n    check_consistent_length(y_true, y_score, sample_weight)\n    y_true = check_array(y_true, ensure_2d=False, accept_sparse=\"csr\")\n    y_score = check_array(y_score, ensure_2d=False)\n    if y_true.shape != y_score.shape:\n        raise ValueError(\"y_true and y_score have different shape\")\n    y_type = type_of_target(y_true, input_name=\"y_true\")\n    if y_type != \"multilabel-indicator\" and not (\n        y_type == \"binary\" and y_true.ndim == 2\n    ):\n        raise ValueError(\"{0} format is not supported\".format(y_type))", "documentation": "    \"\"\"Compute ranking-based average precision.\n\n    Label ranking average precision (LRAP) is the average over each ground\n    truth label assigned to each sample, of the ratio of true vs. total\n    labels with lower score.\n\n    This metric is used in multilabel ranking problem, where the goal\n    is to give better rank to the labels associated to each sample.\n\n    The obtained score is always strictly greater than 0 and\n    the best value is 1.\n\n    Read more in the :ref:`User Guide <label_ranking_average_precision>`.\n\n    Parameters\n    ----------\n    y_true : {array-like, sparse matrix} of shape (n_samples, n_labels)\n        True binary labels in binary indicator format.\n\n    y_score : array-like of shape (n_samples, n_labels)\n        Target scores, can either be probability estimates of the positive\n        class, confidence values, or non-thresholded measure of decisions\n        (as returned by \"decision_function\" on some classifiers).\n        For :term:`decision_function` scores, values greater than or equal to\n        zero should indicate the positive class.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights.\n\n        .. versionadded:: 0.20\n\n    Returns\n    -------\n    score : float\n        Ranking-based average precision score.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.metrics import label_ranking_average_precision_score\n    >>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n    >>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n    >>> label_ranking_average_precision_score(y_true, y_score)\n    0.416\n    \"\"\""}, {"filename": "sklearn/metrics/_ranking.py", "start_line": 1346, "code": "def coverage_error(y_true, y_score, *, sample_weight=None):\n    y_true = check_array(y_true, ensure_2d=True)\n    y_score = check_array(y_score, ensure_2d=True)\n    check_consistent_length(y_true, y_score, sample_weight)\n    y_type = type_of_target(y_true, input_name=\"y_true\")\n    if y_type != \"multilabel-indicator\":\n        raise ValueError(\"{0} format is not supported\".format(y_type))\n    if y_true.shape != y_score.shape:\n        raise ValueError(\"y_true and y_score have different shape\")\n    y_score_mask = np.ma.masked_array(y_score, mask=np.logical_not(y_true))\n    y_min_relevant = y_score_mask.min(axis=1).reshape((-1, 1))", "documentation": "    \"\"\"Coverage error measure.\n\n    Compute how far we need to go through the ranked scores to cover all\n    true labels. The best value is equal to the average number\n    of labels in ``y_true`` per sample.\n\n    Ties in ``y_scores`` are broken by giving maximal rank that would have\n    been assigned to all tied values.\n\n    Note: Our implementation's score is 1 greater than the one given in\n    Tsoumakas et al., 2010. This extends it to handle the degenerate case\n    in which an instance has 0 true labels.\n\n    Read more in the :ref:`User Guide <coverage_error>`.\n\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples, n_labels)\n        True binary labels in binary indicator format.\n\n    y_score : array-like of shape (n_samples, n_labels)\n        Target scores, can either be probability estimates of the positive\n        class, confidence values, or non-thresholded measure of decisions\n        (as returned by \"decision_function\" on some classifiers).\n        For :term:`decision_function` scores, values greater than or equal to\n        zero should indicate the positive class.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights.\n\n    Returns\n    -------\n    coverage_error : float\n        The coverage error.\n\n    References\n    ----------\n    .. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n           Mining multi-label data. In Data mining and knowledge discovery\n           handbook (pp. 667-685). Springer US.\n\n    Examples\n    --------\n    >>> from sklearn.metrics import coverage_error\n    >>> y_true = [[1, 0, 0], [0, 1, 1]]\n    >>> y_score = [[1, 0, 0], [0, 1, 1]]\n    >>> coverage_error(y_true, y_score)\n    1.5\n    \"\"\""}, {"filename": "sklearn/metrics/_ranking.py", "start_line": 1423, "code": "def label_ranking_loss(y_true, y_score, *, sample_weight=None):\n    y_true = check_array(y_true, ensure_2d=False, accept_sparse=\"csr\")\n    y_score = check_array(y_score, ensure_2d=False)\n    check_consistent_length(y_true, y_score, sample_weight)\n    y_type = type_of_target(y_true, input_name=\"y_true\")\n    if y_type not in (\"multilabel-indicator\",):\n        raise ValueError(\"{0} format is not supported\".format(y_type))\n    if y_true.shape != y_score.shape:\n        raise ValueError(\"y_true and y_score have different shape\")\n    n_samples, n_labels = y_true.shape\n    y_true = csr_matrix(y_true)", "documentation": "    \"\"\"Compute Ranking loss measure.\n\n    Compute the average number of label pairs that are incorrectly ordered\n    given y_score weighted by the size of the label set and the number of\n    labels not in the label set.\n\n    This is similar to the error set size, but weighted by the number of\n    relevant and irrelevant labels. The best performance is achieved with\n    a ranking loss of zero.\n\n    Read more in the :ref:`User Guide <label_ranking_loss>`.\n\n    .. versionadded:: 0.17\n       A function *label_ranking_loss*\n\n    Parameters\n    ----------\n    y_true : {array-like, sparse matrix} of shape (n_samples, n_labels)\n        True binary labels in binary indicator format.\n\n    y_score : array-like of shape (n_samples, n_labels)\n        Target scores, can either be probability estimates of the positive\n        class, confidence values, or non-thresholded measure of decisions\n        (as returned by \"decision_function\" on some classifiers).\n        For :term:`decision_function` scores, values greater than or equal to\n        zero should indicate the positive class.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights.\n\n    Returns\n    -------\n    loss : float\n        Average number of label pairs that are incorrectly ordered given\n        y_score weighted by the size of the label set and the number of labels not\n        in the label set.\n\n    References\n    ----------\n    .. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n           Mining multi-label data. In Data mining and knowledge discovery\n           handbook (pp. 667-685). Springer US.\n\n    Examples\n    --------\n    >>> from sklearn.metrics import label_ranking_loss\n    >>> y_true = [[1, 0, 0], [0, 0, 1]]\n    >>> y_score = [[0.75, 0.5, 1], [1, 0.2, 0.1]]\n    >>> label_ranking_loss(y_true, y_score)\n    0.75\n    \"\"\""}, {"filename": "sklearn/metrics/_ranking.py", "start_line": 1517, "code": "def _dcg_sample_scores(y_true, y_score, k=None, log_base=2, ignore_ties=False):\n    discount = 1 / (np.log(np.arange(y_true.shape[1]) + 2) / np.log(log_base))\n    if k is not None:\n        discount[k:] = 0\n    if ignore_ties:\n        ranking = np.argsort(y_score)[:, ::-1]\n        ranked = y_true[np.arange(ranking.shape[0])[:, np.newaxis], ranking]\n        cumulative_gains = discount.dot(ranked.T)\n    else:\n        discount_cumsum = np.cumsum(discount)\n        cumulative_gains = [", "documentation": "    \"\"\"Compute Discounted Cumulative Gain.\n\n    Sum the true scores ranked in the order induced by the predicted scores,\n    after applying a logarithmic discount.\n\n    This ranking metric yields a high value if true labels are ranked high by\n    ``y_score``.\n\n    Parameters\n    ----------\n    y_true : ndarray of shape (n_samples, n_labels)\n        True targets of multilabel classification, or true scores of entities\n        to be ranked.\n\n    y_score : ndarray of shape (n_samples, n_labels)\n        Target scores, can either be probability estimates, confidence values,\n        or non-thresholded measure of decisions (as returned by\n        \"decision_function\" on some classifiers).\n\n    k : int, default=None\n        Only consider the highest k scores in the ranking. If `None`, use all\n        outputs.\n\n    log_base : float, default=2\n        Base of the logarithm used for the discount. A low value means a\n        sharper discount (top results are more important).\n\n    ignore_ties : bool, default=False\n        Assume that there are no ties in y_score (which is likely to be the\n        case if y_score is continuous) for efficiency gains.\n\n    Returns\n    -------\n    discounted_cumulative_gain : ndarray of shape (n_samples,)\n        The DCG score for each sample.\n\n    See Also\n    --------\n    ndcg_score : The Discounted Cumulative Gain divided by the Ideal Discounted\n        Cumulative Gain (the DCG obtained for a perfect ranking), in order to\n        have a score between 0 and 1.\n    \"\"\""}, {"filename": "sklearn/metrics/_ranking.py", "start_line": 1577, "code": "def _tie_averaged_dcg(y_true, y_score, discount_cumsum):\n    _, inv, counts = np.unique(-y_score, return_inverse=True, return_counts=True)\n    ranked = np.zeros(len(counts))\n    np.add.at(ranked, inv, y_true)\n    ranked /= counts\n    groups = np.cumsum(counts) - 1\n    discount_sums = np.empty(len(counts))\n    discount_sums[0] = discount_cumsum[groups[0]]\n    discount_sums[1:] = np.diff(discount_cumsum[groups])\n    return (ranked * discount_sums).sum()", "documentation": "    \"\"\"\n    Compute DCG by averaging over possible permutations of ties.\n\n    The gain (`y_true`) of an index falling inside a tied group (in the order\n    induced by `y_score`) is replaced by the average gain within this group.\n    The discounted gain for a tied group is then the average `y_true` within\n    this group times the sum of discounts of the corresponding ranks.\n\n    This amounts to averaging scores for all possible orderings of the tied\n    groups.\n\n    (note in the case of dcg@k the discount is 0 after index k)\n\n    Parameters\n    ----------\n    y_true : ndarray\n        The true relevance scores.\n\n    y_score : ndarray\n        Predicted scores.\n\n    discount_cumsum : ndarray\n        Precomputed cumulative sum of the discounts.\n\n    Returns\n    -------\n    discounted_cumulative_gain : float\n        The discounted cumulative gain.\n\n    References\n    ----------\n    McSherry, F., & Najork, M. (2008, March). Computing information retrieval\n    performance measures efficiently in the presence of tied scores. In\n    European conference on information retrieval (pp. 414-421). Springer,\n    Berlin, Heidelberg.\n    \"\"\""}, {"filename": "sklearn/metrics/_ranking.py", "start_line": 1760, "code": "def _ndcg_sample_scores(y_true, y_score, k=None, ignore_ties=False):\n    gain = _dcg_sample_scores(y_true, y_score, k, ignore_ties=ignore_ties)\n    normalizing_gain = _dcg_sample_scores(y_true, y_true, k, ignore_ties=True)\n    all_irrelevant = normalizing_gain == 0\n    gain[all_irrelevant] = 0\n    gain[~all_irrelevant] /= normalizing_gain[~all_irrelevant]\n    return gain\n@validate_params(\n    {\n        \"y_true\": [\"array-like\"],\n        \"y_score\": [\"array-like\"],", "documentation": "    \"\"\"Compute Normalized Discounted Cumulative Gain.\n\n    Sum the true scores ranked in the order induced by the predicted scores,\n    after applying a logarithmic discount. Then divide by the best possible\n    score (Ideal DCG, obtained for a perfect ranking) to obtain a score between\n    0 and 1.\n\n    This ranking metric yields a high value if true labels are ranked high by\n    ``y_score``.\n\n    Parameters\n    ----------\n    y_true : ndarray of shape (n_samples, n_labels)\n        True targets of multilabel classification, or true scores of entities\n        to be ranked.\n\n    y_score : ndarray of shape (n_samples, n_labels)\n        Target scores, can either be probability estimates, confidence values,\n        or non-thresholded measure of decisions (as returned by\n        \"decision_function\" on some classifiers).\n\n    k : int, default=None\n        Only consider the highest k scores in the ranking. If None, use all\n        outputs.\n\n    ignore_ties : bool, default=False\n        Assume that there are no ties in y_score (which is likely to be the\n        case if y_score is continuous) for efficiency gains.\n\n    Returns\n    -------\n    normalized_discounted_cumulative_gain : ndarray of shape (n_samples,)\n        The NDCG score for each sample (float in [0., 1.]).\n\n    See Also\n    --------\n    dcg_score : Discounted Cumulative Gain (not normalized).\n\n    \"\"\""}, {"filename": "sklearn/metrics/_ranking.py", "start_line": 1821, "code": "def ndcg_score(y_true, y_score, *, k=None, sample_weight=None, ignore_ties=False):\n    y_true = check_array(y_true, ensure_2d=False)\n    y_score = check_array(y_score, ensure_2d=False)\n    check_consistent_length(y_true, y_score, sample_weight)\n    if y_true.min() < 0:\n        raise ValueError(\"ndcg_score should not be used on negative y_true values.\")\n    if y_true.ndim > 1 and y_true.shape[1] <= 1:\n        raise ValueError(\n            \"Computing NDCG is only meaningful when there is more than 1 document. \"\n            f\"Got {y_true.shape[1]} instead.\"\n        )", "documentation": "    \"\"\"Compute Normalized Discounted Cumulative Gain.\n\n    Sum the true scores ranked in the order induced by the predicted scores,\n    after applying a logarithmic discount. Then divide by the best possible\n    score (Ideal DCG, obtained for a perfect ranking) to obtain a score between\n    0 and 1.\n\n    This ranking metric returns a high value if true labels are ranked high by\n    ``y_score``.\n\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples, n_labels)\n        True targets of multilabel classification, or true scores of entities\n        to be ranked. Negative values in `y_true` may result in an output\n        that is not between 0 and 1.\n\n    y_score : array-like of shape (n_samples, n_labels)\n        Target scores, can either be probability estimates, confidence values,\n        or non-thresholded measure of decisions (as returned by\n        \"decision_function\" on some classifiers).\n\n    k : int, default=None\n        Only consider the highest k scores in the ranking. If `None`, use all\n        outputs.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights. If `None`, all samples are given the same weight.\n\n    ignore_ties : bool, default=False\n        Assume that there are no ties in y_score (which is likely to be the\n        case if y_score is continuous) for efficiency gains.\n\n    Returns\n    -------\n    normalized_discounted_cumulative_gain : float in [0., 1.]\n        The averaged NDCG scores for all samples.\n\n    See Also\n    --------\n    dcg_score : Discounted Cumulative Gain (not normalized).\n\n    References\n    ----------\n    `Wikipedia entry for Discounted Cumulative Gain\n    <https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_\n\n    Jarvelin, K., & Kekalainen, J. (2002).\n    Cumulated gain-based evaluation of IR techniques. ACM Transactions on\n    Information Systems (TOIS), 20(4), 422-446.\n\n    Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).\n    A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th\n    Annual Conference on Learning Theory (COLT 2013)\n\n    McSherry, F., & Najork, M. (2008, March). Computing information retrieval\n    performance measures efficiently in the presence of tied scores. In\n    European conference on information retrieval (pp. 414-421). Springer,\n    Berlin, Heidelberg.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.metrics import ndcg_score\n    >>> # we have ground-truth relevance of some answers to a query:\n    >>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])\n    >>> # we predict some scores (relevance) for the answers\n    >>> scores = np.asarray([[.1, .2, .3, 4, 70]])\n    >>> ndcg_score(true_relevance, scores)\n    0.69\n    >>> scores = np.asarray([[.05, 1.1, 1., .5, .0]])\n    >>> ndcg_score(true_relevance, scores)\n    0.49\n    >>> # we can set k to truncate the sum; only top k answers contribute.\n    >>> ndcg_score(true_relevance, scores, k=4)\n    0.35\n    >>> # the normalization takes k into account so a perfect answer\n    >>> # would still get 1.0\n    >>> ndcg_score(true_relevance, true_relevance, k=4)\n    1.0...\n    >>> # now we have some ties in our prediction\n    >>> scores = np.asarray([[1, 0, 0, 0, 1]])\n    >>> # by default ties are averaged, so here we get the average (normalized)\n    >>> # true relevance of our top predictions: (10 / 10 + 5 / 10) / 2 = .75\n    >>> ndcg_score(true_relevance, scores, k=1)\n    0.75\n    >>> # we can choose to ignore ties for faster results, but only\n    >>> # if we know there aren't ties in our scores, otherwise we get\n    >>> # wrong results:\n    >>> ndcg_score(true_relevance,\n    ...           scores, k=1, ignore_ties=True)\n    0.5...\n    \"\"\""}]}
{"repository": "pandas-dev/pandas", "commit_sha": "565e195681cdffbdad71d34c3fad6ce2430a6c82", "commit_message": "DOC: Fix docstring of DataFrame.eq ('Equal', not 'Not equal') (#64076)", "commit_date": "2026-02-08T18:02:28+00:00", "author": "Cl\u00e9ment Pit-Claudel", "file": "pandas/core/frame.py", "patch": "@@ -9425,7 +9425,7 @@ def _flex_cmp_method(self, other, op, *, axis: Axis = \"columns\", level=None):\n \n     def eq(self, other, axis: Axis = \"columns\", level=None) -> DataFrame:\n         \"\"\"\n-        Get Not equal to of dataframe and other, element-wise (binary operator `eq`).\n+        Get Equal to of dataframe and other, element-wise (binary operator `eq`).\n \n         Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n         operators.", "before_segments": [{"filename": "pandas/core/frame.py", "start_line": 270, "code": "class DataFrame(NDFrame, OpsMixin):\n    _internal_names_set = {\"columns\", \"index\"} | NDFrame._internal_names_set\n    _typ = \"dataframe\"\n    _HANDLED_TYPES = (Series, Index, ExtensionArray, np.ndarray)\n    _accessors: set[str] = {\"sparse\"}\n    _hidden_attrs: frozenset[str] = NDFrame._hidden_attrs | frozenset([])\n    _mgr: BlockManager\n    __pandas_priority__ = 4000\n    @property", "documentation": "    \"\"\"\n    Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\n    Data structure also contains labeled axes (rows and columns).\n    Arithmetic operations align on both row and column labels. Can be\n    thought of as a dict-like container for Series objects. The primary\n    pandas data structure.\n\n    Parameters\n    ----------\n    data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n        Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n        data is a dict, column order follows insertion-order. If a dict contains Series\n        which have an index defined, it is aligned by its index. This alignment also\n        occurs if data is a Series or a DataFrame itself. Alignment is done on\n        Series/DataFrame inputs.\n\n        If data is a list of dicts, column order follows insertion-order.\n\n    index : Index or array-like\n        Index to use for resulting frame. Will default to RangeIndex if\n        no indexing information part of input data and no index provided.\n    columns : Index or array-like\n        Column labels to use for resulting frame when data does not have them,\n        defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n        will perform column selection instead.\n    dtype : dtype, default None\n        Data type to force. Only a single dtype is allowed. If None, infer.\n        If ``data`` is DataFrame then is ignored.\n    copy : bool or None, default None\n        Copy data from inputs.\n        For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n        or 2d ndarray input, the default of None behaves like ``copy=False``.\n        If data is a dict containing one or more Series (possibly of different dtypes),\n        ``copy=False`` will ensure that these inputs are not copied.\n\n    See Also\n    --------\n    DataFrame.from_records : Constructor from tuples, also record arrays.\n    DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n    read_csv : Read a comma-separated values (csv) file into DataFrame.\n    read_table : Read general delimited file into DataFrame.\n    read_clipboard : Read text from clipboard into DataFrame.\n\n    Notes\n    -----\n    Please reference the :ref:`User Guide <basics.dataframe>` for more information.\n\n    Examples\n    --------\n    Constructing DataFrame from a dictionary.\n\n    >>> d = {\"col1\": [1, 2], \"col2\": [3, 4]}\n    >>> df = pd.DataFrame(data=d)\n    >>> df\n       col1  col2\n    0     1     3\n    1     2     4\n\n    Notice that the inferred dtype is int64.\n\n    >>> df.dtypes\n    col1    int64\n    col2    int64\n    dtype: object\n\n    To enforce a single dtype:\n\n    >>> df = pd.DataFrame(data=d, dtype=np.int8)\n    >>> df.dtypes\n    col1    int8\n    col2    int8\n    dtype: object\n\n    Constructing DataFrame from a dictionary including Series:\n\n    >>> d = {\"col1\": [0, 1, 2, 3], \"col2\": pd.Series([2, 3], index=[2, 3])}\n    >>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n       col1  col2\n    0     0   NaN\n    1     1   NaN\n    2     2   2.0\n    3     3   3.0\n\n    Constructing DataFrame from numpy ndarray:\n\n    >>> df2 = pd.DataFrame(\n    ...     np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=[\"a\", \"b\", \"c\"]\n    ... )\n    >>> df2\n       a  b  c\n    0  1  2  3\n    1  4  5  6\n    2  7  8  9\n\n    Constructing DataFrame from a numpy ndarray that has labeled columns:\n\n    >>> data = np.array(\n    ...     [(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n    ...     dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")],\n    ... )\n    >>> df3 = pd.DataFrame(data, columns=[\"c\", \"a\"])\n    >>> df3\n       c  a\n    0  3  1\n    1  6  4\n    2  9  7\n\n    Constructing DataFrame from dataclass:\n\n    >>> from dataclasses import make_dataclass\n    >>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n    >>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n       x  y\n    0  0  0\n    1  0  3\n    2  2  3\n\n    Constructing DataFrame from Series/DataFrame:\n\n    >>> ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n    >>> df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n    >>> df\n       0\n    a  1\n    c  3\n\n    >>> df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n    >>> df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n    >>> df2\n       x\n    a  1\n    c  3\n    \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 741, "code": "    def __arrow_c_stream__(self, requested_schema=None):\n        pa = import_optional_dependency(\"pyarrow\", min_version=\"14.0.0\")\n        if requested_schema is not None:\n            requested_schema = pa.Schema._import_from_c_capsule(requested_schema)\n        table = pa.Table.from_pandas(self, schema=requested_schema)\n        return table.__arrow_c_stream__()\n    @property", "documentation": "        \"\"\"\n        Export the pandas DataFrame as an Arrow C stream PyCapsule.\n\n        This relies on pyarrow to convert the pandas DataFrame to the Arrow\n        format (and follows the default behaviour of ``pyarrow.Table.from_pandas``\n        in its handling of the index, i.e. store the index as a column except\n        for RangeIndex).\n        This conversion is not necessarily zero-copy.\n\n        Parameters\n        ----------\n        requested_schema : PyCapsule, default None\n            The schema to which the dataframe should be casted, passed as a\n            PyCapsule containing a C ArrowSchema representation of the\n            requested schema.\n\n        Returns\n        -------\n        PyCapsule\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 771, "code": "    def axes(self) -> list[Index]:\n        return [self.index, self.columns]\n    @property", "documentation": "        \"\"\"\n        Return a list representing the axes of the DataFrame.\n\n        It has the row axis labels and column axis labels as the only members.\n        They are returned in that order.\n\n        See Also\n        --------\n        DataFrame.index: The index (row labels) of the DataFrame.\n        DataFrame.columns: The column labels of the DataFrame.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"col1\": [1, 2], \"col2\": [3, 4]})\n        >>> df.axes\n        [RangeIndex(start=0, stop=2, step=1), Index(['col1', 'col2'], dtype='str')]\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 792, "code": "    def shape(self) -> tuple[int, int]:\n        return len(self.index), len(self.columns)\n    @property", "documentation": "        \"\"\"\n        Return a tuple representing the dimensionality of the DataFrame.\n\n        Unlike the `len()` method, which only returns the number of rows, `shape`\n        provides both row and column counts, making it a more informative method for\n        understanding dataset size.\n\n        See Also\n        --------\n        numpy.ndarray.shape : Tuple of array dimensions.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"col1\": [1, 2], \"col2\": [3, 4]})\n        >>> df.shape\n        (2, 2)\n\n        >>> df = pd.DataFrame({\"col1\": [1, 2], \"col2\": [3, 4], \"col3\": [5, 6]})\n        >>> df.shape\n        (2, 3)\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 817, "code": "    def _is_homogeneous_type(self) -> bool:\n        return len({block.values.dtype for block in self._mgr.blocks}) <= 1\n    @property", "documentation": "        \"\"\"\n        Whether all the columns in a DataFrame have the same type.\n\n        Returns\n        -------\n        bool\n\n        Examples\n        --------\n        >>> DataFrame({\"A\": [1, 2], \"B\": [3, 4]})._is_homogeneous_type\n        True\n        >>> DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.0]})._is_homogeneous_type\n        False\n\n        Items with the same type but different sizes are considered\n        different types.\n\n        >>> DataFrame(\n        ...     {\n        ...         \"A\": np.array([1, 2], dtype=np.int32),\n        ...         \"B\": np.array([1, 2], dtype=np.int64),\n        ...     }\n        ... )._is_homogeneous_type\n        False\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 847, "code": "    def _can_fast_transpose(self) -> bool:\n        blocks = self._mgr.blocks\n        if len(blocks) != 1:\n            return False\n        dtype = blocks[0].dtype\n        return not is_1d_only_ea_dtype(dtype)\n    @property", "documentation": "        \"\"\"\n        Can we transpose this DataFrame without creating any new array objects.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 860, "code": "    def _values(self) -> np.ndarray | DatetimeArray | TimedeltaArray | PeriodArray:\n        mgr = self._mgr\n        blocks = mgr.blocks\n        if len(blocks) != 1:\n            return ensure_wrapped_if_datetimelike(self.values)\n        arr = blocks[0].values\n        if arr.ndim == 1:\n            return self.values\n        arr = cast(\"np.ndarray | DatetimeArray | TimedeltaArray | PeriodArray\", arr)\n        return arr.T", "documentation": "        \"\"\"\n        Analogue to ._values that may return a 2D ExtensionArray.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 882, "code": "    def _repr_fits_vertical_(self) -> bool:\n        max_rows = get_option(\"display.max_rows\")\n        return len(self) <= max_rows", "documentation": "        \"\"\"\n        Check length against max_rows.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 889, "code": "    def _repr_fits_horizontal_(self) -> bool:\n        width, height = console.get_console_size()\n        max_columns = get_option(\"display.max_columns\")\n        nb_columns = len(self.columns)\n        if (max_columns and nb_columns > max_columns) or (\n            width and nb_columns > (width // 2)\n        ):\n            return False\n        if width is None or not console.in_interactive_session():\n            return True\n        if get_option(\"display.width\") is not None or console.in_ipython_frontend():", "documentation": "        \"\"\"\n        Check if full repr fits in horizontal boundaries imposed by the display\n        options width and max_columns.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 936, "code": "    def _info_repr(self) -> bool:\n        info_repr_option = get_option(\"display.large_repr\") == \"info\"\n        return info_repr_option and not (\n            self._repr_fits_horizontal_() and self._repr_fits_vertical_()\n        )", "documentation": "        \"\"\"\n        True if the repr should show the info view.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 945, "code": "    def __repr__(self) -> str:\n        if self._info_repr():\n            buf = StringIO()\n            self.info(buf=buf)\n            return buf.getvalue()\n        repr_params = fmt.get_dataframe_repr_params()\n        return self.to_string(**repr_params)", "documentation": "        \"\"\"\n        Return a string representation for a particular DataFrame.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 957, "code": "    def _repr_html_(self) -> str | None:\n        if self._info_repr():\n            buf = StringIO()\n            self.info(buf=buf)\n            val = buf.getvalue().replace(\"<\", r\"&lt;\", 1)\n            val = val.replace(\">\", r\"&gt;\", 1)\n            return f\"<pre>{val}</pre>\"\n        if get_option(\"display.notebook_repr_html\"):\n            max_rows = get_option(\"display.max_rows\")\n            min_rows = get_option(\"display.min_rows\")\n            max_cols = get_option(\"display.max_columns\")", "documentation": "        \"\"\"\n        Return a html representation for a particular DataFrame.\n\n        Mainly for IPython notebook.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 1216, "code": "    def style(self) -> Styler:\n        has_jinja2 = import_optional_dependency(\"jinja2\", errors=\"ignore\")\n        if not has_jinja2:\n            raise AttributeError(\"The '.style' accessor requires jinja2\")\n        from pandas.io.formats.style import Styler\n        return Styler(self)\n    _shared_docs[\"items\"] = r\"\"\"\n        Iterate over (column name, Series) pairs.\n        Iterates over the DataFrame columns, returning a tuple with\n        the column name and the content as a Series.\n        Yields", "documentation": "        \"\"\"\n        Returns a Styler object.\n\n        Contains methods for building a styled HTML representation of the DataFrame.\n\n        See Also\n        --------\n        io.formats.style.Styler : Helps style a DataFrame or Series according to the\n            data with HTML and CSS.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"A\": [1, 2, 3]})\n        >>> df.style  # doctest: +SKIP\n\n        Please see\n        `Table Visualization <../../user_guide/style.ipynb>`_ for more examples.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 1346, "code": "    def iterrows(self) -> Iterable[tuple[Hashable, Series]]:\n        columns = self.columns\n        klass = self._constructor_sliced\n        for k, v in zip(self.index, self.values, strict=True):\n            s = klass(v, index=columns, name=k).__finalize__(self)\n            if self._mgr.is_single_block:\n                s._mgr.add_references(self._mgr)\n            yield k, s", "documentation": "        \"\"\"\n        Iterate over DataFrame rows as (index, Series) pairs.\n\n        Yields\n        ------\n        index : label or tuple of label\n            The index of the row. A tuple for a `MultiIndex`.\n        data : Series\n            The data of the row as a Series.\n\n        See Also\n        --------\n        DataFrame.itertuples : Iterate over DataFrame rows as namedtuples of the values.\n        DataFrame.items : Iterate over (column name, Series) pairs.\n\n        Notes\n        -----\n        1. Because ``iterrows`` returns a Series for each row,\n           it does **not** preserve dtypes across the rows (dtypes are\n           preserved across columns for DataFrames).\n\n           To preserve dtypes while iterating over the rows, it is better\n           to use :meth:`itertuples` which returns namedtuples of the values\n           and which is generally faster than ``iterrows``.\n\n        2. You should **never modify** something you are iterating over.\n           This is not guaranteed to work in all cases. Depending on the\n           data types, the iterator returns a copy and not a view, and writing\n           to it will have no effect.\n\n        Examples\n        --------\n\n        >>> df = pd.DataFrame([[1, 1.5]], columns=[\"int\", \"float\"])\n        >>> row = next(df.iterrows())[1]\n        >>> row\n        int      1.0\n        float    1.5\n        Name: 0, dtype: float64\n        >>> print(row[\"int\"].dtype)\n        float64\n        >>> print(df[\"int\"].dtype)\n        int64\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 1481, "code": "    def __len__(self) -> int:\n        return len(self.index)\n    @overload", "documentation": "        \"\"\"\n        Returns length of info axis, but here we use the index.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 1493, "code": "    def dot(self, other: AnyArrayLike | DataFrame) -> DataFrame | Series:\n        if isinstance(other, (Series, DataFrame)):\n            common = self.columns.union(other.index)\n            if len(common) > len(self.columns) or len(common) > len(other.index):\n                raise ValueError(\"matrices are not aligned\")\n            left = self.reindex(columns=common)\n            right = other.reindex(index=common)\n            lvals = left.values\n            rvals = right._values\n        else:\n            left = self", "documentation": "        \"\"\"\n        Compute the matrix multiplication between the DataFrame and other.\n\n        This method computes the matrix product between the DataFrame and the\n        values of an other Series, DataFrame or a numpy array.\n\n        It can also be called using ``self @ other``.\n\n        Parameters\n        ----------\n        other : Series, DataFrame or array-like\n            The other object to compute the matrix product with.\n\n        Returns\n        -------\n        Series or DataFrame\n            If other is a Series, return the matrix product between self and\n            other as a Series. If other is a DataFrame or a numpy.array, return\n            the matrix product of self and other in a DataFrame of a np.array.\n\n        See Also\n        --------\n        Series.dot: Similar method for Series.\n\n        Notes\n        -----\n        The dimensions of DataFrame and other must be compatible in order to\n        compute the matrix multiplication. In addition, the column names of\n        DataFrame and the index of other must contain the same values, as they\n        will be aligned prior to the multiplication.\n\n        The dot method for Series computes the inner product, instead of the\n        matrix product here.\n\n        Examples\n        --------\n        Here we multiply a DataFrame with a Series.\n\n        >>> df = pd.DataFrame([[0, 1, -2, -1], [1, 1, 1, 1]])\n        >>> s = pd.Series([1, 1, 2, 1])\n        >>> df.dot(s)\n        0    -4\n        1     5\n        dtype: int64\n\n        Here we multiply a DataFrame with another DataFrame.\n\n        >>> other = pd.DataFrame([[0, 1], [1, 2], [-1, -1], [2, 0]])\n        >>> df.dot(other)\n            0   1\n        0   1   4\n        1   2   2\n\n        Note that the dot method give the same result as @\n\n        >>> df @ other\n            0   1\n        0   1   4\n        1   2   2\n\n        The dot method works also if other is an np.array.\n\n        >>> arr = np.array([[0, 1], [1, 2], [-1, -1], [2, 0]])\n        >>> df.dot(arr)\n            0   1\n        0   1   4\n        1   2   2\n\n        Note how shuffling of the objects does not change the result.\n\n        >>> s2 = s.reindex([1, 0, 2, 3])\n        >>> df.dot(s2)\n        0    -4\n        1     5\n        dtype: int64\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 1617, "code": "    def __matmul__(self, other: AnyArrayLike | DataFrame) -> DataFrame | Series:\n        return self.dot(other)", "documentation": "        \"\"\"\n        Matrix multiplication using binary `@` operator.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 1623, "code": "    def __rmatmul__(self, other) -> DataFrame:\n        try:\n            return self.T.dot(np.transpose(other)).T\n        except ValueError as err:\n            if \"shape mismatch\" not in str(err):\n                raise\n            msg = f\"shapes {np.shape(other)} and {self.shape} not aligned\"\n            raise ValueError(msg) from err\n    @classmethod", "documentation": "        \"\"\"\n        Matrix multiplication using binary `@` operator.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 2641, "code": "    def to_feather(self, path: FilePath | WriteBuffer[bytes], **kwargs) -> None:\n        from pandas.io.feather_format import to_feather\n        to_feather(self, path, **kwargs)\n    @overload", "documentation": "        \"\"\"\n        Write a DataFrame to the binary Feather format.\n\n        Parameters\n        ----------\n        path : str, path object, file-like object\n            String, path object (implementing ``os.PathLike[str]``), or file-like\n            object implementing a binary ``write()`` function. If a string or a path,\n            it will be used as Root Directory path when writing a partitioned dataset.\n        **kwargs :\n            Additional keywords passed to :func:`pyarrow.feather.write_feather`.\n            This includes the `compression`, `compression_level`, `chunksize`\n            and `version` keywords.\n\n        See Also\n        --------\n        DataFrame.to_parquet : Write a DataFrame to the binary parquet format.\n        DataFrame.to_excel : Write object to an Excel sheet.\n        DataFrame.to_sql : Write to a sql table.\n        DataFrame.to_csv : Write a csv file.\n        DataFrame.to_json : Convert the object to a JSON string.\n        DataFrame.to_html : Render a DataFrame as an HTML table.\n        DataFrame.to_string : Convert DataFrame to a string.\n\n        Notes\n        -----\n        This function writes the dataframe as a `feather file\n        <https://arrow.apache.org/docs/python/feather.html>`_. Requires a default\n        index. For saving the DataFrame with your custom index use a method that\n        supports custom indices e.g. `to_parquet`.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6]])\n        >>> df.to_feather(\"file.feather\")  # doctest: +SKIP\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 3836, "code": "    def memory_usage(self, index: bool = True, deep: bool = False) -> Series:\n        result = self._constructor_sliced(\n            [c.memory_usage(index=False, deep=deep) for col, c in self.items()],\n            index=self.columns,\n            dtype=np.intp,\n        )\n        if index:\n            index_memory_usage = self._constructor_sliced(\n                self.index.memory_usage(deep=deep), index=[\"Index\"]\n            )\n            result = index_memory_usage._append_internal(result)", "documentation": "        \"\"\"\n        Return the memory usage of each column in bytes.\n\n        The memory usage can optionally include the contribution of\n        the index and elements of `object` dtype.\n\n        This value is displayed in `DataFrame.info` by default. This can be\n        suppressed by setting ``pandas.options.display.memory_usage`` to False.\n\n        Parameters\n        ----------\n        index : bool, default True\n            Specifies whether to include the memory usage of the DataFrame's\n            index in returned Series. If ``index=True``, the memory usage of\n            the index is the first item in the output.\n        deep : bool, default False\n            If True, introspect the data deeply by interrogating\n            `object` dtypes for system-level memory consumption, and include\n            it in the returned values.\n\n        Returns\n        -------\n        Series\n            A Series whose index is the original column names and whose values\n            is the memory usage of each column in bytes.\n\n        See Also\n        --------\n        numpy.ndarray.nbytes : Total bytes consumed by the elements of an\n            ndarray.\n        Series.memory_usage : Bytes consumed by a Series.\n        Categorical : Memory-efficient array for string values with\n            many repeated values.\n        DataFrame.info : Concise summary of a DataFrame.\n\n        Notes\n        -----\n        See the :ref:`Frequently Asked Questions <df-memory-usage>` for more\n        details.\n\n        Examples\n        --------\n        >>> dtypes = [\"int64\", \"float64\", \"complex128\", \"object\", \"bool\"]\n        >>> data = dict([(t, np.ones(shape=5000, dtype=int).astype(t)) for t in dtypes])\n        >>> df = pd.DataFrame(data)\n        >>> df.head()\n           int64  float64            complex128  object  bool\n        0      1      1.0              1.0+0.0j       1  True\n        1      1      1.0              1.0+0.0j       1  True\n        2      1      1.0              1.0+0.0j       1  True\n        3      1      1.0              1.0+0.0j       1  True\n        4      1      1.0              1.0+0.0j       1  True\n\n        >>> df.memory_usage()\n        Index           132\n        int64         40000\n        float64       40000\n        complex128    80000\n        object        40000\n        bool           5000\n        dtype: int64\n\n        >>> df.memory_usage(index=False)\n        int64         40000\n        float64       40000\n        complex128    80000\n        object        40000\n        bool           5000\n        dtype: int64\n\n        The memory footprint of `object` dtype columns is ignored by default:\n\n        >>> df.memory_usage(deep=True)\n        Index            132\n        int64          40000\n        float64        40000\n        complex128     80000\n        object        180000\n        bool            5000\n        dtype: int64\n\n        Use a Categorical for efficient storage of an object-dtype column with\n        many repeated values.\n\n        >>> df[\"object\"].astype(\"category\").memory_usage(deep=True)\n        5140\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 4122, "code": "    def T(self) -> DataFrame:\n        return self.transpose()", "documentation": "        \"\"\"\n        The transpose of the DataFrame.\n\n        This property returns a DataFrame with rows and columns interchanged,\n        reflecting the data across the main diagonal.\n\n        Returns\n        -------\n        DataFrame\n            The transposed DataFrame.\n\n        See Also\n        --------\n        DataFrame.transpose : Transpose index and columns.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"col1\": [1, 2], \"col2\": [3, 4]})\n        >>> df\n           col1  col2\n        0     1     3\n        1     2     4\n\n        >>> df.T\n              0  1\n        col1  1  2\n        col2  3  4\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 4156, "code": "    def _ixs(self, i: int, axis: AxisInt = 0) -> Series:\n        if axis == 0:\n            new_mgr = self._mgr.fast_xs(i)\n            result = self._constructor_sliced_from_mgr(new_mgr, axes=new_mgr.axes)\n            result._name = self.index[i]\n            return result.__finalize__(self)\n        else:\n            col_mgr = self._mgr.iget(i)\n            return self._box_col_values(col_mgr, i)", "documentation": "        \"\"\"\n        Parameters\n        ----------\n        i : int\n        axis : int\n\n        Returns\n        -------\n        Series\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 4180, "code": "    def _get_column_array(self, i: int) -> ArrayLike:\n        return self._mgr.iget_values(i)", "documentation": "        \"\"\"\n        Get the values of the i'th column (ndarray or ExtensionArray, as stored\n        in the Block)\n\n        Warning! The returned array is a view but doesn't handle Copy-on-Write,\n        so this should be used with caution (for read-only purposes).\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 4190, "code": "    def _iter_column_arrays(self) -> Iterator[ArrayLike]:\n        for i in range(len(self.columns)):\n            yield self._get_column_array(i)", "documentation": "        \"\"\"\n        Iterate over the arrays of all columns in order.\n        This returns the values as stored in the Block (ndarray or ExtensionArray).\n\n        Warning! The returned array is a view but doesn't handle Copy-on-Write,\n        so this should be used with caution (for read-only purposes).\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 4330, "code": "    def _get_value(self, index, col, takeable: bool = False) -> Scalar:\n        if takeable:\n            series = self._ixs(col, axis=1)\n            return series._values[index]\n        series = self._get_item(col)\n        if not isinstance(self.index, MultiIndex):\n            row = self.index.get_loc(index)\n            return series._values[row]\n        loc = self.index._engine.get_loc(index)\n        return series._values[loc]", "documentation": "        \"\"\"\n        Quickly retrieve single value at passed column and index.\n\n        Parameters\n        ----------\n        index : row label\n        col : column label\n        takeable : interpret the index/col as indexers, default False\n\n        Returns\n        -------\n        scalar\n\n        Notes\n        -----\n        Assumes that both `self.index._index_as_unique` and\n        `self.columns._index_as_unique`; Caller is responsible for checking.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 4367, "code": "    def isetitem(self, loc, value) -> None:\n        if isinstance(value, DataFrame):\n            if is_integer(loc):\n                loc = [loc]\n            if len(loc) != len(value.columns):\n                raise ValueError(\n                    f\"Got {len(loc)} positions but value has {len(value.columns)} \"\n                    f\"columns.\"\n                )\n            for i, idx in enumerate(loc):\n                arraylike, refs = self._sanitize_column(value.iloc[:, i])", "documentation": "        \"\"\"\n        Set the given value in the column with position `loc`.\n\n        This is a positional analogue to ``__setitem__``.\n\n        Parameters\n        ----------\n        loc : int or sequence of ints\n            Index position for the column.\n        value : scalar or arraylike\n            Value(s) for the column.\n\n        See Also\n        --------\n        DataFrame.iloc : Purely integer-location based indexing for selection by\n            position.\n\n        Notes\n        -----\n        ``frame.isetitem(loc, value)`` is an in-place method as it will\n        modify the DataFrame in place (not returning a new object). In contrast to\n        ``frame.iloc[:, i] = value`` which will try to update the existing values in\n        place, ``frame.isetitem(loc, value)`` will not update the values of the column\n        itself in place, it will instead insert a new array.\n\n        In cases where ``frame.columns`` is unique, this is equivalent to\n        ``frame[frame.columns[i]] = value``.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n        >>> df.isetitem(1, [5, 6])\n        >>> df\n              A  B\n        0     1  5\n        1     2  6\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 4423, "code": "    def __setitem__(self, key, value) -> None:\n        if not CHAINED_WARNING_DISABLED:\n            if sys.getrefcount(self) <= REF_COUNT and not com.is_local_in_caller_frame(\n                self\n            ):\n                warnings.warn(\n                    _chained_assignment_msg, ChainedAssignmentError, stacklevel=2\n                )\n        key = com.apply_if_callable(key, self)\n        if isinstance(key, slice):\n            slc = self.index._convert_slice_indexer(key, kind=\"getitem\")", "documentation": "        \"\"\"\n        Set item(s) in DataFrame by key.\n\n        This method allows you to set the values of one or more columns in the\n        DataFrame using a key. If the key does not exist, a new\n        column will be created.\n\n        Parameters\n        ----------\n        key : The object(s) in the index which are to be assigned to\n            Column label(s) to set. Can be a single column name, list of column names,\n            or tuple for MultiIndex columns.\n        value : scalar, array-like, Series, or DataFrame\n            Value(s) to set for the specified key(s).\n\n        Returns\n        -------\n        None\n            This method does not return a value.\n\n        See Also\n        --------\n        DataFrame.loc : Access and set values by label-based indexing.\n        DataFrame.iloc : Access and set values by position-based indexing.\n        DataFrame.assign : Assign new columns to a DataFrame.\n\n        Notes\n        -----\n        When assigning a Series to a DataFrame column, pandas aligns the Series\n        by index labels, not by position. This means:\n\n        * Values from the Series are matched to DataFrame rows by index label\n        * If a Series index label doesn't exist in the DataFrame index, it's ignored\n        * If a DataFrame index label doesn't exist in the Series index, NaN is assigned\n        * The order of values in the Series doesn't matter; only the index labels matter\n\n        Examples\n        --------\n        Basic column assignment:\n\n        >>> df = pd.DataFrame({\"A\": [1, 2, 3]})\n        >>> df[\"B\"] = [4, 5, 6]  # Assigns by position\n        >>> df\n            A  B\n        0  1  4\n        1  2  5\n        2  3  6\n\n        Series assignment with index alignment:\n\n        >>> df = pd.DataFrame({\"A\": [1, 2, 3]}, index=[0, 1, 2])\n        >>> s = pd.Series([10, 20], index=[1, 3])  # Note: index 3 doesn't exist in df\n        >>> df[\"B\"] = s  # Assigns by index label, not position\n        >>> df\n           A     B\n        0  1   NaN\n        1  2  10.0\n        2  3   NaN\n\n        Series assignment with partial index match:\n\n        >>> df = pd.DataFrame({\"A\": [1, 2, 3, 4]}, index=[\"a\", \"b\", \"c\", \"d\"])\n        >>> s = pd.Series([100, 200], index=[\"b\", \"d\"])\n        >>> df[\"B\"] = s\n        >>> df\n           A      B\n        a  1    NaN\n        b  2  100.0\n        c  3    NaN\n        d  4  200.0\n\n        Series index labels NOT in DataFrame, ignored:\n\n        >>> df = pd.DataFrame({\"A\": [1, 2, 3]}, index=[\"x\", \"y\", \"z\"])\n        >>> s = pd.Series([10, 20, 30, 40, 50], index=[\"x\", \"y\", \"a\", \"b\", \"z\"])\n        >>> df[\"B\"] = s\n        >>> df\n           A   B\n        x  1  10\n        y  2  20\n        z  3  50\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 4726, "code": "    def _set_item(self, key, value) -> None:\n        value, refs = self._sanitize_column(value)\n        if (\n            key in self.columns\n            and value.ndim == 1\n            and not isinstance(value.dtype, ExtensionDtype)\n        ):\n            if not self.columns.is_unique or isinstance(self.columns, MultiIndex):\n                existing_piece = self[key]\n                if isinstance(existing_piece, DataFrame):\n                    value = np.tile(value, (len(existing_piece.columns), 1)).T", "documentation": "        \"\"\"\n        Add series to DataFrame in specified column.\n\n        If series is a numpy-array (not a Series/TimeSeries), it must be the\n        same length as the DataFrames index or an error will be thrown.\n\n        Series/TimeSeries will be conformed to the DataFrames index to\n        ensure homogeneity.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 4795, "code": "    def _ensure_valid_index(self, value) -> None:\n        if not len(self.index) and is_list_like(value) and len(value):\n            if not isinstance(value, DataFrame):\n                try:\n                    value = Series(value)\n                except (ValueError, NotImplementedError, TypeError) as err:\n                    raise ValueError(\n                        \"Cannot set a frame with no defined index \"\n                        \"and a value that cannot be converted to a Series\"\n                    ) from err\n            index_copy = value.index.copy()", "documentation": "        \"\"\"\n        Ensure that if we don't have an index, that we can create one from the\n        passed value.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 4818, "code": "    def _box_col_values(self, values: SingleBlockManager, loc: int) -> Series:\n        name = self.columns[loc]\n        obj = self._constructor_sliced_from_mgr(values, axes=values.axes)\n        obj._name = name\n        return obj.__finalize__(self)", "documentation": "        \"\"\"\n        Provide boxed values for a column.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 5087, "code": "    def eval(self, expr: str, *, inplace: bool = False, **kwargs) -> Any | None:\n        from pandas.core.computation.eval import eval as _eval\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        kwargs[\"level\"] = kwargs.pop(\"level\", 0) + 1\n        index_resolvers = self._get_index_resolvers()\n        column_resolvers = self._get_cleaned_column_resolvers()\n        resolvers = column_resolvers, index_resolvers\n        if \"target\" not in kwargs:\n            kwargs[\"target\"] = self\n        kwargs[\"resolvers\"] = tuple(kwargs.get(\"resolvers\", ())) + resolvers\n        return _eval(expr, inplace=inplace, **kwargs)", "documentation": "        \"\"\"\n        Evaluate a string describing operations on DataFrame columns.\n\n        .. warning::\n\n            This method can run arbitrary code which can make you vulnerable to code\n            injection if you pass user input to this function.\n\n        Operates on columns only, not specific rows or elements.  This allows\n        `eval` to run arbitrary code, which can make you vulnerable to code\n        injection if you pass user input to this function.\n\n        Parameters\n        ----------\n        expr : str\n            The expression string to evaluate.\n\n            You can refer to variables\n            in the environment by prefixing them with an '@' character like\n            ``@a + b``.\n\n            You can refer to column names that are not valid Python variable names\n            by surrounding them in backticks. Thus, column names containing spaces\n            or punctuation (besides underscores) or starting with digits must be\n            surrounded by backticks. (For example, a column named \"Area (cm^2)\" would\n            be referenced as ```Area (cm^2)```). Column names which are Python keywords\n            (like \"if\", \"for\", \"import\", etc) cannot be used.\n\n            For example, if one of your columns is called ``a a`` and you want\n            to sum it with ``b``, your query should be ```a a` + b``.\n\n            See the documentation for :func:`eval` for full details of\n            supported operations and functions in the expression string.\n        inplace : bool, default False\n            If the expression contains an assignment, whether to perform the\n            operation inplace and mutate the existing DataFrame. Otherwise,\n            a new DataFrame is returned.\n        **kwargs\n            See the documentation for :func:`eval` for complete details\n            on the keyword arguments accepted by\n            :meth:`~pandas.DataFrame.eval`.\n\n        Returns\n        -------\n        ndarray, scalar, pandas object, or None\n            The result of the evaluation or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.query : Evaluates a boolean expression to query the columns\n            of a frame.\n        DataFrame.assign : Can evaluate an expression or function to create new\n            values for a column.\n        eval : Evaluate a Python expression as a string using various\n            backends.\n\n        Notes\n        -----\n        For more details see the API documentation for :func:`~eval`.\n        For detailed examples see :ref:`enhancing performance with eval\n        <enhancingperf.eval>`.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\"A\": range(1, 6), \"B\": range(10, 0, -2), \"C&C\": range(10, 5, -1)}\n        ... )\n        >>> df\n           A   B  C&C\n        0  1  10   10\n        1  2   8    9\n        2  3   6    8\n        3  4   4    7\n        4  5   2    6\n        >>> df.eval(\"A + B\")\n        0    11\n        1    10\n        2     9\n        3     8\n        4     7\n        dtype: int64\n\n        Assignment is allowed though by default the original DataFrame is not\n        modified.\n\n        >>> df.eval(\"D = A + B\")\n           A   B  C&C   D\n        0  1  10   10  11\n        1  2   8    9  10\n        2  3   6    8   9\n        3  4   4    7   8\n        4  5   2    6   7\n        >>> df\n           A   B  C&C\n        0  1  10   10\n        1  2   8    9\n        2  3   6    8\n        3  4   4    7\n        4  5   2    6\n\n        Multiple columns can be assigned to using multi-line expressions:\n\n        >>> df.eval(\n        ...     '''\n        ... D = A + B\n        ... E = A - B\n        ... '''\n        ... )\n           A   B  C&C   D  E\n        0  1  10   10  11 -9\n        1  2   8    9  10 -6\n        2  3   6    8   9 -3\n        3  4   4    7   8  0\n        4  5   2    6   7  3\n\n        For columns with spaces or other disallowed characters in their name, you can\n        use backtick quoting.\n\n        >>> df.eval(\"B * `C&C`\")\n        0    100\n        1     72\n        2     48\n        3     28\n        4     12\n        dtype: int64\n\n        Local variables shall be explicitly referenced using ``@``\n        character in front of the name:\n\n        >>> local_var = 2\n        >>> df.eval(\"@local_var * A\")\n        0     2\n        1     4\n        2     6\n        3     8\n        4    10\n        Name: A, dtype: int64\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 5239, "code": "    def select_dtypes(self, include=None, exclude=None) -> DataFrame:\n        if not is_list_like(include):\n            include = (include,) if include is not None else ()\n        if not is_list_like(exclude):\n            exclude = (exclude,) if exclude is not None else ()\n        selection = (frozenset(include), frozenset(exclude))\n        if not any(selection):\n            raise ValueError(\"at least one of include or exclude must be nonempty\")", "documentation": "        \"\"\"\n        Return a subset of the DataFrame's columns based on the column dtypes.\n\n        This method allows for filtering columns based on their data types.\n        It is useful when working with heterogeneous DataFrames where operations\n        need to be performed on a specific subset of data types.\n\n        Parameters\n        ----------\n        include, exclude : scalar or list-like\n            A selection of dtypes or strings to be included/excluded. At least\n            one of these parameters must be supplied.\n\n        Returns\n        -------\n        DataFrame\n            The subset of the frame including the dtypes in ``include`` and\n            excluding the dtypes in ``exclude``.\n\n        Raises\n        ------\n        ValueError\n            * If both of ``include`` and ``exclude`` are empty\n            * If ``include`` and ``exclude`` have overlapping elements\n        TypeError\n            * If any kind of string dtype is passed in.\n\n        See Also\n        --------\n        DataFrame.dtypes: Return Series with the data type of each column.\n\n        Notes\n        -----\n        * To select all *numeric* types, use ``np.number`` or ``'number'``\n        * To select strings you must use the ``object`` dtype, but note that\n          this will return *all* object dtype columns. With\n          ``pd.options.future.infer_string`` enabled, using ``\"str\"`` will\n          work to select all string columns.\n        * See the `numpy dtype hierarchy\n          <https://numpy.org/doc/stable/reference/arrays.scalars.html>`__\n        * To select datetimes, use ``np.datetime64``, ``'datetime'`` or\n          ``'datetime64'``\n        * To select timedeltas, use ``np.timedelta64``, ``'timedelta'`` or\n          ``'timedelta64'``\n        * To select Pandas categorical dtypes, use ``'category'``\n        * To select Pandas datetimetz dtypes, use ``'datetimetz'``\n          or ``'datetime64[ns, tz]'``\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\"a\": [1, 2] * 3, \"b\": [True, False] * 3, \"c\": [1.0, 2.0] * 3}\n        ... )\n        >>> df\n                a      b  c\n        0       1   True  1.0\n        1       2  False  2.0\n        2       1   True  1.0\n        3       2  False  2.0\n        4       1   True  1.0\n        5       2  False  2.0\n\n        >>> df.select_dtypes(include=\"bool\")\n           b\n        0  True\n        1  False\n        2  True\n        3  False\n        4  True\n        5  False\n\n        >>> df.select_dtypes(include=[\"float64\"])\n           c\n        0  1.0\n        1  2.0\n        2  1.0\n        3  2.0\n        4  1.0\n        5  2.0\n\n        >>> df.select_dtypes(exclude=[\"int64\"])\n               b    c\n        0   True  1.0\n        1  False  2.0\n        2   True  1.0\n        3  False  2.0\n        4   True  1.0\n        5  False  2.0\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 5582, "code": "    def _sanitize_column(self, value) -> tuple[ArrayLike, BlockValuesRefs | None]:\n        self._ensure_valid_index(value)\n        assert not isinstance(value, DataFrame)\n        if is_dict_like(value):\n            if not isinstance(value, Series):\n                value = Series(value)\n            return _reindex_for_setitem(value, self.index)\n        if is_list_like(value):\n            com.require_length_match(value, self.index)\n        return sanitize_array(value, self.index, copy=True, allow_2d=True), None\n    @property", "documentation": "        \"\"\"\n        Ensures new columns (which go into the BlockManager as new blocks) are\n        always copied (or a reference is being tracked to them under CoW)\n        and converted into an array.\n\n        Parameters\n        ----------\n        value : scalar, Series, or array-like\n\n        Returns\n        -------\n        tuple of numpy.ndarray or ExtensionArray and optional BlockValuesRefs\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 5616, "code": "    def _reindex_multi(self, axes: dict[str, Index], fill_value) -> DataFrame:\n        new_index, row_indexer = self.index.reindex(axes[\"index\"])\n        new_columns, col_indexer = self.columns.reindex(axes[\"columns\"])\n        if row_indexer is not None and col_indexer is not None:\n            indexer = row_indexer, col_indexer\n            new_values = take_2d_multi(self.values, indexer, fill_value=fill_value)\n            return self._constructor(\n                new_values, index=new_index, columns=new_columns, copy=False\n            )\n        else:\n            return self._reindex_with_indexers(", "documentation": "        \"\"\"\n        We are guaranteed non-Nones in the axes.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 6345, "code": "    def pop(self, item: Hashable) -> Series:\n        return super().pop(item=item)", "documentation": "        \"\"\"\n        Return item and drop it from DataFrame. Raise KeyError if not found.\n\n        Parameters\n        ----------\n        item : label\n            Label of column to be popped.\n\n        Returns\n        -------\n        Series\n            Series representing the item that is dropped.\n\n        See Also\n        --------\n        DataFrame.drop: Drop specified labels from rows or columns.\n        DataFrame.drop_duplicates: Return DataFrame with duplicate rows removed.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     [\n        ...         (\"falcon\", \"bird\", 389.0),\n        ...         (\"parrot\", \"bird\", 24.0),\n        ...         (\"lion\", \"mammal\", 80.5),\n        ...         (\"monkey\", \"mammal\", np.nan),\n        ...     ],\n        ...     columns=(\"name\", \"class\", \"max_speed\"),\n        ... )\n        >>> df\n             name   class  max_speed\n        0  falcon    bird      389.0\n        1  parrot    bird       24.0\n        2    lion  mammal       80.5\n        3  monkey  mammal        NaN\n\n        >>> df.pop(\"class\")\n        0      bird\n        1      bird\n        2    mammal\n        3    mammal\n        Name: class, dtype: str\n\n        >>> df\n             name  max_speed\n        0  falcon      389.0\n        1  parrot       24.0\n        2    lion       80.5\n        3  monkey        NaN\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 7204, "code": "    def isna(self) -> DataFrame:\n        res_mgr = self._mgr.isna(func=isna)\n        result = self._constructor_from_mgr(res_mgr, axes=res_mgr.axes)\n        return result.__finalize__(self, method=\"isna\")", "documentation": "        \"\"\"\n        Detect missing values.\n\n        Return a boolean same-sized object indicating if the values are NA.\n        NA values, such as None or :attr:`numpy.NaN`, gets mapped to True\n        values.\n        Everything else gets mapped to False values. Characters such as empty\n        strings ``''`` or :attr:`numpy.inf` are not considered NA values.\n\n        Returns\n        -------\n        Series/DataFrame\n            Mask of bool values for each element in Series/DataFrame\n            that indicates whether an element is an NA value.\n\n        See Also\n        --------\n        Series.isnull : Alias of isna.\n        DataFrame.isnull : Alias of isna.\n        Series.notna : Boolean inverse of isna.\n        DataFrame.notna : Boolean inverse of isna.\n        Series.dropna : Omit axes labels with missing values.\n        DataFrame.dropna : Omit axes labels with missing values.\n        isna : Top-level isna.\n\n        Examples\n        --------\n        Show which entries in a DataFrame are NA.\n\n        >>> df = pd.DataFrame(\n        ...     dict(\n        ...         age=[5, 6, np.nan],\n        ...         born=[\n        ...             pd.NaT,\n        ...             pd.Timestamp(\"1939-05-27\"),\n        ...             pd.Timestamp(\"1940-04-25\"),\n        ...         ],\n        ...         name=[\"Alfred\", \"Batman\", \"\"],\n        ...         toy=[None, \"Batmobile\", \"Joker\"],\n        ...     )\n        ... )\n        >>> df\n           age       born    name        toy\n        0  5.0        NaT  Alfred        NaN\n        1  6.0 1939-05-27  Batman  Batmobile\n        2  NaN 1940-04-25              Joker\n\n        >>> df.isna()\n             age   born   name    toy\n        0  False   True  False   True\n        1  False  False  False  False\n        2   True  False  False  False\n\n        Show which entries in a Series are NA.\n\n        >>> ser = pd.Series([5, 6, np.nan])\n        >>> ser\n        0    5.0\n        1    6.0\n        2    NaN\n        dtype: float64\n\n        >>> ser.isna()\n        0    False\n        1    False\n        2     True\n        dtype: bool\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 7277, "code": "    def isnull(self) -> DataFrame:\n        return self.isna()", "documentation": "        \"\"\"\n        DataFrame.isnull is an alias for DataFrame.isna.\n\n        Detect missing values.\n\n        Return a boolean same-sized object indicating if the values are NA.\n        NA values, such as None or :attr:`numpy.NaN`, gets mapped to True\n        values.\n        Everything else gets mapped to False values. Characters such as empty\n        strings ``''`` or :attr:`numpy.inf` are not considered NA values.\n\n        Returns\n        -------\n        Series/DataFrame\n            Mask of bool values for each element in Series/DataFrame\n            that indicates whether an element is an NA value.\n\n        See Also\n        --------\n        Series.isnull : Alias of isna.\n        DataFrame.isnull : Alias of isna.\n        Series.notna : Boolean inverse of isna.\n        DataFrame.notna : Boolean inverse of isna.\n        Series.dropna : Omit axes labels with missing values.\n        DataFrame.dropna : Omit axes labels with missing values.\n        isna : Top-level isna.\n\n        Examples\n        --------\n        Show which entries in a DataFrame are NA.\n\n        >>> df = pd.DataFrame(\n        ...     dict(\n        ...         age=[5, 6, np.nan],\n        ...         born=[\n        ...             pd.NaT,\n        ...             pd.Timestamp(\"1939-05-27\"),\n        ...             pd.Timestamp(\"1940-04-25\"),\n        ...         ],\n        ...         name=[\"Alfred\", \"Batman\", \"\"],\n        ...         toy=[None, \"Batmobile\", \"Joker\"],\n        ...     )\n        ... )\n        >>> df\n           age       born    name        toy\n        0  5.0        NaT  Alfred        NaN\n        1  6.0 1939-05-27  Batman  Batmobile\n        2  NaN 1940-04-25              Joker\n\n        >>> df.isna()\n             age   born   name    toy\n        0  False   True  False   True\n        1  False  False  False  False\n        2   True  False  False  False\n\n        Show which entries in a Series are NA.\n\n        >>> ser = pd.Series([5, 6, np.nan])\n        >>> ser\n        0    5.0\n        1    6.0\n        2    NaN\n        dtype: float64\n\n        >>> ser.isna()\n        0    False\n        1    False\n        2     True\n        dtype: bool\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 7350, "code": "    def notna(self) -> DataFrame:\n        return ~self.isna()", "documentation": "        \"\"\"\n        Detect existing (non-missing) values.\n\n        Return a boolean same-sized object indicating if the values are not NA.\n        Non-missing values get mapped to True. Characters such as empty\n        strings ``''`` or :attr:`numpy.inf` are not considered NA values.\n        NA values, such as None or :attr:`numpy.NaN`, get mapped to False\n        values.\n\n        Returns\n        -------\n        Series/DataFrame\n            Mask of bool values for each element in Series/DataFrame\n            that indicates whether an element is not an NA value.\n\n        See Also\n        --------\n        Series.notnull : Alias of notna.\n        DataFrame.notnull : Alias of notna.\n        Series.isna : Boolean inverse of notna.\n        DataFrame.isna : Boolean inverse of notna.\n        Series.dropna : Omit axes labels with missing values.\n        DataFrame.dropna : Omit axes labels with missing values.\n        notna : Top-level notna.\n\n        Examples\n        --------\n        Show which entries in a DataFrame are not NA.\n\n        >>> df = pd.DataFrame(\n        ...     dict(\n        ...         age=[5, 6, np.nan],\n        ...         born=[\n        ...             pd.NaT,\n        ...             pd.Timestamp(\"1939-05-27\"),\n        ...             pd.Timestamp(\"1940-04-25\"),\n        ...         ],\n        ...         name=[\"Alfred\", \"Batman\", \"\"],\n        ...         toy=[None, \"Batmobile\", \"Joker\"],\n        ...     )\n        ... )\n        >>> df\n           age       born    name        toy\n        0  5.0        NaT  Alfred        NaN\n        1  6.0 1939-05-27  Batman  Batmobile\n        2  NaN 1940-04-25              Joker\n\n        >>> df.notna()\n             age   born  name    toy\n        0   True  False  True  False\n        1   True   True  True   True\n        2  False   True  True   True\n\n        Show which entries in a Series are not NA.\n\n        >>> ser = pd.Series([5, 6, np.nan])\n        >>> ser\n        0    5.0\n        1    6.0\n        2    NaN\n        dtype: float64\n\n        >>> ser.notna()\n        0     True\n        1     True\n        2    False\n        dtype: bool\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 7421, "code": "    def notnull(self) -> DataFrame:\n        return ~self.isna()\n    @overload", "documentation": "        \"\"\"\n        DataFrame.notnull is an alias for DataFrame.notna.\n\n        Detect existing (non-missing) values.\n\n        Return a boolean same-sized object indicating if the values are not NA.\n        Non-missing values get mapped to True. Characters such as empty\n        strings ``''`` or :attr:`numpy.inf` are not considered NA values.\n        NA values, such as None or :attr:`numpy.NaN`, get mapped to False\n        values.\n\n        Returns\n        -------\n        Series/DataFrame\n            Mask of bool values for each element in Series/DataFrame\n            that indicates whether an element is not an NA value.\n\n        See Also\n        --------\n        Series.notnull : Alias of notna.\n        DataFrame.notnull : Alias of notna.\n        Series.isna : Boolean inverse of notna.\n        DataFrame.isna : Boolean inverse of notna.\n        Series.dropna : Omit axes labels with missing values.\n        DataFrame.dropna : Omit axes labels with missing values.\n        notna : Top-level notna.\n\n        Examples\n        --------\n        Show which entries in a DataFrame are not NA.\n\n        >>> df = pd.DataFrame(\n        ...     dict(\n        ...         age=[5, 6, np.nan],\n        ...         born=[\n        ...             pd.NaT,\n        ...             pd.Timestamp(\"1939-05-27\"),\n        ...             pd.Timestamp(\"1940-04-25\"),\n        ...         ],\n        ...         name=[\"Alfred\", \"Batman\", \"\"],\n        ...         toy=[None, \"Batmobile\", \"Joker\"],\n        ...     )\n        ... )\n        >>> df\n           age       born    name        toy\n        0  5.0        NaT  Alfred        NaN\n        1  6.0 1939-05-27  Batman  Batmobile\n        2  NaN 1940-04-25              Joker\n\n        >>> df.notnull()\n             age   born  name    toy\n        0   True  False  True  False\n        1   True   True  True   True\n        2  False   True  True   True\n\n        Show which entries in a Series are not NA.\n\n        >>> ser = pd.Series([5, 6, np.nan])\n        >>> ser\n        0    5.0\n        1    6.0\n        2    NaN\n        dtype: float64\n\n        >>> ser.notnull()\n        0     True\n        1     True\n        2    False\n        dtype: bool\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 8832, "code": "    def swaplevel(self, i: Axis = -2, j: Axis = -1, axis: Axis = 0) -> DataFrame:\n        result = self.copy(deep=False)\n        axis = self._get_axis_number(axis)\n        if not isinstance(result._get_axis(axis), MultiIndex):  # pragma: no cover\n            raise TypeError(\"Can only swap levels on a hierarchical axis.\")\n        if axis == 0:\n            assert isinstance(result.index, MultiIndex)\n            result.index = result.index.swaplevel(i, j)\n        else:\n            assert isinstance(result.columns, MultiIndex)\n            result.columns = result.columns.swaplevel(i, j)", "documentation": "        \"\"\"\n        Swap levels i and j in a :class:`MultiIndex`.\n\n        Default is to swap the two innermost levels of the index.\n\n        Parameters\n        ----------\n        i, j : int or str\n            Levels of the indices to be swapped. Can pass level name as string.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n                    The axis to swap levels on. 0 or 'index' for row-wise, 1 or\n                    'columns' for column-wise.\n\n        Returns\n        -------\n        DataFrame\n            DataFrame with levels swapped in MultiIndex.\n\n        See Also\n        --------\n        DataFrame.reorder_levels: Reorder levels of MultiIndex.\n        DataFrame.sort_index: Sort MultiIndex.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\"Grade\": [\"A\", \"B\", \"A\", \"C\"]},\n        ...     index=[\n        ...         [\"Final exam\", \"Final exam\", \"Coursework\", \"Coursework\"],\n        ...         [\"History\", \"Geography\", \"History\", \"Geography\"],\n        ...         [\"January\", \"February\", \"March\", \"April\"],\n        ...     ],\n        ... )\n        >>> df\n                                            Grade\n        Final exam  History     January      A\n                    Geography   February     B\n        Coursework  History     March        A\n                    Geography   April        C\n\n        In the following example, we will swap the levels of the indices.\n        Here, we will swap the levels column-wise, but levels can be swapped row-wise\n        in a similar manner. Note that column-wise is the default behaviour.\n        By not supplying any arguments for i and j, we swap the last and second to\n        last indices.\n\n        >>> df.swaplevel()\n                                            Grade\n        Final exam  January     History         A\n                    February    Geography       B\n        Coursework  March       History         A\n                    April       Geography       C\n\n        By supplying one argument, we can choose which index to swap the last\n        index with. We can for example swap the first index with the last one as\n        follows.\n\n        >>> df.swaplevel(0)\n                                            Grade\n        January     History     Final exam      A\n        February    Geography   Final exam      B\n        March       History     Coursework      A\n        April       Geography   Coursework      C\n\n        We can also define explicitly which indices we want to swap by supplying values\n        for both i and j. Here, we for example swap the first and second indices.\n\n        >>> df.swaplevel(0, 1)\n                                            Grade\n        History     Final exam  January         A\n        Geography   Final exam  February        B\n        History     Coursework  March           A\n        Geography   Coursework  April           C\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 8922, "code": "    def reorder_levels(self, order: Sequence[int | str], axis: Axis = 0) -> DataFrame:\n        axis = self._get_axis_number(axis)\n        if not isinstance(self._get_axis(axis), MultiIndex):  # pragma: no cover\n            raise TypeError(\"Can only reorder levels on a hierarchical axis.\")\n        result = self.copy(deep=False)\n        if axis == 0:\n            assert isinstance(result.index, MultiIndex)\n            result.index = result.index.reorder_levels(order)\n        else:\n            assert isinstance(result.columns, MultiIndex)\n            result.columns = result.columns.reorder_levels(order)", "documentation": "        \"\"\"\n        Rearrange index or column levels using input ``order``.\n\n        May not drop or duplicate levels.\n\n        Parameters\n        ----------\n        order : list of int or list of str\n            List representing new level order. Reference level by number\n            (position) or by key (label).\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Where to reorder levels.\n\n        Returns\n        -------\n        DataFrame\n            DataFrame with indices or columns with reordered levels.\n\n        See Also\n        --------\n            DataFrame.swaplevel : Swap levels i and j in a MultiIndex.\n\n        Examples\n        --------\n        >>> data = {\n        ...     \"class\": [\"Mammals\", \"Mammals\", \"Reptiles\"],\n        ...     \"diet\": [\"Omnivore\", \"Carnivore\", \"Carnivore\"],\n        ...     \"species\": [\"Humans\", \"Dogs\", \"Snakes\"],\n        ... }\n        >>> df = pd.DataFrame(data, columns=[\"class\", \"diet\", \"species\"])\n        >>> df = df.set_index([\"class\", \"diet\"])\n        >>> df\n                                          species\n        class      diet\n        Mammals    Omnivore                Humans\n                   Carnivore                 Dogs\n        Reptiles   Carnivore               Snakes\n\n        Let's reorder the levels of the index:\n\n        >>> df.reorder_levels([\"diet\", \"class\"])\n                                          species\n        diet      class\n        Omnivore  Mammals                  Humans\n        Carnivore Mammals                    Dogs\n                  Reptiles                 Snakes\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 9101, "code": "    def _arith_method_with_reindex(self, right: DataFrame, op) -> DataFrame:\n        left = self\n        cols, lcol_indexer, rcol_indexer = left.columns.join(\n            right.columns, how=\"inner\", return_indexers=True\n        )\n        new_left = left if lcol_indexer is None else left.iloc[:, lcol_indexer]\n        new_right = right if rcol_indexer is None else right.iloc[:, rcol_indexer]\n        if isinstance(cols, MultiIndex):\n            new_left = new_left.copy(deep=False)\n            new_right = new_right.copy(deep=False)\n            new_left.columns = cols", "documentation": "        \"\"\"\n        For DataFrame-with-DataFrame operations that require reindexing,\n        operate only on shared columns, then reindex.\n\n        Parameters\n        ----------\n        right : DataFrame\n        op : binary operator\n\n        Returns\n        -------\n        DataFrame\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 9153, "code": "    def _should_reindex_frame_op(self, right, op, axis: int, fill_value, level) -> bool:\n        if op is operator.pow or op is roperator.rpow:\n            return False\n        if not isinstance(right, DataFrame):\n            return False\n        if (\n            (\n                isinstance(self.columns, MultiIndex)\n                or isinstance(right.columns, MultiIndex)\n            )\n            and not self.columns.equals(right.columns)", "documentation": "        \"\"\"\n        Check if this is an operation between DataFrames that will need to reindex.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 9324, "code": "    def _maybe_align_series_as_frame(self, series: Series, axis: AxisInt):\n        rvalues = series._values\n        if not isinstance(rvalues, np.ndarray):\n            if lib.is_np_dtype(rvalues.dtype, \"mM\"):\n                rvalues = np.asarray(rvalues)\n            else:\n                return series\n        if axis == 0:\n            rvalues = rvalues.reshape(-1, 1)\n        else:\n            rvalues = rvalues.reshape(1, -1)", "documentation": "        \"\"\"\n        If the Series operand is not EA-dtype, we can broadcast to 2D and operate\n        blockwise.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 9385, "code": "    def _construct_result(self, result, other) -> DataFrame:\n        out = self._constructor(result, copy=False).__finalize__(self)\n        out.columns = self.columns\n        out.index = self.index\n        out = out.__finalize__(other)\n        return out", "documentation": "        \"\"\"\n        Wrap the result of an arithmetic, comparison, or logical operation.\n\n        Parameters\n        ----------\n        result : DataFrame\n\n        Returns\n        -------\n        DataFrame\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 9425, "code": "    def eq(self, other, axis: Axis = \"columns\", level=None) -> DataFrame:\n        return self._flex_cmp_method(other, operator.eq, axis=axis, level=level)", "documentation": "        \"\"\"\n        Get Not equal to of dataframe and other, element-wise (binary operator `eq`).\n\n        Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n        operators.\n\n        Equivalent to `==`, `!=`, `<=`, `<`, `>=`, `>` with support to choose axis\n        (rows or columns) and level for comparison.\n\n        Parameters\n        ----------\n        other : scalar, sequence, Series, or DataFrame\n            Any single or multiple element data structure, or list-like object.\n        axis : {0 or 'index', 1 or 'columns'}, default 'columns'\n            Whether to compare by the index (0 or 'index') or columns\n            (1 or 'columns').\n        level : int or label\n            Broadcast across a level, matching Index values on the passed\n            MultiIndex level.\n\n        Returns\n        -------\n        DataFrame of bool\n            Result of the comparison.\n\n        See Also\n        --------\n        DataFrame.eq : Compare DataFrames for equality elementwise.\n        DataFrame.ne : Compare DataFrames for inequality elementwise.\n        DataFrame.le : Compare DataFrames for less than inequality\n            or equality elementwise.\n        DataFrame.lt : Compare DataFrames for strictly less than\n            inequality elementwise.\n        DataFrame.ge : Compare DataFrames for greater than inequality\n            or equality elementwise.\n        DataFrame.gt : Compare DataFrames for strictly greater than\n            inequality elementwise.\n\n        Notes\n        -----\n        Mismatched indices will be unioned together.\n        `NaN` values are considered different (i.e. `NaN` != `NaN`).\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\"cost\": [250, 150, 100], \"revenue\": [100, 250, 300]},\n        ...     index=[\"A\", \"B\", \"C\"],\n        ... )\n        >>> df\n           cost  revenue\n        A   250      100\n        B   150      250\n        C   100      300\n\n        Comparison with a scalar, using either the operator or method:\n\n        >>> df == 100\n            cost  revenue\n        A  False     True\n        B  False    False\n        C   True    False\n\n        >>> df.eq(100)\n            cost  revenue\n        A  False     True\n        B  False    False\n        C   True    False\n\n        When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n        with the index of `other` and broadcast:\n\n        >>> df != pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n            cost  revenue\n        A   True     True\n        B   True    False\n        C  False     True\n\n        Use the method to control the broadcast axis:\n\n        >>> df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis=\"index\")\n           cost  revenue\n        A  True    False\n        B  True     True\n        C  True     True\n        D  True     True\n\n        When comparing to an arbitrary sequence, the number of columns must\n        match the number elements in `other`:\n\n        >>> df == [250, 100]\n            cost  revenue\n        A   True     True\n        B  False    False\n        C  False    False\n\n        Use the method to control the axis:\n\n        >>> df.eq([250, 250, 100], axis=\"index\")\n            cost  revenue\n        A   True    False\n        B  False     True\n        C   True    False\n\n        Compare to a DataFrame of different shape.\n\n        >>> other = pd.DataFrame(\n        ...     {\"revenue\": [300, 250, 100, 150]}, index=[\"A\", \"B\", \"C\", \"D\"]\n        ... )\n        >>> other\n           revenue\n        A      300\n        B      250\n        C      100\n        D      150\n\n        >>> df.gt(other)\n            cost  revenue\n        A  False    False\n        B  False    False\n        C  False     True\n        D  False    False\n\n        Compare to a MultiIndex by level.\n\n        >>> df_multindex = pd.DataFrame(\n        ...     {\n        ...         \"cost\": [250, 150, 100, 150, 300, 220],\n        ...         \"revenue\": [100, 250, 300, 200, 175, 225],\n        ...     },\n        ...     index=[\n        ...         [\"Q1\", \"Q1\", \"Q1\", \"Q2\", \"Q2\", \"Q2\"],\n        ...         [\"A\", \"B\", \"C\", \"A\", \"B\", \"C\"],\n        ...     ],\n        ... )\n        >>> df_multindex\n              cost  revenue\n        Q1 A   250      100\n           B   150      250\n           C   100      300\n        Q2 A   150      200\n           B   300      175\n           C   220      225\n\n        >>> df.le(df_multindex, level=1)\n               cost  revenue\n        Q1 A   True     True\n           B   True     True\n           C   True     True\n        Q2 A  False     True\n           B   True    False\n           C   True    False\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 9581, "code": "    def ne(self, other, axis: Axis = \"columns\", level=None) -> DataFrame:\n        return self._flex_cmp_method(other, operator.ne, axis=axis, level=level)", "documentation": "        \"\"\"\n        Get Not equal to of dataframe and other, element-wise (binary operator `ne`).\n\n        Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n        operators.\n\n        Equivalent to `==`, `!=`, `<=`, `<`, `>=`, `>` with support to choose axis\n        (rows or columns) and level for comparison.\n\n        Parameters\n        ----------\n        other : scalar, sequence, Series, or DataFrame\n            Any single or multiple element data structure, or list-like object.\n        axis : {0 or 'index', 1 or 'columns'}, default 'columns'\n            Whether to compare by the index (0 or 'index') or columns\n            (1 or 'columns').\n        level : int or label\n            Broadcast across a level, matching Index values on the passed\n            MultiIndex level.\n\n        Returns\n        -------\n        DataFrame of bool\n            Result of the comparison.\n\n        See Also\n        --------\n        DataFrame.eq : Compare DataFrames for equality elementwise.\n        DataFrame.ne : Compare DataFrames for inequality elementwise.\n        DataFrame.le : Compare DataFrames for less than inequality\n            or equality elementwise.\n        DataFrame.lt : Compare DataFrames for strictly less than\n            inequality elementwise.\n        DataFrame.ge : Compare DataFrames for greater than inequality\n            or equality elementwise.\n        DataFrame.gt : Compare DataFrames for strictly greater than\n            inequality elementwise.\n\n        Notes\n        -----\n        Mismatched indices will be unioned together.\n        `NaN` values are considered different (i.e. `NaN` != `NaN`).\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\"cost\": [250, 150, 100], \"revenue\": [100, 250, 300]},\n        ...     index=[\"A\", \"B\", \"C\"],\n        ... )\n        >>> df\n           cost  revenue\n        A   250      100\n        B   150      250\n        C   100      300\n\n        Comparison with a scalar, using either the operator or method:\n\n        >>> df == 100\n            cost  revenue\n        A  False     True\n        B  False    False\n        C   True    False\n\n        >>> df.eq(100)\n            cost  revenue\n        A  False     True\n        B  False    False\n        C   True    False\n\n        When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n        with the index of `other` and broadcast:\n\n        >>> df != pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n            cost  revenue\n        A   True     True\n        B   True    False\n        C  False     True\n\n        Use the method to control the broadcast axis:\n\n        >>> df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis=\"index\")\n           cost  revenue\n        A  True    False\n        B  True     True\n        C  True     True\n        D  True     True\n\n        When comparing to an arbitrary sequence, the number of columns must\n        match the number elements in `other`:\n\n        >>> df == [250, 100]\n            cost  revenue\n        A   True     True\n        B  False    False\n        C  False    False\n\n        Use the method to control the axis:\n\n        >>> df.eq([250, 250, 100], axis=\"index\")\n            cost  revenue\n        A   True    False\n        B  False     True\n        C   True    False\n\n        Compare to a DataFrame of different shape.\n\n        >>> other = pd.DataFrame(\n        ...     {\"revenue\": [300, 250, 100, 150]}, index=[\"A\", \"B\", \"C\", \"D\"]\n        ... )\n        >>> other\n           revenue\n        A      300\n        B      250\n        C      100\n        D      150\n\n        >>> df.gt(other)\n            cost  revenue\n        A  False    False\n        B  False    False\n        C  False     True\n        D  False    False\n\n        Compare to a MultiIndex by level.\n\n        >>> df_multindex = pd.DataFrame(\n        ...     {\n        ...         \"cost\": [250, 150, 100, 150, 300, 220],\n        ...         \"revenue\": [100, 250, 300, 200, 175, 225],\n        ...     },\n        ...     index=[\n        ...         [\"Q1\", \"Q1\", \"Q1\", \"Q2\", \"Q2\", \"Q2\"],\n        ...         [\"A\", \"B\", \"C\", \"A\", \"B\", \"C\"],\n        ...     ],\n        ... )\n        >>> df_multindex\n              cost  revenue\n        Q1 A   250      100\n           B   150      250\n           C   100      300\n        Q2 A   150      200\n           B   300      175\n           C   220      225\n\n        >>> df.le(df_multindex, level=1)\n               cost  revenue\n        Q1 A   True     True\n           B   True     True\n           C   True     True\n        Q2 A  False     True\n           B   True    False\n           C   True    False\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 9737, "code": "    def le(self, other, axis: Axis = \"columns\", level=None) -> DataFrame:\n        return self._flex_cmp_method(other, operator.le, axis=axis, level=level)", "documentation": "        \"\"\"\n        Get Less than or equal to of dataframe and other, \\\n        element-wise (binary operator `le`).\n\n        Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n        operators.\n\n        Equivalent to ``<=`` with support to choose axis\n        (rows or columns) and level for comparison.\n\n        Parameters\n        ----------\n        other : scalar, sequence, Series, or DataFrame\n            Any single or multiple element data structure, or list-like object.\n        axis : {0 or 'index', 1 or 'columns'}, default 'columns'\n            Whether to compare by the index (0 or 'index') or columns\n            (1 or 'columns').\n        level : int or label\n            Broadcast across a level, matching Index values on the passed\n            MultiIndex level.\n\n        Returns\n        -------\n        DataFrame of bool\n            Result of the comparison.\n\n        See Also\n        --------\n        DataFrame.eq : Compare DataFrames for equality elementwise.\n        DataFrame.ne : Compare DataFrames for inequality elementwise.\n        DataFrame.le : Compare DataFrames for less than inequality\n            or equality elementwise.\n        DataFrame.lt : Compare DataFrames for strictly less than\n            inequality elementwise.\n        DataFrame.ge : Compare DataFrames for greater than inequality\n            or equality elementwise.\n        DataFrame.gt : Compare DataFrames for strictly greater than\n            inequality elementwise.\n\n        Notes\n        -----\n        Mismatched indices will be unioned together.\n        `NaN` values are considered different (i.e. `NaN` != `NaN`).\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'cost': [250, 150, 100],\n        ...                    'revenue': [100, 250, 300]},\n        ...                   index=['A', 'B', 'C'])\n        >>> df\n           cost  revenue\n        A   250      100\n        B   150      250\n        C   100      300\n\n        Comparison with a scalar, using either the operator or method:\n\n        >>> df <= 100\n            cost  revenue\n        A  False     True\n        B  False    False\n        C   True    False\n\n        >>> df.le(100)\n            cost  revenue\n        A  False     True\n        B  False    False\n        C   True    False\n\n        When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n        with the index of `other` and broadcast:\n\n        >>> df <= pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n            cost  revenue\n        A  False     True\n        B  False     True\n        C   True    False\n\n        Use the method to control the broadcast axis:\n\n        >>> df.le(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index')\n            cost  revenue\n        A  False     True\n        B  False    False\n        C  False    False\n        D  False    False\n\n        When comparing to an arbitrary sequence, the number of columns must\n        match the number elements in `other`:\n\n        >>> df <= [250, 100]\n           cost  revenue\n        A  True     True\n        B  True    False\n        C  True    False\n\n        Use the method to control the axis:\n\n        >>> df.le([250, 250, 100], axis='index')\n           cost  revenue\n        A  True     True\n        B  True     True\n        C  True    False\n\n        Compare to a DataFrame of different shape.\n\n        >>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},\n        ...                      index=['A', 'B', 'C', 'D'])\n        >>> other\n           revenue\n        A      300\n        B      250\n        C      100\n        D      150\n\n        >>> df.le(other)\n            cost  revenue\n        A  False     True\n        B  False     True\n        C  False    False\n        D  False    False\n\n        Compare to a MultiIndex by level.\n\n        >>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],\n        ...                              'revenue': [100, 250, 300, 200, 175, 225]},\n        ...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n        ...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n        >>> df_multindex\n              cost  revenue\n        Q1 A   250      100\n           B   150      250\n           C   100      300\n        Q2 A   150      200\n           B   300      175\n           C   220      225\n\n        >>> df.le(df_multindex, level=1)\n               cost  revenue\n        Q1 A   True     True\n           B   True     True\n           C   True     True\n        Q2 A  False     True\n           B   True    False\n           C   True    False\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 9886, "code": "    def lt(self, other, axis: Axis = \"columns\", level=None) -> DataFrame:\n        return self._flex_cmp_method(other, operator.lt, axis=axis, level=level)", "documentation": "        \"\"\"\n        Get Less than of dataframe and other, element-wise (binary operator `lt`).\n\n        Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n        operators.\n\n        Equivalent to ``<`` with support to choose axis\n        (rows or columns) and level for comparison.\n\n        Parameters\n        ----------\n        other : scalar, sequence, Series, or DataFrame\n            Any single or multiple element data structure, or list-like object.\n        axis : {0 or 'index', 1 or 'columns'}, default 'columns'\n            Whether to compare by the index (0 or 'index') or columns\n            (1 or 'columns').\n        level : int or label\n            Broadcast across a level, matching Index values on the passed\n            MultiIndex level.\n\n        Returns\n        -------\n        DataFrame of bool\n            Result of the comparison.\n\n        See Also\n        --------\n        DataFrame.eq : Compare DataFrames for equality elementwise.\n        DataFrame.ne : Compare DataFrames for inequality elementwise.\n        DataFrame.le : Compare DataFrames for less than inequality\n            or equality elementwise.\n        DataFrame.lt : Compare DataFrames for strictly less than\n            inequality elementwise.\n        DataFrame.ge : Compare DataFrames for greater than inequality\n            or equality elementwise.\n        DataFrame.gt : Compare DataFrames for strictly greater than\n            inequality elementwise.\n\n        Notes\n        -----\n        Mismatched indices will be unioned together.\n        `NaN` values are considered different (i.e. `NaN` != `NaN`).\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\"cost\": [250, 150, 100], \"revenue\": [100, 250, 300]},\n        ...     index=[\"A\", \"B\", \"C\"],\n        ... )\n        >>> df\n           cost  revenue\n        A   250      100\n        B   150      250\n        C   100      300\n\n        Comparison with a scalar, using either the operator or method:\n\n        >>> df < 100\n            cost  revenue\n        A  False    False\n        B  False    False\n        C  False    False\n\n        >>> df.lt(100)\n            cost  revenue\n        A  False    False\n        B  False    False\n        C  False    False\n\n        When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n        with the index of `other` and broadcast:\n\n        >>> df < pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n            cost  revenue\n        A  False     True\n        B  False    False\n        C  False    False\n\n        Use the method to control the broadcast axis:\n\n        >>> df.lt(pd.Series([100, 300], index=[\"A\", \"D\"]), axis=\"index\")\n            cost  revenue\n        A  False    False\n        B  False    False\n        C  False    False\n        D  False    False\n\n        When comparing to an arbitrary sequence, the number of columns must\n        match the number elements in `other`:\n\n        >>> df < [250, 100]\n            cost  revenue\n        A  False    False\n        B   True    False\n        C   True    False\n\n        Use the method to control the axis:\n\n        >>> df.lt([250, 250, 100], axis=\"index\")\n            cost  revenue\n        A  False     True\n        B   True    False\n        C  False    False\n\n        Compare to a DataFrame of different shape.\n\n        >>> other = pd.DataFrame(\n        ...     {\"revenue\": [300, 250, 100, 150]}, index=[\"A\", \"B\", \"C\", \"D\"]\n        ... )\n        >>> other\n           revenue\n        A      300\n        B      250\n        C      100\n        D      150\n\n        >>> df.lt(other)\n            cost  revenue\n        A  False     True\n        B  False    False\n        C  False    False\n        D  False    False\n\n        Compare to a MultiIndex by level.\n\n        >>> df_multindex = pd.DataFrame(\n        ...     {\n        ...         \"cost\": [250, 150, 100, 150, 300, 220],\n        ...         \"revenue\": [100, 250, 300, 200, 175, 225],\n        ...     },\n        ...     index=[\n        ...         [\"Q1\", \"Q1\", \"Q1\", \"Q2\", \"Q2\", \"Q2\"],\n        ...         [\"A\", \"B\", \"C\", \"A\", \"B\", \"C\"],\n        ...     ],\n        ... )\n        >>> df_multindex\n              cost  revenue\n        Q1 A   250      100\n           B   150      250\n           C   100      300\n        Q2 A   150      200\n           B   300      175\n           C   220      225\n\n        >>> df.lt(df_multindex, level=1)\n               cost  revenue\n        Q1 A  False    False\n           B  False    False\n           C  False    False\n        Q2 A  False     True\n           B   True    False\n           C   True    False\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 10042, "code": "    def ge(self, other, axis: Axis = \"columns\", level=None) -> DataFrame:\n        return self._flex_cmp_method(other, operator.ge, axis=axis, level=level)", "documentation": "        \"\"\"\n        Get Greater than or equal to of dataframe and other, \\\n        element-wise (binary operator `ge`).\n\n        Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n        operators.\n\n        Equivalent to ``>=`` with support to choose axis\n        (rows or columns) and level for comparison.\n\n        Parameters\n        ----------\n        other : scalar, sequence, Series, or DataFrame\n            Any single or multiple element data structure, or list-like object.\n        axis : {0 or 'index', 1 or 'columns'}, default 'columns'\n            Whether to compare by the index (0 or 'index') or columns\n            (1 or 'columns').\n        level : int or label\n            Broadcast across a level, matching Index values on the passed\n            MultiIndex level.\n\n        Returns\n        -------\n        DataFrame of bool\n            Result of the comparison.\n\n        See Also\n        --------\n        DataFrame.eq : Compare DataFrames for equality elementwise.\n        DataFrame.ne : Compare DataFrames for inequality elementwise.\n        DataFrame.le : Compare DataFrames for less than inequality\n            or equality elementwise.\n        DataFrame.lt : Compare DataFrames for strictly less than\n            inequality elementwise.\n        DataFrame.ge : Compare DataFrames for greater than inequality\n            or equality elementwise.\n        DataFrame.gt : Compare DataFrames for strictly greater than\n            inequality elementwise.\n\n        Notes\n        -----\n        Mismatched indices will be unioned together.\n        `NaN` values are considered different (i.e. `NaN` != `NaN`).\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'cost': [250, 150, 100],\n        ...                    'revenue': [100, 250, 300]},\n        ...                   index=['A', 'B', 'C'])\n        >>> df\n           cost  revenue\n        A   250      100\n        B   150      250\n        C   100      300\n\n        Comparison with a scalar, using either the operator or method:\n\n        >>> df >= 100\n           cost  revenue\n        A  True     True\n        B  True     True\n        C  True     True\n\n        >>> df.ge(100)\n           cost  revenue\n        A  True     True\n        B  True     True\n        C  True     True\n\n        When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n        with the index of `other` and broadcast:\n\n        >>> df >= pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n           cost  revenue\n        A  True    False\n        B  True     True\n        C  True     True\n\n        Use the method to control the broadcast axis:\n\n        >>> df.ge(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index')\n            cost  revenue\n        A   True     True\n        B  False    False\n        C  False    False\n        D  False    False\n\n        When comparing to an arbitrary sequence, the number of columns must\n        match the number elements in `other`:\n\n        >>> df >= [250, 100]\n            cost  revenue\n        A   True     True\n        B  False     True\n        C  False     True\n\n        Use the method to control the axis:\n\n        >>> df.ge([250, 250, 100], axis='index')\n            cost  revenue\n        A   True    False\n        B  False     True\n        C   True     True\n\n        Compare to a DataFrame of different shape.\n\n        >>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},\n        ...                      index=['A', 'B', 'C', 'D'])\n        >>> other\n           revenue\n        A      300\n        B      250\n        C      100\n        D      150\n\n        >>> df.ge(other)\n            cost  revenue\n        A  False    False\n        B  False     True\n        C  False     True\n        D  False    False\n\n        Compare to a MultiIndex by level.\n\n        >>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],\n        ...                              'revenue': [100, 250, 300, 200, 175, 225]},\n        ...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n        ...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n        >>> df_multindex\n              cost  revenue\n        Q1 A   250      100\n           B   150      250\n           C   100      300\n        Q2 A   150      200\n           B   300      175\n           C   220      225\n\n        >>> df.ge(df_multindex, level=1)\n               cost  revenue\n        Q1 A   True     True\n           B   True     True\n           C   True     True\n        Q2 A   True    False\n           B  False     True\n           C  False     True\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 10191, "code": "    def gt(self, other, axis: Axis = \"columns\", level=None) -> DataFrame:\n        return self._flex_cmp_method(other, operator.gt, axis=axis, level=level)", "documentation": "        \"\"\"\n        Get Greater than of dataframe and other, element-wise (binary operator `gt`).\n\n        Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n        operators.\n\n        Equivalent to ``>`` with support to choose axis\n        (rows or columns) and level for comparison.\n\n        Parameters\n        ----------\n        other : scalar, sequence, Series, or DataFrame\n            Any single or multiple element data structure, or list-like object.\n        axis : {0 or 'index', 1 or 'columns'}, default 'columns'\n            Whether to compare by the index (0 or 'index') or columns\n            (1 or 'columns').\n        level : int or label\n            Broadcast across a level, matching Index values on the passed\n            MultiIndex level.\n\n        Returns\n        -------\n        DataFrame of bool\n            Result of the comparison.\n\n        See Also\n        --------\n        DataFrame.eq : Compare DataFrames for equality elementwise.\n        DataFrame.ne : Compare DataFrames for inequality elementwise.\n        DataFrame.le : Compare DataFrames for less than inequality\n            or equality elementwise.\n        DataFrame.lt : Compare DataFrames for strictly less than\n            inequality elementwise.\n        DataFrame.ge : Compare DataFrames for greater than inequality\n            or equality elementwise.\n        DataFrame.gt : Compare DataFrames for strictly greater than\n            inequality elementwise.\n\n        Notes\n        -----\n        Mismatched indices will be unioned together.\n        `NaN` values are considered different (i.e. `NaN` != `NaN`).\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\"cost\": [250, 150, 100], \"revenue\": [100, 250, 300]},\n        ...     index=[\"A\", \"B\", \"C\"],\n        ... )\n        >>> df\n           cost  revenue\n        A   250      100\n        B   150      250\n        C   100      300\n\n        Comparison with a scalar, using either the operator or method:\n\n        >>> df > 100\n            cost  revenue\n        A   True    False\n        B   True     True\n        C  False     True\n\n        >>> df.gt(100)\n            cost  revenue\n        A   True    False\n        B   True     True\n        C  False     True\n\n        When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n        with the index of `other` and broadcast:\n\n        >>> df > pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n            cost  revenue\n        A   True    False\n        B   True    False\n        C  False     True\n\n        Use the method to control the broadcast axis:\n\n        >>> df.gt(pd.Series([100, 300], index=[\"A\", \"D\"]), axis=\"index\")\n            cost  revenue\n        A   True    False\n        B  False    False\n        C  False    False\n        D  False    False\n\n        When comparing to an arbitrary sequence, the number of columns must\n        match the number elements in `other`:\n\n        >>> df > [250, 100]\n            cost  revenue\n        A  False    False\n        B  False     True\n        C  False     True\n\n        Use the method to control the axis:\n\n        >>> df.gt([250, 250, 100], axis=\"index\")\n            cost  revenue\n        A  False    False\n        B  False    False\n        C  False     True\n\n        Compare to a DataFrame of different shape.\n\n        >>> other = pd.DataFrame(\n        ...     {\"revenue\": [300, 250, 100, 150]}, index=[\"A\", \"B\", \"C\", \"D\"]\n        ... )\n        >>> other\n           revenue\n        A      300\n        B      250\n        C      100\n        D      150\n\n        >>> df.gt(other)\n            cost  revenue\n        A  False    False\n        B  False    False\n        C  False     True\n        D  False    False\n\n        Compare to a MultiIndex by level.\n\n        >>> df_multindex = pd.DataFrame(\n        ...     {\n        ...         \"cost\": [250, 150, 100, 150, 300, 220],\n        ...         \"revenue\": [100, 250, 300, 200, 175, 225],\n        ...     },\n        ...     index=[\n        ...         [\"Q1\", \"Q1\", \"Q1\", \"Q2\", \"Q2\", \"Q2\"],\n        ...         [\"A\", \"B\", \"C\", \"A\", \"B\", \"C\"],\n        ...     ],\n        ... )\n        >>> df_multindex\n              cost  revenue\n        Q1 A   250      100\n           B   150      250\n           C   100      300\n        Q2 A   150      200\n           B   300      175\n           C   220      225\n\n        >>> df.gt(df_multindex, level=1)\n               cost  revenue\n        Q1 A  False    False\n           B  False    False\n           C  False    False\n        Q2 A   True    False\n           B  False     True\n           C  False     True\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 12148, "code": "    def combine_first(self, other: DataFrame) -> DataFrame:", "documentation": "        \"\"\"\n        Update null elements with value in the same location in `other`.\n\n        Combine two DataFrame objects by filling null values in one DataFrame\n        with non-null values from other DataFrame. The row and column indexes\n        of the resulting DataFrame will be the union of the two. The resulting\n        dataframe contains the 'first' dataframe values and overrides the\n        second one values where both first.loc[index, col] and\n        second.loc[index, col] are not missing values, upon calling\n        first.combine_first(second).\n\n        Parameters\n        ----------\n        other : DataFrame\n            Provided DataFrame to use to fill null values.\n\n        Returns\n        -------\n        DataFrame\n            The result of combining the provided DataFrame with the other object.\n\n        See Also\n        --------\n        DataFrame.combine : Perform series-wise operation on two DataFrames\n            using a given function.\n\n        Examples\n        --------\n        >>> df1 = pd.DataFrame({\"A\": [None, 0], \"B\": [None, 4]})\n        >>> df2 = pd.DataFrame({\"A\": [1, 1], \"B\": [3, 3]})\n        >>> df1.combine_first(df2)\n             A    B\n        0  1.0  3.0\n        1  0.0  4.0\n\n        Null values still persist if the location of that null value\n        does not exist in `other`\n\n        >>> df1 = pd.DataFrame({\"A\": [None, 0], \"B\": [4, None]})\n        >>> df2 = pd.DataFrame({\"B\": [3, 3], \"C\": [1, 1]}, index=[1, 2])\n        >>> df1.combine_first(df2)\n             A    B    C\n        0  NaN  4.0  NaN\n        1  0.0  3.0  1.0\n        2  NaN  3.0  1.0\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 13562, "code": "    def diff(self, periods: int = 1, axis: Axis = 0) -> DataFrame:\n        if not lib.is_integer(periods):\n            if not (is_float(periods) and periods.is_integer()):\n                raise ValueError(\"periods must be an integer\")\n            periods = int(periods)\n        axis = self._get_axis_number(axis)\n        if axis == 1:\n            if periods != 0:\n                return self - self.shift(periods, axis=axis)\n            axis = 0\n        new_data = self._mgr.diff(n=periods)", "documentation": "        \"\"\"\n        First discrete difference of element.\n\n        Calculates the difference of a DataFrame element compared with another\n        element in the DataFrame (default is element in previous row).\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Periods to shift for calculating difference, accepts negative\n            values.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Take difference over rows (0) or columns (1).\n\n        Returns\n        -------\n        DataFrame\n            First differences of the Series.\n\n        See Also\n        --------\n        DataFrame.pct_change: Percent change over given number of periods.\n        DataFrame.shift: Shift index by desired number of periods with an\n            optional time freq.\n        Series.diff: First discrete difference of object.\n\n        Notes\n        -----\n        For boolean dtypes, this uses :meth:`operator.xor` rather than\n        :meth:`operator.sub`.\n        The result is calculated according to current dtype in DataFrame,\n        however dtype of the result is always float64.\n\n        Examples\n        --------\n\n        Difference with previous row\n\n        >>> df = pd.DataFrame(\n        ...     {\n        ...         \"a\": [1, 2, 3, 4, 5, 6],\n        ...         \"b\": [1, 1, 2, 3, 5, 8],\n        ...         \"c\": [1, 4, 9, 16, 25, 36],\n        ...     }\n        ... )\n        >>> df\n           a  b   c\n        0  1  1   1\n        1  2  1   4\n        2  3  2   9\n        3  4  3  16\n        4  5  5  25\n        5  6  8  36\n        >>> df.diff()\n             a    b     c\n        0  NaN  NaN   NaN\n        1  1.0  0.0   3.0\n        2  1.0  1.0   5.0\n        3  1.0  1.0   7.0\n        4  1.0  2.0   9.0\n        5  1.0  3.0  11.0\n\n        Difference with previous column\n\n        >>> df.diff(axis=1)\n            a  b   c\n        0 NaN  0   0\n        1 NaN -1   3\n        2 NaN -1   7\n        3 NaN -1  13\n        4 NaN  0  20\n        5 NaN  2  28\n\n        Difference with 3rd previous row\n\n        >>> df.diff(periods=3)\n             a    b     c\n        0  NaN  NaN   NaN\n        1  NaN  NaN   NaN\n        2  NaN  NaN   NaN\n        3  3.0  2.0  15.0\n        4  3.0  4.0  21.0\n        5  3.0  6.0  27.0\n\n        Difference with following row\n\n        >>> df.diff(periods=-1)\n             a    b     c\n        0 -1.0  0.0  -3.0\n        1 -1.0 -1.0  -5.0\n        2 -1.0 -1.0  -7.0\n        3 -1.0 -2.0  -9.0\n        4 -1.0 -3.0 -11.0\n        5  NaN  NaN   NaN\n\n        Overflow in input dtype\n\n        >>> df = pd.DataFrame({\"a\": [1, 0]}, dtype=np.uint8)\n        >>> df.diff()\n               a\n        0    NaN\n        1  255.0\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 13773, "code": "    def aggregate(self, func=None, axis: Axis = 0, *args, **kwargs):\n        from pandas.core.apply import frame_apply\n        axis = self._get_axis_number(axis)\n        op = frame_apply(self, func=func, axis=axis, args=args, kwargs=kwargs)\n        result = op.agg()\n        result = reconstruct_and_relabel_result(result, func, **kwargs)\n        return result\n    agg = aggregate", "documentation": "        \"\"\"\n        Aggregate using one or more operations over the specified axis.\n\n        Parameters\n        ----------\n        func : function, str, list or dict\n            Function to use for aggregating the data. If a function, must either\n            work when passed a DataFrame or when passed to DataFrame.apply.\n\n            Accepted combinations are:\n\n            - function\n            - string function name\n            - list of functions and/or function names, e.g. ``[np.sum, 'mean']``\n            - dict of axis labels -> functions, function names or list of such.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n                If 0 or 'index': apply function to each column.\n                If 1 or 'columns': apply function to each row.\n        *args\n            Positional arguments to pass to `func`.\n        **kwargs\n            Keyword arguments to pass to `func`.\n\n        Returns\n        -------\n        scalar, Series or DataFrame\n\n            The return can be:\n\n            * scalar : when Series.agg is called with single function\n            * Series : when DataFrame.agg is called with a single function\n            * DataFrame : when DataFrame.agg is called with several functions\n\n        See Also\n        --------\n        DataFrame.apply : Perform any type of operations.\n        DataFrame.transform : Perform transformation type operations.\n        DataFrame.groupby : Perform operations over groups.\n        DataFrame.resample : Perform operations over resampled bins.\n        DataFrame.rolling : Perform operations over rolling window.\n        DataFrame.expanding : Perform operations over expanding window.\n        core.window.ewm.ExponentialMovingWindow : Perform operation over exponential\n            weighted window.\n\n        Notes\n        -----\n        The aggregation operations are always performed over an axis, either the\n        index (default) or the column axis. This behavior is different from\n        `numpy` aggregation functions (`mean`, `median`, `prod`, `sum`, `std`,\n        `var`), where the default is to compute the aggregation of the flattened\n        array, e.g., ``numpy.mean(arr_2d)`` as opposed to\n        ``numpy.mean(arr_2d, axis=0)``.\n\n        `agg` is an alias for `aggregate`. Use the alias.\n\n        Functions that mutate the passed object can produce unexpected\n        behavior or errors and are not supported. See :ref:`gotchas.udf-mutation`\n        for more details.\n\n        A passed user-defined-function will be passed a Series for evaluation.\n\n        If ``func`` defines an index relabeling, ``axis`` must be ``0`` or ``index``.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     [[1, 2, 3], [4, 5, 6], [7, 8, 9], [np.nan, np.nan, np.nan]],\n        ...     columns=[\"A\", \"B\", \"C\"],\n        ... )\n\n        Aggregate these functions over the rows.\n\n        >>> df.agg([\"sum\", \"min\"])\n                A     B     C\n        sum  12.0  15.0  18.0\n        min   1.0   2.0   3.0\n\n        Different aggregations per column.\n\n        >>> df.agg({\"A\": [\"sum\", \"min\"], \"B\": [\"min\", \"max\"]})\n                A    B\n        sum  12.0  NaN\n        min   1.0  2.0\n        max   NaN  8.0\n\n        Aggregate different functions over the columns and rename the index of\n        the resulting DataFrame.\n\n        >>> df.agg(x=(\"A\", \"max\"), y=(\"B\", \"min\"), z=(\"C\", \"mean\"))\n             A    B    C\n        x  7.0  NaN  NaN\n        y  NaN  2.0  NaN\n        z  NaN  NaN  6.0\n\n        Aggregate over the columns.\n\n        >>> df.agg(\"mean\", axis=\"columns\")\n        0    2.0\n        1    5.0\n        2    8.0\n        3    NaN\n        dtype: float64\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 15500, "code": "    def count(self, axis: Axis = 0, numeric_only: bool = False) -> Series:\n        axis = self._get_axis_number(axis)\n        if numeric_only:\n            frame = self._get_numeric_data()\n        else:\n            frame = self\n        if len(frame._get_axis(axis)) == 0:\n            result = self._constructor_sliced(0, index=frame._get_agg_axis(axis))\n        else:\n            result = notna(frame).sum(axis=axis)\n        return result.astype(\"int64\").__finalize__(self, method=\"count\")", "documentation": "        \"\"\"\n        Count non-NA cells for each column or row.\n\n        The values `None`, `NaN`, `NaT`, ``pandas.NA`` are considered NA.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            If 0 or 'index' counts are generated for each column.\n            If 1 or 'columns' counts are generated for each row.\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n        Returns\n        -------\n        Series\n            For each column/row the number of non-NA/null entries.\n\n        See Also\n        --------\n        Series.count: Number of non-NA elements in a Series.\n        DataFrame.value_counts: Count unique combinations of columns.\n        DataFrame.shape: Number of DataFrame rows and columns (including NA\n            elements).\n        DataFrame.isna: Boolean same-sized DataFrame showing places of NA\n            elements.\n\n        Examples\n        --------\n        Constructing DataFrame from a dictionary:\n\n        >>> df = pd.DataFrame(\n        ...     {\n        ...         \"Person\": [\"John\", \"Myla\", \"Lewis\", \"John\", \"Myla\"],\n        ...         \"Age\": [24.0, np.nan, 21.0, 33, 26],\n        ...         \"Single\": [False, True, True, True, False],\n        ...     }\n        ... )\n        >>> df\n           Person   Age  Single\n        0    John  24.0   False\n        1    Myla   NaN    True\n        2   Lewis  21.0    True\n        3    John  33.0    True\n        4    Myla  26.0   False\n\n        Notice the uncounted NA values:\n\n        >>> df.count()\n        Person    5\n        Age       4\n        Single    5\n        dtype: int64\n\n        Counts for each **row**:\n\n        >>> df.count(axis=\"columns\")\n        0    3\n        1    2\n        2    3\n        3    3\n        4    3\n        dtype: int64\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 15691, "code": "    def _reduce_axis1(self, name: str, func, skipna: bool) -> Series:\n        if name == \"all\":\n            result = np.ones(len(self), dtype=bool)\n            ufunc = np.logical_and\n        elif name == \"any\":\n            result = np.zeros(len(self), dtype=bool)\n            ufunc = np.logical_or  # type: ignore[assignment]\n        else:\n            raise NotImplementedError(name)\n        for blocks in self._mgr.blocks:\n            middle = func(blocks.values, axis=0, skipna=skipna)", "documentation": "        \"\"\"\n        Special case for _reduce to try to avoid a potentially-expensive transpose.\n\n        Apply the reduction block-wise along axis=1 and then reduce the resulting\n        1D arrays.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 17699, "code": "    def nunique(self, axis: Axis = 0, dropna: bool = True) -> Series:\n        return self.apply(Series.nunique, axis=axis, dropna=dropna)", "documentation": "        \"\"\"\n        Count number of distinct elements in specified axis.\n\n        Return Series with number of distinct elements. Can ignore NaN\n        values.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for\n            column-wise.\n        dropna : bool, default True\n            Don't include NaN in the counts.\n\n        Returns\n        -------\n        Series\n            Series with counts of unique values per row or column, depending on `axis`.\n\n        See Also\n        --------\n        Series.nunique: Method nunique for Series.\n        DataFrame.count: Count non-NA cells for each column or row.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"A\": [4, 5, 6], \"B\": [4, 1, 1]})\n        >>> df.nunique()\n        A    3\n        B    2\n        dtype: int64\n\n        >>> df.nunique(axis=1)\n        0    1\n        1    2\n        2    2\n        dtype: int64\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 17942, "code": "    def _get_agg_axis(self, axis_num: int) -> Index:\n        if axis_num == 0:\n            return self.columns\n        elif axis_num == 1:\n            return self.index\n        else:\n            raise ValueError(f\"Axis must be 0 or 1 (got {axis_num!r})\")", "documentation": "        \"\"\"\n        Let's be explicit about this.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 18422, "code": "    def isin(self, values: Series | DataFrame | Sequence | Mapping) -> DataFrame:\n        if isinstance(values, dict):\n            from pandas.core.reshape.concat import concat\n            values = collections.defaultdict(list, values)\n            result = concat(\n                (\n                    self.iloc[:, [i]].isin(values[col])\n                    for i, col in enumerate(self.columns)\n                ),\n                axis=1,\n            )", "documentation": "        \"\"\"\n        Whether each element in the DataFrame is contained in values.\n\n        Parameters\n        ----------\n        values : iterable, Series, DataFrame or dict\n            The result will only be true at a location if all the\n            labels match. If `values` is a Series, that's the index. If\n            `values` is a dict, the keys must be the column names,\n            which must match. If `values` is a DataFrame,\n            then both the index and column labels must match.\n\n        Returns\n        -------\n        DataFrame\n            DataFrame of booleans showing whether each element in the DataFrame\n            is contained in values.\n\n        See Also\n        --------\n        DataFrame.eq: Equality test for DataFrame.\n        Series.isin: Equivalent method on Series.\n        Series.str.contains: Test if pattern or regex is contained within a\n            string of a Series or Index.\n\n        Notes\n        -----\n            ``__iter__`` is used (and not ``__contains__``) to iterate over values\n            when checking if it contains the elements in DataFrame.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\"num_legs\": [2, 4], \"num_wings\": [2, 0]}, index=[\"falcon\", \"dog\"]\n        ... )\n        >>> df\n                num_legs  num_wings\n        falcon         2          2\n        dog            4          0\n\n        When ``values`` is a list check whether every value in the DataFrame\n        is present in the list (which animals have 0 or 2 legs or wings)\n\n        >>> df.isin([0, 2])\n                num_legs  num_wings\n        falcon      True       True\n        dog        False       True\n\n        To check if ``values`` is *not* in the DataFrame, use the ``~`` operator:\n\n        >>> ~df.isin([0, 2])\n                num_legs  num_wings\n        falcon     False      False\n        dog         True      False\n\n        When ``values`` is a dict, we can pass values to check for each\n        column separately:\n\n        >>> df.isin({\"num_wings\": [0, 3]})\n                num_legs  num_wings\n        falcon     False      False\n        dog        False       True\n\n        When ``values`` is a Series or DataFrame the index and column must\n        match. Note that 'falcon' does not match based on the number of legs\n        in other.\n\n        >>> other = pd.DataFrame(\n        ...     {\"num_legs\": [8, 3], \"num_wings\": [0, 2]}, index=[\"spider\", \"falcon\"]\n        ... )\n        >>> df.isin(other)\n                num_legs  num_wings\n        falcon     False       True\n        dog        False      False\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 18642, "code": "    def _to_dict_of_blocks(self):\n        mgr = self._mgr\n        return {\n            k: self._constructor_from_mgr(v, axes=v.axes).__finalize__(self)\n            for k, v in mgr.to_iter_dict()\n        }\n    @property", "documentation": "        \"\"\"\n        Return a dict of dtype -> Constructor Types that\n        each is a homogeneous dtype.\n\n        Internal ONLY.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 18656, "code": "    def values(self) -> np.ndarray:\n        return self._mgr.as_array()", "documentation": "        \"\"\"\n        Return a Numpy representation of the DataFrame.\n\n        .. warning::\n\n           We recommend using :meth:`DataFrame.to_numpy` instead.\n\n        Only the values in the DataFrame will be returned, the axes labels\n        will be removed.\n\n        Returns\n        -------\n        numpy.ndarray\n            The values of the DataFrame.\n\n        See Also\n        --------\n        DataFrame.to_numpy : Recommended alternative to this method.\n        DataFrame.index : Retrieve the index labels.\n        DataFrame.columns : Retrieving the column names.\n\n        Notes\n        -----\n        The dtype will be a lower-common-denominator dtype (implicit\n        upcasting); that is to say if the dtypes (even of numeric types)\n        are mixed, the one that accommodates all will be chosen. Use this\n        with care if you are not dealing with the blocks.\n\n        e.g. If the dtypes are float16 and float32, dtype will be upcast to\n        float32.  If dtypes are int32 and uint8, dtype will be upcast to\n        int32. By :func:`numpy.find_common_type` convention, mixing int64\n        and uint64 will result in a float64 dtype.\n\n        Examples\n        --------\n        A DataFrame where all columns are the same type (e.g., int64) results\n        in an array of the same type.\n\n        >>> df = pd.DataFrame(\n        ...     {\"age\": [3, 29], \"height\": [94, 170], \"weight\": [31, 115]}\n        ... )\n        >>> df\n           age  height  weight\n        0    3      94      31\n        1   29     170     115\n        >>> df.dtypes\n        age       int64\n        height    int64\n        weight    int64\n        dtype: object\n        >>> df.values\n        array([[  3,  94,  31],\n               [ 29, 170, 115]])\n\n        A DataFrame with mixed type columns(e.g., str/object, int64, float32)\n        results in an ndarray of the broadest type that accommodates these\n        mixed types (e.g., object).\n\n        >>> df2 = pd.DataFrame(\n        ...     [\n        ...         (\"parrot\", 24.0, \"second\"),\n        ...         (\"lion\", 80.5, 1),\n        ...         (\"monkey\", np.nan, None),\n        ...     ],\n        ...     columns=(\"name\", \"max_speed\", \"rank\"),\n        ... )\n        >>> df2.dtypes\n        name             str\n        max_speed    float64\n        rank          object\n        dtype: object\n        >>> df2.values\n        array([['parrot', 24.0, 'second'],\n               ['lion', 80.5, 1],\n               ['monkey', nan, None]], dtype=object)\n        \"\"\""}], "after_segments": [{"filename": "pandas/core/frame.py", "start_line": 270, "code": "class DataFrame(NDFrame, OpsMixin):\n    _internal_names_set = {\"columns\", \"index\"} | NDFrame._internal_names_set\n    _typ = \"dataframe\"\n    _HANDLED_TYPES = (Series, Index, ExtensionArray, np.ndarray)\n    _accessors: set[str] = {\"sparse\"}\n    _hidden_attrs: frozenset[str] = NDFrame._hidden_attrs | frozenset([])\n    _mgr: BlockManager\n    __pandas_priority__ = 4000\n    @property", "documentation": "    \"\"\"\n    Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\n    Data structure also contains labeled axes (rows and columns).\n    Arithmetic operations align on both row and column labels. Can be\n    thought of as a dict-like container for Series objects. The primary\n    pandas data structure.\n\n    Parameters\n    ----------\n    data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n        Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n        data is a dict, column order follows insertion-order. If a dict contains Series\n        which have an index defined, it is aligned by its index. This alignment also\n        occurs if data is a Series or a DataFrame itself. Alignment is done on\n        Series/DataFrame inputs.\n\n        If data is a list of dicts, column order follows insertion-order.\n\n    index : Index or array-like\n        Index to use for resulting frame. Will default to RangeIndex if\n        no indexing information part of input data and no index provided.\n    columns : Index or array-like\n        Column labels to use for resulting frame when data does not have them,\n        defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n        will perform column selection instead.\n    dtype : dtype, default None\n        Data type to force. Only a single dtype is allowed. If None, infer.\n        If ``data`` is DataFrame then is ignored.\n    copy : bool or None, default None\n        Copy data from inputs.\n        For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n        or 2d ndarray input, the default of None behaves like ``copy=False``.\n        If data is a dict containing one or more Series (possibly of different dtypes),\n        ``copy=False`` will ensure that these inputs are not copied.\n\n    See Also\n    --------\n    DataFrame.from_records : Constructor from tuples, also record arrays.\n    DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n    read_csv : Read a comma-separated values (csv) file into DataFrame.\n    read_table : Read general delimited file into DataFrame.\n    read_clipboard : Read text from clipboard into DataFrame.\n\n    Notes\n    -----\n    Please reference the :ref:`User Guide <basics.dataframe>` for more information.\n\n    Examples\n    --------\n    Constructing DataFrame from a dictionary.\n\n    >>> d = {\"col1\": [1, 2], \"col2\": [3, 4]}\n    >>> df = pd.DataFrame(data=d)\n    >>> df\n       col1  col2\n    0     1     3\n    1     2     4\n\n    Notice that the inferred dtype is int64.\n\n    >>> df.dtypes\n    col1    int64\n    col2    int64\n    dtype: object\n\n    To enforce a single dtype:\n\n    >>> df = pd.DataFrame(data=d, dtype=np.int8)\n    >>> df.dtypes\n    col1    int8\n    col2    int8\n    dtype: object\n\n    Constructing DataFrame from a dictionary including Series:\n\n    >>> d = {\"col1\": [0, 1, 2, 3], \"col2\": pd.Series([2, 3], index=[2, 3])}\n    >>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n       col1  col2\n    0     0   NaN\n    1     1   NaN\n    2     2   2.0\n    3     3   3.0\n\n    Constructing DataFrame from numpy ndarray:\n\n    >>> df2 = pd.DataFrame(\n    ...     np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=[\"a\", \"b\", \"c\"]\n    ... )\n    >>> df2\n       a  b  c\n    0  1  2  3\n    1  4  5  6\n    2  7  8  9\n\n    Constructing DataFrame from a numpy ndarray that has labeled columns:\n\n    >>> data = np.array(\n    ...     [(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n    ...     dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")],\n    ... )\n    >>> df3 = pd.DataFrame(data, columns=[\"c\", \"a\"])\n    >>> df3\n       c  a\n    0  3  1\n    1  6  4\n    2  9  7\n\n    Constructing DataFrame from dataclass:\n\n    >>> from dataclasses import make_dataclass\n    >>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n    >>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n       x  y\n    0  0  0\n    1  0  3\n    2  2  3\n\n    Constructing DataFrame from Series/DataFrame:\n\n    >>> ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n    >>> df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n    >>> df\n       0\n    a  1\n    c  3\n\n    >>> df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n    >>> df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n    >>> df2\n       x\n    a  1\n    c  3\n    \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 741, "code": "    def __arrow_c_stream__(self, requested_schema=None):\n        pa = import_optional_dependency(\"pyarrow\", min_version=\"14.0.0\")\n        if requested_schema is not None:\n            requested_schema = pa.Schema._import_from_c_capsule(requested_schema)\n        table = pa.Table.from_pandas(self, schema=requested_schema)\n        return table.__arrow_c_stream__()\n    @property", "documentation": "        \"\"\"\n        Export the pandas DataFrame as an Arrow C stream PyCapsule.\n\n        This relies on pyarrow to convert the pandas DataFrame to the Arrow\n        format (and follows the default behaviour of ``pyarrow.Table.from_pandas``\n        in its handling of the index, i.e. store the index as a column except\n        for RangeIndex).\n        This conversion is not necessarily zero-copy.\n\n        Parameters\n        ----------\n        requested_schema : PyCapsule, default None\n            The schema to which the dataframe should be casted, passed as a\n            PyCapsule containing a C ArrowSchema representation of the\n            requested schema.\n\n        Returns\n        -------\n        PyCapsule\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 771, "code": "    def axes(self) -> list[Index]:\n        return [self.index, self.columns]\n    @property", "documentation": "        \"\"\"\n        Return a list representing the axes of the DataFrame.\n\n        It has the row axis labels and column axis labels as the only members.\n        They are returned in that order.\n\n        See Also\n        --------\n        DataFrame.index: The index (row labels) of the DataFrame.\n        DataFrame.columns: The column labels of the DataFrame.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"col1\": [1, 2], \"col2\": [3, 4]})\n        >>> df.axes\n        [RangeIndex(start=0, stop=2, step=1), Index(['col1', 'col2'], dtype='str')]\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 792, "code": "    def shape(self) -> tuple[int, int]:\n        return len(self.index), len(self.columns)\n    @property", "documentation": "        \"\"\"\n        Return a tuple representing the dimensionality of the DataFrame.\n\n        Unlike the `len()` method, which only returns the number of rows, `shape`\n        provides both row and column counts, making it a more informative method for\n        understanding dataset size.\n\n        See Also\n        --------\n        numpy.ndarray.shape : Tuple of array dimensions.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"col1\": [1, 2], \"col2\": [3, 4]})\n        >>> df.shape\n        (2, 2)\n\n        >>> df = pd.DataFrame({\"col1\": [1, 2], \"col2\": [3, 4], \"col3\": [5, 6]})\n        >>> df.shape\n        (2, 3)\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 817, "code": "    def _is_homogeneous_type(self) -> bool:\n        return len({block.values.dtype for block in self._mgr.blocks}) <= 1\n    @property", "documentation": "        \"\"\"\n        Whether all the columns in a DataFrame have the same type.\n\n        Returns\n        -------\n        bool\n\n        Examples\n        --------\n        >>> DataFrame({\"A\": [1, 2], \"B\": [3, 4]})._is_homogeneous_type\n        True\n        >>> DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.0]})._is_homogeneous_type\n        False\n\n        Items with the same type but different sizes are considered\n        different types.\n\n        >>> DataFrame(\n        ...     {\n        ...         \"A\": np.array([1, 2], dtype=np.int32),\n        ...         \"B\": np.array([1, 2], dtype=np.int64),\n        ...     }\n        ... )._is_homogeneous_type\n        False\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 847, "code": "    def _can_fast_transpose(self) -> bool:\n        blocks = self._mgr.blocks\n        if len(blocks) != 1:\n            return False\n        dtype = blocks[0].dtype\n        return not is_1d_only_ea_dtype(dtype)\n    @property", "documentation": "        \"\"\"\n        Can we transpose this DataFrame without creating any new array objects.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 860, "code": "    def _values(self) -> np.ndarray | DatetimeArray | TimedeltaArray | PeriodArray:\n        mgr = self._mgr\n        blocks = mgr.blocks\n        if len(blocks) != 1:\n            return ensure_wrapped_if_datetimelike(self.values)\n        arr = blocks[0].values\n        if arr.ndim == 1:\n            return self.values\n        arr = cast(\"np.ndarray | DatetimeArray | TimedeltaArray | PeriodArray\", arr)\n        return arr.T", "documentation": "        \"\"\"\n        Analogue to ._values that may return a 2D ExtensionArray.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 882, "code": "    def _repr_fits_vertical_(self) -> bool:\n        max_rows = get_option(\"display.max_rows\")\n        return len(self) <= max_rows", "documentation": "        \"\"\"\n        Check length against max_rows.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 889, "code": "    def _repr_fits_horizontal_(self) -> bool:\n        width, height = console.get_console_size()\n        max_columns = get_option(\"display.max_columns\")\n        nb_columns = len(self.columns)\n        if (max_columns and nb_columns > max_columns) or (\n            width and nb_columns > (width // 2)\n        ):\n            return False\n        if width is None or not console.in_interactive_session():\n            return True\n        if get_option(\"display.width\") is not None or console.in_ipython_frontend():", "documentation": "        \"\"\"\n        Check if full repr fits in horizontal boundaries imposed by the display\n        options width and max_columns.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 936, "code": "    def _info_repr(self) -> bool:\n        info_repr_option = get_option(\"display.large_repr\") == \"info\"\n        return info_repr_option and not (\n            self._repr_fits_horizontal_() and self._repr_fits_vertical_()\n        )", "documentation": "        \"\"\"\n        True if the repr should show the info view.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 945, "code": "    def __repr__(self) -> str:\n        if self._info_repr():\n            buf = StringIO()\n            self.info(buf=buf)\n            return buf.getvalue()\n        repr_params = fmt.get_dataframe_repr_params()\n        return self.to_string(**repr_params)", "documentation": "        \"\"\"\n        Return a string representation for a particular DataFrame.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 957, "code": "    def _repr_html_(self) -> str | None:\n        if self._info_repr():\n            buf = StringIO()\n            self.info(buf=buf)\n            val = buf.getvalue().replace(\"<\", r\"&lt;\", 1)\n            val = val.replace(\">\", r\"&gt;\", 1)\n            return f\"<pre>{val}</pre>\"\n        if get_option(\"display.notebook_repr_html\"):\n            max_rows = get_option(\"display.max_rows\")\n            min_rows = get_option(\"display.min_rows\")\n            max_cols = get_option(\"display.max_columns\")", "documentation": "        \"\"\"\n        Return a html representation for a particular DataFrame.\n\n        Mainly for IPython notebook.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 1216, "code": "    def style(self) -> Styler:\n        has_jinja2 = import_optional_dependency(\"jinja2\", errors=\"ignore\")\n        if not has_jinja2:\n            raise AttributeError(\"The '.style' accessor requires jinja2\")\n        from pandas.io.formats.style import Styler\n        return Styler(self)\n    _shared_docs[\"items\"] = r\"\"\"\n        Iterate over (column name, Series) pairs.\n        Iterates over the DataFrame columns, returning a tuple with\n        the column name and the content as a Series.\n        Yields", "documentation": "        \"\"\"\n        Returns a Styler object.\n\n        Contains methods for building a styled HTML representation of the DataFrame.\n\n        See Also\n        --------\n        io.formats.style.Styler : Helps style a DataFrame or Series according to the\n            data with HTML and CSS.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"A\": [1, 2, 3]})\n        >>> df.style  # doctest: +SKIP\n\n        Please see\n        `Table Visualization <../../user_guide/style.ipynb>`_ for more examples.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 1346, "code": "    def iterrows(self) -> Iterable[tuple[Hashable, Series]]:\n        columns = self.columns\n        klass = self._constructor_sliced\n        for k, v in zip(self.index, self.values, strict=True):\n            s = klass(v, index=columns, name=k).__finalize__(self)\n            if self._mgr.is_single_block:\n                s._mgr.add_references(self._mgr)\n            yield k, s", "documentation": "        \"\"\"\n        Iterate over DataFrame rows as (index, Series) pairs.\n\n        Yields\n        ------\n        index : label or tuple of label\n            The index of the row. A tuple for a `MultiIndex`.\n        data : Series\n            The data of the row as a Series.\n\n        See Also\n        --------\n        DataFrame.itertuples : Iterate over DataFrame rows as namedtuples of the values.\n        DataFrame.items : Iterate over (column name, Series) pairs.\n\n        Notes\n        -----\n        1. Because ``iterrows`` returns a Series for each row,\n           it does **not** preserve dtypes across the rows (dtypes are\n           preserved across columns for DataFrames).\n\n           To preserve dtypes while iterating over the rows, it is better\n           to use :meth:`itertuples` which returns namedtuples of the values\n           and which is generally faster than ``iterrows``.\n\n        2. You should **never modify** something you are iterating over.\n           This is not guaranteed to work in all cases. Depending on the\n           data types, the iterator returns a copy and not a view, and writing\n           to it will have no effect.\n\n        Examples\n        --------\n\n        >>> df = pd.DataFrame([[1, 1.5]], columns=[\"int\", \"float\"])\n        >>> row = next(df.iterrows())[1]\n        >>> row\n        int      1.0\n        float    1.5\n        Name: 0, dtype: float64\n        >>> print(row[\"int\"].dtype)\n        float64\n        >>> print(df[\"int\"].dtype)\n        int64\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 1481, "code": "    def __len__(self) -> int:\n        return len(self.index)\n    @overload", "documentation": "        \"\"\"\n        Returns length of info axis, but here we use the index.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 1493, "code": "    def dot(self, other: AnyArrayLike | DataFrame) -> DataFrame | Series:\n        if isinstance(other, (Series, DataFrame)):\n            common = self.columns.union(other.index)\n            if len(common) > len(self.columns) or len(common) > len(other.index):\n                raise ValueError(\"matrices are not aligned\")\n            left = self.reindex(columns=common)\n            right = other.reindex(index=common)\n            lvals = left.values\n            rvals = right._values\n        else:\n            left = self", "documentation": "        \"\"\"\n        Compute the matrix multiplication between the DataFrame and other.\n\n        This method computes the matrix product between the DataFrame and the\n        values of an other Series, DataFrame or a numpy array.\n\n        It can also be called using ``self @ other``.\n\n        Parameters\n        ----------\n        other : Series, DataFrame or array-like\n            The other object to compute the matrix product with.\n\n        Returns\n        -------\n        Series or DataFrame\n            If other is a Series, return the matrix product between self and\n            other as a Series. If other is a DataFrame or a numpy.array, return\n            the matrix product of self and other in a DataFrame of a np.array.\n\n        See Also\n        --------\n        Series.dot: Similar method for Series.\n\n        Notes\n        -----\n        The dimensions of DataFrame and other must be compatible in order to\n        compute the matrix multiplication. In addition, the column names of\n        DataFrame and the index of other must contain the same values, as they\n        will be aligned prior to the multiplication.\n\n        The dot method for Series computes the inner product, instead of the\n        matrix product here.\n\n        Examples\n        --------\n        Here we multiply a DataFrame with a Series.\n\n        >>> df = pd.DataFrame([[0, 1, -2, -1], [1, 1, 1, 1]])\n        >>> s = pd.Series([1, 1, 2, 1])\n        >>> df.dot(s)\n        0    -4\n        1     5\n        dtype: int64\n\n        Here we multiply a DataFrame with another DataFrame.\n\n        >>> other = pd.DataFrame([[0, 1], [1, 2], [-1, -1], [2, 0]])\n        >>> df.dot(other)\n            0   1\n        0   1   4\n        1   2   2\n\n        Note that the dot method give the same result as @\n\n        >>> df @ other\n            0   1\n        0   1   4\n        1   2   2\n\n        The dot method works also if other is an np.array.\n\n        >>> arr = np.array([[0, 1], [1, 2], [-1, -1], [2, 0]])\n        >>> df.dot(arr)\n            0   1\n        0   1   4\n        1   2   2\n\n        Note how shuffling of the objects does not change the result.\n\n        >>> s2 = s.reindex([1, 0, 2, 3])\n        >>> df.dot(s2)\n        0    -4\n        1     5\n        dtype: int64\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 1617, "code": "    def __matmul__(self, other: AnyArrayLike | DataFrame) -> DataFrame | Series:\n        return self.dot(other)", "documentation": "        \"\"\"\n        Matrix multiplication using binary `@` operator.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 1623, "code": "    def __rmatmul__(self, other) -> DataFrame:\n        try:\n            return self.T.dot(np.transpose(other)).T\n        except ValueError as err:\n            if \"shape mismatch\" not in str(err):\n                raise\n            msg = f\"shapes {np.shape(other)} and {self.shape} not aligned\"\n            raise ValueError(msg) from err\n    @classmethod", "documentation": "        \"\"\"\n        Matrix multiplication using binary `@` operator.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 2641, "code": "    def to_feather(self, path: FilePath | WriteBuffer[bytes], **kwargs) -> None:\n        from pandas.io.feather_format import to_feather\n        to_feather(self, path, **kwargs)\n    @overload", "documentation": "        \"\"\"\n        Write a DataFrame to the binary Feather format.\n\n        Parameters\n        ----------\n        path : str, path object, file-like object\n            String, path object (implementing ``os.PathLike[str]``), or file-like\n            object implementing a binary ``write()`` function. If a string or a path,\n            it will be used as Root Directory path when writing a partitioned dataset.\n        **kwargs :\n            Additional keywords passed to :func:`pyarrow.feather.write_feather`.\n            This includes the `compression`, `compression_level`, `chunksize`\n            and `version` keywords.\n\n        See Also\n        --------\n        DataFrame.to_parquet : Write a DataFrame to the binary parquet format.\n        DataFrame.to_excel : Write object to an Excel sheet.\n        DataFrame.to_sql : Write to a sql table.\n        DataFrame.to_csv : Write a csv file.\n        DataFrame.to_json : Convert the object to a JSON string.\n        DataFrame.to_html : Render a DataFrame as an HTML table.\n        DataFrame.to_string : Convert DataFrame to a string.\n\n        Notes\n        -----\n        This function writes the dataframe as a `feather file\n        <https://arrow.apache.org/docs/python/feather.html>`_. Requires a default\n        index. For saving the DataFrame with your custom index use a method that\n        supports custom indices e.g. `to_parquet`.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6]])\n        >>> df.to_feather(\"file.feather\")  # doctest: +SKIP\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 3836, "code": "    def memory_usage(self, index: bool = True, deep: bool = False) -> Series:\n        result = self._constructor_sliced(\n            [c.memory_usage(index=False, deep=deep) for col, c in self.items()],\n            index=self.columns,\n            dtype=np.intp,\n        )\n        if index:\n            index_memory_usage = self._constructor_sliced(\n                self.index.memory_usage(deep=deep), index=[\"Index\"]\n            )\n            result = index_memory_usage._append_internal(result)", "documentation": "        \"\"\"\n        Return the memory usage of each column in bytes.\n\n        The memory usage can optionally include the contribution of\n        the index and elements of `object` dtype.\n\n        This value is displayed in `DataFrame.info` by default. This can be\n        suppressed by setting ``pandas.options.display.memory_usage`` to False.\n\n        Parameters\n        ----------\n        index : bool, default True\n            Specifies whether to include the memory usage of the DataFrame's\n            index in returned Series. If ``index=True``, the memory usage of\n            the index is the first item in the output.\n        deep : bool, default False\n            If True, introspect the data deeply by interrogating\n            `object` dtypes for system-level memory consumption, and include\n            it in the returned values.\n\n        Returns\n        -------\n        Series\n            A Series whose index is the original column names and whose values\n            is the memory usage of each column in bytes.\n\n        See Also\n        --------\n        numpy.ndarray.nbytes : Total bytes consumed by the elements of an\n            ndarray.\n        Series.memory_usage : Bytes consumed by a Series.\n        Categorical : Memory-efficient array for string values with\n            many repeated values.\n        DataFrame.info : Concise summary of a DataFrame.\n\n        Notes\n        -----\n        See the :ref:`Frequently Asked Questions <df-memory-usage>` for more\n        details.\n\n        Examples\n        --------\n        >>> dtypes = [\"int64\", \"float64\", \"complex128\", \"object\", \"bool\"]\n        >>> data = dict([(t, np.ones(shape=5000, dtype=int).astype(t)) for t in dtypes])\n        >>> df = pd.DataFrame(data)\n        >>> df.head()\n           int64  float64            complex128  object  bool\n        0      1      1.0              1.0+0.0j       1  True\n        1      1      1.0              1.0+0.0j       1  True\n        2      1      1.0              1.0+0.0j       1  True\n        3      1      1.0              1.0+0.0j       1  True\n        4      1      1.0              1.0+0.0j       1  True\n\n        >>> df.memory_usage()\n        Index           132\n        int64         40000\n        float64       40000\n        complex128    80000\n        object        40000\n        bool           5000\n        dtype: int64\n\n        >>> df.memory_usage(index=False)\n        int64         40000\n        float64       40000\n        complex128    80000\n        object        40000\n        bool           5000\n        dtype: int64\n\n        The memory footprint of `object` dtype columns is ignored by default:\n\n        >>> df.memory_usage(deep=True)\n        Index            132\n        int64          40000\n        float64        40000\n        complex128     80000\n        object        180000\n        bool            5000\n        dtype: int64\n\n        Use a Categorical for efficient storage of an object-dtype column with\n        many repeated values.\n\n        >>> df[\"object\"].astype(\"category\").memory_usage(deep=True)\n        5140\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 4122, "code": "    def T(self) -> DataFrame:\n        return self.transpose()", "documentation": "        \"\"\"\n        The transpose of the DataFrame.\n\n        This property returns a DataFrame with rows and columns interchanged,\n        reflecting the data across the main diagonal.\n\n        Returns\n        -------\n        DataFrame\n            The transposed DataFrame.\n\n        See Also\n        --------\n        DataFrame.transpose : Transpose index and columns.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"col1\": [1, 2], \"col2\": [3, 4]})\n        >>> df\n           col1  col2\n        0     1     3\n        1     2     4\n\n        >>> df.T\n              0  1\n        col1  1  2\n        col2  3  4\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 4156, "code": "    def _ixs(self, i: int, axis: AxisInt = 0) -> Series:\n        if axis == 0:\n            new_mgr = self._mgr.fast_xs(i)\n            result = self._constructor_sliced_from_mgr(new_mgr, axes=new_mgr.axes)\n            result._name = self.index[i]\n            return result.__finalize__(self)\n        else:\n            col_mgr = self._mgr.iget(i)\n            return self._box_col_values(col_mgr, i)", "documentation": "        \"\"\"\n        Parameters\n        ----------\n        i : int\n        axis : int\n\n        Returns\n        -------\n        Series\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 4180, "code": "    def _get_column_array(self, i: int) -> ArrayLike:\n        return self._mgr.iget_values(i)", "documentation": "        \"\"\"\n        Get the values of the i'th column (ndarray or ExtensionArray, as stored\n        in the Block)\n\n        Warning! The returned array is a view but doesn't handle Copy-on-Write,\n        so this should be used with caution (for read-only purposes).\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 4190, "code": "    def _iter_column_arrays(self) -> Iterator[ArrayLike]:\n        for i in range(len(self.columns)):\n            yield self._get_column_array(i)", "documentation": "        \"\"\"\n        Iterate over the arrays of all columns in order.\n        This returns the values as stored in the Block (ndarray or ExtensionArray).\n\n        Warning! The returned array is a view but doesn't handle Copy-on-Write,\n        so this should be used with caution (for read-only purposes).\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 4330, "code": "    def _get_value(self, index, col, takeable: bool = False) -> Scalar:\n        if takeable:\n            series = self._ixs(col, axis=1)\n            return series._values[index]\n        series = self._get_item(col)\n        if not isinstance(self.index, MultiIndex):\n            row = self.index.get_loc(index)\n            return series._values[row]\n        loc = self.index._engine.get_loc(index)\n        return series._values[loc]", "documentation": "        \"\"\"\n        Quickly retrieve single value at passed column and index.\n\n        Parameters\n        ----------\n        index : row label\n        col : column label\n        takeable : interpret the index/col as indexers, default False\n\n        Returns\n        -------\n        scalar\n\n        Notes\n        -----\n        Assumes that both `self.index._index_as_unique` and\n        `self.columns._index_as_unique`; Caller is responsible for checking.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 4367, "code": "    def isetitem(self, loc, value) -> None:\n        if isinstance(value, DataFrame):\n            if is_integer(loc):\n                loc = [loc]\n            if len(loc) != len(value.columns):\n                raise ValueError(\n                    f\"Got {len(loc)} positions but value has {len(value.columns)} \"\n                    f\"columns.\"\n                )\n            for i, idx in enumerate(loc):\n                arraylike, refs = self._sanitize_column(value.iloc[:, i])", "documentation": "        \"\"\"\n        Set the given value in the column with position `loc`.\n\n        This is a positional analogue to ``__setitem__``.\n\n        Parameters\n        ----------\n        loc : int or sequence of ints\n            Index position for the column.\n        value : scalar or arraylike\n            Value(s) for the column.\n\n        See Also\n        --------\n        DataFrame.iloc : Purely integer-location based indexing for selection by\n            position.\n\n        Notes\n        -----\n        ``frame.isetitem(loc, value)`` is an in-place method as it will\n        modify the DataFrame in place (not returning a new object). In contrast to\n        ``frame.iloc[:, i] = value`` which will try to update the existing values in\n        place, ``frame.isetitem(loc, value)`` will not update the values of the column\n        itself in place, it will instead insert a new array.\n\n        In cases where ``frame.columns`` is unique, this is equivalent to\n        ``frame[frame.columns[i]] = value``.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n        >>> df.isetitem(1, [5, 6])\n        >>> df\n              A  B\n        0     1  5\n        1     2  6\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 4423, "code": "    def __setitem__(self, key, value) -> None:\n        if not CHAINED_WARNING_DISABLED:\n            if sys.getrefcount(self) <= REF_COUNT and not com.is_local_in_caller_frame(\n                self\n            ):\n                warnings.warn(\n                    _chained_assignment_msg, ChainedAssignmentError, stacklevel=2\n                )\n        key = com.apply_if_callable(key, self)\n        if isinstance(key, slice):\n            slc = self.index._convert_slice_indexer(key, kind=\"getitem\")", "documentation": "        \"\"\"\n        Set item(s) in DataFrame by key.\n\n        This method allows you to set the values of one or more columns in the\n        DataFrame using a key. If the key does not exist, a new\n        column will be created.\n\n        Parameters\n        ----------\n        key : The object(s) in the index which are to be assigned to\n            Column label(s) to set. Can be a single column name, list of column names,\n            or tuple for MultiIndex columns.\n        value : scalar, array-like, Series, or DataFrame\n            Value(s) to set for the specified key(s).\n\n        Returns\n        -------\n        None\n            This method does not return a value.\n\n        See Also\n        --------\n        DataFrame.loc : Access and set values by label-based indexing.\n        DataFrame.iloc : Access and set values by position-based indexing.\n        DataFrame.assign : Assign new columns to a DataFrame.\n\n        Notes\n        -----\n        When assigning a Series to a DataFrame column, pandas aligns the Series\n        by index labels, not by position. This means:\n\n        * Values from the Series are matched to DataFrame rows by index label\n        * If a Series index label doesn't exist in the DataFrame index, it's ignored\n        * If a DataFrame index label doesn't exist in the Series index, NaN is assigned\n        * The order of values in the Series doesn't matter; only the index labels matter\n\n        Examples\n        --------\n        Basic column assignment:\n\n        >>> df = pd.DataFrame({\"A\": [1, 2, 3]})\n        >>> df[\"B\"] = [4, 5, 6]  # Assigns by position\n        >>> df\n            A  B\n        0  1  4\n        1  2  5\n        2  3  6\n\n        Series assignment with index alignment:\n\n        >>> df = pd.DataFrame({\"A\": [1, 2, 3]}, index=[0, 1, 2])\n        >>> s = pd.Series([10, 20], index=[1, 3])  # Note: index 3 doesn't exist in df\n        >>> df[\"B\"] = s  # Assigns by index label, not position\n        >>> df\n           A     B\n        0  1   NaN\n        1  2  10.0\n        2  3   NaN\n\n        Series assignment with partial index match:\n\n        >>> df = pd.DataFrame({\"A\": [1, 2, 3, 4]}, index=[\"a\", \"b\", \"c\", \"d\"])\n        >>> s = pd.Series([100, 200], index=[\"b\", \"d\"])\n        >>> df[\"B\"] = s\n        >>> df\n           A      B\n        a  1    NaN\n        b  2  100.0\n        c  3    NaN\n        d  4  200.0\n\n        Series index labels NOT in DataFrame, ignored:\n\n        >>> df = pd.DataFrame({\"A\": [1, 2, 3]}, index=[\"x\", \"y\", \"z\"])\n        >>> s = pd.Series([10, 20, 30, 40, 50], index=[\"x\", \"y\", \"a\", \"b\", \"z\"])\n        >>> df[\"B\"] = s\n        >>> df\n           A   B\n        x  1  10\n        y  2  20\n        z  3  50\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 4726, "code": "    def _set_item(self, key, value) -> None:\n        value, refs = self._sanitize_column(value)\n        if (\n            key in self.columns\n            and value.ndim == 1\n            and not isinstance(value.dtype, ExtensionDtype)\n        ):\n            if not self.columns.is_unique or isinstance(self.columns, MultiIndex):\n                existing_piece = self[key]\n                if isinstance(existing_piece, DataFrame):\n                    value = np.tile(value, (len(existing_piece.columns), 1)).T", "documentation": "        \"\"\"\n        Add series to DataFrame in specified column.\n\n        If series is a numpy-array (not a Series/TimeSeries), it must be the\n        same length as the DataFrames index or an error will be thrown.\n\n        Series/TimeSeries will be conformed to the DataFrames index to\n        ensure homogeneity.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 4795, "code": "    def _ensure_valid_index(self, value) -> None:\n        if not len(self.index) and is_list_like(value) and len(value):\n            if not isinstance(value, DataFrame):\n                try:\n                    value = Series(value)\n                except (ValueError, NotImplementedError, TypeError) as err:\n                    raise ValueError(\n                        \"Cannot set a frame with no defined index \"\n                        \"and a value that cannot be converted to a Series\"\n                    ) from err\n            index_copy = value.index.copy()", "documentation": "        \"\"\"\n        Ensure that if we don't have an index, that we can create one from the\n        passed value.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 4818, "code": "    def _box_col_values(self, values: SingleBlockManager, loc: int) -> Series:\n        name = self.columns[loc]\n        obj = self._constructor_sliced_from_mgr(values, axes=values.axes)\n        obj._name = name\n        return obj.__finalize__(self)", "documentation": "        \"\"\"\n        Provide boxed values for a column.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 5087, "code": "    def eval(self, expr: str, *, inplace: bool = False, **kwargs) -> Any | None:\n        from pandas.core.computation.eval import eval as _eval\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        kwargs[\"level\"] = kwargs.pop(\"level\", 0) + 1\n        index_resolvers = self._get_index_resolvers()\n        column_resolvers = self._get_cleaned_column_resolvers()\n        resolvers = column_resolvers, index_resolvers\n        if \"target\" not in kwargs:\n            kwargs[\"target\"] = self\n        kwargs[\"resolvers\"] = tuple(kwargs.get(\"resolvers\", ())) + resolvers\n        return _eval(expr, inplace=inplace, **kwargs)", "documentation": "        \"\"\"\n        Evaluate a string describing operations on DataFrame columns.\n\n        .. warning::\n\n            This method can run arbitrary code which can make you vulnerable to code\n            injection if you pass user input to this function.\n\n        Operates on columns only, not specific rows or elements.  This allows\n        `eval` to run arbitrary code, which can make you vulnerable to code\n        injection if you pass user input to this function.\n\n        Parameters\n        ----------\n        expr : str\n            The expression string to evaluate.\n\n            You can refer to variables\n            in the environment by prefixing them with an '@' character like\n            ``@a + b``.\n\n            You can refer to column names that are not valid Python variable names\n            by surrounding them in backticks. Thus, column names containing spaces\n            or punctuation (besides underscores) or starting with digits must be\n            surrounded by backticks. (For example, a column named \"Area (cm^2)\" would\n            be referenced as ```Area (cm^2)```). Column names which are Python keywords\n            (like \"if\", \"for\", \"import\", etc) cannot be used.\n\n            For example, if one of your columns is called ``a a`` and you want\n            to sum it with ``b``, your query should be ```a a` + b``.\n\n            See the documentation for :func:`eval` for full details of\n            supported operations and functions in the expression string.\n        inplace : bool, default False\n            If the expression contains an assignment, whether to perform the\n            operation inplace and mutate the existing DataFrame. Otherwise,\n            a new DataFrame is returned.\n        **kwargs\n            See the documentation for :func:`eval` for complete details\n            on the keyword arguments accepted by\n            :meth:`~pandas.DataFrame.eval`.\n\n        Returns\n        -------\n        ndarray, scalar, pandas object, or None\n            The result of the evaluation or None if ``inplace=True``.\n\n        See Also\n        --------\n        DataFrame.query : Evaluates a boolean expression to query the columns\n            of a frame.\n        DataFrame.assign : Can evaluate an expression or function to create new\n            values for a column.\n        eval : Evaluate a Python expression as a string using various\n            backends.\n\n        Notes\n        -----\n        For more details see the API documentation for :func:`~eval`.\n        For detailed examples see :ref:`enhancing performance with eval\n        <enhancingperf.eval>`.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\"A\": range(1, 6), \"B\": range(10, 0, -2), \"C&C\": range(10, 5, -1)}\n        ... )\n        >>> df\n           A   B  C&C\n        0  1  10   10\n        1  2   8    9\n        2  3   6    8\n        3  4   4    7\n        4  5   2    6\n        >>> df.eval(\"A + B\")\n        0    11\n        1    10\n        2     9\n        3     8\n        4     7\n        dtype: int64\n\n        Assignment is allowed though by default the original DataFrame is not\n        modified.\n\n        >>> df.eval(\"D = A + B\")\n           A   B  C&C   D\n        0  1  10   10  11\n        1  2   8    9  10\n        2  3   6    8   9\n        3  4   4    7   8\n        4  5   2    6   7\n        >>> df\n           A   B  C&C\n        0  1  10   10\n        1  2   8    9\n        2  3   6    8\n        3  4   4    7\n        4  5   2    6\n\n        Multiple columns can be assigned to using multi-line expressions:\n\n        >>> df.eval(\n        ...     '''\n        ... D = A + B\n        ... E = A - B\n        ... '''\n        ... )\n           A   B  C&C   D  E\n        0  1  10   10  11 -9\n        1  2   8    9  10 -6\n        2  3   6    8   9 -3\n        3  4   4    7   8  0\n        4  5   2    6   7  3\n\n        For columns with spaces or other disallowed characters in their name, you can\n        use backtick quoting.\n\n        >>> df.eval(\"B * `C&C`\")\n        0    100\n        1     72\n        2     48\n        3     28\n        4     12\n        dtype: int64\n\n        Local variables shall be explicitly referenced using ``@``\n        character in front of the name:\n\n        >>> local_var = 2\n        >>> df.eval(\"@local_var * A\")\n        0     2\n        1     4\n        2     6\n        3     8\n        4    10\n        Name: A, dtype: int64\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 5239, "code": "    def select_dtypes(self, include=None, exclude=None) -> DataFrame:\n        if not is_list_like(include):\n            include = (include,) if include is not None else ()\n        if not is_list_like(exclude):\n            exclude = (exclude,) if exclude is not None else ()\n        selection = (frozenset(include), frozenset(exclude))\n        if not any(selection):\n            raise ValueError(\"at least one of include or exclude must be nonempty\")", "documentation": "        \"\"\"\n        Return a subset of the DataFrame's columns based on the column dtypes.\n\n        This method allows for filtering columns based on their data types.\n        It is useful when working with heterogeneous DataFrames where operations\n        need to be performed on a specific subset of data types.\n\n        Parameters\n        ----------\n        include, exclude : scalar or list-like\n            A selection of dtypes or strings to be included/excluded. At least\n            one of these parameters must be supplied.\n\n        Returns\n        -------\n        DataFrame\n            The subset of the frame including the dtypes in ``include`` and\n            excluding the dtypes in ``exclude``.\n\n        Raises\n        ------\n        ValueError\n            * If both of ``include`` and ``exclude`` are empty\n            * If ``include`` and ``exclude`` have overlapping elements\n        TypeError\n            * If any kind of string dtype is passed in.\n\n        See Also\n        --------\n        DataFrame.dtypes: Return Series with the data type of each column.\n\n        Notes\n        -----\n        * To select all *numeric* types, use ``np.number`` or ``'number'``\n        * To select strings you must use the ``object`` dtype, but note that\n          this will return *all* object dtype columns. With\n          ``pd.options.future.infer_string`` enabled, using ``\"str\"`` will\n          work to select all string columns.\n        * See the `numpy dtype hierarchy\n          <https://numpy.org/doc/stable/reference/arrays.scalars.html>`__\n        * To select datetimes, use ``np.datetime64``, ``'datetime'`` or\n          ``'datetime64'``\n        * To select timedeltas, use ``np.timedelta64``, ``'timedelta'`` or\n          ``'timedelta64'``\n        * To select Pandas categorical dtypes, use ``'category'``\n        * To select Pandas datetimetz dtypes, use ``'datetimetz'``\n          or ``'datetime64[ns, tz]'``\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\"a\": [1, 2] * 3, \"b\": [True, False] * 3, \"c\": [1.0, 2.0] * 3}\n        ... )\n        >>> df\n                a      b  c\n        0       1   True  1.0\n        1       2  False  2.0\n        2       1   True  1.0\n        3       2  False  2.0\n        4       1   True  1.0\n        5       2  False  2.0\n\n        >>> df.select_dtypes(include=\"bool\")\n           b\n        0  True\n        1  False\n        2  True\n        3  False\n        4  True\n        5  False\n\n        >>> df.select_dtypes(include=[\"float64\"])\n           c\n        0  1.0\n        1  2.0\n        2  1.0\n        3  2.0\n        4  1.0\n        5  2.0\n\n        >>> df.select_dtypes(exclude=[\"int64\"])\n               b    c\n        0   True  1.0\n        1  False  2.0\n        2   True  1.0\n        3  False  2.0\n        4   True  1.0\n        5  False  2.0\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 5582, "code": "    def _sanitize_column(self, value) -> tuple[ArrayLike, BlockValuesRefs | None]:\n        self._ensure_valid_index(value)\n        assert not isinstance(value, DataFrame)\n        if is_dict_like(value):\n            if not isinstance(value, Series):\n                value = Series(value)\n            return _reindex_for_setitem(value, self.index)\n        if is_list_like(value):\n            com.require_length_match(value, self.index)\n        return sanitize_array(value, self.index, copy=True, allow_2d=True), None\n    @property", "documentation": "        \"\"\"\n        Ensures new columns (which go into the BlockManager as new blocks) are\n        always copied (or a reference is being tracked to them under CoW)\n        and converted into an array.\n\n        Parameters\n        ----------\n        value : scalar, Series, or array-like\n\n        Returns\n        -------\n        tuple of numpy.ndarray or ExtensionArray and optional BlockValuesRefs\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 5616, "code": "    def _reindex_multi(self, axes: dict[str, Index], fill_value) -> DataFrame:\n        new_index, row_indexer = self.index.reindex(axes[\"index\"])\n        new_columns, col_indexer = self.columns.reindex(axes[\"columns\"])\n        if row_indexer is not None and col_indexer is not None:\n            indexer = row_indexer, col_indexer\n            new_values = take_2d_multi(self.values, indexer, fill_value=fill_value)\n            return self._constructor(\n                new_values, index=new_index, columns=new_columns, copy=False\n            )\n        else:\n            return self._reindex_with_indexers(", "documentation": "        \"\"\"\n        We are guaranteed non-Nones in the axes.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 6345, "code": "    def pop(self, item: Hashable) -> Series:\n        return super().pop(item=item)", "documentation": "        \"\"\"\n        Return item and drop it from DataFrame. Raise KeyError if not found.\n\n        Parameters\n        ----------\n        item : label\n            Label of column to be popped.\n\n        Returns\n        -------\n        Series\n            Series representing the item that is dropped.\n\n        See Also\n        --------\n        DataFrame.drop: Drop specified labels from rows or columns.\n        DataFrame.drop_duplicates: Return DataFrame with duplicate rows removed.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     [\n        ...         (\"falcon\", \"bird\", 389.0),\n        ...         (\"parrot\", \"bird\", 24.0),\n        ...         (\"lion\", \"mammal\", 80.5),\n        ...         (\"monkey\", \"mammal\", np.nan),\n        ...     ],\n        ...     columns=(\"name\", \"class\", \"max_speed\"),\n        ... )\n        >>> df\n             name   class  max_speed\n        0  falcon    bird      389.0\n        1  parrot    bird       24.0\n        2    lion  mammal       80.5\n        3  monkey  mammal        NaN\n\n        >>> df.pop(\"class\")\n        0      bird\n        1      bird\n        2    mammal\n        3    mammal\n        Name: class, dtype: str\n\n        >>> df\n             name  max_speed\n        0  falcon      389.0\n        1  parrot       24.0\n        2    lion       80.5\n        3  monkey        NaN\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 7204, "code": "    def isna(self) -> DataFrame:\n        res_mgr = self._mgr.isna(func=isna)\n        result = self._constructor_from_mgr(res_mgr, axes=res_mgr.axes)\n        return result.__finalize__(self, method=\"isna\")", "documentation": "        \"\"\"\n        Detect missing values.\n\n        Return a boolean same-sized object indicating if the values are NA.\n        NA values, such as None or :attr:`numpy.NaN`, gets mapped to True\n        values.\n        Everything else gets mapped to False values. Characters such as empty\n        strings ``''`` or :attr:`numpy.inf` are not considered NA values.\n\n        Returns\n        -------\n        Series/DataFrame\n            Mask of bool values for each element in Series/DataFrame\n            that indicates whether an element is an NA value.\n\n        See Also\n        --------\n        Series.isnull : Alias of isna.\n        DataFrame.isnull : Alias of isna.\n        Series.notna : Boolean inverse of isna.\n        DataFrame.notna : Boolean inverse of isna.\n        Series.dropna : Omit axes labels with missing values.\n        DataFrame.dropna : Omit axes labels with missing values.\n        isna : Top-level isna.\n\n        Examples\n        --------\n        Show which entries in a DataFrame are NA.\n\n        >>> df = pd.DataFrame(\n        ...     dict(\n        ...         age=[5, 6, np.nan],\n        ...         born=[\n        ...             pd.NaT,\n        ...             pd.Timestamp(\"1939-05-27\"),\n        ...             pd.Timestamp(\"1940-04-25\"),\n        ...         ],\n        ...         name=[\"Alfred\", \"Batman\", \"\"],\n        ...         toy=[None, \"Batmobile\", \"Joker\"],\n        ...     )\n        ... )\n        >>> df\n           age       born    name        toy\n        0  5.0        NaT  Alfred        NaN\n        1  6.0 1939-05-27  Batman  Batmobile\n        2  NaN 1940-04-25              Joker\n\n        >>> df.isna()\n             age   born   name    toy\n        0  False   True  False   True\n        1  False  False  False  False\n        2   True  False  False  False\n\n        Show which entries in a Series are NA.\n\n        >>> ser = pd.Series([5, 6, np.nan])\n        >>> ser\n        0    5.0\n        1    6.0\n        2    NaN\n        dtype: float64\n\n        >>> ser.isna()\n        0    False\n        1    False\n        2     True\n        dtype: bool\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 7277, "code": "    def isnull(self) -> DataFrame:\n        return self.isna()", "documentation": "        \"\"\"\n        DataFrame.isnull is an alias for DataFrame.isna.\n\n        Detect missing values.\n\n        Return a boolean same-sized object indicating if the values are NA.\n        NA values, such as None or :attr:`numpy.NaN`, gets mapped to True\n        values.\n        Everything else gets mapped to False values. Characters such as empty\n        strings ``''`` or :attr:`numpy.inf` are not considered NA values.\n\n        Returns\n        -------\n        Series/DataFrame\n            Mask of bool values for each element in Series/DataFrame\n            that indicates whether an element is an NA value.\n\n        See Also\n        --------\n        Series.isnull : Alias of isna.\n        DataFrame.isnull : Alias of isna.\n        Series.notna : Boolean inverse of isna.\n        DataFrame.notna : Boolean inverse of isna.\n        Series.dropna : Omit axes labels with missing values.\n        DataFrame.dropna : Omit axes labels with missing values.\n        isna : Top-level isna.\n\n        Examples\n        --------\n        Show which entries in a DataFrame are NA.\n\n        >>> df = pd.DataFrame(\n        ...     dict(\n        ...         age=[5, 6, np.nan],\n        ...         born=[\n        ...             pd.NaT,\n        ...             pd.Timestamp(\"1939-05-27\"),\n        ...             pd.Timestamp(\"1940-04-25\"),\n        ...         ],\n        ...         name=[\"Alfred\", \"Batman\", \"\"],\n        ...         toy=[None, \"Batmobile\", \"Joker\"],\n        ...     )\n        ... )\n        >>> df\n           age       born    name        toy\n        0  5.0        NaT  Alfred        NaN\n        1  6.0 1939-05-27  Batman  Batmobile\n        2  NaN 1940-04-25              Joker\n\n        >>> df.isna()\n             age   born   name    toy\n        0  False   True  False   True\n        1  False  False  False  False\n        2   True  False  False  False\n\n        Show which entries in a Series are NA.\n\n        >>> ser = pd.Series([5, 6, np.nan])\n        >>> ser\n        0    5.0\n        1    6.0\n        2    NaN\n        dtype: float64\n\n        >>> ser.isna()\n        0    False\n        1    False\n        2     True\n        dtype: bool\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 7350, "code": "    def notna(self) -> DataFrame:\n        return ~self.isna()", "documentation": "        \"\"\"\n        Detect existing (non-missing) values.\n\n        Return a boolean same-sized object indicating if the values are not NA.\n        Non-missing values get mapped to True. Characters such as empty\n        strings ``''`` or :attr:`numpy.inf` are not considered NA values.\n        NA values, such as None or :attr:`numpy.NaN`, get mapped to False\n        values.\n\n        Returns\n        -------\n        Series/DataFrame\n            Mask of bool values for each element in Series/DataFrame\n            that indicates whether an element is not an NA value.\n\n        See Also\n        --------\n        Series.notnull : Alias of notna.\n        DataFrame.notnull : Alias of notna.\n        Series.isna : Boolean inverse of notna.\n        DataFrame.isna : Boolean inverse of notna.\n        Series.dropna : Omit axes labels with missing values.\n        DataFrame.dropna : Omit axes labels with missing values.\n        notna : Top-level notna.\n\n        Examples\n        --------\n        Show which entries in a DataFrame are not NA.\n\n        >>> df = pd.DataFrame(\n        ...     dict(\n        ...         age=[5, 6, np.nan],\n        ...         born=[\n        ...             pd.NaT,\n        ...             pd.Timestamp(\"1939-05-27\"),\n        ...             pd.Timestamp(\"1940-04-25\"),\n        ...         ],\n        ...         name=[\"Alfred\", \"Batman\", \"\"],\n        ...         toy=[None, \"Batmobile\", \"Joker\"],\n        ...     )\n        ... )\n        >>> df\n           age       born    name        toy\n        0  5.0        NaT  Alfred        NaN\n        1  6.0 1939-05-27  Batman  Batmobile\n        2  NaN 1940-04-25              Joker\n\n        >>> df.notna()\n             age   born  name    toy\n        0   True  False  True  False\n        1   True   True  True   True\n        2  False   True  True   True\n\n        Show which entries in a Series are not NA.\n\n        >>> ser = pd.Series([5, 6, np.nan])\n        >>> ser\n        0    5.0\n        1    6.0\n        2    NaN\n        dtype: float64\n\n        >>> ser.notna()\n        0     True\n        1     True\n        2    False\n        dtype: bool\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 7421, "code": "    def notnull(self) -> DataFrame:\n        return ~self.isna()\n    @overload", "documentation": "        \"\"\"\n        DataFrame.notnull is an alias for DataFrame.notna.\n\n        Detect existing (non-missing) values.\n\n        Return a boolean same-sized object indicating if the values are not NA.\n        Non-missing values get mapped to True. Characters such as empty\n        strings ``''`` or :attr:`numpy.inf` are not considered NA values.\n        NA values, such as None or :attr:`numpy.NaN`, get mapped to False\n        values.\n\n        Returns\n        -------\n        Series/DataFrame\n            Mask of bool values for each element in Series/DataFrame\n            that indicates whether an element is not an NA value.\n\n        See Also\n        --------\n        Series.notnull : Alias of notna.\n        DataFrame.notnull : Alias of notna.\n        Series.isna : Boolean inverse of notna.\n        DataFrame.isna : Boolean inverse of notna.\n        Series.dropna : Omit axes labels with missing values.\n        DataFrame.dropna : Omit axes labels with missing values.\n        notna : Top-level notna.\n\n        Examples\n        --------\n        Show which entries in a DataFrame are not NA.\n\n        >>> df = pd.DataFrame(\n        ...     dict(\n        ...         age=[5, 6, np.nan],\n        ...         born=[\n        ...             pd.NaT,\n        ...             pd.Timestamp(\"1939-05-27\"),\n        ...             pd.Timestamp(\"1940-04-25\"),\n        ...         ],\n        ...         name=[\"Alfred\", \"Batman\", \"\"],\n        ...         toy=[None, \"Batmobile\", \"Joker\"],\n        ...     )\n        ... )\n        >>> df\n           age       born    name        toy\n        0  5.0        NaT  Alfred        NaN\n        1  6.0 1939-05-27  Batman  Batmobile\n        2  NaN 1940-04-25              Joker\n\n        >>> df.notnull()\n             age   born  name    toy\n        0   True  False  True  False\n        1   True   True  True   True\n        2  False   True  True   True\n\n        Show which entries in a Series are not NA.\n\n        >>> ser = pd.Series([5, 6, np.nan])\n        >>> ser\n        0    5.0\n        1    6.0\n        2    NaN\n        dtype: float64\n\n        >>> ser.notnull()\n        0     True\n        1     True\n        2    False\n        dtype: bool\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 8832, "code": "    def swaplevel(self, i: Axis = -2, j: Axis = -1, axis: Axis = 0) -> DataFrame:\n        result = self.copy(deep=False)\n        axis = self._get_axis_number(axis)\n        if not isinstance(result._get_axis(axis), MultiIndex):  # pragma: no cover\n            raise TypeError(\"Can only swap levels on a hierarchical axis.\")\n        if axis == 0:\n            assert isinstance(result.index, MultiIndex)\n            result.index = result.index.swaplevel(i, j)\n        else:\n            assert isinstance(result.columns, MultiIndex)\n            result.columns = result.columns.swaplevel(i, j)", "documentation": "        \"\"\"\n        Swap levels i and j in a :class:`MultiIndex`.\n\n        Default is to swap the two innermost levels of the index.\n\n        Parameters\n        ----------\n        i, j : int or str\n            Levels of the indices to be swapped. Can pass level name as string.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n                    The axis to swap levels on. 0 or 'index' for row-wise, 1 or\n                    'columns' for column-wise.\n\n        Returns\n        -------\n        DataFrame\n            DataFrame with levels swapped in MultiIndex.\n\n        See Also\n        --------\n        DataFrame.reorder_levels: Reorder levels of MultiIndex.\n        DataFrame.sort_index: Sort MultiIndex.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\"Grade\": [\"A\", \"B\", \"A\", \"C\"]},\n        ...     index=[\n        ...         [\"Final exam\", \"Final exam\", \"Coursework\", \"Coursework\"],\n        ...         [\"History\", \"Geography\", \"History\", \"Geography\"],\n        ...         [\"January\", \"February\", \"March\", \"April\"],\n        ...     ],\n        ... )\n        >>> df\n                                            Grade\n        Final exam  History     January      A\n                    Geography   February     B\n        Coursework  History     March        A\n                    Geography   April        C\n\n        In the following example, we will swap the levels of the indices.\n        Here, we will swap the levels column-wise, but levels can be swapped row-wise\n        in a similar manner. Note that column-wise is the default behaviour.\n        By not supplying any arguments for i and j, we swap the last and second to\n        last indices.\n\n        >>> df.swaplevel()\n                                            Grade\n        Final exam  January     History         A\n                    February    Geography       B\n        Coursework  March       History         A\n                    April       Geography       C\n\n        By supplying one argument, we can choose which index to swap the last\n        index with. We can for example swap the first index with the last one as\n        follows.\n\n        >>> df.swaplevel(0)\n                                            Grade\n        January     History     Final exam      A\n        February    Geography   Final exam      B\n        March       History     Coursework      A\n        April       Geography   Coursework      C\n\n        We can also define explicitly which indices we want to swap by supplying values\n        for both i and j. Here, we for example swap the first and second indices.\n\n        >>> df.swaplevel(0, 1)\n                                            Grade\n        History     Final exam  January         A\n        Geography   Final exam  February        B\n        History     Coursework  March           A\n        Geography   Coursework  April           C\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 8922, "code": "    def reorder_levels(self, order: Sequence[int | str], axis: Axis = 0) -> DataFrame:\n        axis = self._get_axis_number(axis)\n        if not isinstance(self._get_axis(axis), MultiIndex):  # pragma: no cover\n            raise TypeError(\"Can only reorder levels on a hierarchical axis.\")\n        result = self.copy(deep=False)\n        if axis == 0:\n            assert isinstance(result.index, MultiIndex)\n            result.index = result.index.reorder_levels(order)\n        else:\n            assert isinstance(result.columns, MultiIndex)\n            result.columns = result.columns.reorder_levels(order)", "documentation": "        \"\"\"\n        Rearrange index or column levels using input ``order``.\n\n        May not drop or duplicate levels.\n\n        Parameters\n        ----------\n        order : list of int or list of str\n            List representing new level order. Reference level by number\n            (position) or by key (label).\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Where to reorder levels.\n\n        Returns\n        -------\n        DataFrame\n            DataFrame with indices or columns with reordered levels.\n\n        See Also\n        --------\n            DataFrame.swaplevel : Swap levels i and j in a MultiIndex.\n\n        Examples\n        --------\n        >>> data = {\n        ...     \"class\": [\"Mammals\", \"Mammals\", \"Reptiles\"],\n        ...     \"diet\": [\"Omnivore\", \"Carnivore\", \"Carnivore\"],\n        ...     \"species\": [\"Humans\", \"Dogs\", \"Snakes\"],\n        ... }\n        >>> df = pd.DataFrame(data, columns=[\"class\", \"diet\", \"species\"])\n        >>> df = df.set_index([\"class\", \"diet\"])\n        >>> df\n                                          species\n        class      diet\n        Mammals    Omnivore                Humans\n                   Carnivore                 Dogs\n        Reptiles   Carnivore               Snakes\n\n        Let's reorder the levels of the index:\n\n        >>> df.reorder_levels([\"diet\", \"class\"])\n                                          species\n        diet      class\n        Omnivore  Mammals                  Humans\n        Carnivore Mammals                    Dogs\n                  Reptiles                 Snakes\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 9101, "code": "    def _arith_method_with_reindex(self, right: DataFrame, op) -> DataFrame:\n        left = self\n        cols, lcol_indexer, rcol_indexer = left.columns.join(\n            right.columns, how=\"inner\", return_indexers=True\n        )\n        new_left = left if lcol_indexer is None else left.iloc[:, lcol_indexer]\n        new_right = right if rcol_indexer is None else right.iloc[:, rcol_indexer]\n        if isinstance(cols, MultiIndex):\n            new_left = new_left.copy(deep=False)\n            new_right = new_right.copy(deep=False)\n            new_left.columns = cols", "documentation": "        \"\"\"\n        For DataFrame-with-DataFrame operations that require reindexing,\n        operate only on shared columns, then reindex.\n\n        Parameters\n        ----------\n        right : DataFrame\n        op : binary operator\n\n        Returns\n        -------\n        DataFrame\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 9153, "code": "    def _should_reindex_frame_op(self, right, op, axis: int, fill_value, level) -> bool:\n        if op is operator.pow or op is roperator.rpow:\n            return False\n        if not isinstance(right, DataFrame):\n            return False\n        if (\n            (\n                isinstance(self.columns, MultiIndex)\n                or isinstance(right.columns, MultiIndex)\n            )\n            and not self.columns.equals(right.columns)", "documentation": "        \"\"\"\n        Check if this is an operation between DataFrames that will need to reindex.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 9324, "code": "    def _maybe_align_series_as_frame(self, series: Series, axis: AxisInt):\n        rvalues = series._values\n        if not isinstance(rvalues, np.ndarray):\n            if lib.is_np_dtype(rvalues.dtype, \"mM\"):\n                rvalues = np.asarray(rvalues)\n            else:\n                return series\n        if axis == 0:\n            rvalues = rvalues.reshape(-1, 1)\n        else:\n            rvalues = rvalues.reshape(1, -1)", "documentation": "        \"\"\"\n        If the Series operand is not EA-dtype, we can broadcast to 2D and operate\n        blockwise.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 9385, "code": "    def _construct_result(self, result, other) -> DataFrame:\n        out = self._constructor(result, copy=False).__finalize__(self)\n        out.columns = self.columns\n        out.index = self.index\n        out = out.__finalize__(other)\n        return out", "documentation": "        \"\"\"\n        Wrap the result of an arithmetic, comparison, or logical operation.\n\n        Parameters\n        ----------\n        result : DataFrame\n\n        Returns\n        -------\n        DataFrame\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 9425, "code": "    def eq(self, other, axis: Axis = \"columns\", level=None) -> DataFrame:\n        return self._flex_cmp_method(other, operator.eq, axis=axis, level=level)", "documentation": "        \"\"\"\n        Get Equal to of dataframe and other, element-wise (binary operator `eq`).\n\n        Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n        operators.\n\n        Equivalent to `==`, `!=`, `<=`, `<`, `>=`, `>` with support to choose axis\n        (rows or columns) and level for comparison.\n\n        Parameters\n        ----------\n        other : scalar, sequence, Series, or DataFrame\n            Any single or multiple element data structure, or list-like object.\n        axis : {0 or 'index', 1 or 'columns'}, default 'columns'\n            Whether to compare by the index (0 or 'index') or columns\n            (1 or 'columns').\n        level : int or label\n            Broadcast across a level, matching Index values on the passed\n            MultiIndex level.\n\n        Returns\n        -------\n        DataFrame of bool\n            Result of the comparison.\n\n        See Also\n        --------\n        DataFrame.eq : Compare DataFrames for equality elementwise.\n        DataFrame.ne : Compare DataFrames for inequality elementwise.\n        DataFrame.le : Compare DataFrames for less than inequality\n            or equality elementwise.\n        DataFrame.lt : Compare DataFrames for strictly less than\n            inequality elementwise.\n        DataFrame.ge : Compare DataFrames for greater than inequality\n            or equality elementwise.\n        DataFrame.gt : Compare DataFrames for strictly greater than\n            inequality elementwise.\n\n        Notes\n        -----\n        Mismatched indices will be unioned together.\n        `NaN` values are considered different (i.e. `NaN` != `NaN`).\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\"cost\": [250, 150, 100], \"revenue\": [100, 250, 300]},\n        ...     index=[\"A\", \"B\", \"C\"],\n        ... )\n        >>> df\n           cost  revenue\n        A   250      100\n        B   150      250\n        C   100      300\n\n        Comparison with a scalar, using either the operator or method:\n\n        >>> df == 100\n            cost  revenue\n        A  False     True\n        B  False    False\n        C   True    False\n\n        >>> df.eq(100)\n            cost  revenue\n        A  False     True\n        B  False    False\n        C   True    False\n\n        When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n        with the index of `other` and broadcast:\n\n        >>> df != pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n            cost  revenue\n        A   True     True\n        B   True    False\n        C  False     True\n\n        Use the method to control the broadcast axis:\n\n        >>> df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis=\"index\")\n           cost  revenue\n        A  True    False\n        B  True     True\n        C  True     True\n        D  True     True\n\n        When comparing to an arbitrary sequence, the number of columns must\n        match the number elements in `other`:\n\n        >>> df == [250, 100]\n            cost  revenue\n        A   True     True\n        B  False    False\n        C  False    False\n\n        Use the method to control the axis:\n\n        >>> df.eq([250, 250, 100], axis=\"index\")\n            cost  revenue\n        A   True    False\n        B  False     True\n        C   True    False\n\n        Compare to a DataFrame of different shape.\n\n        >>> other = pd.DataFrame(\n        ...     {\"revenue\": [300, 250, 100, 150]}, index=[\"A\", \"B\", \"C\", \"D\"]\n        ... )\n        >>> other\n           revenue\n        A      300\n        B      250\n        C      100\n        D      150\n\n        >>> df.gt(other)\n            cost  revenue\n        A  False    False\n        B  False    False\n        C  False     True\n        D  False    False\n\n        Compare to a MultiIndex by level.\n\n        >>> df_multindex = pd.DataFrame(\n        ...     {\n        ...         \"cost\": [250, 150, 100, 150, 300, 220],\n        ...         \"revenue\": [100, 250, 300, 200, 175, 225],\n        ...     },\n        ...     index=[\n        ...         [\"Q1\", \"Q1\", \"Q1\", \"Q2\", \"Q2\", \"Q2\"],\n        ...         [\"A\", \"B\", \"C\", \"A\", \"B\", \"C\"],\n        ...     ],\n        ... )\n        >>> df_multindex\n              cost  revenue\n        Q1 A   250      100\n           B   150      250\n           C   100      300\n        Q2 A   150      200\n           B   300      175\n           C   220      225\n\n        >>> df.le(df_multindex, level=1)\n               cost  revenue\n        Q1 A   True     True\n           B   True     True\n           C   True     True\n        Q2 A  False     True\n           B   True    False\n           C   True    False\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 9581, "code": "    def ne(self, other, axis: Axis = \"columns\", level=None) -> DataFrame:\n        return self._flex_cmp_method(other, operator.ne, axis=axis, level=level)", "documentation": "        \"\"\"\n        Get Not equal to of dataframe and other, element-wise (binary operator `ne`).\n\n        Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n        operators.\n\n        Equivalent to `==`, `!=`, `<=`, `<`, `>=`, `>` with support to choose axis\n        (rows or columns) and level for comparison.\n\n        Parameters\n        ----------\n        other : scalar, sequence, Series, or DataFrame\n            Any single or multiple element data structure, or list-like object.\n        axis : {0 or 'index', 1 or 'columns'}, default 'columns'\n            Whether to compare by the index (0 or 'index') or columns\n            (1 or 'columns').\n        level : int or label\n            Broadcast across a level, matching Index values on the passed\n            MultiIndex level.\n\n        Returns\n        -------\n        DataFrame of bool\n            Result of the comparison.\n\n        See Also\n        --------\n        DataFrame.eq : Compare DataFrames for equality elementwise.\n        DataFrame.ne : Compare DataFrames for inequality elementwise.\n        DataFrame.le : Compare DataFrames for less than inequality\n            or equality elementwise.\n        DataFrame.lt : Compare DataFrames for strictly less than\n            inequality elementwise.\n        DataFrame.ge : Compare DataFrames for greater than inequality\n            or equality elementwise.\n        DataFrame.gt : Compare DataFrames for strictly greater than\n            inequality elementwise.\n\n        Notes\n        -----\n        Mismatched indices will be unioned together.\n        `NaN` values are considered different (i.e. `NaN` != `NaN`).\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\"cost\": [250, 150, 100], \"revenue\": [100, 250, 300]},\n        ...     index=[\"A\", \"B\", \"C\"],\n        ... )\n        >>> df\n           cost  revenue\n        A   250      100\n        B   150      250\n        C   100      300\n\n        Comparison with a scalar, using either the operator or method:\n\n        >>> df == 100\n            cost  revenue\n        A  False     True\n        B  False    False\n        C   True    False\n\n        >>> df.eq(100)\n            cost  revenue\n        A  False     True\n        B  False    False\n        C   True    False\n\n        When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n        with the index of `other` and broadcast:\n\n        >>> df != pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n            cost  revenue\n        A   True     True\n        B   True    False\n        C  False     True\n\n        Use the method to control the broadcast axis:\n\n        >>> df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis=\"index\")\n           cost  revenue\n        A  True    False\n        B  True     True\n        C  True     True\n        D  True     True\n\n        When comparing to an arbitrary sequence, the number of columns must\n        match the number elements in `other`:\n\n        >>> df == [250, 100]\n            cost  revenue\n        A   True     True\n        B  False    False\n        C  False    False\n\n        Use the method to control the axis:\n\n        >>> df.eq([250, 250, 100], axis=\"index\")\n            cost  revenue\n        A   True    False\n        B  False     True\n        C   True    False\n\n        Compare to a DataFrame of different shape.\n\n        >>> other = pd.DataFrame(\n        ...     {\"revenue\": [300, 250, 100, 150]}, index=[\"A\", \"B\", \"C\", \"D\"]\n        ... )\n        >>> other\n           revenue\n        A      300\n        B      250\n        C      100\n        D      150\n\n        >>> df.gt(other)\n            cost  revenue\n        A  False    False\n        B  False    False\n        C  False     True\n        D  False    False\n\n        Compare to a MultiIndex by level.\n\n        >>> df_multindex = pd.DataFrame(\n        ...     {\n        ...         \"cost\": [250, 150, 100, 150, 300, 220],\n        ...         \"revenue\": [100, 250, 300, 200, 175, 225],\n        ...     },\n        ...     index=[\n        ...         [\"Q1\", \"Q1\", \"Q1\", \"Q2\", \"Q2\", \"Q2\"],\n        ...         [\"A\", \"B\", \"C\", \"A\", \"B\", \"C\"],\n        ...     ],\n        ... )\n        >>> df_multindex\n              cost  revenue\n        Q1 A   250      100\n           B   150      250\n           C   100      300\n        Q2 A   150      200\n           B   300      175\n           C   220      225\n\n        >>> df.le(df_multindex, level=1)\n               cost  revenue\n        Q1 A   True     True\n           B   True     True\n           C   True     True\n        Q2 A  False     True\n           B   True    False\n           C   True    False\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 9737, "code": "    def le(self, other, axis: Axis = \"columns\", level=None) -> DataFrame:\n        return self._flex_cmp_method(other, operator.le, axis=axis, level=level)", "documentation": "        \"\"\"\n        Get Less than or equal to of dataframe and other, \\\n        element-wise (binary operator `le`).\n\n        Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n        operators.\n\n        Equivalent to ``<=`` with support to choose axis\n        (rows or columns) and level for comparison.\n\n        Parameters\n        ----------\n        other : scalar, sequence, Series, or DataFrame\n            Any single or multiple element data structure, or list-like object.\n        axis : {0 or 'index', 1 or 'columns'}, default 'columns'\n            Whether to compare by the index (0 or 'index') or columns\n            (1 or 'columns').\n        level : int or label\n            Broadcast across a level, matching Index values on the passed\n            MultiIndex level.\n\n        Returns\n        -------\n        DataFrame of bool\n            Result of the comparison.\n\n        See Also\n        --------\n        DataFrame.eq : Compare DataFrames for equality elementwise.\n        DataFrame.ne : Compare DataFrames for inequality elementwise.\n        DataFrame.le : Compare DataFrames for less than inequality\n            or equality elementwise.\n        DataFrame.lt : Compare DataFrames for strictly less than\n            inequality elementwise.\n        DataFrame.ge : Compare DataFrames for greater than inequality\n            or equality elementwise.\n        DataFrame.gt : Compare DataFrames for strictly greater than\n            inequality elementwise.\n\n        Notes\n        -----\n        Mismatched indices will be unioned together.\n        `NaN` values are considered different (i.e. `NaN` != `NaN`).\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'cost': [250, 150, 100],\n        ...                    'revenue': [100, 250, 300]},\n        ...                   index=['A', 'B', 'C'])\n        >>> df\n           cost  revenue\n        A   250      100\n        B   150      250\n        C   100      300\n\n        Comparison with a scalar, using either the operator or method:\n\n        >>> df <= 100\n            cost  revenue\n        A  False     True\n        B  False    False\n        C   True    False\n\n        >>> df.le(100)\n            cost  revenue\n        A  False     True\n        B  False    False\n        C   True    False\n\n        When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n        with the index of `other` and broadcast:\n\n        >>> df <= pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n            cost  revenue\n        A  False     True\n        B  False     True\n        C   True    False\n\n        Use the method to control the broadcast axis:\n\n        >>> df.le(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index')\n            cost  revenue\n        A  False     True\n        B  False    False\n        C  False    False\n        D  False    False\n\n        When comparing to an arbitrary sequence, the number of columns must\n        match the number elements in `other`:\n\n        >>> df <= [250, 100]\n           cost  revenue\n        A  True     True\n        B  True    False\n        C  True    False\n\n        Use the method to control the axis:\n\n        >>> df.le([250, 250, 100], axis='index')\n           cost  revenue\n        A  True     True\n        B  True     True\n        C  True    False\n\n        Compare to a DataFrame of different shape.\n\n        >>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},\n        ...                      index=['A', 'B', 'C', 'D'])\n        >>> other\n           revenue\n        A      300\n        B      250\n        C      100\n        D      150\n\n        >>> df.le(other)\n            cost  revenue\n        A  False     True\n        B  False     True\n        C  False    False\n        D  False    False\n\n        Compare to a MultiIndex by level.\n\n        >>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],\n        ...                              'revenue': [100, 250, 300, 200, 175, 225]},\n        ...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n        ...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n        >>> df_multindex\n              cost  revenue\n        Q1 A   250      100\n           B   150      250\n           C   100      300\n        Q2 A   150      200\n           B   300      175\n           C   220      225\n\n        >>> df.le(df_multindex, level=1)\n               cost  revenue\n        Q1 A   True     True\n           B   True     True\n           C   True     True\n        Q2 A  False     True\n           B   True    False\n           C   True    False\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 9886, "code": "    def lt(self, other, axis: Axis = \"columns\", level=None) -> DataFrame:\n        return self._flex_cmp_method(other, operator.lt, axis=axis, level=level)", "documentation": "        \"\"\"\n        Get Less than of dataframe and other, element-wise (binary operator `lt`).\n\n        Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n        operators.\n\n        Equivalent to ``<`` with support to choose axis\n        (rows or columns) and level for comparison.\n\n        Parameters\n        ----------\n        other : scalar, sequence, Series, or DataFrame\n            Any single or multiple element data structure, or list-like object.\n        axis : {0 or 'index', 1 or 'columns'}, default 'columns'\n            Whether to compare by the index (0 or 'index') or columns\n            (1 or 'columns').\n        level : int or label\n            Broadcast across a level, matching Index values on the passed\n            MultiIndex level.\n\n        Returns\n        -------\n        DataFrame of bool\n            Result of the comparison.\n\n        See Also\n        --------\n        DataFrame.eq : Compare DataFrames for equality elementwise.\n        DataFrame.ne : Compare DataFrames for inequality elementwise.\n        DataFrame.le : Compare DataFrames for less than inequality\n            or equality elementwise.\n        DataFrame.lt : Compare DataFrames for strictly less than\n            inequality elementwise.\n        DataFrame.ge : Compare DataFrames for greater than inequality\n            or equality elementwise.\n        DataFrame.gt : Compare DataFrames for strictly greater than\n            inequality elementwise.\n\n        Notes\n        -----\n        Mismatched indices will be unioned together.\n        `NaN` values are considered different (i.e. `NaN` != `NaN`).\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\"cost\": [250, 150, 100], \"revenue\": [100, 250, 300]},\n        ...     index=[\"A\", \"B\", \"C\"],\n        ... )\n        >>> df\n           cost  revenue\n        A   250      100\n        B   150      250\n        C   100      300\n\n        Comparison with a scalar, using either the operator or method:\n\n        >>> df < 100\n            cost  revenue\n        A  False    False\n        B  False    False\n        C  False    False\n\n        >>> df.lt(100)\n            cost  revenue\n        A  False    False\n        B  False    False\n        C  False    False\n\n        When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n        with the index of `other` and broadcast:\n\n        >>> df < pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n            cost  revenue\n        A  False     True\n        B  False    False\n        C  False    False\n\n        Use the method to control the broadcast axis:\n\n        >>> df.lt(pd.Series([100, 300], index=[\"A\", \"D\"]), axis=\"index\")\n            cost  revenue\n        A  False    False\n        B  False    False\n        C  False    False\n        D  False    False\n\n        When comparing to an arbitrary sequence, the number of columns must\n        match the number elements in `other`:\n\n        >>> df < [250, 100]\n            cost  revenue\n        A  False    False\n        B   True    False\n        C   True    False\n\n        Use the method to control the axis:\n\n        >>> df.lt([250, 250, 100], axis=\"index\")\n            cost  revenue\n        A  False     True\n        B   True    False\n        C  False    False\n\n        Compare to a DataFrame of different shape.\n\n        >>> other = pd.DataFrame(\n        ...     {\"revenue\": [300, 250, 100, 150]}, index=[\"A\", \"B\", \"C\", \"D\"]\n        ... )\n        >>> other\n           revenue\n        A      300\n        B      250\n        C      100\n        D      150\n\n        >>> df.lt(other)\n            cost  revenue\n        A  False     True\n        B  False    False\n        C  False    False\n        D  False    False\n\n        Compare to a MultiIndex by level.\n\n        >>> df_multindex = pd.DataFrame(\n        ...     {\n        ...         \"cost\": [250, 150, 100, 150, 300, 220],\n        ...         \"revenue\": [100, 250, 300, 200, 175, 225],\n        ...     },\n        ...     index=[\n        ...         [\"Q1\", \"Q1\", \"Q1\", \"Q2\", \"Q2\", \"Q2\"],\n        ...         [\"A\", \"B\", \"C\", \"A\", \"B\", \"C\"],\n        ...     ],\n        ... )\n        >>> df_multindex\n              cost  revenue\n        Q1 A   250      100\n           B   150      250\n           C   100      300\n        Q2 A   150      200\n           B   300      175\n           C   220      225\n\n        >>> df.lt(df_multindex, level=1)\n               cost  revenue\n        Q1 A  False    False\n           B  False    False\n           C  False    False\n        Q2 A  False     True\n           B   True    False\n           C   True    False\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 10042, "code": "    def ge(self, other, axis: Axis = \"columns\", level=None) -> DataFrame:\n        return self._flex_cmp_method(other, operator.ge, axis=axis, level=level)", "documentation": "        \"\"\"\n        Get Greater than or equal to of dataframe and other, \\\n        element-wise (binary operator `ge`).\n\n        Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n        operators.\n\n        Equivalent to ``>=`` with support to choose axis\n        (rows or columns) and level for comparison.\n\n        Parameters\n        ----------\n        other : scalar, sequence, Series, or DataFrame\n            Any single or multiple element data structure, or list-like object.\n        axis : {0 or 'index', 1 or 'columns'}, default 'columns'\n            Whether to compare by the index (0 or 'index') or columns\n            (1 or 'columns').\n        level : int or label\n            Broadcast across a level, matching Index values on the passed\n            MultiIndex level.\n\n        Returns\n        -------\n        DataFrame of bool\n            Result of the comparison.\n\n        See Also\n        --------\n        DataFrame.eq : Compare DataFrames for equality elementwise.\n        DataFrame.ne : Compare DataFrames for inequality elementwise.\n        DataFrame.le : Compare DataFrames for less than inequality\n            or equality elementwise.\n        DataFrame.lt : Compare DataFrames for strictly less than\n            inequality elementwise.\n        DataFrame.ge : Compare DataFrames for greater than inequality\n            or equality elementwise.\n        DataFrame.gt : Compare DataFrames for strictly greater than\n            inequality elementwise.\n\n        Notes\n        -----\n        Mismatched indices will be unioned together.\n        `NaN` values are considered different (i.e. `NaN` != `NaN`).\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'cost': [250, 150, 100],\n        ...                    'revenue': [100, 250, 300]},\n        ...                   index=['A', 'B', 'C'])\n        >>> df\n           cost  revenue\n        A   250      100\n        B   150      250\n        C   100      300\n\n        Comparison with a scalar, using either the operator or method:\n\n        >>> df >= 100\n           cost  revenue\n        A  True     True\n        B  True     True\n        C  True     True\n\n        >>> df.ge(100)\n           cost  revenue\n        A  True     True\n        B  True     True\n        C  True     True\n\n        When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n        with the index of `other` and broadcast:\n\n        >>> df >= pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n           cost  revenue\n        A  True    False\n        B  True     True\n        C  True     True\n\n        Use the method to control the broadcast axis:\n\n        >>> df.ge(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index')\n            cost  revenue\n        A   True     True\n        B  False    False\n        C  False    False\n        D  False    False\n\n        When comparing to an arbitrary sequence, the number of columns must\n        match the number elements in `other`:\n\n        >>> df >= [250, 100]\n            cost  revenue\n        A   True     True\n        B  False     True\n        C  False     True\n\n        Use the method to control the axis:\n\n        >>> df.ge([250, 250, 100], axis='index')\n            cost  revenue\n        A   True    False\n        B  False     True\n        C   True     True\n\n        Compare to a DataFrame of different shape.\n\n        >>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},\n        ...                      index=['A', 'B', 'C', 'D'])\n        >>> other\n           revenue\n        A      300\n        B      250\n        C      100\n        D      150\n\n        >>> df.ge(other)\n            cost  revenue\n        A  False    False\n        B  False     True\n        C  False     True\n        D  False    False\n\n        Compare to a MultiIndex by level.\n\n        >>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],\n        ...                              'revenue': [100, 250, 300, 200, 175, 225]},\n        ...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n        ...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n        >>> df_multindex\n              cost  revenue\n        Q1 A   250      100\n           B   150      250\n           C   100      300\n        Q2 A   150      200\n           B   300      175\n           C   220      225\n\n        >>> df.ge(df_multindex, level=1)\n               cost  revenue\n        Q1 A   True     True\n           B   True     True\n           C   True     True\n        Q2 A   True    False\n           B  False     True\n           C  False     True\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 10191, "code": "    def gt(self, other, axis: Axis = \"columns\", level=None) -> DataFrame:\n        return self._flex_cmp_method(other, operator.gt, axis=axis, level=level)", "documentation": "        \"\"\"\n        Get Greater than of dataframe and other, element-wise (binary operator `gt`).\n\n        Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n        operators.\n\n        Equivalent to ``>`` with support to choose axis\n        (rows or columns) and level for comparison.\n\n        Parameters\n        ----------\n        other : scalar, sequence, Series, or DataFrame\n            Any single or multiple element data structure, or list-like object.\n        axis : {0 or 'index', 1 or 'columns'}, default 'columns'\n            Whether to compare by the index (0 or 'index') or columns\n            (1 or 'columns').\n        level : int or label\n            Broadcast across a level, matching Index values on the passed\n            MultiIndex level.\n\n        Returns\n        -------\n        DataFrame of bool\n            Result of the comparison.\n\n        See Also\n        --------\n        DataFrame.eq : Compare DataFrames for equality elementwise.\n        DataFrame.ne : Compare DataFrames for inequality elementwise.\n        DataFrame.le : Compare DataFrames for less than inequality\n            or equality elementwise.\n        DataFrame.lt : Compare DataFrames for strictly less than\n            inequality elementwise.\n        DataFrame.ge : Compare DataFrames for greater than inequality\n            or equality elementwise.\n        DataFrame.gt : Compare DataFrames for strictly greater than\n            inequality elementwise.\n\n        Notes\n        -----\n        Mismatched indices will be unioned together.\n        `NaN` values are considered different (i.e. `NaN` != `NaN`).\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\"cost\": [250, 150, 100], \"revenue\": [100, 250, 300]},\n        ...     index=[\"A\", \"B\", \"C\"],\n        ... )\n        >>> df\n           cost  revenue\n        A   250      100\n        B   150      250\n        C   100      300\n\n        Comparison with a scalar, using either the operator or method:\n\n        >>> df > 100\n            cost  revenue\n        A   True    False\n        B   True     True\n        C  False     True\n\n        >>> df.gt(100)\n            cost  revenue\n        A   True    False\n        B   True     True\n        C  False     True\n\n        When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n        with the index of `other` and broadcast:\n\n        >>> df > pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n            cost  revenue\n        A   True    False\n        B   True    False\n        C  False     True\n\n        Use the method to control the broadcast axis:\n\n        >>> df.gt(pd.Series([100, 300], index=[\"A\", \"D\"]), axis=\"index\")\n            cost  revenue\n        A   True    False\n        B  False    False\n        C  False    False\n        D  False    False\n\n        When comparing to an arbitrary sequence, the number of columns must\n        match the number elements in `other`:\n\n        >>> df > [250, 100]\n            cost  revenue\n        A  False    False\n        B  False     True\n        C  False     True\n\n        Use the method to control the axis:\n\n        >>> df.gt([250, 250, 100], axis=\"index\")\n            cost  revenue\n        A  False    False\n        B  False    False\n        C  False     True\n\n        Compare to a DataFrame of different shape.\n\n        >>> other = pd.DataFrame(\n        ...     {\"revenue\": [300, 250, 100, 150]}, index=[\"A\", \"B\", \"C\", \"D\"]\n        ... )\n        >>> other\n           revenue\n        A      300\n        B      250\n        C      100\n        D      150\n\n        >>> df.gt(other)\n            cost  revenue\n        A  False    False\n        B  False    False\n        C  False     True\n        D  False    False\n\n        Compare to a MultiIndex by level.\n\n        >>> df_multindex = pd.DataFrame(\n        ...     {\n        ...         \"cost\": [250, 150, 100, 150, 300, 220],\n        ...         \"revenue\": [100, 250, 300, 200, 175, 225],\n        ...     },\n        ...     index=[\n        ...         [\"Q1\", \"Q1\", \"Q1\", \"Q2\", \"Q2\", \"Q2\"],\n        ...         [\"A\", \"B\", \"C\", \"A\", \"B\", \"C\"],\n        ...     ],\n        ... )\n        >>> df_multindex\n              cost  revenue\n        Q1 A   250      100\n           B   150      250\n           C   100      300\n        Q2 A   150      200\n           B   300      175\n           C   220      225\n\n        >>> df.gt(df_multindex, level=1)\n               cost  revenue\n        Q1 A  False    False\n           B  False    False\n           C  False    False\n        Q2 A   True    False\n           B  False     True\n           C  False     True\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 12148, "code": "    def combine_first(self, other: DataFrame) -> DataFrame:", "documentation": "        \"\"\"\n        Update null elements with value in the same location in `other`.\n\n        Combine two DataFrame objects by filling null values in one DataFrame\n        with non-null values from other DataFrame. The row and column indexes\n        of the resulting DataFrame will be the union of the two. The resulting\n        dataframe contains the 'first' dataframe values and overrides the\n        second one values where both first.loc[index, col] and\n        second.loc[index, col] are not missing values, upon calling\n        first.combine_first(second).\n\n        Parameters\n        ----------\n        other : DataFrame\n            Provided DataFrame to use to fill null values.\n\n        Returns\n        -------\n        DataFrame\n            The result of combining the provided DataFrame with the other object.\n\n        See Also\n        --------\n        DataFrame.combine : Perform series-wise operation on two DataFrames\n            using a given function.\n\n        Examples\n        --------\n        >>> df1 = pd.DataFrame({\"A\": [None, 0], \"B\": [None, 4]})\n        >>> df2 = pd.DataFrame({\"A\": [1, 1], \"B\": [3, 3]})\n        >>> df1.combine_first(df2)\n             A    B\n        0  1.0  3.0\n        1  0.0  4.0\n\n        Null values still persist if the location of that null value\n        does not exist in `other`\n\n        >>> df1 = pd.DataFrame({\"A\": [None, 0], \"B\": [4, None]})\n        >>> df2 = pd.DataFrame({\"B\": [3, 3], \"C\": [1, 1]}, index=[1, 2])\n        >>> df1.combine_first(df2)\n             A    B    C\n        0  NaN  4.0  NaN\n        1  0.0  3.0  1.0\n        2  NaN  3.0  1.0\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 13562, "code": "    def diff(self, periods: int = 1, axis: Axis = 0) -> DataFrame:\n        if not lib.is_integer(periods):\n            if not (is_float(periods) and periods.is_integer()):\n                raise ValueError(\"periods must be an integer\")\n            periods = int(periods)\n        axis = self._get_axis_number(axis)\n        if axis == 1:\n            if periods != 0:\n                return self - self.shift(periods, axis=axis)\n            axis = 0\n        new_data = self._mgr.diff(n=periods)", "documentation": "        \"\"\"\n        First discrete difference of element.\n\n        Calculates the difference of a DataFrame element compared with another\n        element in the DataFrame (default is element in previous row).\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Periods to shift for calculating difference, accepts negative\n            values.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            Take difference over rows (0) or columns (1).\n\n        Returns\n        -------\n        DataFrame\n            First differences of the Series.\n\n        See Also\n        --------\n        DataFrame.pct_change: Percent change over given number of periods.\n        DataFrame.shift: Shift index by desired number of periods with an\n            optional time freq.\n        Series.diff: First discrete difference of object.\n\n        Notes\n        -----\n        For boolean dtypes, this uses :meth:`operator.xor` rather than\n        :meth:`operator.sub`.\n        The result is calculated according to current dtype in DataFrame,\n        however dtype of the result is always float64.\n\n        Examples\n        --------\n\n        Difference with previous row\n\n        >>> df = pd.DataFrame(\n        ...     {\n        ...         \"a\": [1, 2, 3, 4, 5, 6],\n        ...         \"b\": [1, 1, 2, 3, 5, 8],\n        ...         \"c\": [1, 4, 9, 16, 25, 36],\n        ...     }\n        ... )\n        >>> df\n           a  b   c\n        0  1  1   1\n        1  2  1   4\n        2  3  2   9\n        3  4  3  16\n        4  5  5  25\n        5  6  8  36\n        >>> df.diff()\n             a    b     c\n        0  NaN  NaN   NaN\n        1  1.0  0.0   3.0\n        2  1.0  1.0   5.0\n        3  1.0  1.0   7.0\n        4  1.0  2.0   9.0\n        5  1.0  3.0  11.0\n\n        Difference with previous column\n\n        >>> df.diff(axis=1)\n            a  b   c\n        0 NaN  0   0\n        1 NaN -1   3\n        2 NaN -1   7\n        3 NaN -1  13\n        4 NaN  0  20\n        5 NaN  2  28\n\n        Difference with 3rd previous row\n\n        >>> df.diff(periods=3)\n             a    b     c\n        0  NaN  NaN   NaN\n        1  NaN  NaN   NaN\n        2  NaN  NaN   NaN\n        3  3.0  2.0  15.0\n        4  3.0  4.0  21.0\n        5  3.0  6.0  27.0\n\n        Difference with following row\n\n        >>> df.diff(periods=-1)\n             a    b     c\n        0 -1.0  0.0  -3.0\n        1 -1.0 -1.0  -5.0\n        2 -1.0 -1.0  -7.0\n        3 -1.0 -2.0  -9.0\n        4 -1.0 -3.0 -11.0\n        5  NaN  NaN   NaN\n\n        Overflow in input dtype\n\n        >>> df = pd.DataFrame({\"a\": [1, 0]}, dtype=np.uint8)\n        >>> df.diff()\n               a\n        0    NaN\n        1  255.0\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 13773, "code": "    def aggregate(self, func=None, axis: Axis = 0, *args, **kwargs):\n        from pandas.core.apply import frame_apply\n        axis = self._get_axis_number(axis)\n        op = frame_apply(self, func=func, axis=axis, args=args, kwargs=kwargs)\n        result = op.agg()\n        result = reconstruct_and_relabel_result(result, func, **kwargs)\n        return result\n    agg = aggregate", "documentation": "        \"\"\"\n        Aggregate using one or more operations over the specified axis.\n\n        Parameters\n        ----------\n        func : function, str, list or dict\n            Function to use for aggregating the data. If a function, must either\n            work when passed a DataFrame or when passed to DataFrame.apply.\n\n            Accepted combinations are:\n\n            - function\n            - string function name\n            - list of functions and/or function names, e.g. ``[np.sum, 'mean']``\n            - dict of axis labels -> functions, function names or list of such.\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n                If 0 or 'index': apply function to each column.\n                If 1 or 'columns': apply function to each row.\n        *args\n            Positional arguments to pass to `func`.\n        **kwargs\n            Keyword arguments to pass to `func`.\n\n        Returns\n        -------\n        scalar, Series or DataFrame\n\n            The return can be:\n\n            * scalar : when Series.agg is called with single function\n            * Series : when DataFrame.agg is called with a single function\n            * DataFrame : when DataFrame.agg is called with several functions\n\n        See Also\n        --------\n        DataFrame.apply : Perform any type of operations.\n        DataFrame.transform : Perform transformation type operations.\n        DataFrame.groupby : Perform operations over groups.\n        DataFrame.resample : Perform operations over resampled bins.\n        DataFrame.rolling : Perform operations over rolling window.\n        DataFrame.expanding : Perform operations over expanding window.\n        core.window.ewm.ExponentialMovingWindow : Perform operation over exponential\n            weighted window.\n\n        Notes\n        -----\n        The aggregation operations are always performed over an axis, either the\n        index (default) or the column axis. This behavior is different from\n        `numpy` aggregation functions (`mean`, `median`, `prod`, `sum`, `std`,\n        `var`), where the default is to compute the aggregation of the flattened\n        array, e.g., ``numpy.mean(arr_2d)`` as opposed to\n        ``numpy.mean(arr_2d, axis=0)``.\n\n        `agg` is an alias for `aggregate`. Use the alias.\n\n        Functions that mutate the passed object can produce unexpected\n        behavior or errors and are not supported. See :ref:`gotchas.udf-mutation`\n        for more details.\n\n        A passed user-defined-function will be passed a Series for evaluation.\n\n        If ``func`` defines an index relabeling, ``axis`` must be ``0`` or ``index``.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     [[1, 2, 3], [4, 5, 6], [7, 8, 9], [np.nan, np.nan, np.nan]],\n        ...     columns=[\"A\", \"B\", \"C\"],\n        ... )\n\n        Aggregate these functions over the rows.\n\n        >>> df.agg([\"sum\", \"min\"])\n                A     B     C\n        sum  12.0  15.0  18.0\n        min   1.0   2.0   3.0\n\n        Different aggregations per column.\n\n        >>> df.agg({\"A\": [\"sum\", \"min\"], \"B\": [\"min\", \"max\"]})\n                A    B\n        sum  12.0  NaN\n        min   1.0  2.0\n        max   NaN  8.0\n\n        Aggregate different functions over the columns and rename the index of\n        the resulting DataFrame.\n\n        >>> df.agg(x=(\"A\", \"max\"), y=(\"B\", \"min\"), z=(\"C\", \"mean\"))\n             A    B    C\n        x  7.0  NaN  NaN\n        y  NaN  2.0  NaN\n        z  NaN  NaN  6.0\n\n        Aggregate over the columns.\n\n        >>> df.agg(\"mean\", axis=\"columns\")\n        0    2.0\n        1    5.0\n        2    8.0\n        3    NaN\n        dtype: float64\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 15500, "code": "    def count(self, axis: Axis = 0, numeric_only: bool = False) -> Series:\n        axis = self._get_axis_number(axis)\n        if numeric_only:\n            frame = self._get_numeric_data()\n        else:\n            frame = self\n        if len(frame._get_axis(axis)) == 0:\n            result = self._constructor_sliced(0, index=frame._get_agg_axis(axis))\n        else:\n            result = notna(frame).sum(axis=axis)\n        return result.astype(\"int64\").__finalize__(self, method=\"count\")", "documentation": "        \"\"\"\n        Count non-NA cells for each column or row.\n\n        The values `None`, `NaN`, `NaT`, ``pandas.NA`` are considered NA.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            If 0 or 'index' counts are generated for each column.\n            If 1 or 'columns' counts are generated for each row.\n        numeric_only : bool, default False\n            Include only `float`, `int` or `boolean` data.\n\n        Returns\n        -------\n        Series\n            For each column/row the number of non-NA/null entries.\n\n        See Also\n        --------\n        Series.count: Number of non-NA elements in a Series.\n        DataFrame.value_counts: Count unique combinations of columns.\n        DataFrame.shape: Number of DataFrame rows and columns (including NA\n            elements).\n        DataFrame.isna: Boolean same-sized DataFrame showing places of NA\n            elements.\n\n        Examples\n        --------\n        Constructing DataFrame from a dictionary:\n\n        >>> df = pd.DataFrame(\n        ...     {\n        ...         \"Person\": [\"John\", \"Myla\", \"Lewis\", \"John\", \"Myla\"],\n        ...         \"Age\": [24.0, np.nan, 21.0, 33, 26],\n        ...         \"Single\": [False, True, True, True, False],\n        ...     }\n        ... )\n        >>> df\n           Person   Age  Single\n        0    John  24.0   False\n        1    Myla   NaN    True\n        2   Lewis  21.0    True\n        3    John  33.0    True\n        4    Myla  26.0   False\n\n        Notice the uncounted NA values:\n\n        >>> df.count()\n        Person    5\n        Age       4\n        Single    5\n        dtype: int64\n\n        Counts for each **row**:\n\n        >>> df.count(axis=\"columns\")\n        0    3\n        1    2\n        2    3\n        3    3\n        4    3\n        dtype: int64\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 15691, "code": "    def _reduce_axis1(self, name: str, func, skipna: bool) -> Series:\n        if name == \"all\":\n            result = np.ones(len(self), dtype=bool)\n            ufunc = np.logical_and\n        elif name == \"any\":\n            result = np.zeros(len(self), dtype=bool)\n            ufunc = np.logical_or  # type: ignore[assignment]\n        else:\n            raise NotImplementedError(name)\n        for blocks in self._mgr.blocks:\n            middle = func(blocks.values, axis=0, skipna=skipna)", "documentation": "        \"\"\"\n        Special case for _reduce to try to avoid a potentially-expensive transpose.\n\n        Apply the reduction block-wise along axis=1 and then reduce the resulting\n        1D arrays.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 17699, "code": "    def nunique(self, axis: Axis = 0, dropna: bool = True) -> Series:\n        return self.apply(Series.nunique, axis=axis, dropna=dropna)", "documentation": "        \"\"\"\n        Count number of distinct elements in specified axis.\n\n        Return Series with number of distinct elements. Can ignore NaN\n        values.\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for\n            column-wise.\n        dropna : bool, default True\n            Don't include NaN in the counts.\n\n        Returns\n        -------\n        Series\n            Series with counts of unique values per row or column, depending on `axis`.\n\n        See Also\n        --------\n        Series.nunique: Method nunique for Series.\n        DataFrame.count: Count non-NA cells for each column or row.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"A\": [4, 5, 6], \"B\": [4, 1, 1]})\n        >>> df.nunique()\n        A    3\n        B    2\n        dtype: int64\n\n        >>> df.nunique(axis=1)\n        0    1\n        1    2\n        2    2\n        dtype: int64\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 17942, "code": "    def _get_agg_axis(self, axis_num: int) -> Index:\n        if axis_num == 0:\n            return self.columns\n        elif axis_num == 1:\n            return self.index\n        else:\n            raise ValueError(f\"Axis must be 0 or 1 (got {axis_num!r})\")", "documentation": "        \"\"\"\n        Let's be explicit about this.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 18422, "code": "    def isin(self, values: Series | DataFrame | Sequence | Mapping) -> DataFrame:\n        if isinstance(values, dict):\n            from pandas.core.reshape.concat import concat\n            values = collections.defaultdict(list, values)\n            result = concat(\n                (\n                    self.iloc[:, [i]].isin(values[col])\n                    for i, col in enumerate(self.columns)\n                ),\n                axis=1,\n            )", "documentation": "        \"\"\"\n        Whether each element in the DataFrame is contained in values.\n\n        Parameters\n        ----------\n        values : iterable, Series, DataFrame or dict\n            The result will only be true at a location if all the\n            labels match. If `values` is a Series, that's the index. If\n            `values` is a dict, the keys must be the column names,\n            which must match. If `values` is a DataFrame,\n            then both the index and column labels must match.\n\n        Returns\n        -------\n        DataFrame\n            DataFrame of booleans showing whether each element in the DataFrame\n            is contained in values.\n\n        See Also\n        --------\n        DataFrame.eq: Equality test for DataFrame.\n        Series.isin: Equivalent method on Series.\n        Series.str.contains: Test if pattern or regex is contained within a\n            string of a Series or Index.\n\n        Notes\n        -----\n            ``__iter__`` is used (and not ``__contains__``) to iterate over values\n            when checking if it contains the elements in DataFrame.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\"num_legs\": [2, 4], \"num_wings\": [2, 0]}, index=[\"falcon\", \"dog\"]\n        ... )\n        >>> df\n                num_legs  num_wings\n        falcon         2          2\n        dog            4          0\n\n        When ``values`` is a list check whether every value in the DataFrame\n        is present in the list (which animals have 0 or 2 legs or wings)\n\n        >>> df.isin([0, 2])\n                num_legs  num_wings\n        falcon      True       True\n        dog        False       True\n\n        To check if ``values`` is *not* in the DataFrame, use the ``~`` operator:\n\n        >>> ~df.isin([0, 2])\n                num_legs  num_wings\n        falcon     False      False\n        dog         True      False\n\n        When ``values`` is a dict, we can pass values to check for each\n        column separately:\n\n        >>> df.isin({\"num_wings\": [0, 3]})\n                num_legs  num_wings\n        falcon     False      False\n        dog        False       True\n\n        When ``values`` is a Series or DataFrame the index and column must\n        match. Note that 'falcon' does not match based on the number of legs\n        in other.\n\n        >>> other = pd.DataFrame(\n        ...     {\"num_legs\": [8, 3], \"num_wings\": [0, 2]}, index=[\"spider\", \"falcon\"]\n        ... )\n        >>> df.isin(other)\n                num_legs  num_wings\n        falcon     False       True\n        dog        False      False\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 18642, "code": "    def _to_dict_of_blocks(self):\n        mgr = self._mgr\n        return {\n            k: self._constructor_from_mgr(v, axes=v.axes).__finalize__(self)\n            for k, v in mgr.to_iter_dict()\n        }\n    @property", "documentation": "        \"\"\"\n        Return a dict of dtype -> Constructor Types that\n        each is a homogeneous dtype.\n\n        Internal ONLY.\n        \"\"\""}, {"filename": "pandas/core/frame.py", "start_line": 18656, "code": "    def values(self) -> np.ndarray:\n        return self._mgr.as_array()", "documentation": "        \"\"\"\n        Return a Numpy representation of the DataFrame.\n\n        .. warning::\n\n           We recommend using :meth:`DataFrame.to_numpy` instead.\n\n        Only the values in the DataFrame will be returned, the axes labels\n        will be removed.\n\n        Returns\n        -------\n        numpy.ndarray\n            The values of the DataFrame.\n\n        See Also\n        --------\n        DataFrame.to_numpy : Recommended alternative to this method.\n        DataFrame.index : Retrieve the index labels.\n        DataFrame.columns : Retrieving the column names.\n\n        Notes\n        -----\n        The dtype will be a lower-common-denominator dtype (implicit\n        upcasting); that is to say if the dtypes (even of numeric types)\n        are mixed, the one that accommodates all will be chosen. Use this\n        with care if you are not dealing with the blocks.\n\n        e.g. If the dtypes are float16 and float32, dtype will be upcast to\n        float32.  If dtypes are int32 and uint8, dtype will be upcast to\n        int32. By :func:`numpy.find_common_type` convention, mixing int64\n        and uint64 will result in a float64 dtype.\n\n        Examples\n        --------\n        A DataFrame where all columns are the same type (e.g., int64) results\n        in an array of the same type.\n\n        >>> df = pd.DataFrame(\n        ...     {\"age\": [3, 29], \"height\": [94, 170], \"weight\": [31, 115]}\n        ... )\n        >>> df\n           age  height  weight\n        0    3      94      31\n        1   29     170     115\n        >>> df.dtypes\n        age       int64\n        height    int64\n        weight    int64\n        dtype: object\n        >>> df.values\n        array([[  3,  94,  31],\n               [ 29, 170, 115]])\n\n        A DataFrame with mixed type columns(e.g., str/object, int64, float32)\n        results in an ndarray of the broadest type that accommodates these\n        mixed types (e.g., object).\n\n        >>> df2 = pd.DataFrame(\n        ...     [\n        ...         (\"parrot\", 24.0, \"second\"),\n        ...         (\"lion\", 80.5, 1),\n        ...         (\"monkey\", np.nan, None),\n        ...     ],\n        ...     columns=(\"name\", \"max_speed\", \"rank\"),\n        ... )\n        >>> df2.dtypes\n        name             str\n        max_speed    float64\n        rank          object\n        dtype: object\n        >>> df2.values\n        array([['parrot', 24.0, 'second'],\n               ['lion', 80.5, 1],\n               ['monkey', nan, None]], dtype=object)\n        \"\"\""}]}
{"repository": "numpy/numpy", "commit_sha": "4656d66f2c976a517ebfb5b7bbd97d0261850bc9", "commit_message": "Fix comment capitalization for clarity", "commit_date": "2026-02-10T13:28:12+00:00", "author": "mdrdope", "file": "numpy/_core/numeric.py", "patch": "@@ -2549,7 +2549,7 @@ def array_equal(a1, a2, equal_nan=False):\n     if cannot_have_nan:\n         return builtins.bool(asarray(a1 == a2).all())\n \n-    # fast path for a1 and a2 being all NaN arrays\n+    # Fast path for a1 and a2 being all NaN arrays\n     a1nan = isnan(a1)\n     if a1nan.all():\n         return builtins.bool(isnan(a2).all())", "before_segments": [{"filename": "numpy/_core/numeric.py", "start_line": 171, "code": "def ones(shape, dtype=None, order='C', *, device=None, like=None):\n    if like is not None:\n        return _ones_with_like(\n            like, shape, dtype=dtype, order=order, device=device\n        )\n    a = empty(shape, dtype, order, device=device)\n    multiarray.copyto(a, 1, casting='unsafe')\n    return a\n_ones_with_like = array_function_dispatch()(ones)", "documentation": "    \"\"\"\n    Return a new array of given shape and type, filled with ones.\n\n    Parameters\n    ----------\n    shape : int or sequence of ints\n        Shape of the new array, e.g., ``(2, 3)`` or ``2``.\n    dtype : data-type, optional\n        The desired data-type for the array, e.g., `numpy.int8`.  Default is\n        `numpy.float64`.\n    order : {'C', 'F'}, optional, default: C\n        Whether to store multi-dimensional data in row-major\n        (C-style) or column-major (Fortran-style) order in\n        memory.\n    device : str, optional\n        The device on which to place the created array. Default: None.\n        For Array-API interoperability only, so must be ``\"cpu\"`` if passed.\n\n        .. versionadded:: 2.0.0\n    ${ARRAY_FUNCTION_LIKE}\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    out : ndarray\n        Array of ones with the given shape, dtype, and order.\n\n    See Also\n    --------\n    ones_like : Return an array of ones with shape and type of input.\n    empty : Return a new uninitialized array.\n    zeros : Return a new array setting values to zero.\n    full : Return a new array of given shape filled with value.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.ones(5)\n    array([1., 1., 1., 1., 1.])\n\n    >>> np.ones((5,), dtype=np.int_)\n    array([1, 1, 1, 1, 1])\n\n    >>> np.ones((2, 1))\n    array([[1.],\n           [1.]])\n\n    >>> s = (2,2)\n    >>> np.ones(s)\n    array([[1.,  1.],\n           [1.,  1.]])\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 324, "code": "def full(shape, fill_value, dtype=None, order='C', *, device=None, like=None):\n    if like is not None:\n        return _full_with_like(\n            like, shape, fill_value, dtype=dtype, order=order, device=device\n        )\n    if dtype is None:\n        fill_value = asarray(fill_value)\n        dtype = fill_value.dtype\n    a = empty(shape, dtype, order, device=device)\n    multiarray.copyto(a, fill_value, casting='unsafe')\n    return a", "documentation": "    \"\"\"\n    Return a new array of given shape and type, filled with `fill_value`.\n\n    Parameters\n    ----------\n    shape : int or sequence of ints\n        Shape of the new array, e.g., ``(2, 3)`` or ``2``.\n    fill_value : scalar or array_like\n        Fill value.\n    dtype : data-type, optional\n        The desired data-type for the array  The default, None, means\n         ``np.array(fill_value).dtype``.\n    order : {'C', 'F'}, optional\n        Whether to store multidimensional data in C- or Fortran-contiguous\n        (row- or column-wise) order in memory.\n    device : str, optional\n        The device on which to place the created array. Default: None.\n        For Array-API interoperability only, so must be ``\"cpu\"`` if passed.\n\n        .. versionadded:: 2.0.0\n    ${ARRAY_FUNCTION_LIKE}\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    out : ndarray\n        Array of `fill_value` with the given shape, dtype, and order.\n\n    See Also\n    --------\n    full_like : Return a new array with shape of input filled with value.\n    empty : Return a new uninitialized array.\n    ones : Return a new array setting values to one.\n    zeros : Return a new array setting values to zero.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.full((2, 2), np.inf)\n    array([[inf, inf],\n           [inf, inf]])\n    >>> np.full((2, 2), 10)\n    array([[10, 10],\n           [10, 10]])\n\n    >>> np.full((2, 2), [1, 2])\n    array([[1, 2],\n           [1, 2]])\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 483, "code": "def count_nonzero(a, axis=None, *, keepdims=False):\n    if axis is None and not keepdims:\n        return multiarray.count_nonzero(a)\n    a = asanyarray(a)\n    if np.issubdtype(a.dtype, np.character):\n        a_bool = a != a.dtype.type()\n    else:\n        a_bool = a.astype(np.bool, copy=False)\n    return a_bool.sum(axis=axis, dtype=np.intp, keepdims=keepdims)\n@set_module('numpy')", "documentation": "    \"\"\"\n    Counts the number of non-zero values in the array ``a``.\n\n    The word \"non-zero\" is in reference to the Python 2.x\n    built-in method ``__nonzero__()`` (renamed ``__bool__()``\n    in Python 3.x) of Python objects that tests an object's\n    \"truthfulness\". For example, any number is considered\n    truthful if it is nonzero, whereas any string is considered\n    truthful if it is not the empty string. Thus, this function\n    (recursively) counts how many elements in ``a`` (and in\n    sub-arrays thereof) have their ``__nonzero__()`` or ``__bool__()``\n    method evaluated to ``True``.\n\n    Parameters\n    ----------\n    a : array_like\n        The array for which to count non-zeros.\n    axis : int or tuple, optional\n        Axis or tuple of axes along which to count non-zeros.\n        Default is None, meaning that non-zeros will be counted\n        along a flattened version of ``a``.\n    keepdims : bool, optional\n        If this is set to True, the axes that are counted are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input array.\n\n    Returns\n    -------\n    count : int or array of int\n        Number of non-zero values in the array along a given axis.\n        Otherwise, the total number of non-zero values in the array\n        is returned.\n\n    See Also\n    --------\n    nonzero : Return the coordinates of all the non-zero values.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.count_nonzero(np.eye(4))\n    np.int64(4)\n    >>> a = np.array([[0, 1, 7, 0],\n    ...               [3, 0, 2, 19]])\n    >>> np.count_nonzero(a)\n    np.int64(5)\n    >>> np.count_nonzero(a, axis=0)\n    array([1, 1, 2, 1])\n    >>> np.count_nonzero(a, axis=1)\n    array([2, 3])\n    >>> np.count_nonzero(a, axis=1, keepdims=True)\n    array([[2],\n           [3]])\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 553, "code": "def isfortran(a):\n    return a.flags.fnc", "documentation": "    \"\"\"\n    Check if the array is Fortran contiguous but *not* C contiguous.\n\n    This function is obsolete. If you only want to check if an array is Fortran\n    contiguous use ``a.flags.f_contiguous`` instead.\n\n    Parameters\n    ----------\n    a : ndarray\n        Input array.\n\n    Returns\n    -------\n    isfortran : bool\n        Returns True if the array is Fortran contiguous but *not* C contiguous.\n\n\n    Examples\n    --------\n\n    np.array allows to specify whether the array is written in C-contiguous\n    order (last index varies the fastest), or FORTRAN-contiguous order in\n    memory (first index varies the fastest).\n\n    >>> import numpy as np\n    >>> a = np.array([[1, 2, 3], [4, 5, 6]], order='C')\n    >>> a\n    array([[1, 2, 3],\n           [4, 5, 6]])\n    >>> np.isfortran(a)\n    False\n\n    >>> b = np.array([[1, 2, 3], [4, 5, 6]], order='F')\n    >>> b\n    array([[1, 2, 3],\n           [4, 5, 6]])\n    >>> np.isfortran(b)\n    True\n\n\n    The transpose of a C-ordered array is a FORTRAN-ordered array.\n\n    >>> a = np.array([[1, 2, 3], [4, 5, 6]], order='C')\n    >>> a\n    array([[1, 2, 3],\n           [4, 5, 6]])\n    >>> np.isfortran(a)\n    False\n    >>> b = a.T\n    >>> b\n    array([[1, 4],\n           [2, 5],\n           [3, 6]])\n    >>> np.isfortran(b)\n    True\n\n    C-ordered arrays evaluate as False even if they are also FORTRAN-ordered.\n\n    >>> np.isfortran(np.array([1, 2], order='F'))\n    False\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 624, "code": "def argwhere(a):\n    if np.ndim(a) == 0:\n        a = shape_base.atleast_1d(a)\n        return argwhere(a)[:, :0]\n    return transpose(nonzero(a))", "documentation": "    \"\"\"\n    Find the indices of array elements that are non-zero, grouped by element.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n\n    Returns\n    -------\n    index_array : (N, a.ndim) ndarray\n        Indices of elements that are non-zero. Indices are grouped by element.\n        This array will have shape ``(N, a.ndim)`` where ``N`` is the number of\n        non-zero items.\n\n    See Also\n    --------\n    where, nonzero\n\n    Notes\n    -----\n    ``np.argwhere(a)`` is almost the same as ``np.transpose(np.nonzero(a))``,\n    but produces a result of the correct shape for a 0D array.\n\n    The output of ``argwhere`` is not suitable for indexing arrays.\n    For this purpose use ``nonzero(a)`` instead.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> x = np.arange(6).reshape(2,3)\n    >>> x\n    array([[0, 1, 2],\n           [3, 4, 5]])\n    >>> np.argwhere(x>1)\n    array([[0, 2],\n           [1, 0],\n           [1, 1],\n           [1, 2]])\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 679, "code": "def flatnonzero(a):\n    return np.nonzero(np.ravel(a))[0]", "documentation": "    \"\"\"\n    Return indices that are non-zero in the flattened version of a.\n\n    This is equivalent to ``np.nonzero(np.ravel(a))[0]``.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n\n    Returns\n    -------\n    res : ndarray\n        Output array, containing the indices of the elements of ``a.ravel()``\n        that are non-zero.\n\n    See Also\n    --------\n    nonzero : Return the indices of the non-zero elements of the input array.\n    ravel : Return a 1-D array containing the elements of the input array.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> x = np.arange(-2, 3)\n    >>> x\n    array([-2, -1,  0,  1,  2])\n    >>> np.flatnonzero(x)\n    array([0, 1, 3, 4])\n\n    Use the indices of the non-zero elements as an index array to extract\n    these elements:\n\n    >>> x.ravel()[np.flatnonzero(x)]\n    array([-2, -1,  1,  2])\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 805, "code": "def convolve(a, v, mode='full'):\n    a, v = array(a, copy=None, ndmin=1), array(v, copy=None, ndmin=1)\n    if len(a) == 0:\n        raise ValueError('a cannot be empty')\n    if len(v) == 0:\n        raise ValueError('v cannot be empty')\n    if len(v) > len(a):\n        a, v = v, a\n    return multiarray.correlate(a, v[::-1], mode)", "documentation": "    \"\"\"\n    Returns the discrete, linear convolution of two one-dimensional sequences.\n\n    The convolution operator is often seen in signal processing, where it\n    models the effect of a linear time-invariant system on a signal [1]_.  In\n    probability theory, the sum of two independent random variables is\n    distributed according to the convolution of their individual\n    distributions.\n\n    If `v` is longer than `a`, the arrays are swapped before computation.\n\n    Parameters\n    ----------\n    a : (N,) array_like\n        First one-dimensional input array.\n    v : (M,) array_like\n        Second one-dimensional input array.\n    mode : {'full', 'valid', 'same'}, optional\n        'full':\n          By default, mode is 'full'.  This returns the convolution\n          at each point of overlap, with an output shape of (N+M-1,). At\n          the end-points of the convolution, the signals do not overlap\n          completely, and boundary effects may be seen.\n\n        'same':\n          Mode 'same' returns output of length ``max(M, N)``.  Boundary\n          effects are still visible.\n\n        'valid':\n          Mode 'valid' returns output of length\n          ``max(M, N) - min(M, N) + 1``.  The convolution product is only given\n          for points where the signals overlap completely.  Values outside\n          the signal boundary have no effect.\n\n    Returns\n    -------\n    out : ndarray\n        Discrete, linear convolution of `a` and `v`.\n\n    See Also\n    --------\n    scipy.signal.fftconvolve : Convolve two arrays using the Fast Fourier\n                               Transform.\n    scipy.linalg.toeplitz : Used to construct the convolution operator.\n    polymul : Polynomial multiplication. Same output as convolve, but also\n              accepts poly1d objects as input.\n\n    Notes\n    -----\n    The discrete convolution operation is defined as\n\n    .. math:: (a * v)_n = \\\\sum_{m = -\\\\infty}^{\\\\infty} a_m v_{n - m}\n\n    It can be shown that a convolution :math:`x(t) * y(t)` in time/space\n    is equivalent to the multiplication :math:`X(f) Y(f)` in the Fourier\n    domain, after appropriate padding (padding is necessary to prevent\n    circular convolution).  Since multiplication is more efficient (faster)\n    than convolution, the function `scipy.signal.fftconvolve` exploits the\n    FFT to calculate the convolution of large data-sets.\n\n    References\n    ----------\n    .. [1] Wikipedia, \"Convolution\",\n        https://en.wikipedia.org/wiki/Convolution\n\n    Examples\n    --------\n    Note how the convolution operator flips the second array\n    before \"sliding\" the two across one another:\n\n    >>> import numpy as np\n    >>> np.convolve([1, 2, 3], [0, 1, 0.5])\n    array([0. , 1. , 2.5, 4. , 1.5])\n\n    Only return the middle values of the convolution.\n    Contains boundary effects, where zeros are taken\n    into account:\n\n    >>> np.convolve([1,2,3],[0,1,0.5], 'same')\n    array([1. ,  2.5,  4. ])\n\n    The two arrays are of the same length, so there\n    is only one position where they completely overlap:\n\n    >>> np.convolve([1,2,3],[0,1,0.5], 'valid')\n    array([2.5])\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 909, "code": "def outer(a, b, out=None):\n    a = asarray(a)\n    b = asarray(b)\n    return multiply(a.ravel()[:, newaxis], b.ravel()[newaxis, :], out)", "documentation": "    \"\"\"\n    Compute the outer product of two vectors.\n\n    Given two vectors `a` and `b` of length ``M`` and ``N``, respectively,\n    the outer product [1]_ is::\n\n      [[a_0*b_0  a_0*b_1 ... a_0*b_{N-1} ]\n       [a_1*b_0    .\n       [ ...          .\n       [a_{M-1}*b_0            a_{M-1}*b_{N-1} ]]\n\n    Parameters\n    ----------\n    a : (M,) array_like\n        First input vector.  Input is flattened if\n        not already 1-dimensional.\n    b : (N,) array_like\n        Second input vector.  Input is flattened if\n        not already 1-dimensional.\n    out : (M, N) ndarray, optional\n        A location where the result is stored\n\n    Returns\n    -------\n    out : (M, N) ndarray\n        ``out[i, j] = a[i] * b[j]``\n\n    See also\n    --------\n    inner\n    einsum : ``einsum('i,j->ij', a.ravel(), b.ravel())`` is the equivalent.\n    ufunc.outer : A generalization to dimensions other than 1D and other\n                  operations. ``np.multiply.outer(a.ravel(), b.ravel())``\n                  is the equivalent.\n    linalg.outer : An Array API compatible variation of ``np.outer``,\n                   which accepts 1-dimensional inputs only.\n    tensordot : ``np.tensordot(a.ravel(), b.ravel(), axes=((), ()))``\n                is the equivalent.\n\n    References\n    ----------\n    .. [1] G. H. Golub and C. F. Van Loan, *Matrix Computations*, 3rd\n           ed., Baltimore, MD, Johns Hopkins University Press, 1996,\n           pg. 8.\n\n    Examples\n    --------\n    Make a (*very* coarse) grid for computing a Mandelbrot set:\n\n    >>> import numpy as np\n    >>> rl = np.outer(np.ones((5,)), np.linspace(-2, 2, 5))\n    >>> rl\n    array([[-2., -1.,  0.,  1.,  2.],\n           [-2., -1.,  0.,  1.,  2.],\n           [-2., -1.,  0.,  1.,  2.],\n           [-2., -1.,  0.,  1.,  2.],\n           [-2., -1.,  0.,  1.,  2.]])\n    >>> im = np.outer(1j*np.linspace(2, -2, 5), np.ones((5,)))\n    >>> im\n    array([[0.+2.j, 0.+2.j, 0.+2.j, 0.+2.j, 0.+2.j],\n           [0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j],\n           [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n           [0.-1.j, 0.-1.j, 0.-1.j, 0.-1.j, 0.-1.j],\n           [0.-2.j, 0.-2.j, 0.-2.j, 0.-2.j, 0.-2.j]])\n    >>> grid = rl + im\n    >>> grid\n    array([[-2.+2.j, -1.+2.j,  0.+2.j,  1.+2.j,  2.+2.j],\n           [-2.+1.j, -1.+1.j,  0.+1.j,  1.+1.j,  2.+1.j],\n           [-2.+0.j, -1.+0.j,  0.+0.j,  1.+0.j,  2.+0.j],\n           [-2.-1.j, -1.-1.j,  0.-1.j,  1.-1.j,  2.-1.j],\n           [-2.-2.j, -1.-2.j,  0.-2.j,  1.-2.j,  2.-2.j]])\n\n    An example using a \"vector\" of letters:\n\n    >>> x = np.array(['a', 'b', 'c'], dtype=np.object_)\n    >>> np.outer(x, [1, 2, 3])\n    array([['a', 'aa', 'aaa'],\n           ['b', 'bb', 'bbb'],\n           ['c', 'cc', 'ccc']], dtype=object)\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 1001, "code": "def tensordot(a, b, axes=2):\n    try:\n        iter(axes)\n    except Exception:\n        axes_a = list(range(-axes, 0))\n        axes_b = list(range(axes))\n    else:\n        axes_a, axes_b = axes\n    try:\n        na = len(axes_a)\n        axes_a = list(axes_a)", "documentation": "    \"\"\"\n    Compute tensor dot product along specified axes.\n\n    Given two tensors, `a` and `b`, and an array_like object containing\n    two array_like objects, ``(a_axes, b_axes)``, sum the products of\n    `a`'s and `b`'s elements (components) over the axes specified by\n    ``a_axes`` and ``b_axes``. The third argument can be a single non-negative\n    integer_like scalar, ``N``; if it is such, then the last ``N`` dimensions\n    of `a` and the first ``N`` dimensions of `b` are summed over.\n\n    Parameters\n    ----------\n    a, b : array_like\n        Tensors to \"dot\".\n\n    axes : int or (2,) array_like\n        * integer_like\n          If an int N, sum over the last N axes of `a` and the first N axes\n          of `b` in order. The sizes of the corresponding axes must match.\n        * (2,) array_like\n          Or, a list of axes to be summed over, first sequence applying to `a`,\n          second to `b`. Both elements array_like must be of the same length.\n          Each axis may appear at most once; repeated axes are not allowed.\n          For example, ``axes=([1, 1], [0, 0])`` is invalid.\n    Returns\n    -------\n    output : ndarray\n        The tensor dot product of the input.\n\n    See Also\n    --------\n    dot, einsum\n\n    Notes\n    -----\n    Three common use cases are:\n        * ``axes = 0`` : tensor product :math:`a\\\\otimes b`\n        * ``axes = 1`` : tensor dot product :math:`a\\\\cdot b`\n        * ``axes = 2`` : (default) tensor double contraction :math:`a:b`\n\n    When `axes` is integer_like, the sequence of axes for evaluation\n    will be: from the -Nth axis to the -1th axis in `a`,\n    and from the 0th axis to (N-1)th axis in `b`.\n    For example, ``axes = 2`` is the equal to\n    ``axes = [[-2, -1], [0, 1]]``.\n    When N-1 is smaller than 0, or when -N is larger than -1,\n    the element of `a` and `b` are defined as the `axes`.\n\n    When there is more than one axis to sum over - and they are not the last\n    (first) axes of `a` (`b`) - the argument `axes` should consist of\n    two sequences of the same length, with the first axis to sum over given\n    first in both sequences, the second axis second, and so forth.\n    The calculation can be referred to ``numpy.einsum``.\n\n    For example, if ``a.shape == (2, 3, 4)`` and ``b.shape == (3, 4, 5)``,\n    then ``axes=([1, 2], [0, 1])`` sums over the ``(3, 4)`` dimensions of\n    both arrays and produces an output of shape ``(2, 5)``.\n\n    Each summation axis corresponds to a distinct contraction index; repeating\n    an axis (for example ``axes=([1, 1], [0, 0])``) is invalid.\n\n    The shape of the result consists of the non-contracted axes of the\n    first tensor, followed by the non-contracted axes of the second.\n\n    Examples\n    --------\n    An example on integer_like:\n\n    >>> a_0 = np.array([[1, 2], [3, 4]])\n    >>> b_0 = np.array([[5, 6], [7, 8]])\n    >>> c_0 = np.tensordot(a_0, b_0, axes=0)\n    >>> c_0.shape\n    (2, 2, 2, 2)\n    >>> c_0\n    array([[[[ 5,  6],\n             [ 7,  8]],\n            [[10, 12],\n             [14, 16]]],\n           [[[15, 18],\n             [21, 24]],\n            [[20, 24],\n             [28, 32]]]])\n\n    An example on array_like:\n\n    >>> a = np.arange(60.).reshape(3,4,5)\n    >>> b = np.arange(24.).reshape(4,3,2)\n    >>> c = np.tensordot(a,b, axes=([1,0],[0,1]))\n    >>> c.shape\n    (5, 2)\n    >>> c\n    array([[4400., 4730.],\n           [4532., 4874.],\n           [4664., 5018.],\n           [4796., 5162.],\n           [4928., 5306.]])\n\n    A slower but equivalent way of computing the same...\n\n    >>> d = np.zeros((5,2))\n    >>> for i in range(5):\n    ...   for j in range(2):\n    ...     for k in range(3):\n    ...       for n in range(4):\n    ...         d[i,j] += a[k,n,i] * b[n,k,j]\n    >>> c == d\n    array([[ True,  True],\n           [ True,  True],\n           [ True,  True],\n           [ True,  True],\n           [ True,  True]])\n\n    An extended example taking advantage of the overloading of + and \\\\*:\n\n    >>> a = np.array(range(1, 9)).reshape((2, 2, 2))\n    >>> A = np.array(('a', 'b', 'c', 'd'), dtype=np.object_)\n    >>> A = A.reshape((2, 2))\n    >>> a; A\n    array([[[1, 2],\n            [3, 4]],\n           [[5, 6],\n            [7, 8]]])\n    array([['a', 'b'],\n           ['c', 'd']], dtype=object)\n\n    >>> np.tensordot(a, A) # third argument default is 2 for double-contraction\n    array(['abbcccdddd', 'aaaaabbbbbbcccccccdddddddd'], dtype=object)\n\n    >>> np.tensordot(a, A, 1)\n    array([[['acc', 'bdd'],\n            ['aaacccc', 'bbbdddd']],\n           [['aaaaacccccc', 'bbbbbdddddd'],\n            ['aaaaaaacccccccc', 'bbbbbbbdddddddd']]], dtype=object)\n\n    >>> np.tensordot(a, A, 0) # tensor product (result too long to incl.)\n    array([[[[['a', 'b'],\n              ['c', 'd']],\n              ...\n\n    >>> np.tensordot(a, A, (0, 1))\n    array([[['abbbbb', 'cddddd'],\n            ['aabbbbbb', 'ccdddddd']],\n           [['aaabbbbbbb', 'cccddddddd'],\n            ['aaaabbbbbbbb', 'ccccdddddddd']]], dtype=object)\n\n    >>> np.tensordot(a, A, (2, 1))\n    array([[['abb', 'cdd'],\n            ['aaabbbb', 'cccdddd']],\n           [['aaaaabbbbbb', 'cccccdddddd'],\n            ['aaaaaaabbbbbbbb', 'cccccccdddddddd']]], dtype=object)\n\n    >>> np.tensordot(a, A, ((0, 1), (0, 1)))\n    array(['abbbcccccddddddd', 'aabbbbccccccdddddddd'], dtype=object)\n\n    >>> np.tensordot(a, A, ((2, 1), (1, 0)))\n    array(['acccbbdddd', 'aaaaacccccccbbbbbbdddddddd'], dtype=object)\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 1230, "code": "def roll(a, shift, axis=None):\n    a = asanyarray(a)\n    if axis is None:\n        return roll(a.ravel(), shift, 0).reshape(a.shape)\n    else:\n        axis = normalize_axis_tuple(axis, a.ndim, allow_duplicate=True)\n        broadcasted = broadcast(shift, axis)\n        if broadcasted.ndim > 1:\n            raise ValueError(\n                \"'shift' and 'axis' should be scalars or 1D sequences\")\n        shifts = dict.fromkeys(range(a.ndim), 0)", "documentation": "    \"\"\"\n    Roll array elements along a given axis.\n\n    Elements that roll beyond the last position are re-introduced at\n    the first.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    shift : int or tuple of ints\n        The number of places by which elements are shifted.  If a tuple,\n        then `axis` must be a tuple of the same size, and each of the\n        given axes is shifted by the corresponding number.  If an int\n        while `axis` is a tuple of ints, then the same value is used for\n        all given axes.\n    axis : int or tuple of ints, optional\n        Axis or axes along which elements are shifted.  By default, the\n        array is flattened before shifting, after which the original\n        shape is restored.\n\n    Returns\n    -------\n    res : ndarray\n        Output array, with the same shape as `a`.\n\n    See Also\n    --------\n    rollaxis : Roll the specified axis backwards, until it lies in a\n               given position.\n\n    Notes\n    -----\n    Supports rolling over multiple dimensions simultaneously.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> x = np.arange(10)\n    >>> np.roll(x, 2)\n    array([8, 9, 0, 1, 2, 3, 4, 5, 6, 7])\n    >>> np.roll(x, -2)\n    array([2, 3, 4, 5, 6, 7, 8, 9, 0, 1])\n\n    >>> x2 = np.reshape(x, (2, 5))\n    >>> x2\n    array([[0, 1, 2, 3, 4],\n           [5, 6, 7, 8, 9]])\n    >>> np.roll(x2, 1)\n    array([[9, 0, 1, 2, 3],\n           [4, 5, 6, 7, 8]])\n    >>> np.roll(x2, -1)\n    array([[1, 2, 3, 4, 5],\n           [6, 7, 8, 9, 0]])\n    >>> np.roll(x2, 1, axis=0)\n    array([[5, 6, 7, 8, 9],\n           [0, 1, 2, 3, 4]])\n    >>> np.roll(x2, -1, axis=0)\n    array([[5, 6, 7, 8, 9],\n           [0, 1, 2, 3, 4]])\n    >>> np.roll(x2, 1, axis=1)\n    array([[4, 0, 1, 2, 3],\n           [9, 5, 6, 7, 8]])\n    >>> np.roll(x2, -1, axis=1)\n    array([[1, 2, 3, 4, 0],\n           [6, 7, 8, 9, 5]])\n    >>> np.roll(x2, (1, 1), axis=(1, 0))\n    array([[9, 5, 6, 7, 8],\n           [4, 0, 1, 2, 3]])\n    >>> np.roll(x2, (2, 1), axis=(1, 0))\n    array([[8, 9, 5, 6, 7],\n           [3, 4, 0, 1, 2]])\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 1340, "code": "def rollaxis(a, axis, start=0):\n    n = a.ndim\n    axis = normalize_axis_index(axis, n)\n    if start < 0:\n        start += n\n    msg = \"'%s' arg requires %d <= %s < %d, but %d was passed in\"\n    if not (0 <= start < n + 1):\n        raise AxisError(msg % ('start', -n, 'start', n + 1, start))\n    if axis < start:\n        start -= 1\n    if axis == start:", "documentation": "    \"\"\"\n    Roll the specified axis backwards, until it lies in a given position.\n\n    This function continues to be supported for backward compatibility, but you\n    should prefer `moveaxis`. The `moveaxis` function was added in NumPy\n    1.11.\n\n    Parameters\n    ----------\n    a : ndarray\n        Input array.\n    axis : int\n        The axis to be rolled. The positions of the other axes do not\n        change relative to one another.\n    start : int, optional\n        When ``start <= axis``, the axis is rolled back until it lies in\n        this position. When ``start > axis``, the axis is rolled until it\n        lies before this position. The default, 0, results in a \"complete\"\n        roll. The following table describes how negative values of ``start``\n        are interpreted:\n\n        .. table::\n           :align: left\n\n           +-------------------+----------------------+\n           |     ``start``     | Normalized ``start`` |\n           +===================+======================+\n           | ``-(arr.ndim+1)`` | raise ``AxisError``  |\n           +-------------------+----------------------+\n           | ``-arr.ndim``     | 0                    |\n           +-------------------+----------------------+\n           | |vdots|           | |vdots|              |\n           +-------------------+----------------------+\n           | ``-1``            | ``arr.ndim-1``       |\n           +-------------------+----------------------+\n           | ``0``             | ``0``                |\n           +-------------------+----------------------+\n           | |vdots|           | |vdots|              |\n           +-------------------+----------------------+\n           | ``arr.ndim``      | ``arr.ndim``         |\n           +-------------------+----------------------+\n           | ``arr.ndim + 1``  | raise ``AxisError``  |\n           +-------------------+----------------------+\n\n        .. |vdots|   unicode:: U+22EE .. Vertical Ellipsis\n\n    Returns\n    -------\n    res : ndarray\n        For NumPy >= 1.10.0 a view of `a` is always returned. For earlier\n        NumPy versions a view of `a` is returned only if the order of the\n        axes is changed, otherwise the input array is returned.\n\n    See Also\n    --------\n    moveaxis : Move array axes to new positions.\n    roll : Roll the elements of an array by a number of positions along a\n        given axis.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> a = np.ones((3,4,5,6))\n    >>> np.rollaxis(a, 3, 1).shape\n    (3, 6, 4, 5)\n    >>> np.rollaxis(a, 2).shape\n    (5, 3, 4, 6)\n    >>> np.rollaxis(a, 1, 4).shape\n    (3, 5, 6, 4)\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 1431, "code": "def normalize_axis_tuple(axis, ndim, argname=None, allow_duplicate=False):\n    if not isinstance(axis, (tuple, list)):\n        try:\n            axis = [operator.index(axis)]\n        except TypeError:\n            pass\n    axis = tuple(normalize_axis_index(ax, ndim, argname) for ax in axis)\n    if not allow_duplicate and len(set(axis)) != len(axis):\n        if argname:\n            raise ValueError(f'repeated axis in `{argname}` argument')\n        else:", "documentation": "    \"\"\"\n    Normalizes an axis argument into a tuple of non-negative integer axes.\n\n    This handles shorthands such as ``1`` and converts them to ``(1,)``,\n    as well as performing the handling of negative indices covered by\n    `normalize_axis_index`.\n\n    By default, this forbids axes from being specified multiple times.\n\n    Used internally by multi-axis-checking logic.\n\n    Parameters\n    ----------\n    axis : int, iterable of int\n        The un-normalized index or indices of the axis.\n    ndim : int\n        The number of dimensions of the array that `axis` should be normalized\n        against.\n    argname : str, optional\n        A prefix to put before the error message, typically the name of the\n        argument.\n    allow_duplicate : bool, optional\n        If False, the default, disallow an axis from being specified twice.\n\n    Returns\n    -------\n    normalized_axes : tuple of int\n        The normalized axis index, such that `0 <= normalized_axis < ndim`\n\n    Raises\n    ------\n    AxisError\n        If any axis provided is out of range\n    ValueError\n        If an axis is repeated\n\n    See also\n    --------\n    normalize_axis_index : normalizing a single scalar axis\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 1493, "code": "def moveaxis(a, source, destination):\n    try:\n        transpose = a.transpose\n    except AttributeError:\n        a = asarray(a)\n        transpose = a.transpose\n    source = normalize_axis_tuple(source, a.ndim, 'source')\n    destination = normalize_axis_tuple(destination, a.ndim, 'destination')\n    if len(source) != len(destination):\n        raise ValueError('`source` and `destination` arguments must have '\n                         'the same number of elements')", "documentation": "    \"\"\"\n    Move axes of an array to new positions.\n\n    Other axes remain in their original order.\n\n    Parameters\n    ----------\n    a : np.ndarray\n        The array whose axes should be reordered.\n    source : int or sequence of int\n        Original positions of the axes to move. These must be unique.\n    destination : int or sequence of int\n        Destination positions for each of the original axes. These must also be\n        unique.\n\n    Returns\n    -------\n    result : np.ndarray\n        Array with moved axes. This array is a view of the input array.\n\n    See Also\n    --------\n    transpose : Permute the dimensions of an array.\n    swapaxes : Interchange two axes of an array.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> x = np.zeros((3, 4, 5))\n    >>> np.moveaxis(x, 0, -1).shape\n    (4, 5, 3)\n    >>> np.moveaxis(x, -1, 0).shape\n    (5, 3, 4)\n\n    These all achieve the same result:\n\n    >>> np.transpose(x).shape\n    (5, 4, 3)\n    >>> np.swapaxes(x, 0, -1).shape\n    (5, 4, 3)\n    >>> np.moveaxis(x, [0, 1], [-1, -2]).shape\n    (5, 4, 3)\n    >>> np.moveaxis(x, [0, 1, 2], [-1, -2, -3]).shape\n    (5, 4, 3)\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 1567, "code": "def cross(a, b, axisa=-1, axisb=-1, axisc=-1, axis=None):\n    if axis is not None:\n        axisa, axisb, axisc = (axis,) * 3\n    a = asarray(a)\n    b = asarray(b)\n    if (a.ndim < 1) or (b.ndim < 1):\n        raise ValueError(\"At least one array has zero dimension\")\n    axisa = normalize_axis_index(axisa, a.ndim, msg_prefix='axisa')\n    axisb = normalize_axis_index(axisb, b.ndim, msg_prefix='axisb')\n    a = moveaxis(a, axisa, -1)\n    b = moveaxis(b, axisb, -1)", "documentation": "    \"\"\"\n    Return the cross product of two (arrays of) vectors.\n\n    The cross product of `a` and `b` in :math:`R^3` is a vector perpendicular\n    to both `a` and `b`.  If `a` and `b` are arrays of vectors, the vectors\n    are defined by the last axis of `a` and `b` by default, and these axes\n    must have 3 dimensions.\n\n    Parameters\n    ----------\n    a : array_like\n        Components of the first vector(s).\n    b : array_like\n        Components of the second vector(s).\n    axisa : int, optional\n        Axis of `a` that defines the vector(s).  By default, the last axis.\n    axisb : int, optional\n        Axis of `b` that defines the vector(s).  By default, the last axis.\n    axisc : int, optional\n        Axis of `c` containing the cross product vector(s).  By default, the last axis.\n    axis : int, optional\n        If defined, the axis of `a`, `b` and `c` that defines the vector(s)\n        and cross product(s).  Overrides `axisa`, `axisb` and `axisc`.\n\n    Returns\n    -------\n    c : ndarray\n        Vector cross product(s).\n\n    Raises\n    ------\n    ValueError\n        When the dimension of the vector(s) in `a` or `b` does not equal 3.\n\n    See Also\n    --------\n    inner : Inner product\n    outer : Outer product.\n    linalg.cross : An Array API compatible variation of ``np.cross``.\n    ix_ : Construct index arrays.\n\n    Notes\n    -----\n    Supports full broadcasting of the inputs.\n\n    Examples\n    --------\n    Vector cross-product.\n\n    >>> import numpy as np\n    >>> x = [1, 2, 3]\n    >>> y = [4, 5, 6]\n    >>> np.cross(x, y)\n    array([-3,  6, -3])\n\n    One vector with dimension 2.\n\n    >>> x = [1, 2, 0]\n    >>> y = [4, 5, 6]\n    >>> np.cross(x, y)\n    array([12, -6, -3])\n\n    Both vectors with dimension 2.\n\n    >>> x = [1, 2, 0]\n    >>> y = [4, 5, 0]\n    >>> np.cross(x, y)\n    array([0, 0, -3])\n\n    Multiple vector cross-products. Note that the direction of the cross\n    product vector is defined by the *right-hand rule*.\n\n    >>> x = np.array([[1,2,3], [4,5,6]])\n    >>> y = np.array([[4,5,6], [1,2,3]])\n    >>> np.cross(x, y)\n    array([[-3,  6, -3],\n           [ 3, -6,  3]])\n\n    The orientation of `c` can be changed using the `axisc` keyword.\n\n    >>> np.cross(x, y, axisc=0)\n    array([[-3,  3],\n           [ 6, -6],\n           [-3,  3]])\n\n    Change the vector definition of `x` and `y` using `axisa` and `axisb`.\n\n    >>> x = np.array([[1,2,3], [4,5,6], [7, 8, 9]])\n    >>> y = np.array([[7, 8, 9], [4,5,6], [1,2,3]])\n    >>> np.cross(x, y)\n    array([[ -6,  12,  -6],\n           [  0,   0,   0],\n           [  6, -12,   6]])\n    >>> np.cross(x, y, axisa=0, axisb=0)\n    array([[-24,  48, -24],\n           [-30,  60, -30],\n           [-36,  72, -36]])\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 1730, "code": "def indices(dimensions, dtype=int, sparse=False):\n    dimensions = tuple(dimensions)\n    N = len(dimensions)\n    shape = (1,) * N\n    if sparse:\n        res = ()\n    else:\n        res = empty((N,) + dimensions, dtype=dtype)\n    for i, dim in enumerate(dimensions):\n        idx = arange(dim, dtype=dtype).reshape(\n            shape[:i] + (dim,) + shape[i + 1:]", "documentation": "    \"\"\"\n    Return an array representing the indices of a grid.\n\n    Compute an array where the subarrays contain index values 0, 1, ...\n    varying only along the corresponding axis.\n\n    Parameters\n    ----------\n    dimensions : sequence of ints\n        The shape of the grid.\n    dtype : dtype, optional\n        Data type of the result.\n    sparse : boolean, optional\n        Return a sparse representation of the grid instead of a dense\n        representation. Default is False.\n\n    Returns\n    -------\n    grid : one ndarray or tuple of ndarrays\n        If sparse is False:\n            Returns one array of grid indices,\n            ``grid.shape = (len(dimensions),) + tuple(dimensions)``.\n        If sparse is True:\n            Returns a tuple of arrays, with\n            ``grid[i].shape = (1, ..., 1, dimensions[i], 1, ..., 1)`` with\n            dimensions[i] in the ith place\n\n    See Also\n    --------\n    mgrid, ogrid, meshgrid\n\n    Notes\n    -----\n    The output shape in the dense case is obtained by prepending the number\n    of dimensions in front of the tuple of dimensions, i.e. if `dimensions`\n    is a tuple ``(r0, ..., rN-1)`` of length ``N``, the output shape is\n    ``(N, r0, ..., rN-1)``.\n\n    The subarrays ``grid[k]`` contains the N-D array of indices along the\n    ``k-th`` axis. Explicitly::\n\n        grid[k, i0, i1, ..., iN-1] = ik\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> grid = np.indices((2, 3))\n    >>> grid.shape\n    (2, 2, 3)\n    >>> grid[0]        # row indices\n    array([[0, 0, 0],\n           [1, 1, 1]])\n    >>> grid[1]        # column indices\n    array([[0, 1, 2],\n           [0, 1, 2]])\n\n    The indices can be used as an index into an array.\n\n    >>> x = np.arange(20).reshape(5, 4)\n    >>> row, col = np.indices((2, 3))\n    >>> x[row, col]\n    array([[0, 1, 2],\n           [4, 5, 6]])\n\n    Note that it would be more straightforward in the above example to\n    extract the required elements directly with ``x[:2, :3]``.\n\n    If sparse is set to true, the grid will be returned in a sparse\n    representation.\n\n    >>> i, j = np.indices((2, 3), sparse=True)\n    >>> i.shape\n    (2, 1)\n    >>> j.shape\n    (1, 3)\n    >>> i        # row indices\n    array([[0],\n           [1]])\n    >>> j        # column indices\n    array([[0, 1, 2]])\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 1833, "code": "def fromfunction(function, shape, *, dtype=float, like=None, **kwargs):\n    if like is not None:\n        return _fromfunction_with_like(\n                like, function, shape, dtype=dtype, **kwargs)\n    args = indices(shape, dtype=dtype)\n    return function(*args, **kwargs)\n_fromfunction_with_like = array_function_dispatch()(fromfunction)", "documentation": "    \"\"\"\n    Construct an array by executing a function over each coordinate.\n\n    The resulting array therefore has a value ``fn(x, y, z)`` at\n    coordinate ``(x, y, z)``.\n\n    Parameters\n    ----------\n    function : callable\n        The function is called with N parameters, where N is the rank of\n        `shape`.  Each parameter represents the coordinates of the array\n        varying along a specific axis.  For example, if `shape`\n        were ``(2, 2)``, then the parameters would be\n        ``array([[0, 0], [1, 1]])`` and ``array([[0, 1], [0, 1]])``\n    shape : (N,) tuple of ints\n        Shape of the output array, which also determines the shape of\n        the coordinate arrays passed to `function`.\n    dtype : data-type, optional\n        Data-type of the coordinate arrays passed to `function`.\n        By default, `dtype` is float.\n    ${ARRAY_FUNCTION_LIKE}\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    fromfunction : any\n        The result of the call to `function` is passed back directly.\n        Therefore the shape of `fromfunction` is completely determined by\n        `function`.  If `function` returns a scalar value, the shape of\n        `fromfunction` would not match the `shape` parameter.\n\n    See Also\n    --------\n    indices, meshgrid\n\n    Notes\n    -----\n    Keywords other than `dtype` and `like` are passed to `function`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.fromfunction(lambda i, j: i, (2, 2), dtype=np.float64)\n    array([[0., 0.],\n           [1., 1.]])\n\n    >>> np.fromfunction(lambda i, j: j, (2, 2), dtype=np.float64)\n    array([[0., 1.],\n           [0., 1.]])\n\n    >>> np.fromfunction(lambda i, j: i == j, (3, 3), dtype=np.int_)\n    array([[ True, False, False],\n           [False,  True, False],\n           [False, False,  True]])\n\n    >>> np.fromfunction(lambda i, j: i + j, (3, 3), dtype=np.int_)\n    array([[0, 1, 2],\n           [1, 2, 3],\n           [2, 3, 4]])\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 1915, "code": "def isscalar(element):\n    return (isinstance(element, generic)\n            or type(element) in ScalarType\n            or isinstance(element, numbers.Number))\n@set_module('numpy')", "documentation": "    \"\"\"\n    Returns True if the type of `element` is a scalar type.\n\n    Parameters\n    ----------\n    element : any\n        Input argument, can be of any type and shape.\n\n    Returns\n    -------\n    val : bool\n        True if `element` is a scalar type, False if it is not.\n\n    See Also\n    --------\n    ndim : Get the number of dimensions of an array\n\n    Notes\n    -----\n    If you need a stricter way to identify a *numerical* scalar, use\n    ``isinstance(x, numbers.Number)``, as that returns ``False`` for most\n    non-numerical elements such as strings.\n\n    In most cases ``np.ndim(x) == 0`` should be used instead of this function,\n    as that will also return true for 0d arrays. This is how numpy overloads\n    functions in the style of the ``dx`` arguments to `gradient` and\n    the ``bins`` argument to `histogram`. Some key differences:\n\n    +------------------------------------+---------------+-------------------+\n    | x                                  |``isscalar(x)``|``np.ndim(x) == 0``|\n    +====================================+===============+===================+\n    | PEP 3141 numeric objects           | ``True``      | ``True``          |\n    | (including builtins)               |               |                   |\n    +------------------------------------+---------------+-------------------+\n    | builtin string and buffer objects  | ``True``      | ``True``          |\n    +------------------------------------+---------------+-------------------+\n    | other builtin objects, like        | ``False``     | ``True``          |\n    | `pathlib.Path`, `Exception`,       |               |                   |\n    | the result of `re.compile`         |               |                   |\n    +------------------------------------+---------------+-------------------+\n    | third-party objects like           | ``False``     | ``True``          |\n    | `matplotlib.figure.Figure`         |               |                   |\n    +------------------------------------+---------------+-------------------+\n    | zero-dimensional numpy arrays      | ``False``     | ``True``          |\n    +------------------------------------+---------------+-------------------+\n    | other numpy arrays                 | ``False``     | ``False``         |\n    +------------------------------------+---------------+-------------------+\n    | `list`, `tuple`, and other         | ``False``     | ``False``         |\n    | sequence objects                   |               |                   |\n    +------------------------------------+---------------+-------------------+\n\n    Examples\n    --------\n    >>> import numpy as np\n\n    >>> np.isscalar(3.1)\n    True\n\n    >>> np.isscalar(np.array(3.1))\n    False\n\n    >>> np.isscalar([3.1])\n    False\n\n    >>> np.isscalar(False)\n    True\n\n    >>> np.isscalar('numpy')\n    True\n\n    NumPy supports PEP 3141 numbers:\n\n    >>> from fractions import Fraction\n    >>> np.isscalar(Fraction(5, 17))\n    True\n    >>> from numbers import Number\n    >>> np.isscalar(Number())\n    True\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 2002, "code": "def binary_repr(num, width=None):", "documentation": "    \"\"\"\n    Return the binary representation of the input number as a string.\n\n    For negative numbers, if width is not given, a minus sign is added to the\n    front. If width is given, the two's complement of the number is\n    returned, with respect to that width.\n\n    In a two's-complement system negative numbers are represented by the two's\n    complement of the absolute value. This is the most common method of\n    representing signed integers on computers [1]_. A N-bit two's-complement\n    system can represent every integer in the range\n    :math:`-2^{N-1}` to :math:`+2^{N-1}-1`.\n\n    Parameters\n    ----------\n    num : int\n        Only an integer decimal number can be used.\n    width : int, optional\n        The length of the returned string if `num` is positive, or the length\n        of the two's complement if `num` is negative, provided that `width` is\n        at least a sufficient number of bits for `num` to be represented in\n        the designated form. If the `width` value is insufficient, an error is\n        raised.\n\n    Returns\n    -------\n    bin : str\n        Binary representation of `num` or two's complement of `num`.\n\n    See Also\n    --------\n    base_repr: Return a string representation of a number in the given base\n               system.\n    bin: Python's built-in binary representation generator of an integer.\n\n    Notes\n    -----\n    `binary_repr` is equivalent to using `base_repr` with base 2, but about 25x\n    faster.\n\n    References\n    ----------\n    .. [1] Wikipedia, \"Two's complement\",\n        https://en.wikipedia.org/wiki/Two's_complement\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.binary_repr(3)\n    '11'\n    >>> np.binary_repr(-3)\n    '-11'\n    >>> np.binary_repr(3, width=4)\n    '0011'\n\n    The two's complement is returned when the input number is negative and\n    width is specified:\n\n    >>> np.binary_repr(-3, width=3)\n    '101'\n    >>> np.binary_repr(-3, width=5)\n    '11101'\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 2109, "code": "def base_repr(number, base=2, padding=0):\n    digits = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n    if base > len(digits):\n        raise ValueError(\"Bases greater than 36 not handled in base_repr.\")\n    elif base < 2:\n        raise ValueError(\"Bases less than 2 not handled in base_repr.\")\n    num = abs(int(number))\n    res = []\n    while num:\n        res.append(digits[num % base])\n        num //= base", "documentation": "    \"\"\"\n    Return a string representation of a number in the given base system.\n\n    Parameters\n    ----------\n    number : int\n        The value to convert. Positive and negative values are handled.\n    base : int, optional\n        Convert `number` to the `base` number system. The valid range is 2-36,\n        the default value is 2.\n    padding : int, optional\n        Number of zeros padded on the left. Default is 0 (no padding).\n\n    Returns\n    -------\n    out : str\n        String representation of `number` in `base` system.\n\n    See Also\n    --------\n    binary_repr : Faster version of `base_repr` for base 2.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.base_repr(5)\n    '101'\n    >>> np.base_repr(6, 5)\n    '11'\n    >>> np.base_repr(7, base=5, padding=3)\n    '00012'\n\n    >>> np.base_repr(10, base=16)\n    'A'\n    >>> np.base_repr(32, base=16)\n    '20'\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 2183, "code": "def identity(n, dtype=None, *, like=None):\n    if like is not None:\n        return _identity_with_like(like, n, dtype=dtype)\n    from numpy import eye\n    return eye(n, dtype=dtype, like=like)\n_identity_with_like = array_function_dispatch()(identity)", "documentation": "    \"\"\"\n    Return the identity array.\n\n    The identity array is a square array with ones on\n    the main diagonal.\n\n    Parameters\n    ----------\n    n : int\n        Number of rows (and columns) in `n` x `n` output.\n    dtype : data-type, optional\n        Data-type of the output.  Defaults to ``float``.\n    ${ARRAY_FUNCTION_LIKE}\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    out : ndarray\n        `n` x `n` array with its main diagonal set to one,\n        and all other elements 0.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.identity(3)\n    array([[1.,  0.,  0.],\n           [0.,  1.,  0.],\n           [0.,  0.,  1.]])\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 2230, "code": "def allclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):\n    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n    return builtins.bool(res)", "documentation": "    \"\"\"\n    Returns True if two arrays are element-wise equal within a tolerance.\n\n    The tolerance values are positive, typically very small numbers.  The\n    relative difference (`rtol` * abs(`b`)) and the absolute difference\n    `atol` are added together to compare against the absolute difference\n    between `a` and `b`.\n\n    .. warning:: The default `atol` is not appropriate for comparing numbers\n                 with magnitudes much smaller than one (see Notes).\n\n    NaNs are treated as equal if they are in the same place and if\n    ``equal_nan=True``.  Infs are treated as equal if they are in the same\n    place and of the same sign in both arrays.\n\n    Parameters\n    ----------\n    a, b : array_like\n        Input arrays to compare.\n    rtol : array_like\n        The relative tolerance parameter (see Notes).\n    atol : array_like\n        The absolute tolerance parameter (see Notes).\n    equal_nan : bool\n        Whether to compare NaN's as equal.  If True, NaN's in `a` will be\n        considered equal to NaN's in `b` in the output array.\n\n    Returns\n    -------\n    allclose : bool\n        Returns True if the two arrays are equal within the given\n        tolerance; False otherwise.\n\n    See Also\n    --------\n    isclose, all, any, equal\n\n    Notes\n    -----\n    If the following equation is element-wise True, then allclose returns\n    True.::\n\n     absolute(a - b) <= (atol + rtol * absolute(b))\n\n    The above equation is not symmetric in `a` and `b`, so that\n    ``allclose(a, b)`` might be different from ``allclose(b, a)`` in\n    some rare cases.\n\n    The default value of `atol` is not appropriate when the reference value\n    `b` has magnitude smaller than one. For example, it is unlikely that\n    ``a = 1e-9`` and ``b = 2e-9`` should be considered \"close\", yet\n    ``allclose(1e-9, 2e-9)`` is ``True`` with default settings. Be sure\n    to select `atol` for the use case at hand, especially for defining the\n    threshold below which a non-zero value in `a` will be considered \"close\"\n    to a very small or zero value in `b`.\n\n    The comparison of `a` and `b` uses standard broadcasting, which\n    means that `a` and `b` need not have the same shape in order for\n    ``allclose(a, b)`` to evaluate to True.  The same is true for\n    `equal` but not `array_equal`.\n\n    `allclose` is not defined for non-numeric data types.\n    `bool` is considered a numeric data-type for this purpose.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.allclose([1e10,1e-7], [1.00001e10,1e-8])\n    False\n\n    >>> np.allclose([1e10,1e-8], [1.00001e10,1e-9])\n    True\n\n    >>> np.allclose([1e10,1e-8], [1.0001e10,1e-9])\n    False\n\n    >>> np.allclose([1.0, np.nan], [1.0, np.nan])\n    False\n\n    >>> np.allclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)\n    True\n\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 2324, "code": "def isclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):\n    x, y, atol, rtol = (\n        a if isinstance(a, (int, float, complex)) else asanyarray(a)\n        for a in (a, b, atol, rtol))\n    if (dtype := getattr(y, \"dtype\", None)) is not None and dtype.kind != \"m\":\n        dt = multiarray.result_type(y, 1.)\n        y = asanyarray(y, dtype=dt)\n    elif isinstance(y, int):\n        y = float(y)\n    if not (np.all(np.isfinite(atol)) and np.all(np.isfinite(rtol))):\n        err_s = np.geterr()[\"invalid\"]", "documentation": "    \"\"\"\n    Returns a boolean array where two arrays are element-wise equal within a\n    tolerance.\n\n    The tolerance values are positive, typically very small numbers.  The\n    relative difference (`rtol` * abs(`b`)) and the absolute difference\n    `atol` are added together to compare against the absolute difference\n    between `a` and `b`.\n\n    .. warning:: The default `atol` is not appropriate for comparing numbers\n                 with magnitudes much smaller than one (see Notes).\n\n    Parameters\n    ----------\n    a, b : array_like\n        Input arrays to compare.\n    rtol : array_like\n        The relative tolerance parameter (see Notes).\n    atol : array_like\n        The absolute tolerance parameter (see Notes).\n    equal_nan : bool\n        Whether to compare NaN's as equal.  If True, NaN's in `a` will be\n        considered equal to NaN's in `b` in the output array.\n\n    Returns\n    -------\n    y : array_like\n        Returns a boolean array of where `a` and `b` are equal within the\n        given tolerance. If both `a` and `b` are scalars, returns a single\n        boolean value.\n\n    See Also\n    --------\n    allclose\n    math.isclose\n\n    Notes\n    -----\n    For finite values, isclose uses the following equation to test whether\n    two floating point values are equivalent.::\n\n     absolute(a - b) <= (atol + rtol * absolute(b))\n\n    Unlike the built-in `math.isclose`, the above equation is not symmetric\n    in `a` and `b` -- it assumes `b` is the reference value -- so that\n    `isclose(a, b)` might be different from `isclose(b, a)`.\n\n    The default value of `atol` is not appropriate when the reference value\n    `b` has magnitude smaller than one. For example, it is unlikely that\n    ``a = 1e-9`` and ``b = 2e-9`` should be considered \"close\", yet\n    ``isclose(1e-9, 2e-9)`` is ``True`` with default settings. Be sure\n    to select `atol` for the use case at hand, especially for defining the\n    threshold below which a non-zero value in `a` will be considered \"close\"\n    to a very small or zero value in `b`.\n\n    `isclose` is not defined for non-numeric data types.\n    :class:`bool` is considered a numeric data-type for this purpose.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.isclose([1e10,1e-7], [1.00001e10,1e-8])\n    array([ True, False])\n\n    >>> np.isclose([1e10,1e-8], [1.00001e10,1e-9])\n    array([ True, True])\n\n    >>> np.isclose([1e10,1e-8], [1.0001e10,1e-9])\n    array([False,  True])\n\n    >>> np.isclose([1.0, np.nan], [1.0, np.nan])\n    array([ True, False])\n\n    >>> np.isclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)\n    array([ True, True])\n\n    >>> np.isclose([1e-8, 1e-7], [0.0, 0.0])\n    array([ True, False])\n\n    >>> np.isclose([1e-100, 1e-7], [0.0, 0.0], atol=0.0)\n    array([False, False])\n\n    >>> np.isclose([1e-10, 1e-10], [1e-20, 0.0])\n    array([ True,  True])\n\n    >>> np.isclose([1e-10, 1e-10], [1e-20, 0.999999e-10], atol=0.0)\n    array([False,  True])\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 2475, "code": "def array_equal(a1, a2, equal_nan=False):\n    try:\n        a1, a2 = asarray(a1), asarray(a2)\n    except Exception:\n        return False\n    if a1.shape != a2.shape:\n        return False\n    if not equal_nan:\n        return builtins.bool((asanyarray(a1 == a2)).all())\n    if a1 is a2:\n        return True", "documentation": "    \"\"\"\n    True if two arrays have the same shape and elements, False otherwise.\n\n    Parameters\n    ----------\n    a1, a2 : array_like\n        Input arrays.\n    equal_nan : bool\n        Whether to compare NaN's as equal. If the dtype of a1 and a2 is\n        complex, values will be considered equal if either the real or the\n        imaginary component of a given value is ``nan``.\n\n    Returns\n    -------\n    b : bool\n        Returns True if the arrays are equal.\n\n    See Also\n    --------\n    allclose: Returns True if two arrays are element-wise equal within a\n              tolerance.\n    array_equiv: Returns True if input arrays are shape consistent and all\n                 elements equal.\n\n    Examples\n    --------\n    >>> import numpy as np\n\n    >>> np.array_equal([1, 2], [1, 2])\n    True\n\n    >>> np.array_equal(np.array([1, 2]), np.array([1, 2]))\n    True\n\n    >>> np.array_equal([1, 2], [1, 2, 3])\n    False\n\n    >>> np.array_equal([1, 2], [1, 4])\n    False\n\n    >>> a = np.array([1, np.nan])\n    >>> np.array_equal(a, a)\n    False\n\n    >>> np.array_equal(a, a, equal_nan=True)\n    True\n\n    When ``equal_nan`` is True, complex values with nan components are\n    considered equal if either the real *or* the imaginary components are nan.\n\n    >>> a = np.array([1 + 1j])\n    >>> b = a.copy()\n    >>> a.real = np.nan\n    >>> b.imag = np.nan\n    >>> np.array_equal(a, b, equal_nan=True)\n    True\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 2565, "code": "def array_equiv(a1, a2):\n    try:\n        a1, a2 = asarray(a1), asarray(a2)\n    except Exception:\n        return False\n    try:\n        multiarray.broadcast(a1, a2)\n    except Exception:\n        return False\n    return builtins.bool(asanyarray(a1 == a2).all())", "documentation": "    \"\"\"\n    Returns True if input arrays are shape consistent and all elements equal.\n\n    Shape consistent means they are either the same shape, or one input array\n    can be broadcasted to create the same shape as the other one.\n\n    Parameters\n    ----------\n    a1, a2 : array_like\n        Input arrays.\n\n    Returns\n    -------\n    out : bool\n        True if equivalent, False otherwise.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.array_equiv([1, 2], [1, 2])\n    True\n    >>> np.array_equiv([1, 2], [1, 3])\n    False\n\n    Showing the shape equivalence:\n\n    >>> np.array_equiv([1, 2], [[1, 2], [1, 2]])\n    True\n    >>> np.array_equiv([1, 2], [[1, 2, 1, 2], [1, 2, 1, 2]])\n    False\n\n    >>> np.array_equiv([1, 2], [[1, 2], [1, 3]])\n    False\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 2618, "code": "def astype(x, dtype, /, *, copy=True, device=None):\n    if not (isinstance(x, np.ndarray) or isscalar(x)):\n        raise TypeError(\n            \"Input should be a NumPy array or scalar. \"\n            f\"It is a {type(x)} instead.\"\n        )\n    if device is not None and device != \"cpu\":\n        raise ValueError(\n            'Device not understood. Only \"cpu\" is allowed, but received:'\n            f' {device}'\n        )", "documentation": "    \"\"\"\n    Copies an array to a specified data type.\n\n    This function is an Array API compatible alternative to\n    `numpy.ndarray.astype`.\n\n    Parameters\n    ----------\n    x : ndarray\n        Input NumPy array to cast. ``array_likes`` are explicitly not\n        supported here.\n    dtype : dtype\n        Data type of the result.\n    copy : bool, optional\n        Specifies whether to copy an array when the specified dtype matches\n        the data type of the input array ``x``. If ``True``, a newly allocated\n        array must always be returned. If ``False`` and the specified dtype\n        matches the data type of the input array, the input array must be\n        returned; otherwise, a newly allocated array must be returned.\n        Defaults to ``True``.\n    device : str, optional\n        The device on which to place the returned array. Default: None.\n        For Array-API interoperability only, so must be ``\"cpu\"`` if passed.\n\n        .. versionadded:: 2.1.0\n\n    Returns\n    -------\n    out : ndarray\n        An array having the specified data type.\n\n    See Also\n    --------\n    ndarray.astype\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> arr = np.array([1, 2, 3]); arr\n    array([1, 2, 3])\n    >>> np.astype(arr, np.float64)\n    array([1., 2., 3.])\n\n    Non-copy case:\n\n    >>> arr = np.array([1, 2, 3])\n    >>> arr_noncpy = np.astype(arr, arr.dtype, copy=False)\n    >>> np.shares_memory(arr, arr_noncpy)\n    True\n\n    \"\"\""}], "after_segments": [{"filename": "numpy/_core/numeric.py", "start_line": 171, "code": "def ones(shape, dtype=None, order='C', *, device=None, like=None):\n    if like is not None:\n        return _ones_with_like(\n            like, shape, dtype=dtype, order=order, device=device\n        )\n    a = empty(shape, dtype, order, device=device)\n    multiarray.copyto(a, 1, casting='unsafe')\n    return a\n_ones_with_like = array_function_dispatch()(ones)", "documentation": "    \"\"\"\n    Return a new array of given shape and type, filled with ones.\n\n    Parameters\n    ----------\n    shape : int or sequence of ints\n        Shape of the new array, e.g., ``(2, 3)`` or ``2``.\n    dtype : data-type, optional\n        The desired data-type for the array, e.g., `numpy.int8`.  Default is\n        `numpy.float64`.\n    order : {'C', 'F'}, optional, default: C\n        Whether to store multi-dimensional data in row-major\n        (C-style) or column-major (Fortran-style) order in\n        memory.\n    device : str, optional\n        The device on which to place the created array. Default: None.\n        For Array-API interoperability only, so must be ``\"cpu\"`` if passed.\n\n        .. versionadded:: 2.0.0\n    ${ARRAY_FUNCTION_LIKE}\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    out : ndarray\n        Array of ones with the given shape, dtype, and order.\n\n    See Also\n    --------\n    ones_like : Return an array of ones with shape and type of input.\n    empty : Return a new uninitialized array.\n    zeros : Return a new array setting values to zero.\n    full : Return a new array of given shape filled with value.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.ones(5)\n    array([1., 1., 1., 1., 1.])\n\n    >>> np.ones((5,), dtype=np.int_)\n    array([1, 1, 1, 1, 1])\n\n    >>> np.ones((2, 1))\n    array([[1.],\n           [1.]])\n\n    >>> s = (2,2)\n    >>> np.ones(s)\n    array([[1.,  1.],\n           [1.,  1.]])\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 324, "code": "def full(shape, fill_value, dtype=None, order='C', *, device=None, like=None):\n    if like is not None:\n        return _full_with_like(\n            like, shape, fill_value, dtype=dtype, order=order, device=device\n        )\n    if dtype is None:\n        fill_value = asarray(fill_value)\n        dtype = fill_value.dtype\n    a = empty(shape, dtype, order, device=device)\n    multiarray.copyto(a, fill_value, casting='unsafe')\n    return a", "documentation": "    \"\"\"\n    Return a new array of given shape and type, filled with `fill_value`.\n\n    Parameters\n    ----------\n    shape : int or sequence of ints\n        Shape of the new array, e.g., ``(2, 3)`` or ``2``.\n    fill_value : scalar or array_like\n        Fill value.\n    dtype : data-type, optional\n        The desired data-type for the array  The default, None, means\n         ``np.array(fill_value).dtype``.\n    order : {'C', 'F'}, optional\n        Whether to store multidimensional data in C- or Fortran-contiguous\n        (row- or column-wise) order in memory.\n    device : str, optional\n        The device on which to place the created array. Default: None.\n        For Array-API interoperability only, so must be ``\"cpu\"`` if passed.\n\n        .. versionadded:: 2.0.0\n    ${ARRAY_FUNCTION_LIKE}\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    out : ndarray\n        Array of `fill_value` with the given shape, dtype, and order.\n\n    See Also\n    --------\n    full_like : Return a new array with shape of input filled with value.\n    empty : Return a new uninitialized array.\n    ones : Return a new array setting values to one.\n    zeros : Return a new array setting values to zero.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.full((2, 2), np.inf)\n    array([[inf, inf],\n           [inf, inf]])\n    >>> np.full((2, 2), 10)\n    array([[10, 10],\n           [10, 10]])\n\n    >>> np.full((2, 2), [1, 2])\n    array([[1, 2],\n           [1, 2]])\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 483, "code": "def count_nonzero(a, axis=None, *, keepdims=False):\n    if axis is None and not keepdims:\n        return multiarray.count_nonzero(a)\n    a = asanyarray(a)\n    if np.issubdtype(a.dtype, np.character):\n        a_bool = a != a.dtype.type()\n    else:\n        a_bool = a.astype(np.bool, copy=False)\n    return a_bool.sum(axis=axis, dtype=np.intp, keepdims=keepdims)\n@set_module('numpy')", "documentation": "    \"\"\"\n    Counts the number of non-zero values in the array ``a``.\n\n    The word \"non-zero\" is in reference to the Python 2.x\n    built-in method ``__nonzero__()`` (renamed ``__bool__()``\n    in Python 3.x) of Python objects that tests an object's\n    \"truthfulness\". For example, any number is considered\n    truthful if it is nonzero, whereas any string is considered\n    truthful if it is not the empty string. Thus, this function\n    (recursively) counts how many elements in ``a`` (and in\n    sub-arrays thereof) have their ``__nonzero__()`` or ``__bool__()``\n    method evaluated to ``True``.\n\n    Parameters\n    ----------\n    a : array_like\n        The array for which to count non-zeros.\n    axis : int or tuple, optional\n        Axis or tuple of axes along which to count non-zeros.\n        Default is None, meaning that non-zeros will be counted\n        along a flattened version of ``a``.\n    keepdims : bool, optional\n        If this is set to True, the axes that are counted are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input array.\n\n    Returns\n    -------\n    count : int or array of int\n        Number of non-zero values in the array along a given axis.\n        Otherwise, the total number of non-zero values in the array\n        is returned.\n\n    See Also\n    --------\n    nonzero : Return the coordinates of all the non-zero values.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.count_nonzero(np.eye(4))\n    np.int64(4)\n    >>> a = np.array([[0, 1, 7, 0],\n    ...               [3, 0, 2, 19]])\n    >>> np.count_nonzero(a)\n    np.int64(5)\n    >>> np.count_nonzero(a, axis=0)\n    array([1, 1, 2, 1])\n    >>> np.count_nonzero(a, axis=1)\n    array([2, 3])\n    >>> np.count_nonzero(a, axis=1, keepdims=True)\n    array([[2],\n           [3]])\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 553, "code": "def isfortran(a):\n    return a.flags.fnc", "documentation": "    \"\"\"\n    Check if the array is Fortran contiguous but *not* C contiguous.\n\n    This function is obsolete. If you only want to check if an array is Fortran\n    contiguous use ``a.flags.f_contiguous`` instead.\n\n    Parameters\n    ----------\n    a : ndarray\n        Input array.\n\n    Returns\n    -------\n    isfortran : bool\n        Returns True if the array is Fortran contiguous but *not* C contiguous.\n\n\n    Examples\n    --------\n\n    np.array allows to specify whether the array is written in C-contiguous\n    order (last index varies the fastest), or FORTRAN-contiguous order in\n    memory (first index varies the fastest).\n\n    >>> import numpy as np\n    >>> a = np.array([[1, 2, 3], [4, 5, 6]], order='C')\n    >>> a\n    array([[1, 2, 3],\n           [4, 5, 6]])\n    >>> np.isfortran(a)\n    False\n\n    >>> b = np.array([[1, 2, 3], [4, 5, 6]], order='F')\n    >>> b\n    array([[1, 2, 3],\n           [4, 5, 6]])\n    >>> np.isfortran(b)\n    True\n\n\n    The transpose of a C-ordered array is a FORTRAN-ordered array.\n\n    >>> a = np.array([[1, 2, 3], [4, 5, 6]], order='C')\n    >>> a\n    array([[1, 2, 3],\n           [4, 5, 6]])\n    >>> np.isfortran(a)\n    False\n    >>> b = a.T\n    >>> b\n    array([[1, 4],\n           [2, 5],\n           [3, 6]])\n    >>> np.isfortran(b)\n    True\n\n    C-ordered arrays evaluate as False even if they are also FORTRAN-ordered.\n\n    >>> np.isfortran(np.array([1, 2], order='F'))\n    False\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 624, "code": "def argwhere(a):\n    if np.ndim(a) == 0:\n        a = shape_base.atleast_1d(a)\n        return argwhere(a)[:, :0]\n    return transpose(nonzero(a))", "documentation": "    \"\"\"\n    Find the indices of array elements that are non-zero, grouped by element.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n\n    Returns\n    -------\n    index_array : (N, a.ndim) ndarray\n        Indices of elements that are non-zero. Indices are grouped by element.\n        This array will have shape ``(N, a.ndim)`` where ``N`` is the number of\n        non-zero items.\n\n    See Also\n    --------\n    where, nonzero\n\n    Notes\n    -----\n    ``np.argwhere(a)`` is almost the same as ``np.transpose(np.nonzero(a))``,\n    but produces a result of the correct shape for a 0D array.\n\n    The output of ``argwhere`` is not suitable for indexing arrays.\n    For this purpose use ``nonzero(a)`` instead.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> x = np.arange(6).reshape(2,3)\n    >>> x\n    array([[0, 1, 2],\n           [3, 4, 5]])\n    >>> np.argwhere(x>1)\n    array([[0, 2],\n           [1, 0],\n           [1, 1],\n           [1, 2]])\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 679, "code": "def flatnonzero(a):\n    return np.nonzero(np.ravel(a))[0]", "documentation": "    \"\"\"\n    Return indices that are non-zero in the flattened version of a.\n\n    This is equivalent to ``np.nonzero(np.ravel(a))[0]``.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n\n    Returns\n    -------\n    res : ndarray\n        Output array, containing the indices of the elements of ``a.ravel()``\n        that are non-zero.\n\n    See Also\n    --------\n    nonzero : Return the indices of the non-zero elements of the input array.\n    ravel : Return a 1-D array containing the elements of the input array.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> x = np.arange(-2, 3)\n    >>> x\n    array([-2, -1,  0,  1,  2])\n    >>> np.flatnonzero(x)\n    array([0, 1, 3, 4])\n\n    Use the indices of the non-zero elements as an index array to extract\n    these elements:\n\n    >>> x.ravel()[np.flatnonzero(x)]\n    array([-2, -1,  1,  2])\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 805, "code": "def convolve(a, v, mode='full'):\n    a, v = array(a, copy=None, ndmin=1), array(v, copy=None, ndmin=1)\n    if len(a) == 0:\n        raise ValueError('a cannot be empty')\n    if len(v) == 0:\n        raise ValueError('v cannot be empty')\n    if len(v) > len(a):\n        a, v = v, a\n    return multiarray.correlate(a, v[::-1], mode)", "documentation": "    \"\"\"\n    Returns the discrete, linear convolution of two one-dimensional sequences.\n\n    The convolution operator is often seen in signal processing, where it\n    models the effect of a linear time-invariant system on a signal [1]_.  In\n    probability theory, the sum of two independent random variables is\n    distributed according to the convolution of their individual\n    distributions.\n\n    If `v` is longer than `a`, the arrays are swapped before computation.\n\n    Parameters\n    ----------\n    a : (N,) array_like\n        First one-dimensional input array.\n    v : (M,) array_like\n        Second one-dimensional input array.\n    mode : {'full', 'valid', 'same'}, optional\n        'full':\n          By default, mode is 'full'.  This returns the convolution\n          at each point of overlap, with an output shape of (N+M-1,). At\n          the end-points of the convolution, the signals do not overlap\n          completely, and boundary effects may be seen.\n\n        'same':\n          Mode 'same' returns output of length ``max(M, N)``.  Boundary\n          effects are still visible.\n\n        'valid':\n          Mode 'valid' returns output of length\n          ``max(M, N) - min(M, N) + 1``.  The convolution product is only given\n          for points where the signals overlap completely.  Values outside\n          the signal boundary have no effect.\n\n    Returns\n    -------\n    out : ndarray\n        Discrete, linear convolution of `a` and `v`.\n\n    See Also\n    --------\n    scipy.signal.fftconvolve : Convolve two arrays using the Fast Fourier\n                               Transform.\n    scipy.linalg.toeplitz : Used to construct the convolution operator.\n    polymul : Polynomial multiplication. Same output as convolve, but also\n              accepts poly1d objects as input.\n\n    Notes\n    -----\n    The discrete convolution operation is defined as\n\n    .. math:: (a * v)_n = \\\\sum_{m = -\\\\infty}^{\\\\infty} a_m v_{n - m}\n\n    It can be shown that a convolution :math:`x(t) * y(t)` in time/space\n    is equivalent to the multiplication :math:`X(f) Y(f)` in the Fourier\n    domain, after appropriate padding (padding is necessary to prevent\n    circular convolution).  Since multiplication is more efficient (faster)\n    than convolution, the function `scipy.signal.fftconvolve` exploits the\n    FFT to calculate the convolution of large data-sets.\n\n    References\n    ----------\n    .. [1] Wikipedia, \"Convolution\",\n        https://en.wikipedia.org/wiki/Convolution\n\n    Examples\n    --------\n    Note how the convolution operator flips the second array\n    before \"sliding\" the two across one another:\n\n    >>> import numpy as np\n    >>> np.convolve([1, 2, 3], [0, 1, 0.5])\n    array([0. , 1. , 2.5, 4. , 1.5])\n\n    Only return the middle values of the convolution.\n    Contains boundary effects, where zeros are taken\n    into account:\n\n    >>> np.convolve([1,2,3],[0,1,0.5], 'same')\n    array([1. ,  2.5,  4. ])\n\n    The two arrays are of the same length, so there\n    is only one position where they completely overlap:\n\n    >>> np.convolve([1,2,3],[0,1,0.5], 'valid')\n    array([2.5])\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 909, "code": "def outer(a, b, out=None):\n    a = asarray(a)\n    b = asarray(b)\n    return multiply(a.ravel()[:, newaxis], b.ravel()[newaxis, :], out)", "documentation": "    \"\"\"\n    Compute the outer product of two vectors.\n\n    Given two vectors `a` and `b` of length ``M`` and ``N``, respectively,\n    the outer product [1]_ is::\n\n      [[a_0*b_0  a_0*b_1 ... a_0*b_{N-1} ]\n       [a_1*b_0    .\n       [ ...          .\n       [a_{M-1}*b_0            a_{M-1}*b_{N-1} ]]\n\n    Parameters\n    ----------\n    a : (M,) array_like\n        First input vector.  Input is flattened if\n        not already 1-dimensional.\n    b : (N,) array_like\n        Second input vector.  Input is flattened if\n        not already 1-dimensional.\n    out : (M, N) ndarray, optional\n        A location where the result is stored\n\n    Returns\n    -------\n    out : (M, N) ndarray\n        ``out[i, j] = a[i] * b[j]``\n\n    See also\n    --------\n    inner\n    einsum : ``einsum('i,j->ij', a.ravel(), b.ravel())`` is the equivalent.\n    ufunc.outer : A generalization to dimensions other than 1D and other\n                  operations. ``np.multiply.outer(a.ravel(), b.ravel())``\n                  is the equivalent.\n    linalg.outer : An Array API compatible variation of ``np.outer``,\n                   which accepts 1-dimensional inputs only.\n    tensordot : ``np.tensordot(a.ravel(), b.ravel(), axes=((), ()))``\n                is the equivalent.\n\n    References\n    ----------\n    .. [1] G. H. Golub and C. F. Van Loan, *Matrix Computations*, 3rd\n           ed., Baltimore, MD, Johns Hopkins University Press, 1996,\n           pg. 8.\n\n    Examples\n    --------\n    Make a (*very* coarse) grid for computing a Mandelbrot set:\n\n    >>> import numpy as np\n    >>> rl = np.outer(np.ones((5,)), np.linspace(-2, 2, 5))\n    >>> rl\n    array([[-2., -1.,  0.,  1.,  2.],\n           [-2., -1.,  0.,  1.,  2.],\n           [-2., -1.,  0.,  1.,  2.],\n           [-2., -1.,  0.,  1.,  2.],\n           [-2., -1.,  0.,  1.,  2.]])\n    >>> im = np.outer(1j*np.linspace(2, -2, 5), np.ones((5,)))\n    >>> im\n    array([[0.+2.j, 0.+2.j, 0.+2.j, 0.+2.j, 0.+2.j],\n           [0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j],\n           [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n           [0.-1.j, 0.-1.j, 0.-1.j, 0.-1.j, 0.-1.j],\n           [0.-2.j, 0.-2.j, 0.-2.j, 0.-2.j, 0.-2.j]])\n    >>> grid = rl + im\n    >>> grid\n    array([[-2.+2.j, -1.+2.j,  0.+2.j,  1.+2.j,  2.+2.j],\n           [-2.+1.j, -1.+1.j,  0.+1.j,  1.+1.j,  2.+1.j],\n           [-2.+0.j, -1.+0.j,  0.+0.j,  1.+0.j,  2.+0.j],\n           [-2.-1.j, -1.-1.j,  0.-1.j,  1.-1.j,  2.-1.j],\n           [-2.-2.j, -1.-2.j,  0.-2.j,  1.-2.j,  2.-2.j]])\n\n    An example using a \"vector\" of letters:\n\n    >>> x = np.array(['a', 'b', 'c'], dtype=np.object_)\n    >>> np.outer(x, [1, 2, 3])\n    array([['a', 'aa', 'aaa'],\n           ['b', 'bb', 'bbb'],\n           ['c', 'cc', 'ccc']], dtype=object)\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 1001, "code": "def tensordot(a, b, axes=2):\n    try:\n        iter(axes)\n    except Exception:\n        axes_a = list(range(-axes, 0))\n        axes_b = list(range(axes))\n    else:\n        axes_a, axes_b = axes\n    try:\n        na = len(axes_a)\n        axes_a = list(axes_a)", "documentation": "    \"\"\"\n    Compute tensor dot product along specified axes.\n\n    Given two tensors, `a` and `b`, and an array_like object containing\n    two array_like objects, ``(a_axes, b_axes)``, sum the products of\n    `a`'s and `b`'s elements (components) over the axes specified by\n    ``a_axes`` and ``b_axes``. The third argument can be a single non-negative\n    integer_like scalar, ``N``; if it is such, then the last ``N`` dimensions\n    of `a` and the first ``N`` dimensions of `b` are summed over.\n\n    Parameters\n    ----------\n    a, b : array_like\n        Tensors to \"dot\".\n\n    axes : int or (2,) array_like\n        * integer_like\n          If an int N, sum over the last N axes of `a` and the first N axes\n          of `b` in order. The sizes of the corresponding axes must match.\n        * (2,) array_like\n          Or, a list of axes to be summed over, first sequence applying to `a`,\n          second to `b`. Both elements array_like must be of the same length.\n          Each axis may appear at most once; repeated axes are not allowed.\n          For example, ``axes=([1, 1], [0, 0])`` is invalid.\n    Returns\n    -------\n    output : ndarray\n        The tensor dot product of the input.\n\n    See Also\n    --------\n    dot, einsum\n\n    Notes\n    -----\n    Three common use cases are:\n        * ``axes = 0`` : tensor product :math:`a\\\\otimes b`\n        * ``axes = 1`` : tensor dot product :math:`a\\\\cdot b`\n        * ``axes = 2`` : (default) tensor double contraction :math:`a:b`\n\n    When `axes` is integer_like, the sequence of axes for evaluation\n    will be: from the -Nth axis to the -1th axis in `a`,\n    and from the 0th axis to (N-1)th axis in `b`.\n    For example, ``axes = 2`` is the equal to\n    ``axes = [[-2, -1], [0, 1]]``.\n    When N-1 is smaller than 0, or when -N is larger than -1,\n    the element of `a` and `b` are defined as the `axes`.\n\n    When there is more than one axis to sum over - and they are not the last\n    (first) axes of `a` (`b`) - the argument `axes` should consist of\n    two sequences of the same length, with the first axis to sum over given\n    first in both sequences, the second axis second, and so forth.\n    The calculation can be referred to ``numpy.einsum``.\n\n    For example, if ``a.shape == (2, 3, 4)`` and ``b.shape == (3, 4, 5)``,\n    then ``axes=([1, 2], [0, 1])`` sums over the ``(3, 4)`` dimensions of\n    both arrays and produces an output of shape ``(2, 5)``.\n\n    Each summation axis corresponds to a distinct contraction index; repeating\n    an axis (for example ``axes=([1, 1], [0, 0])``) is invalid.\n\n    The shape of the result consists of the non-contracted axes of the\n    first tensor, followed by the non-contracted axes of the second.\n\n    Examples\n    --------\n    An example on integer_like:\n\n    >>> a_0 = np.array([[1, 2], [3, 4]])\n    >>> b_0 = np.array([[5, 6], [7, 8]])\n    >>> c_0 = np.tensordot(a_0, b_0, axes=0)\n    >>> c_0.shape\n    (2, 2, 2, 2)\n    >>> c_0\n    array([[[[ 5,  6],\n             [ 7,  8]],\n            [[10, 12],\n             [14, 16]]],\n           [[[15, 18],\n             [21, 24]],\n            [[20, 24],\n             [28, 32]]]])\n\n    An example on array_like:\n\n    >>> a = np.arange(60.).reshape(3,4,5)\n    >>> b = np.arange(24.).reshape(4,3,2)\n    >>> c = np.tensordot(a,b, axes=([1,0],[0,1]))\n    >>> c.shape\n    (5, 2)\n    >>> c\n    array([[4400., 4730.],\n           [4532., 4874.],\n           [4664., 5018.],\n           [4796., 5162.],\n           [4928., 5306.]])\n\n    A slower but equivalent way of computing the same...\n\n    >>> d = np.zeros((5,2))\n    >>> for i in range(5):\n    ...   for j in range(2):\n    ...     for k in range(3):\n    ...       for n in range(4):\n    ...         d[i,j] += a[k,n,i] * b[n,k,j]\n    >>> c == d\n    array([[ True,  True],\n           [ True,  True],\n           [ True,  True],\n           [ True,  True],\n           [ True,  True]])\n\n    An extended example taking advantage of the overloading of + and \\\\*:\n\n    >>> a = np.array(range(1, 9)).reshape((2, 2, 2))\n    >>> A = np.array(('a', 'b', 'c', 'd'), dtype=np.object_)\n    >>> A = A.reshape((2, 2))\n    >>> a; A\n    array([[[1, 2],\n            [3, 4]],\n           [[5, 6],\n            [7, 8]]])\n    array([['a', 'b'],\n           ['c', 'd']], dtype=object)\n\n    >>> np.tensordot(a, A) # third argument default is 2 for double-contraction\n    array(['abbcccdddd', 'aaaaabbbbbbcccccccdddddddd'], dtype=object)\n\n    >>> np.tensordot(a, A, 1)\n    array([[['acc', 'bdd'],\n            ['aaacccc', 'bbbdddd']],\n           [['aaaaacccccc', 'bbbbbdddddd'],\n            ['aaaaaaacccccccc', 'bbbbbbbdddddddd']]], dtype=object)\n\n    >>> np.tensordot(a, A, 0) # tensor product (result too long to incl.)\n    array([[[[['a', 'b'],\n              ['c', 'd']],\n              ...\n\n    >>> np.tensordot(a, A, (0, 1))\n    array([[['abbbbb', 'cddddd'],\n            ['aabbbbbb', 'ccdddddd']],\n           [['aaabbbbbbb', 'cccddddddd'],\n            ['aaaabbbbbbbb', 'ccccdddddddd']]], dtype=object)\n\n    >>> np.tensordot(a, A, (2, 1))\n    array([[['abb', 'cdd'],\n            ['aaabbbb', 'cccdddd']],\n           [['aaaaabbbbbb', 'cccccdddddd'],\n            ['aaaaaaabbbbbbbb', 'cccccccdddddddd']]], dtype=object)\n\n    >>> np.tensordot(a, A, ((0, 1), (0, 1)))\n    array(['abbbcccccddddddd', 'aabbbbccccccdddddddd'], dtype=object)\n\n    >>> np.tensordot(a, A, ((2, 1), (1, 0)))\n    array(['acccbbdddd', 'aaaaacccccccbbbbbbdddddddd'], dtype=object)\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 1230, "code": "def roll(a, shift, axis=None):\n    a = asanyarray(a)\n    if axis is None:\n        return roll(a.ravel(), shift, 0).reshape(a.shape)\n    else:\n        axis = normalize_axis_tuple(axis, a.ndim, allow_duplicate=True)\n        broadcasted = broadcast(shift, axis)\n        if broadcasted.ndim > 1:\n            raise ValueError(\n                \"'shift' and 'axis' should be scalars or 1D sequences\")\n        shifts = dict.fromkeys(range(a.ndim), 0)", "documentation": "    \"\"\"\n    Roll array elements along a given axis.\n\n    Elements that roll beyond the last position are re-introduced at\n    the first.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    shift : int or tuple of ints\n        The number of places by which elements are shifted.  If a tuple,\n        then `axis` must be a tuple of the same size, and each of the\n        given axes is shifted by the corresponding number.  If an int\n        while `axis` is a tuple of ints, then the same value is used for\n        all given axes.\n    axis : int or tuple of ints, optional\n        Axis or axes along which elements are shifted.  By default, the\n        array is flattened before shifting, after which the original\n        shape is restored.\n\n    Returns\n    -------\n    res : ndarray\n        Output array, with the same shape as `a`.\n\n    See Also\n    --------\n    rollaxis : Roll the specified axis backwards, until it lies in a\n               given position.\n\n    Notes\n    -----\n    Supports rolling over multiple dimensions simultaneously.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> x = np.arange(10)\n    >>> np.roll(x, 2)\n    array([8, 9, 0, 1, 2, 3, 4, 5, 6, 7])\n    >>> np.roll(x, -2)\n    array([2, 3, 4, 5, 6, 7, 8, 9, 0, 1])\n\n    >>> x2 = np.reshape(x, (2, 5))\n    >>> x2\n    array([[0, 1, 2, 3, 4],\n           [5, 6, 7, 8, 9]])\n    >>> np.roll(x2, 1)\n    array([[9, 0, 1, 2, 3],\n           [4, 5, 6, 7, 8]])\n    >>> np.roll(x2, -1)\n    array([[1, 2, 3, 4, 5],\n           [6, 7, 8, 9, 0]])\n    >>> np.roll(x2, 1, axis=0)\n    array([[5, 6, 7, 8, 9],\n           [0, 1, 2, 3, 4]])\n    >>> np.roll(x2, -1, axis=0)\n    array([[5, 6, 7, 8, 9],\n           [0, 1, 2, 3, 4]])\n    >>> np.roll(x2, 1, axis=1)\n    array([[4, 0, 1, 2, 3],\n           [9, 5, 6, 7, 8]])\n    >>> np.roll(x2, -1, axis=1)\n    array([[1, 2, 3, 4, 0],\n           [6, 7, 8, 9, 5]])\n    >>> np.roll(x2, (1, 1), axis=(1, 0))\n    array([[9, 5, 6, 7, 8],\n           [4, 0, 1, 2, 3]])\n    >>> np.roll(x2, (2, 1), axis=(1, 0))\n    array([[8, 9, 5, 6, 7],\n           [3, 4, 0, 1, 2]])\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 1340, "code": "def rollaxis(a, axis, start=0):\n    n = a.ndim\n    axis = normalize_axis_index(axis, n)\n    if start < 0:\n        start += n\n    msg = \"'%s' arg requires %d <= %s < %d, but %d was passed in\"\n    if not (0 <= start < n + 1):\n        raise AxisError(msg % ('start', -n, 'start', n + 1, start))\n    if axis < start:\n        start -= 1\n    if axis == start:", "documentation": "    \"\"\"\n    Roll the specified axis backwards, until it lies in a given position.\n\n    This function continues to be supported for backward compatibility, but you\n    should prefer `moveaxis`. The `moveaxis` function was added in NumPy\n    1.11.\n\n    Parameters\n    ----------\n    a : ndarray\n        Input array.\n    axis : int\n        The axis to be rolled. The positions of the other axes do not\n        change relative to one another.\n    start : int, optional\n        When ``start <= axis``, the axis is rolled back until it lies in\n        this position. When ``start > axis``, the axis is rolled until it\n        lies before this position. The default, 0, results in a \"complete\"\n        roll. The following table describes how negative values of ``start``\n        are interpreted:\n\n        .. table::\n           :align: left\n\n           +-------------------+----------------------+\n           |     ``start``     | Normalized ``start`` |\n           +===================+======================+\n           | ``-(arr.ndim+1)`` | raise ``AxisError``  |\n           +-------------------+----------------------+\n           | ``-arr.ndim``     | 0                    |\n           +-------------------+----------------------+\n           | |vdots|           | |vdots|              |\n           +-------------------+----------------------+\n           | ``-1``            | ``arr.ndim-1``       |\n           +-------------------+----------------------+\n           | ``0``             | ``0``                |\n           +-------------------+----------------------+\n           | |vdots|           | |vdots|              |\n           +-------------------+----------------------+\n           | ``arr.ndim``      | ``arr.ndim``         |\n           +-------------------+----------------------+\n           | ``arr.ndim + 1``  | raise ``AxisError``  |\n           +-------------------+----------------------+\n\n        .. |vdots|   unicode:: U+22EE .. Vertical Ellipsis\n\n    Returns\n    -------\n    res : ndarray\n        For NumPy >= 1.10.0 a view of `a` is always returned. For earlier\n        NumPy versions a view of `a` is returned only if the order of the\n        axes is changed, otherwise the input array is returned.\n\n    See Also\n    --------\n    moveaxis : Move array axes to new positions.\n    roll : Roll the elements of an array by a number of positions along a\n        given axis.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> a = np.ones((3,4,5,6))\n    >>> np.rollaxis(a, 3, 1).shape\n    (3, 6, 4, 5)\n    >>> np.rollaxis(a, 2).shape\n    (5, 3, 4, 6)\n    >>> np.rollaxis(a, 1, 4).shape\n    (3, 5, 6, 4)\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 1431, "code": "def normalize_axis_tuple(axis, ndim, argname=None, allow_duplicate=False):\n    if not isinstance(axis, (tuple, list)):\n        try:\n            axis = [operator.index(axis)]\n        except TypeError:\n            pass\n    axis = tuple(normalize_axis_index(ax, ndim, argname) for ax in axis)\n    if not allow_duplicate and len(set(axis)) != len(axis):\n        if argname:\n            raise ValueError(f'repeated axis in `{argname}` argument')\n        else:", "documentation": "    \"\"\"\n    Normalizes an axis argument into a tuple of non-negative integer axes.\n\n    This handles shorthands such as ``1`` and converts them to ``(1,)``,\n    as well as performing the handling of negative indices covered by\n    `normalize_axis_index`.\n\n    By default, this forbids axes from being specified multiple times.\n\n    Used internally by multi-axis-checking logic.\n\n    Parameters\n    ----------\n    axis : int, iterable of int\n        The un-normalized index or indices of the axis.\n    ndim : int\n        The number of dimensions of the array that `axis` should be normalized\n        against.\n    argname : str, optional\n        A prefix to put before the error message, typically the name of the\n        argument.\n    allow_duplicate : bool, optional\n        If False, the default, disallow an axis from being specified twice.\n\n    Returns\n    -------\n    normalized_axes : tuple of int\n        The normalized axis index, such that `0 <= normalized_axis < ndim`\n\n    Raises\n    ------\n    AxisError\n        If any axis provided is out of range\n    ValueError\n        If an axis is repeated\n\n    See also\n    --------\n    normalize_axis_index : normalizing a single scalar axis\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 1493, "code": "def moveaxis(a, source, destination):\n    try:\n        transpose = a.transpose\n    except AttributeError:\n        a = asarray(a)\n        transpose = a.transpose\n    source = normalize_axis_tuple(source, a.ndim, 'source')\n    destination = normalize_axis_tuple(destination, a.ndim, 'destination')\n    if len(source) != len(destination):\n        raise ValueError('`source` and `destination` arguments must have '\n                         'the same number of elements')", "documentation": "    \"\"\"\n    Move axes of an array to new positions.\n\n    Other axes remain in their original order.\n\n    Parameters\n    ----------\n    a : np.ndarray\n        The array whose axes should be reordered.\n    source : int or sequence of int\n        Original positions of the axes to move. These must be unique.\n    destination : int or sequence of int\n        Destination positions for each of the original axes. These must also be\n        unique.\n\n    Returns\n    -------\n    result : np.ndarray\n        Array with moved axes. This array is a view of the input array.\n\n    See Also\n    --------\n    transpose : Permute the dimensions of an array.\n    swapaxes : Interchange two axes of an array.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> x = np.zeros((3, 4, 5))\n    >>> np.moveaxis(x, 0, -1).shape\n    (4, 5, 3)\n    >>> np.moveaxis(x, -1, 0).shape\n    (5, 3, 4)\n\n    These all achieve the same result:\n\n    >>> np.transpose(x).shape\n    (5, 4, 3)\n    >>> np.swapaxes(x, 0, -1).shape\n    (5, 4, 3)\n    >>> np.moveaxis(x, [0, 1], [-1, -2]).shape\n    (5, 4, 3)\n    >>> np.moveaxis(x, [0, 1, 2], [-1, -2, -3]).shape\n    (5, 4, 3)\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 1567, "code": "def cross(a, b, axisa=-1, axisb=-1, axisc=-1, axis=None):\n    if axis is not None:\n        axisa, axisb, axisc = (axis,) * 3\n    a = asarray(a)\n    b = asarray(b)\n    if (a.ndim < 1) or (b.ndim < 1):\n        raise ValueError(\"At least one array has zero dimension\")\n    axisa = normalize_axis_index(axisa, a.ndim, msg_prefix='axisa')\n    axisb = normalize_axis_index(axisb, b.ndim, msg_prefix='axisb')\n    a = moveaxis(a, axisa, -1)\n    b = moveaxis(b, axisb, -1)", "documentation": "    \"\"\"\n    Return the cross product of two (arrays of) vectors.\n\n    The cross product of `a` and `b` in :math:`R^3` is a vector perpendicular\n    to both `a` and `b`.  If `a` and `b` are arrays of vectors, the vectors\n    are defined by the last axis of `a` and `b` by default, and these axes\n    must have 3 dimensions.\n\n    Parameters\n    ----------\n    a : array_like\n        Components of the first vector(s).\n    b : array_like\n        Components of the second vector(s).\n    axisa : int, optional\n        Axis of `a` that defines the vector(s).  By default, the last axis.\n    axisb : int, optional\n        Axis of `b` that defines the vector(s).  By default, the last axis.\n    axisc : int, optional\n        Axis of `c` containing the cross product vector(s).  By default, the last axis.\n    axis : int, optional\n        If defined, the axis of `a`, `b` and `c` that defines the vector(s)\n        and cross product(s).  Overrides `axisa`, `axisb` and `axisc`.\n\n    Returns\n    -------\n    c : ndarray\n        Vector cross product(s).\n\n    Raises\n    ------\n    ValueError\n        When the dimension of the vector(s) in `a` or `b` does not equal 3.\n\n    See Also\n    --------\n    inner : Inner product\n    outer : Outer product.\n    linalg.cross : An Array API compatible variation of ``np.cross``.\n    ix_ : Construct index arrays.\n\n    Notes\n    -----\n    Supports full broadcasting of the inputs.\n\n    Examples\n    --------\n    Vector cross-product.\n\n    >>> import numpy as np\n    >>> x = [1, 2, 3]\n    >>> y = [4, 5, 6]\n    >>> np.cross(x, y)\n    array([-3,  6, -3])\n\n    One vector with dimension 2.\n\n    >>> x = [1, 2, 0]\n    >>> y = [4, 5, 6]\n    >>> np.cross(x, y)\n    array([12, -6, -3])\n\n    Both vectors with dimension 2.\n\n    >>> x = [1, 2, 0]\n    >>> y = [4, 5, 0]\n    >>> np.cross(x, y)\n    array([0, 0, -3])\n\n    Multiple vector cross-products. Note that the direction of the cross\n    product vector is defined by the *right-hand rule*.\n\n    >>> x = np.array([[1,2,3], [4,5,6]])\n    >>> y = np.array([[4,5,6], [1,2,3]])\n    >>> np.cross(x, y)\n    array([[-3,  6, -3],\n           [ 3, -6,  3]])\n\n    The orientation of `c` can be changed using the `axisc` keyword.\n\n    >>> np.cross(x, y, axisc=0)\n    array([[-3,  3],\n           [ 6, -6],\n           [-3,  3]])\n\n    Change the vector definition of `x` and `y` using `axisa` and `axisb`.\n\n    >>> x = np.array([[1,2,3], [4,5,6], [7, 8, 9]])\n    >>> y = np.array([[7, 8, 9], [4,5,6], [1,2,3]])\n    >>> np.cross(x, y)\n    array([[ -6,  12,  -6],\n           [  0,   0,   0],\n           [  6, -12,   6]])\n    >>> np.cross(x, y, axisa=0, axisb=0)\n    array([[-24,  48, -24],\n           [-30,  60, -30],\n           [-36,  72, -36]])\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 1730, "code": "def indices(dimensions, dtype=int, sparse=False):\n    dimensions = tuple(dimensions)\n    N = len(dimensions)\n    shape = (1,) * N\n    if sparse:\n        res = ()\n    else:\n        res = empty((N,) + dimensions, dtype=dtype)\n    for i, dim in enumerate(dimensions):\n        idx = arange(dim, dtype=dtype).reshape(\n            shape[:i] + (dim,) + shape[i + 1:]", "documentation": "    \"\"\"\n    Return an array representing the indices of a grid.\n\n    Compute an array where the subarrays contain index values 0, 1, ...\n    varying only along the corresponding axis.\n\n    Parameters\n    ----------\n    dimensions : sequence of ints\n        The shape of the grid.\n    dtype : dtype, optional\n        Data type of the result.\n    sparse : boolean, optional\n        Return a sparse representation of the grid instead of a dense\n        representation. Default is False.\n\n    Returns\n    -------\n    grid : one ndarray or tuple of ndarrays\n        If sparse is False:\n            Returns one array of grid indices,\n            ``grid.shape = (len(dimensions),) + tuple(dimensions)``.\n        If sparse is True:\n            Returns a tuple of arrays, with\n            ``grid[i].shape = (1, ..., 1, dimensions[i], 1, ..., 1)`` with\n            dimensions[i] in the ith place\n\n    See Also\n    --------\n    mgrid, ogrid, meshgrid\n\n    Notes\n    -----\n    The output shape in the dense case is obtained by prepending the number\n    of dimensions in front of the tuple of dimensions, i.e. if `dimensions`\n    is a tuple ``(r0, ..., rN-1)`` of length ``N``, the output shape is\n    ``(N, r0, ..., rN-1)``.\n\n    The subarrays ``grid[k]`` contains the N-D array of indices along the\n    ``k-th`` axis. Explicitly::\n\n        grid[k, i0, i1, ..., iN-1] = ik\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> grid = np.indices((2, 3))\n    >>> grid.shape\n    (2, 2, 3)\n    >>> grid[0]        # row indices\n    array([[0, 0, 0],\n           [1, 1, 1]])\n    >>> grid[1]        # column indices\n    array([[0, 1, 2],\n           [0, 1, 2]])\n\n    The indices can be used as an index into an array.\n\n    >>> x = np.arange(20).reshape(5, 4)\n    >>> row, col = np.indices((2, 3))\n    >>> x[row, col]\n    array([[0, 1, 2],\n           [4, 5, 6]])\n\n    Note that it would be more straightforward in the above example to\n    extract the required elements directly with ``x[:2, :3]``.\n\n    If sparse is set to true, the grid will be returned in a sparse\n    representation.\n\n    >>> i, j = np.indices((2, 3), sparse=True)\n    >>> i.shape\n    (2, 1)\n    >>> j.shape\n    (1, 3)\n    >>> i        # row indices\n    array([[0],\n           [1]])\n    >>> j        # column indices\n    array([[0, 1, 2]])\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 1833, "code": "def fromfunction(function, shape, *, dtype=float, like=None, **kwargs):\n    if like is not None:\n        return _fromfunction_with_like(\n                like, function, shape, dtype=dtype, **kwargs)\n    args = indices(shape, dtype=dtype)\n    return function(*args, **kwargs)\n_fromfunction_with_like = array_function_dispatch()(fromfunction)", "documentation": "    \"\"\"\n    Construct an array by executing a function over each coordinate.\n\n    The resulting array therefore has a value ``fn(x, y, z)`` at\n    coordinate ``(x, y, z)``.\n\n    Parameters\n    ----------\n    function : callable\n        The function is called with N parameters, where N is the rank of\n        `shape`.  Each parameter represents the coordinates of the array\n        varying along a specific axis.  For example, if `shape`\n        were ``(2, 2)``, then the parameters would be\n        ``array([[0, 0], [1, 1]])`` and ``array([[0, 1], [0, 1]])``\n    shape : (N,) tuple of ints\n        Shape of the output array, which also determines the shape of\n        the coordinate arrays passed to `function`.\n    dtype : data-type, optional\n        Data-type of the coordinate arrays passed to `function`.\n        By default, `dtype` is float.\n    ${ARRAY_FUNCTION_LIKE}\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    fromfunction : any\n        The result of the call to `function` is passed back directly.\n        Therefore the shape of `fromfunction` is completely determined by\n        `function`.  If `function` returns a scalar value, the shape of\n        `fromfunction` would not match the `shape` parameter.\n\n    See Also\n    --------\n    indices, meshgrid\n\n    Notes\n    -----\n    Keywords other than `dtype` and `like` are passed to `function`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.fromfunction(lambda i, j: i, (2, 2), dtype=np.float64)\n    array([[0., 0.],\n           [1., 1.]])\n\n    >>> np.fromfunction(lambda i, j: j, (2, 2), dtype=np.float64)\n    array([[0., 1.],\n           [0., 1.]])\n\n    >>> np.fromfunction(lambda i, j: i == j, (3, 3), dtype=np.int_)\n    array([[ True, False, False],\n           [False,  True, False],\n           [False, False,  True]])\n\n    >>> np.fromfunction(lambda i, j: i + j, (3, 3), dtype=np.int_)\n    array([[0, 1, 2],\n           [1, 2, 3],\n           [2, 3, 4]])\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 1915, "code": "def isscalar(element):\n    return (isinstance(element, generic)\n            or type(element) in ScalarType\n            or isinstance(element, numbers.Number))\n@set_module('numpy')", "documentation": "    \"\"\"\n    Returns True if the type of `element` is a scalar type.\n\n    Parameters\n    ----------\n    element : any\n        Input argument, can be of any type and shape.\n\n    Returns\n    -------\n    val : bool\n        True if `element` is a scalar type, False if it is not.\n\n    See Also\n    --------\n    ndim : Get the number of dimensions of an array\n\n    Notes\n    -----\n    If you need a stricter way to identify a *numerical* scalar, use\n    ``isinstance(x, numbers.Number)``, as that returns ``False`` for most\n    non-numerical elements such as strings.\n\n    In most cases ``np.ndim(x) == 0`` should be used instead of this function,\n    as that will also return true for 0d arrays. This is how numpy overloads\n    functions in the style of the ``dx`` arguments to `gradient` and\n    the ``bins`` argument to `histogram`. Some key differences:\n\n    +------------------------------------+---------------+-------------------+\n    | x                                  |``isscalar(x)``|``np.ndim(x) == 0``|\n    +====================================+===============+===================+\n    | PEP 3141 numeric objects           | ``True``      | ``True``          |\n    | (including builtins)               |               |                   |\n    +------------------------------------+---------------+-------------------+\n    | builtin string and buffer objects  | ``True``      | ``True``          |\n    +------------------------------------+---------------+-------------------+\n    | other builtin objects, like        | ``False``     | ``True``          |\n    | `pathlib.Path`, `Exception`,       |               |                   |\n    | the result of `re.compile`         |               |                   |\n    +------------------------------------+---------------+-------------------+\n    | third-party objects like           | ``False``     | ``True``          |\n    | `matplotlib.figure.Figure`         |               |                   |\n    +------------------------------------+---------------+-------------------+\n    | zero-dimensional numpy arrays      | ``False``     | ``True``          |\n    +------------------------------------+---------------+-------------------+\n    | other numpy arrays                 | ``False``     | ``False``         |\n    +------------------------------------+---------------+-------------------+\n    | `list`, `tuple`, and other         | ``False``     | ``False``         |\n    | sequence objects                   |               |                   |\n    +------------------------------------+---------------+-------------------+\n\n    Examples\n    --------\n    >>> import numpy as np\n\n    >>> np.isscalar(3.1)\n    True\n\n    >>> np.isscalar(np.array(3.1))\n    False\n\n    >>> np.isscalar([3.1])\n    False\n\n    >>> np.isscalar(False)\n    True\n\n    >>> np.isscalar('numpy')\n    True\n\n    NumPy supports PEP 3141 numbers:\n\n    >>> from fractions import Fraction\n    >>> np.isscalar(Fraction(5, 17))\n    True\n    >>> from numbers import Number\n    >>> np.isscalar(Number())\n    True\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 2002, "code": "def binary_repr(num, width=None):", "documentation": "    \"\"\"\n    Return the binary representation of the input number as a string.\n\n    For negative numbers, if width is not given, a minus sign is added to the\n    front. If width is given, the two's complement of the number is\n    returned, with respect to that width.\n\n    In a two's-complement system negative numbers are represented by the two's\n    complement of the absolute value. This is the most common method of\n    representing signed integers on computers [1]_. A N-bit two's-complement\n    system can represent every integer in the range\n    :math:`-2^{N-1}` to :math:`+2^{N-1}-1`.\n\n    Parameters\n    ----------\n    num : int\n        Only an integer decimal number can be used.\n    width : int, optional\n        The length of the returned string if `num` is positive, or the length\n        of the two's complement if `num` is negative, provided that `width` is\n        at least a sufficient number of bits for `num` to be represented in\n        the designated form. If the `width` value is insufficient, an error is\n        raised.\n\n    Returns\n    -------\n    bin : str\n        Binary representation of `num` or two's complement of `num`.\n\n    See Also\n    --------\n    base_repr: Return a string representation of a number in the given base\n               system.\n    bin: Python's built-in binary representation generator of an integer.\n\n    Notes\n    -----\n    `binary_repr` is equivalent to using `base_repr` with base 2, but about 25x\n    faster.\n\n    References\n    ----------\n    .. [1] Wikipedia, \"Two's complement\",\n        https://en.wikipedia.org/wiki/Two's_complement\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.binary_repr(3)\n    '11'\n    >>> np.binary_repr(-3)\n    '-11'\n    >>> np.binary_repr(3, width=4)\n    '0011'\n\n    The two's complement is returned when the input number is negative and\n    width is specified:\n\n    >>> np.binary_repr(-3, width=3)\n    '101'\n    >>> np.binary_repr(-3, width=5)\n    '11101'\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 2109, "code": "def base_repr(number, base=2, padding=0):\n    digits = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n    if base > len(digits):\n        raise ValueError(\"Bases greater than 36 not handled in base_repr.\")\n    elif base < 2:\n        raise ValueError(\"Bases less than 2 not handled in base_repr.\")\n    num = abs(int(number))\n    res = []\n    while num:\n        res.append(digits[num % base])\n        num //= base", "documentation": "    \"\"\"\n    Return a string representation of a number in the given base system.\n\n    Parameters\n    ----------\n    number : int\n        The value to convert. Positive and negative values are handled.\n    base : int, optional\n        Convert `number` to the `base` number system. The valid range is 2-36,\n        the default value is 2.\n    padding : int, optional\n        Number of zeros padded on the left. Default is 0 (no padding).\n\n    Returns\n    -------\n    out : str\n        String representation of `number` in `base` system.\n\n    See Also\n    --------\n    binary_repr : Faster version of `base_repr` for base 2.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.base_repr(5)\n    '101'\n    >>> np.base_repr(6, 5)\n    '11'\n    >>> np.base_repr(7, base=5, padding=3)\n    '00012'\n\n    >>> np.base_repr(10, base=16)\n    'A'\n    >>> np.base_repr(32, base=16)\n    '20'\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 2183, "code": "def identity(n, dtype=None, *, like=None):\n    if like is not None:\n        return _identity_with_like(like, n, dtype=dtype)\n    from numpy import eye\n    return eye(n, dtype=dtype, like=like)\n_identity_with_like = array_function_dispatch()(identity)", "documentation": "    \"\"\"\n    Return the identity array.\n\n    The identity array is a square array with ones on\n    the main diagonal.\n\n    Parameters\n    ----------\n    n : int\n        Number of rows (and columns) in `n` x `n` output.\n    dtype : data-type, optional\n        Data-type of the output.  Defaults to ``float``.\n    ${ARRAY_FUNCTION_LIKE}\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    out : ndarray\n        `n` x `n` array with its main diagonal set to one,\n        and all other elements 0.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.identity(3)\n    array([[1.,  0.,  0.],\n           [0.,  1.,  0.],\n           [0.,  0.,  1.]])\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 2230, "code": "def allclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):\n    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n    return builtins.bool(res)", "documentation": "    \"\"\"\n    Returns True if two arrays are element-wise equal within a tolerance.\n\n    The tolerance values are positive, typically very small numbers.  The\n    relative difference (`rtol` * abs(`b`)) and the absolute difference\n    `atol` are added together to compare against the absolute difference\n    between `a` and `b`.\n\n    .. warning:: The default `atol` is not appropriate for comparing numbers\n                 with magnitudes much smaller than one (see Notes).\n\n    NaNs are treated as equal if they are in the same place and if\n    ``equal_nan=True``.  Infs are treated as equal if they are in the same\n    place and of the same sign in both arrays.\n\n    Parameters\n    ----------\n    a, b : array_like\n        Input arrays to compare.\n    rtol : array_like\n        The relative tolerance parameter (see Notes).\n    atol : array_like\n        The absolute tolerance parameter (see Notes).\n    equal_nan : bool\n        Whether to compare NaN's as equal.  If True, NaN's in `a` will be\n        considered equal to NaN's in `b` in the output array.\n\n    Returns\n    -------\n    allclose : bool\n        Returns True if the two arrays are equal within the given\n        tolerance; False otherwise.\n\n    See Also\n    --------\n    isclose, all, any, equal\n\n    Notes\n    -----\n    If the following equation is element-wise True, then allclose returns\n    True.::\n\n     absolute(a - b) <= (atol + rtol * absolute(b))\n\n    The above equation is not symmetric in `a` and `b`, so that\n    ``allclose(a, b)`` might be different from ``allclose(b, a)`` in\n    some rare cases.\n\n    The default value of `atol` is not appropriate when the reference value\n    `b` has magnitude smaller than one. For example, it is unlikely that\n    ``a = 1e-9`` and ``b = 2e-9`` should be considered \"close\", yet\n    ``allclose(1e-9, 2e-9)`` is ``True`` with default settings. Be sure\n    to select `atol` for the use case at hand, especially for defining the\n    threshold below which a non-zero value in `a` will be considered \"close\"\n    to a very small or zero value in `b`.\n\n    The comparison of `a` and `b` uses standard broadcasting, which\n    means that `a` and `b` need not have the same shape in order for\n    ``allclose(a, b)`` to evaluate to True.  The same is true for\n    `equal` but not `array_equal`.\n\n    `allclose` is not defined for non-numeric data types.\n    `bool` is considered a numeric data-type for this purpose.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.allclose([1e10,1e-7], [1.00001e10,1e-8])\n    False\n\n    >>> np.allclose([1e10,1e-8], [1.00001e10,1e-9])\n    True\n\n    >>> np.allclose([1e10,1e-8], [1.0001e10,1e-9])\n    False\n\n    >>> np.allclose([1.0, np.nan], [1.0, np.nan])\n    False\n\n    >>> np.allclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)\n    True\n\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 2324, "code": "def isclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):\n    x, y, atol, rtol = (\n        a if isinstance(a, (int, float, complex)) else asanyarray(a)\n        for a in (a, b, atol, rtol))\n    if (dtype := getattr(y, \"dtype\", None)) is not None and dtype.kind != \"m\":\n        dt = multiarray.result_type(y, 1.)\n        y = asanyarray(y, dtype=dt)\n    elif isinstance(y, int):\n        y = float(y)\n    if not (np.all(np.isfinite(atol)) and np.all(np.isfinite(rtol))):\n        err_s = np.geterr()[\"invalid\"]", "documentation": "    \"\"\"\n    Returns a boolean array where two arrays are element-wise equal within a\n    tolerance.\n\n    The tolerance values are positive, typically very small numbers.  The\n    relative difference (`rtol` * abs(`b`)) and the absolute difference\n    `atol` are added together to compare against the absolute difference\n    between `a` and `b`.\n\n    .. warning:: The default `atol` is not appropriate for comparing numbers\n                 with magnitudes much smaller than one (see Notes).\n\n    Parameters\n    ----------\n    a, b : array_like\n        Input arrays to compare.\n    rtol : array_like\n        The relative tolerance parameter (see Notes).\n    atol : array_like\n        The absolute tolerance parameter (see Notes).\n    equal_nan : bool\n        Whether to compare NaN's as equal.  If True, NaN's in `a` will be\n        considered equal to NaN's in `b` in the output array.\n\n    Returns\n    -------\n    y : array_like\n        Returns a boolean array of where `a` and `b` are equal within the\n        given tolerance. If both `a` and `b` are scalars, returns a single\n        boolean value.\n\n    See Also\n    --------\n    allclose\n    math.isclose\n\n    Notes\n    -----\n    For finite values, isclose uses the following equation to test whether\n    two floating point values are equivalent.::\n\n     absolute(a - b) <= (atol + rtol * absolute(b))\n\n    Unlike the built-in `math.isclose`, the above equation is not symmetric\n    in `a` and `b` -- it assumes `b` is the reference value -- so that\n    `isclose(a, b)` might be different from `isclose(b, a)`.\n\n    The default value of `atol` is not appropriate when the reference value\n    `b` has magnitude smaller than one. For example, it is unlikely that\n    ``a = 1e-9`` and ``b = 2e-9`` should be considered \"close\", yet\n    ``isclose(1e-9, 2e-9)`` is ``True`` with default settings. Be sure\n    to select `atol` for the use case at hand, especially for defining the\n    threshold below which a non-zero value in `a` will be considered \"close\"\n    to a very small or zero value in `b`.\n\n    `isclose` is not defined for non-numeric data types.\n    :class:`bool` is considered a numeric data-type for this purpose.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.isclose([1e10,1e-7], [1.00001e10,1e-8])\n    array([ True, False])\n\n    >>> np.isclose([1e10,1e-8], [1.00001e10,1e-9])\n    array([ True, True])\n\n    >>> np.isclose([1e10,1e-8], [1.0001e10,1e-9])\n    array([False,  True])\n\n    >>> np.isclose([1.0, np.nan], [1.0, np.nan])\n    array([ True, False])\n\n    >>> np.isclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)\n    array([ True, True])\n\n    >>> np.isclose([1e-8, 1e-7], [0.0, 0.0])\n    array([ True, False])\n\n    >>> np.isclose([1e-100, 1e-7], [0.0, 0.0], atol=0.0)\n    array([False, False])\n\n    >>> np.isclose([1e-10, 1e-10], [1e-20, 0.0])\n    array([ True,  True])\n\n    >>> np.isclose([1e-10, 1e-10], [1e-20, 0.999999e-10], atol=0.0)\n    array([False,  True])\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 2475, "code": "def array_equal(a1, a2, equal_nan=False):\n    try:\n        a1, a2 = asarray(a1), asarray(a2)\n    except Exception:\n        return False\n    if a1.shape != a2.shape:\n        return False\n    if not equal_nan:\n        return builtins.bool((asanyarray(a1 == a2)).all())\n    if a1 is a2:\n        return True", "documentation": "    \"\"\"\n    True if two arrays have the same shape and elements, False otherwise.\n\n    Parameters\n    ----------\n    a1, a2 : array_like\n        Input arrays.\n    equal_nan : bool\n        Whether to compare NaN's as equal. If the dtype of a1 and a2 is\n        complex, values will be considered equal if either the real or the\n        imaginary component of a given value is ``nan``.\n\n    Returns\n    -------\n    b : bool\n        Returns True if the arrays are equal.\n\n    See Also\n    --------\n    allclose: Returns True if two arrays are element-wise equal within a\n              tolerance.\n    array_equiv: Returns True if input arrays are shape consistent and all\n                 elements equal.\n\n    Examples\n    --------\n    >>> import numpy as np\n\n    >>> np.array_equal([1, 2], [1, 2])\n    True\n\n    >>> np.array_equal(np.array([1, 2]), np.array([1, 2]))\n    True\n\n    >>> np.array_equal([1, 2], [1, 2, 3])\n    False\n\n    >>> np.array_equal([1, 2], [1, 4])\n    False\n\n    >>> a = np.array([1, np.nan])\n    >>> np.array_equal(a, a)\n    False\n\n    >>> np.array_equal(a, a, equal_nan=True)\n    True\n\n    When ``equal_nan`` is True, complex values with nan components are\n    considered equal if either the real *or* the imaginary components are nan.\n\n    >>> a = np.array([1 + 1j])\n    >>> b = a.copy()\n    >>> a.real = np.nan\n    >>> b.imag = np.nan\n    >>> np.array_equal(a, b, equal_nan=True)\n    True\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 2565, "code": "def array_equiv(a1, a2):\n    try:\n        a1, a2 = asarray(a1), asarray(a2)\n    except Exception:\n        return False\n    try:\n        multiarray.broadcast(a1, a2)\n    except Exception:\n        return False\n    return builtins.bool(asanyarray(a1 == a2).all())", "documentation": "    \"\"\"\n    Returns True if input arrays are shape consistent and all elements equal.\n\n    Shape consistent means they are either the same shape, or one input array\n    can be broadcasted to create the same shape as the other one.\n\n    Parameters\n    ----------\n    a1, a2 : array_like\n        Input arrays.\n\n    Returns\n    -------\n    out : bool\n        True if equivalent, False otherwise.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.array_equiv([1, 2], [1, 2])\n    True\n    >>> np.array_equiv([1, 2], [1, 3])\n    False\n\n    Showing the shape equivalence:\n\n    >>> np.array_equiv([1, 2], [[1, 2], [1, 2]])\n    True\n    >>> np.array_equiv([1, 2], [[1, 2, 1, 2], [1, 2, 1, 2]])\n    False\n\n    >>> np.array_equiv([1, 2], [[1, 2], [1, 3]])\n    False\n\n    \"\"\""}, {"filename": "numpy/_core/numeric.py", "start_line": 2618, "code": "def astype(x, dtype, /, *, copy=True, device=None):\n    if not (isinstance(x, np.ndarray) or isscalar(x)):\n        raise TypeError(\n            \"Input should be a NumPy array or scalar. \"\n            f\"It is a {type(x)} instead.\"\n        )\n    if device is not None and device != \"cpu\":\n        raise ValueError(\n            'Device not understood. Only \"cpu\" is allowed, but received:'\n            f' {device}'\n        )", "documentation": "    \"\"\"\n    Copies an array to a specified data type.\n\n    This function is an Array API compatible alternative to\n    `numpy.ndarray.astype`.\n\n    Parameters\n    ----------\n    x : ndarray\n        Input NumPy array to cast. ``array_likes`` are explicitly not\n        supported here.\n    dtype : dtype\n        Data type of the result.\n    copy : bool, optional\n        Specifies whether to copy an array when the specified dtype matches\n        the data type of the input array ``x``. If ``True``, a newly allocated\n        array must always be returned. If ``False`` and the specified dtype\n        matches the data type of the input array, the input array must be\n        returned; otherwise, a newly allocated array must be returned.\n        Defaults to ``True``.\n    device : str, optional\n        The device on which to place the returned array. Default: None.\n        For Array-API interoperability only, so must be ``\"cpu\"`` if passed.\n\n        .. versionadded:: 2.1.0\n\n    Returns\n    -------\n    out : ndarray\n        An array having the specified data type.\n\n    See Also\n    --------\n    ndarray.astype\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> arr = np.array([1, 2, 3]); arr\n    array([1, 2, 3])\n    >>> np.astype(arr, np.float64)\n    array([1., 2., 3.])\n\n    Non-copy case:\n\n    >>> arr = np.array([1, 2, 3])\n    >>> arr_noncpy = np.astype(arr, arr.dtype, copy=False)\n    >>> np.shares_memory(arr, arr_noncpy)\n    True\n\n    \"\"\""}]}
{"repository": "numpy/numpy", "commit_sha": "9b9a167d617dfe255df4b74cd343f049333a7036", "commit_message": "DOC/DEP: update docs to reflect deprecation of ``chararray``", "commit_date": "2026-01-08T10:48:48+00:00", "author": "jorenham", "file": "numpy/_core/defchararray.py", "patch": "@@ -409,6 +409,10 @@ class chararray(ndarray):\n \n     Provides a convenient view on arrays of string and unicode values.\n \n+    .. deprecated:: 2.5\n+       ``chararray`` is deprecated. Use an ``ndarray`` with a string or\n+       bytes dtype instead.\n+\n     .. note::\n        The `chararray` class exists for backwards compatibility with\n        Numarray, it is not recommended for new development. Starting from numpy", "before_segments": [{"filename": "numpy/_core/defchararray.py", "start_line": 61, "code": "def equal(x1, x2):\n    return compare_chararrays(x1, x2, '==', True)\n@array_function_dispatch(_binary_op_dispatcher)", "documentation": "    \"\"\"\n    Return (x1 == x2) element-wise.\n\n    Unlike `numpy.equal`, this comparison is performed by first\n    stripping whitespace characters from the end of the string.  This\n    behavior is provided for backward-compatibility with numarray.\n\n    Parameters\n    ----------\n    x1, x2 : array_like of str or unicode\n        Input arrays of the same shape.\n\n    Returns\n    -------\n    out : ndarray\n        Output array of bools.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> y = \"aa \"\n    >>> x = \"aa\"\n    >>> np.char.equal(x, y)\n    array(True)\n\n    See Also\n    --------\n    not_equal, greater_equal, less_equal, greater, less\n    \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 95, "code": "def not_equal(x1, x2):\n    return compare_chararrays(x1, x2, '!=', True)\n@array_function_dispatch(_binary_op_dispatcher)", "documentation": "    \"\"\"\n    Return (x1 != x2) element-wise.\n\n    Unlike `numpy.not_equal`, this comparison is performed by first\n    stripping whitespace characters from the end of the string.  This\n    behavior is provided for backward-compatibility with numarray.\n\n    Parameters\n    ----------\n    x1, x2 : array_like of str or unicode\n        Input arrays of the same shape.\n\n    Returns\n    -------\n    out : ndarray\n        Output array of bools.\n\n    See Also\n    --------\n    equal, greater_equal, less_equal, greater, less\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> x1 = np.array(['a', 'b', 'c'])\n    >>> np.char.not_equal(x1, 'b')\n    array([ True, False,  True])\n\n    \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 129, "code": "def greater_equal(x1, x2):\n    return compare_chararrays(x1, x2, '>=', True)\n@array_function_dispatch(_binary_op_dispatcher)", "documentation": "    \"\"\"\n    Return (x1 >= x2) element-wise.\n\n    Unlike `numpy.greater_equal`, this comparison is performed by\n    first stripping whitespace characters from the end of the string.\n    This behavior is provided for backward-compatibility with\n    numarray.\n\n    Parameters\n    ----------\n    x1, x2 : array_like of str or unicode\n        Input arrays of the same shape.\n\n    Returns\n    -------\n    out : ndarray\n        Output array of bools.\n\n    See Also\n    --------\n    equal, not_equal, less_equal, greater, less\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> x1 = np.array(['a', 'b', 'c'])\n    >>> np.char.greater_equal(x1, 'b')\n    array([False,  True,  True])\n\n    \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 164, "code": "def less_equal(x1, x2):\n    return compare_chararrays(x1, x2, '<=', True)\n@array_function_dispatch(_binary_op_dispatcher)", "documentation": "    \"\"\"\n    Return (x1 <= x2) element-wise.\n\n    Unlike `numpy.less_equal`, this comparison is performed by first\n    stripping whitespace characters from the end of the string.  This\n    behavior is provided for backward-compatibility with numarray.\n\n    Parameters\n    ----------\n    x1, x2 : array_like of str or unicode\n        Input arrays of the same shape.\n\n    Returns\n    -------\n    out : ndarray\n        Output array of bools.\n\n    See Also\n    --------\n    equal, not_equal, greater_equal, greater, less\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> x1 = np.array(['a', 'b', 'c'])\n    >>> np.char.less_equal(x1, 'b')\n    array([ True,  True, False])\n\n    \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 198, "code": "def greater(x1, x2):\n    return compare_chararrays(x1, x2, '>', True)\n@array_function_dispatch(_binary_op_dispatcher)", "documentation": "    \"\"\"\n    Return (x1 > x2) element-wise.\n\n    Unlike `numpy.greater`, this comparison is performed by first\n    stripping whitespace characters from the end of the string.  This\n    behavior is provided for backward-compatibility with numarray.\n\n    Parameters\n    ----------\n    x1, x2 : array_like of str or unicode\n        Input arrays of the same shape.\n\n    Returns\n    -------\n    out : ndarray\n        Output array of bools.\n\n    See Also\n    --------\n    equal, not_equal, greater_equal, less_equal, less\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> x1 = np.array(['a', 'b', 'c'])\n    >>> np.char.greater(x1, 'b')\n    array([False, False,  True])\n\n    \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 232, "code": "def less(x1, x2):\n    return compare_chararrays(x1, x2, '<', True)\n@set_module(\"numpy.char\")", "documentation": "    \"\"\"\n    Return (x1 < x2) element-wise.\n\n    Unlike `numpy.greater`, this comparison is performed by first\n    stripping whitespace characters from the end of the string.  This\n    behavior is provided for backward-compatibility with numarray.\n\n    Parameters\n    ----------\n    x1, x2 : array_like of str or unicode\n        Input arrays of the same shape.\n\n    Returns\n    -------\n    out : ndarray\n        Output array of bools.\n\n    See Also\n    --------\n    equal, not_equal, greater_equal, less_equal, greater\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> x1 = np.array(['a', 'b', 'c'])\n    >>> np.char.less(x1, 'b')\n    array([True, False, False])\n\n    \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 266, "code": "def multiply(a, i):\n    try:\n        return strings_multiply(a, i)\n    except TypeError:\n        raise ValueError(\"Can only multiply by integers\")\n@set_module(\"numpy.char\")", "documentation": "    \"\"\"\n    Return (a * i), that is string multiple concatenation,\n    element-wise.\n\n    Values in ``i`` of less than 0 are treated as 0 (which yields an\n    empty string).\n\n    Parameters\n    ----------\n    a : array_like, with `np.bytes_` or `np.str_` dtype\n\n    i : array_like, with any integer dtype\n\n    Returns\n    -------\n    out : ndarray\n        Output array of str or unicode, depending on input types\n\n    Notes\n    -----\n    This is a thin wrapper around np.strings.multiply that raises\n    `ValueError` when ``i`` is not an integer. It only\n    exists for backwards-compatibility.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> a = np.array([\"a\", \"b\", \"c\"])\n    >>> np.strings.multiply(a, 3)\n    array(['aaa', 'bbb', 'ccc'], dtype='<U3')\n    >>> i = np.array([1, 2, 3])\n    >>> np.strings.multiply(a, i)\n    array(['a', 'bb', 'ccc'], dtype='<U3')\n    >>> np.strings.multiply(np.array(['a']), i)\n    array(['a', 'aa', 'aaa'], dtype='<U3')\n    >>> a = np.array(['a', 'b', 'c', 'd', 'e', 'f']).reshape((2, 3))\n    >>> np.strings.multiply(a, 3)\n    array([['aaa', 'bbb', 'ccc'],\n           ['ddd', 'eee', 'fff']], dtype='<U3')\n    >>> np.strings.multiply(a, i)\n    array([['a', 'bb', 'ccc'],\n           ['d', 'ee', 'fff']], dtype='<U3')\n\n    \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 318, "code": "def partition(a, sep):\n    return np.stack(strings_partition(a, sep), axis=-1)\n@set_module(\"numpy.char\")", "documentation": "    \"\"\"\n    Partition each element in `a` around `sep`.\n\n    Calls :meth:`str.partition` element-wise.\n\n    For each element in `a`, split the element as the first\n    occurrence of `sep`, and return 3 strings containing the part\n    before the separator, the separator itself, and the part after\n    the separator. If the separator is not found, return 3 strings\n    containing the string itself, followed by two empty strings.\n\n    Parameters\n    ----------\n    a : array-like, with ``StringDType``, ``bytes_``, or ``str_`` dtype\n        Input array\n    sep : {str, unicode}\n        Separator to split each string element in `a`.\n\n    Returns\n    -------\n    out : ndarray\n        Output array of ``StringDType``, ``bytes_`` or ``str_`` dtype,\n        depending on input types. The output array will have an extra\n        dimension with 3 elements per input element.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> x = np.array([\"Numpy is nice!\"])\n    >>> np.char.partition(x, \" \")\n    array([['Numpy', ' ', 'is nice!']], dtype='<U8')\n\n    See Also\n    --------\n    str.partition\n\n    \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 360, "code": "def rpartition(a, sep):\n    return np.stack(strings_rpartition(a, sep), axis=-1)\n@set_module(\"numpy.char\")", "documentation": "    \"\"\"\n    Partition (split) each element around the right-most separator.\n\n    Calls :meth:`str.rpartition` element-wise.\n\n    For each element in `a`, split the element as the last\n    occurrence of `sep`, and return 3 strings containing the part\n    before the separator, the separator itself, and the part after\n    the separator. If the separator is not found, return 3 strings\n    containing the string itself, followed by two empty strings.\n\n    Parameters\n    ----------\n    a : array-like, with ``StringDType``, ``bytes_``, or ``str_`` dtype\n        Input array\n    sep : str or unicode\n        Right-most separator to split each element in array.\n\n    Returns\n    -------\n    out : ndarray\n        Output array of ``StringDType``, ``bytes_`` or ``str_`` dtype,\n        depending on input types. The output array will have an extra\n        dimension with 3 elements per input element.\n\n    See Also\n    --------\n    str.rpartition\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> a = np.array(['aAaAaA', '  aA  ', 'abBABba'])\n    >>> np.char.rpartition(a, 'A')\n    array([['aAaAa', 'A', ''],\n       ['  a', 'A', '  '],\n       ['abB', 'A', 'Bba']], dtype='<U5')\n\n    \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 404, "code": "class chararray(ndarray):", "documentation": "    \"\"\"\n    chararray(shape, itemsize=1, unicode=False, buffer=None, offset=0,\n              strides=None, order=None)\n\n    Provides a convenient view on arrays of string and unicode values.\n\n    .. note::\n       The `chararray` class exists for backwards compatibility with\n       Numarray, it is not recommended for new development. Starting from numpy\n       1.4, if one needs arrays of strings, it is recommended to use arrays of\n       `dtype` `~numpy.object_`, `~numpy.bytes_` or `~numpy.str_`, and use\n       the free functions in the `numpy.char` module for fast vectorized\n       string operations.\n\n    Versus a NumPy array of dtype `~numpy.bytes_` or `~numpy.str_`, this\n    class adds the following functionality:\n\n    1) values automatically have whitespace removed from the end\n       when indexed\n\n    2) comparison operators automatically remove whitespace from the\n       end when comparing values\n\n    3) vectorized string operations are provided as methods\n       (e.g. `.endswith`) and infix operators (e.g. ``\"+\", \"*\", \"%\"``)\n\n    chararrays should be created using `numpy.char.array` or\n    `numpy.char.asarray`, rather than this constructor directly.\n\n    This constructor creates the array, using `buffer` (with `offset`\n    and `strides`) if it is not ``None``. If `buffer` is ``None``, then\n    constructs a new array with `strides` in \"C order\", unless both\n    ``len(shape) >= 2`` and ``order='F'``, in which case `strides`\n    is in \"Fortran order\".\n\n    Methods\n    -------\n    astype\n    argsort\n    copy\n    count\n    decode\n    dump\n    dumps\n    encode\n    endswith\n    expandtabs\n    fill\n    find\n    flatten\n    getfield\n    index\n    isalnum\n    isalpha\n    isdecimal\n    isdigit\n    islower\n    isnumeric\n    isspace\n    istitle\n    isupper\n    item\n    join\n    ljust\n    lower\n    lstrip\n    nonzero\n    put\n    ravel\n    repeat\n    replace\n    reshape\n    resize\n    rfind\n    rindex\n    rjust\n    rsplit\n    rstrip\n    searchsorted\n    setfield\n    setflags\n    sort\n    split\n    splitlines\n    squeeze\n    startswith\n    strip\n    swapaxes\n    swapcase\n    take\n    title\n    tofile\n    tolist\n    translate\n    transpose\n    upper\n    view\n    zfill\n\n    Parameters\n    ----------\n    shape : tuple\n        Shape of the array.\n    itemsize : int, optional\n        Length of each array element, in number of characters. Default is 1.\n    unicode : bool, optional\n        Are the array elements of type unicode (True) or string (False).\n        Default is False.\n    buffer : object exposing the buffer interface or str, optional\n        Memory address of the start of the array data.  Default is None,\n        in which case a new array is created.\n    offset : int, optional\n        Fixed stride displacement from the beginning of an axis?\n        Default is 0. Needs to be >=0.\n    strides : array_like of ints, optional\n        Strides for the array (see `~numpy.ndarray.strides` for\n        full description). Default is None.\n    order : {'C', 'F'}, optional\n        The order in which the array data is stored in memory: 'C' ->\n        \"row major\" order (the default), 'F' -> \"column major\"\n        (Fortran) order.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> charar = np.char.chararray((3, 3))\n    >>> charar[:] = 'a'\n    >>> charar\n    chararray([[b'a', b'a', b'a'],\n               [b'a', b'a', b'a'],\n               [b'a', b'a', b'a']], dtype='|S1')\n\n    >>> charar = np.char.chararray(charar.shape, itemsize=5)\n    >>> charar[:] = 'abc'\n    >>> charar\n    chararray([[b'abc', b'abc', b'abc'],\n               [b'abc', b'abc', b'abc'],\n               [b'abc', b'abc', b'abc']], dtype='|S5')\n\n    \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 601, "code": "    def __eq__(self, other):\n        return equal(self, other)", "documentation": "        \"\"\"\n        Return (self == other) element-wise.\n\n        See Also\n        --------\n        equal\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 611, "code": "    def __ne__(self, other):\n        return not_equal(self, other)", "documentation": "        \"\"\"\n        Return (self != other) element-wise.\n\n        See Also\n        --------\n        not_equal\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 621, "code": "    def __ge__(self, other):\n        return greater_equal(self, other)", "documentation": "        \"\"\"\n        Return (self >= other) element-wise.\n\n        See Also\n        --------\n        greater_equal\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 631, "code": "    def __le__(self, other):\n        return less_equal(self, other)", "documentation": "        \"\"\"\n        Return (self <= other) element-wise.\n\n        See Also\n        --------\n        less_equal\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 641, "code": "    def __gt__(self, other):\n        return greater(self, other)", "documentation": "        \"\"\"\n        Return (self > other) element-wise.\n\n        See Also\n        --------\n        greater\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 651, "code": "    def __lt__(self, other):\n        return less(self, other)", "documentation": "        \"\"\"\n        Return (self < other) element-wise.\n\n        See Also\n        --------\n        less\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 661, "code": "    def __add__(self, other):\n        return add(self, other)", "documentation": "        \"\"\"\n        Return (self + other), that is string concatenation,\n        element-wise for a pair of array_likes of str or unicode.\n\n        See Also\n        --------\n        add\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 672, "code": "    def __radd__(self, other):\n        return add(other, self)", "documentation": "        \"\"\"\n        Return (other + self), that is string concatenation,\n        element-wise for a pair of array_likes of `bytes_` or `str_`.\n\n        See Also\n        --------\n        add\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 683, "code": "    def __mul__(self, i):\n        return asarray(multiply(self, i))", "documentation": "        \"\"\"\n        Return (self * i), that is string multiple concatenation,\n        element-wise.\n\n        See Also\n        --------\n        multiply\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 694, "code": "    def __rmul__(self, i):\n        return asarray(multiply(self, i))", "documentation": "        \"\"\"\n        Return (self * i), that is string multiple concatenation,\n        element-wise.\n\n        See Also\n        --------\n        multiply\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 705, "code": "    def __mod__(self, i):\n        return asarray(mod(self, i))", "documentation": "        \"\"\"\n        Return (self % i), that is pre-Python 2.6 string formatting\n        (interpolation), element-wise for a pair of array_likes of `bytes_`\n        or `str_`.\n\n        See Also\n        --------\n        mod\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 720, "code": "    def argsort(self, axis=-1, kind=None, order=None, *, stable=None):\n        return self.__array__().argsort(axis, kind, order, stable=stable)\n    argsort.__doc__ = ndarray.argsort.__doc__", "documentation": "        \"\"\"\n        Return the indices that sort the array lexicographically.\n\n        For full documentation see `numpy.argsort`, for which this method is\n        in fact merely a \"thin wrapper.\"\n\n        Examples\n        --------\n        >>> c = np.array(['a1b c', '1b ca', 'b ca1', 'Ca1b'], 'S5')\n        >>> c = c.view(np.char.chararray); c\n        chararray(['a1b c', '1b ca', 'b ca1', 'Ca1b'],\n              dtype='|S5')\n        >>> c[c.argsort()]\n        chararray(['1b ca', 'Ca1b', 'a1b c', 'b ca1'],\n              dtype='|S5')\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 741, "code": "    def capitalize(self):\n        return asarray(capitalize(self))", "documentation": "        \"\"\"\n        Return a copy of `self` with only the first character of each element\n        capitalized.\n\n        See Also\n        --------\n        char.capitalize\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 753, "code": "    def center(self, width, fillchar=' '):\n        return asarray(center(self, width, fillchar))", "documentation": "        \"\"\"\n        Return a copy of `self` with its elements centered in a\n        string of length `width`.\n\n        See Also\n        --------\n        center\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 764, "code": "    def count(self, sub, start=0, end=None):\n        return count(self, sub, start, end)", "documentation": "        \"\"\"\n        Returns an array with the number of non-overlapping occurrences of\n        substring `sub` in the range [`start`, `end`].\n\n        See Also\n        --------\n        char.count\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 776, "code": "    def decode(self, encoding=None, errors=None):\n        return decode(self, encoding, errors)", "documentation": "        \"\"\"\n        Calls ``bytes.decode`` element-wise.\n\n        See Also\n        --------\n        char.decode\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 787, "code": "    def encode(self, encoding=None, errors=None):\n        return encode(self, encoding, errors)", "documentation": "        \"\"\"\n        Calls :meth:`str.encode` element-wise.\n\n        See Also\n        --------\n        char.encode\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 798, "code": "    def endswith(self, suffix, start=0, end=None):\n        return endswith(self, suffix, start, end)", "documentation": "        \"\"\"\n        Returns a boolean array which is `True` where the string element\n        in `self` ends with `suffix`, otherwise `False`.\n\n        See Also\n        --------\n        char.endswith\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 810, "code": "    def expandtabs(self, tabsize=8):\n        return asarray(expandtabs(self, tabsize))", "documentation": "        \"\"\"\n        Return a copy of each string element where all tab characters are\n        replaced by one or more spaces.\n\n        See Also\n        --------\n        char.expandtabs\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 822, "code": "    def find(self, sub, start=0, end=None):\n        return find(self, sub, start, end)", "documentation": "        \"\"\"\n        For each element, return the lowest index in the string where\n        substring `sub` is found.\n\n        See Also\n        --------\n        char.find\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 834, "code": "    def index(self, sub, start=0, end=None):\n        return index(self, sub, start, end)", "documentation": "        \"\"\"\n        Like `find`, but raises :exc:`ValueError` when the substring is not\n        found.\n\n        See Also\n        --------\n        char.index\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 846, "code": "    def isalnum(self):\n        return isalnum(self)", "documentation": "        \"\"\"\n        Returns true for each element if all characters in the string\n        are alphanumeric and there is at least one character, false\n        otherwise.\n\n        See Also\n        --------\n        char.isalnum\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 859, "code": "    def isalpha(self):\n        return isalpha(self)", "documentation": "        \"\"\"\n        Returns true for each element if all characters in the string\n        are alphabetic and there is at least one character, false\n        otherwise.\n\n        See Also\n        --------\n        char.isalpha\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 872, "code": "    def isdigit(self):\n        return isdigit(self)", "documentation": "        \"\"\"\n        Returns true for each element if all characters in the string are\n        digits and there is at least one character, false otherwise.\n\n        See Also\n        --------\n        char.isdigit\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 884, "code": "    def islower(self):\n        return islower(self)", "documentation": "        \"\"\"\n        Returns true for each element if all cased characters in the\n        string are lowercase and there is at least one cased character,\n        false otherwise.\n\n        See Also\n        --------\n        char.islower\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 897, "code": "    def isspace(self):\n        return isspace(self)", "documentation": "        \"\"\"\n        Returns true for each element if there are only whitespace\n        characters in the string and there is at least one character,\n        false otherwise.\n\n        See Also\n        --------\n        char.isspace\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 910, "code": "    def istitle(self):\n        return istitle(self)", "documentation": "        \"\"\"\n        Returns true for each element if the element is a titlecased\n        string and there is at least one character, false otherwise.\n\n        See Also\n        --------\n        char.istitle\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 922, "code": "    def isupper(self):\n        return isupper(self)", "documentation": "        \"\"\"\n        Returns true for each element if all cased characters in the\n        string are uppercase and there is at least one character, false\n        otherwise.\n\n        See Also\n        --------\n        char.isupper\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 935, "code": "    def join(self, seq):\n        return join(self, seq)", "documentation": "        \"\"\"\n        Return a string which is the concatenation of the strings in the\n        sequence `seq`.\n\n        See Also\n        --------\n        char.join\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 947, "code": "    def ljust(self, width, fillchar=' '):\n        return asarray(ljust(self, width, fillchar))", "documentation": "        \"\"\"\n        Return an array with the elements of `self` left-justified in a\n        string of length `width`.\n\n        See Also\n        --------\n        char.ljust\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 959, "code": "    def lower(self):\n        return asarray(lower(self))", "documentation": "        \"\"\"\n        Return an array with the elements of `self` converted to\n        lowercase.\n\n        See Also\n        --------\n        char.lower\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 971, "code": "    def lstrip(self, chars=None):\n        return lstrip(self, chars)", "documentation": "        \"\"\"\n        For each element in `self`, return a copy with the leading characters\n        removed.\n\n        See Also\n        --------\n        char.lstrip\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 983, "code": "    def partition(self, sep):\n        return asarray(partition(self, sep))", "documentation": "        \"\"\"\n        Partition each element in `self` around `sep`.\n\n        See Also\n        --------\n        partition\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 993, "code": "    def replace(self, old, new, count=None):\n        return replace(self, old, new, count if count is not None else -1)", "documentation": "        \"\"\"\n        For each element in `self`, return a copy of the string with all\n        occurrences of substring `old` replaced by `new`.\n\n        See Also\n        --------\n        char.replace\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1005, "code": "    def rfind(self, sub, start=0, end=None):\n        return rfind(self, sub, start, end)", "documentation": "        \"\"\"\n        For each element in `self`, return the highest index in the string\n        where substring `sub` is found, such that `sub` is contained\n        within [`start`, `end`].\n\n        See Also\n        --------\n        char.rfind\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1018, "code": "    def rindex(self, sub, start=0, end=None):\n        return rindex(self, sub, start, end)", "documentation": "        \"\"\"\n        Like `rfind`, but raises :exc:`ValueError` when the substring `sub` is\n        not found.\n\n        See Also\n        --------\n        char.rindex\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1030, "code": "    def rjust(self, width, fillchar=' '):\n        return asarray(rjust(self, width, fillchar))", "documentation": "        \"\"\"\n        Return an array with the elements of `self`\n        right-justified in a string of length `width`.\n\n        See Also\n        --------\n        char.rjust\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1042, "code": "    def rpartition(self, sep):\n        return asarray(rpartition(self, sep))", "documentation": "        \"\"\"\n        Partition each element in `self` around `sep`.\n\n        See Also\n        --------\n        rpartition\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1052, "code": "    def rsplit(self, sep=None, maxsplit=None):\n        return rsplit(self, sep, maxsplit)", "documentation": "        \"\"\"\n        For each element in `self`, return a list of the words in\n        the string, using `sep` as the delimiter string.\n\n        See Also\n        --------\n        char.rsplit\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1064, "code": "    def rstrip(self, chars=None):\n        return rstrip(self, chars)", "documentation": "        \"\"\"\n        For each element in `self`, return a copy with the trailing\n        characters removed.\n\n        See Also\n        --------\n        char.rstrip\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1076, "code": "    def split(self, sep=None, maxsplit=None):\n        return split(self, sep, maxsplit)", "documentation": "        \"\"\"\n        For each element in `self`, return a list of the words in the\n        string, using `sep` as the delimiter string.\n\n        See Also\n        --------\n        char.split\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1088, "code": "    def splitlines(self, keepends=None):\n        return splitlines(self, keepends)", "documentation": "        \"\"\"\n        For each element in `self`, return a list of the lines in the\n        element, breaking at line boundaries.\n\n        See Also\n        --------\n        char.splitlines\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1100, "code": "    def startswith(self, prefix, start=0, end=None):\n        return startswith(self, prefix, start, end)", "documentation": "        \"\"\"\n        Returns a boolean array which is `True` where the string element\n        in `self` starts with `prefix`, otherwise `False`.\n\n        See Also\n        --------\n        char.startswith\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1112, "code": "    def strip(self, chars=None):\n        return strip(self, chars)", "documentation": "        \"\"\"\n        For each element in `self`, return a copy with the leading and\n        trailing characters removed.\n\n        See Also\n        --------\n        char.strip\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1124, "code": "    def swapcase(self):\n        return asarray(swapcase(self))", "documentation": "        \"\"\"\n        For each element in `self`, return a copy of the string with\n        uppercase characters converted to lowercase and vice versa.\n\n        See Also\n        --------\n        char.swapcase\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1136, "code": "    def title(self):\n        return asarray(title(self))", "documentation": "        \"\"\"\n        For each element in `self`, return a titlecased version of the\n        string: words start with uppercase characters, all remaining cased\n        characters are lowercase.\n\n        See Also\n        --------\n        char.title\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1149, "code": "    def translate(self, table, deletechars=None):\n        return asarray(translate(self, table, deletechars))", "documentation": "        \"\"\"\n        For each element in `self`, return a copy of the string where\n        all characters occurring in the optional argument\n        `deletechars` are removed, and the remaining characters have\n        been mapped through the given translation table.\n\n        See Also\n        --------\n        char.translate\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1163, "code": "    def upper(self):\n        return asarray(upper(self))", "documentation": "        \"\"\"\n        Return an array with the elements of `self` converted to\n        uppercase.\n\n        See Also\n        --------\n        char.upper\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1175, "code": "    def zfill(self, width):\n        return asarray(zfill(self, width))", "documentation": "        \"\"\"\n        Return the numeric string left-filled with zeros in a string of\n        length `width`.\n\n        See Also\n        --------\n        char.zfill\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1187, "code": "    def isnumeric(self):\n        return isnumeric(self)", "documentation": "        \"\"\"\n        For each element in `self`, return True if there are only\n        numeric characters in the element.\n\n        See Also\n        --------\n        char.isnumeric\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1199, "code": "    def isdecimal(self):\n        return isdecimal(self)\n@set_module(\"numpy.char\")", "documentation": "        \"\"\"\n        For each element in `self`, return True if there are only\n        decimal characters in the element.\n\n        See Also\n        --------\n        char.isdecimal\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1213, "code": "def array(obj, itemsize=None, copy=True, unicode=None, order=None):\n    if isinstance(obj, (bytes, str)):\n        if unicode is None:\n            if isinstance(obj, str):\n                unicode = True\n            else:\n                unicode = False\n        if itemsize is None:\n            itemsize = len(obj)\n        shape = len(obj) // itemsize\n        return chararray(shape, itemsize=itemsize, unicode=unicode,", "documentation": "    \"\"\"\n    Create a `~numpy.char.chararray`.\n\n    .. note::\n       This class is provided for numarray backward-compatibility.\n       New code (not concerned with numarray compatibility) should use\n       arrays of type `bytes_` or `str_` and use the free functions\n       in :mod:`numpy.char` for fast vectorized string operations instead.\n\n    Versus a NumPy array of dtype `bytes_` or `str_`, this\n    class adds the following functionality:\n\n    1) values automatically have whitespace removed from the end\n       when indexed\n\n    2) comparison operators automatically remove whitespace from the\n       end when comparing values\n\n    3) vectorized string operations are provided as methods\n       (e.g. `chararray.endswith <numpy.char.chararray.endswith>`)\n       and infix operators (e.g. ``+, *, %``)\n\n    Parameters\n    ----------\n    obj : array of str or unicode-like\n\n    itemsize : int, optional\n        `itemsize` is the number of characters per scalar in the\n        resulting array.  If `itemsize` is None, and `obj` is an\n        object array or a Python list, the `itemsize` will be\n        automatically determined.  If `itemsize` is provided and `obj`\n        is of type str or unicode, then the `obj` string will be\n        chunked into `itemsize` pieces.\n\n    copy : bool, optional\n        If true (default), then the object is copied.  Otherwise, a copy\n        will only be made if ``__array__`` returns a copy, if obj is a\n        nested sequence, or if a copy is needed to satisfy any of the other\n        requirements (`itemsize`, unicode, `order`, etc.).\n\n    unicode : bool, optional\n        When true, the resulting `~numpy.char.chararray` can contain Unicode\n        characters, when false only 8-bit characters.  If unicode is\n        None and `obj` is one of the following:\n\n        - a `~numpy.char.chararray`,\n        - an ndarray of type :class:`str_` or :class:`bytes_`\n        - a Python :class:`str` or :class:`bytes` object,\n\n        then the unicode setting of the output array will be\n        automatically determined.\n\n    order : {'C', 'F', 'A'}, optional\n        Specify the order of the array.  If order is 'C' (default), then the\n        array will be in C-contiguous order (last-index varies the\n        fastest).  If order is 'F', then the returned array\n        will be in Fortran-contiguous order (first-index varies the\n        fastest).  If order is 'A', then the returned array may\n        be in any order (either C-, Fortran-contiguous, or even\n        discontiguous).\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> char_array = np.char.array(['hello', 'world', 'numpy','array'])\n    >>> char_array\n    chararray(['hello', 'world', 'numpy', 'array'], dtype='<U5')\n\n    \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1356, "code": "def asarray(obj, itemsize=None, unicode=None, order=None):\n    return array(obj, itemsize, copy=False,\n                 unicode=unicode, order=order)", "documentation": "    \"\"\"\n    Convert the input to a `~numpy.char.chararray`, copying the data only if\n    necessary.\n\n    Versus a NumPy array of dtype `bytes_` or `str_`, this\n    class adds the following functionality:\n\n    1) values automatically have whitespace removed from the end\n       when indexed\n\n    2) comparison operators automatically remove whitespace from the\n       end when comparing values\n\n    3) vectorized string operations are provided as methods\n       (e.g. `chararray.endswith <numpy.char.chararray.endswith>`)\n       and infix operators (e.g. ``+``, ``*``, ``%``)\n\n    Parameters\n    ----------\n    obj : array of str or unicode-like\n\n    itemsize : int, optional\n        `itemsize` is the number of characters per scalar in the\n        resulting array.  If `itemsize` is None, and `obj` is an\n        object array or a Python list, the `itemsize` will be\n        automatically determined.  If `itemsize` is provided and `obj`\n        is of type str or unicode, then the `obj` string will be\n        chunked into `itemsize` pieces.\n\n    unicode : bool, optional\n        When true, the resulting `~numpy.char.chararray` can contain Unicode\n        characters, when false only 8-bit characters.  If unicode is\n        None and `obj` is one of the following:\n\n        - a `~numpy.char.chararray`,\n        - an ndarray of type `str_` or `unicode_`\n        - a Python str or unicode object,\n\n        then the unicode setting of the output array will be\n        automatically determined.\n\n    order : {'C', 'F'}, optional\n        Specify the order of the array.  If order is 'C' (default), then the\n        array will be in C-contiguous order (last-index varies the\n        fastest).  If order is 'F', then the returned array\n        will be in Fortran-contiguous order (first-index varies the\n        fastest).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.char.asarray(['hello', 'world'])\n    chararray(['hello', 'world'], dtype='<U5')\n\n    \"\"\""}], "after_segments": [{"filename": "numpy/_core/defchararray.py", "start_line": 61, "code": "def equal(x1, x2):\n    return compare_chararrays(x1, x2, '==', True)\n@array_function_dispatch(_binary_op_dispatcher)", "documentation": "    \"\"\"\n    Return (x1 == x2) element-wise.\n\n    Unlike `numpy.equal`, this comparison is performed by first\n    stripping whitespace characters from the end of the string.  This\n    behavior is provided for backward-compatibility with numarray.\n\n    Parameters\n    ----------\n    x1, x2 : array_like of str or unicode\n        Input arrays of the same shape.\n\n    Returns\n    -------\n    out : ndarray\n        Output array of bools.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> y = \"aa \"\n    >>> x = \"aa\"\n    >>> np.char.equal(x, y)\n    array(True)\n\n    See Also\n    --------\n    not_equal, greater_equal, less_equal, greater, less\n    \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 95, "code": "def not_equal(x1, x2):\n    return compare_chararrays(x1, x2, '!=', True)\n@array_function_dispatch(_binary_op_dispatcher)", "documentation": "    \"\"\"\n    Return (x1 != x2) element-wise.\n\n    Unlike `numpy.not_equal`, this comparison is performed by first\n    stripping whitespace characters from the end of the string.  This\n    behavior is provided for backward-compatibility with numarray.\n\n    Parameters\n    ----------\n    x1, x2 : array_like of str or unicode\n        Input arrays of the same shape.\n\n    Returns\n    -------\n    out : ndarray\n        Output array of bools.\n\n    See Also\n    --------\n    equal, greater_equal, less_equal, greater, less\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> x1 = np.array(['a', 'b', 'c'])\n    >>> np.char.not_equal(x1, 'b')\n    array([ True, False,  True])\n\n    \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 129, "code": "def greater_equal(x1, x2):\n    return compare_chararrays(x1, x2, '>=', True)\n@array_function_dispatch(_binary_op_dispatcher)", "documentation": "    \"\"\"\n    Return (x1 >= x2) element-wise.\n\n    Unlike `numpy.greater_equal`, this comparison is performed by\n    first stripping whitespace characters from the end of the string.\n    This behavior is provided for backward-compatibility with\n    numarray.\n\n    Parameters\n    ----------\n    x1, x2 : array_like of str or unicode\n        Input arrays of the same shape.\n\n    Returns\n    -------\n    out : ndarray\n        Output array of bools.\n\n    See Also\n    --------\n    equal, not_equal, less_equal, greater, less\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> x1 = np.array(['a', 'b', 'c'])\n    >>> np.char.greater_equal(x1, 'b')\n    array([False,  True,  True])\n\n    \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 164, "code": "def less_equal(x1, x2):\n    return compare_chararrays(x1, x2, '<=', True)\n@array_function_dispatch(_binary_op_dispatcher)", "documentation": "    \"\"\"\n    Return (x1 <= x2) element-wise.\n\n    Unlike `numpy.less_equal`, this comparison is performed by first\n    stripping whitespace characters from the end of the string.  This\n    behavior is provided for backward-compatibility with numarray.\n\n    Parameters\n    ----------\n    x1, x2 : array_like of str or unicode\n        Input arrays of the same shape.\n\n    Returns\n    -------\n    out : ndarray\n        Output array of bools.\n\n    See Also\n    --------\n    equal, not_equal, greater_equal, greater, less\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> x1 = np.array(['a', 'b', 'c'])\n    >>> np.char.less_equal(x1, 'b')\n    array([ True,  True, False])\n\n    \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 198, "code": "def greater(x1, x2):\n    return compare_chararrays(x1, x2, '>', True)\n@array_function_dispatch(_binary_op_dispatcher)", "documentation": "    \"\"\"\n    Return (x1 > x2) element-wise.\n\n    Unlike `numpy.greater`, this comparison is performed by first\n    stripping whitespace characters from the end of the string.  This\n    behavior is provided for backward-compatibility with numarray.\n\n    Parameters\n    ----------\n    x1, x2 : array_like of str or unicode\n        Input arrays of the same shape.\n\n    Returns\n    -------\n    out : ndarray\n        Output array of bools.\n\n    See Also\n    --------\n    equal, not_equal, greater_equal, less_equal, less\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> x1 = np.array(['a', 'b', 'c'])\n    >>> np.char.greater(x1, 'b')\n    array([False, False,  True])\n\n    \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 232, "code": "def less(x1, x2):\n    return compare_chararrays(x1, x2, '<', True)\n@set_module(\"numpy.char\")", "documentation": "    \"\"\"\n    Return (x1 < x2) element-wise.\n\n    Unlike `numpy.greater`, this comparison is performed by first\n    stripping whitespace characters from the end of the string.  This\n    behavior is provided for backward-compatibility with numarray.\n\n    Parameters\n    ----------\n    x1, x2 : array_like of str or unicode\n        Input arrays of the same shape.\n\n    Returns\n    -------\n    out : ndarray\n        Output array of bools.\n\n    See Also\n    --------\n    equal, not_equal, greater_equal, less_equal, greater\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> x1 = np.array(['a', 'b', 'c'])\n    >>> np.char.less(x1, 'b')\n    array([True, False, False])\n\n    \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 266, "code": "def multiply(a, i):\n    try:\n        return strings_multiply(a, i)\n    except TypeError:\n        raise ValueError(\"Can only multiply by integers\")\n@set_module(\"numpy.char\")", "documentation": "    \"\"\"\n    Return (a * i), that is string multiple concatenation,\n    element-wise.\n\n    Values in ``i`` of less than 0 are treated as 0 (which yields an\n    empty string).\n\n    Parameters\n    ----------\n    a : array_like, with `np.bytes_` or `np.str_` dtype\n\n    i : array_like, with any integer dtype\n\n    Returns\n    -------\n    out : ndarray\n        Output array of str or unicode, depending on input types\n\n    Notes\n    -----\n    This is a thin wrapper around np.strings.multiply that raises\n    `ValueError` when ``i`` is not an integer. It only\n    exists for backwards-compatibility.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> a = np.array([\"a\", \"b\", \"c\"])\n    >>> np.strings.multiply(a, 3)\n    array(['aaa', 'bbb', 'ccc'], dtype='<U3')\n    >>> i = np.array([1, 2, 3])\n    >>> np.strings.multiply(a, i)\n    array(['a', 'bb', 'ccc'], dtype='<U3')\n    >>> np.strings.multiply(np.array(['a']), i)\n    array(['a', 'aa', 'aaa'], dtype='<U3')\n    >>> a = np.array(['a', 'b', 'c', 'd', 'e', 'f']).reshape((2, 3))\n    >>> np.strings.multiply(a, 3)\n    array([['aaa', 'bbb', 'ccc'],\n           ['ddd', 'eee', 'fff']], dtype='<U3')\n    >>> np.strings.multiply(a, i)\n    array([['a', 'bb', 'ccc'],\n           ['d', 'ee', 'fff']], dtype='<U3')\n\n    \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 318, "code": "def partition(a, sep):\n    return np.stack(strings_partition(a, sep), axis=-1)\n@set_module(\"numpy.char\")", "documentation": "    \"\"\"\n    Partition each element in `a` around `sep`.\n\n    Calls :meth:`str.partition` element-wise.\n\n    For each element in `a`, split the element as the first\n    occurrence of `sep`, and return 3 strings containing the part\n    before the separator, the separator itself, and the part after\n    the separator. If the separator is not found, return 3 strings\n    containing the string itself, followed by two empty strings.\n\n    Parameters\n    ----------\n    a : array-like, with ``StringDType``, ``bytes_``, or ``str_`` dtype\n        Input array\n    sep : {str, unicode}\n        Separator to split each string element in `a`.\n\n    Returns\n    -------\n    out : ndarray\n        Output array of ``StringDType``, ``bytes_`` or ``str_`` dtype,\n        depending on input types. The output array will have an extra\n        dimension with 3 elements per input element.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> x = np.array([\"Numpy is nice!\"])\n    >>> np.char.partition(x, \" \")\n    array([['Numpy', ' ', 'is nice!']], dtype='<U8')\n\n    See Also\n    --------\n    str.partition\n\n    \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 360, "code": "def rpartition(a, sep):\n    return np.stack(strings_rpartition(a, sep), axis=-1)\n@set_module(\"numpy.char\")", "documentation": "    \"\"\"\n    Partition (split) each element around the right-most separator.\n\n    Calls :meth:`str.rpartition` element-wise.\n\n    For each element in `a`, split the element as the last\n    occurrence of `sep`, and return 3 strings containing the part\n    before the separator, the separator itself, and the part after\n    the separator. If the separator is not found, return 3 strings\n    containing the string itself, followed by two empty strings.\n\n    Parameters\n    ----------\n    a : array-like, with ``StringDType``, ``bytes_``, or ``str_`` dtype\n        Input array\n    sep : str or unicode\n        Right-most separator to split each element in array.\n\n    Returns\n    -------\n    out : ndarray\n        Output array of ``StringDType``, ``bytes_`` or ``str_`` dtype,\n        depending on input types. The output array will have an extra\n        dimension with 3 elements per input element.\n\n    See Also\n    --------\n    str.rpartition\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> a = np.array(['aAaAaA', '  aA  ', 'abBABba'])\n    >>> np.char.rpartition(a, 'A')\n    array([['aAaAa', 'A', ''],\n       ['  a', 'A', '  '],\n       ['abB', 'A', 'Bba']], dtype='<U5')\n\n    \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 404, "code": "class chararray(ndarray):", "documentation": "    \"\"\"\n    chararray(shape, itemsize=1, unicode=False, buffer=None, offset=0,\n              strides=None, order=None)\n\n    Provides a convenient view on arrays of string and unicode values.\n\n    .. deprecated:: 2.5\n       ``chararray`` is deprecated. Use an ``ndarray`` with a string or\n       bytes dtype instead.\n\n    .. note::\n       The `chararray` class exists for backwards compatibility with\n       Numarray, it is not recommended for new development. Starting from numpy\n       1.4, if one needs arrays of strings, it is recommended to use arrays of\n       `dtype` `~numpy.object_`, `~numpy.bytes_` or `~numpy.str_`, and use\n       the free functions in the `numpy.char` module for fast vectorized\n       string operations.\n\n    Versus a NumPy array of dtype `~numpy.bytes_` or `~numpy.str_`, this\n    class adds the following functionality:\n\n    1) values automatically have whitespace removed from the end\n       when indexed\n\n    2) comparison operators automatically remove whitespace from the\n       end when comparing values\n\n    3) vectorized string operations are provided as methods\n       (e.g. `.endswith`) and infix operators (e.g. ``\"+\", \"*\", \"%\"``)\n\n    chararrays should be created using `numpy.char.array` or\n    `numpy.char.asarray`, rather than this constructor directly.\n\n    This constructor creates the array, using `buffer` (with `offset`\n    and `strides`) if it is not ``None``. If `buffer` is ``None``, then\n    constructs a new array with `strides` in \"C order\", unless both\n    ``len(shape) >= 2`` and ``order='F'``, in which case `strides`\n    is in \"Fortran order\".\n\n    Methods\n    -------\n    astype\n    argsort\n    copy\n    count\n    decode\n    dump\n    dumps\n    encode\n    endswith\n    expandtabs\n    fill\n    find\n    flatten\n    getfield\n    index\n    isalnum\n    isalpha\n    isdecimal\n    isdigit\n    islower\n    isnumeric\n    isspace\n    istitle\n    isupper\n    item\n    join\n    ljust\n    lower\n    lstrip\n    nonzero\n    put\n    ravel\n    repeat\n    replace\n    reshape\n    resize\n    rfind\n    rindex\n    rjust\n    rsplit\n    rstrip\n    searchsorted\n    setfield\n    setflags\n    sort\n    split\n    splitlines\n    squeeze\n    startswith\n    strip\n    swapaxes\n    swapcase\n    take\n    title\n    tofile\n    tolist\n    translate\n    transpose\n    upper\n    view\n    zfill\n\n    Parameters\n    ----------\n    shape : tuple\n        Shape of the array.\n    itemsize : int, optional\n        Length of each array element, in number of characters. Default is 1.\n    unicode : bool, optional\n        Are the array elements of type unicode (True) or string (False).\n        Default is False.\n    buffer : object exposing the buffer interface or str, optional\n        Memory address of the start of the array data.  Default is None,\n        in which case a new array is created.\n    offset : int, optional\n        Fixed stride displacement from the beginning of an axis?\n        Default is 0. Needs to be >=0.\n    strides : array_like of ints, optional\n        Strides for the array (see `~numpy.ndarray.strides` for\n        full description). Default is None.\n    order : {'C', 'F'}, optional\n        The order in which the array data is stored in memory: 'C' ->\n        \"row major\" order (the default), 'F' -> \"column major\"\n        (Fortran) order.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> charar = np.char.chararray((3, 3))\n    >>> charar[:] = 'a'\n    >>> charar\n    chararray([[b'a', b'a', b'a'],\n               [b'a', b'a', b'a'],\n               [b'a', b'a', b'a']], dtype='|S1')\n\n    >>> charar = np.char.chararray(charar.shape, itemsize=5)\n    >>> charar[:] = 'abc'\n    >>> charar\n    chararray([[b'abc', b'abc', b'abc'],\n               [b'abc', b'abc', b'abc'],\n               [b'abc', b'abc', b'abc']], dtype='|S5')\n\n    \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 605, "code": "    def __eq__(self, other):\n        return equal(self, other)", "documentation": "        \"\"\"\n        Return (self == other) element-wise.\n\n        See Also\n        --------\n        equal\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 615, "code": "    def __ne__(self, other):\n        return not_equal(self, other)", "documentation": "        \"\"\"\n        Return (self != other) element-wise.\n\n        See Also\n        --------\n        not_equal\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 625, "code": "    def __ge__(self, other):\n        return greater_equal(self, other)", "documentation": "        \"\"\"\n        Return (self >= other) element-wise.\n\n        See Also\n        --------\n        greater_equal\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 635, "code": "    def __le__(self, other):\n        return less_equal(self, other)", "documentation": "        \"\"\"\n        Return (self <= other) element-wise.\n\n        See Also\n        --------\n        less_equal\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 645, "code": "    def __gt__(self, other):\n        return greater(self, other)", "documentation": "        \"\"\"\n        Return (self > other) element-wise.\n\n        See Also\n        --------\n        greater\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 655, "code": "    def __lt__(self, other):\n        return less(self, other)", "documentation": "        \"\"\"\n        Return (self < other) element-wise.\n\n        See Also\n        --------\n        less\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 665, "code": "    def __add__(self, other):\n        return add(self, other)", "documentation": "        \"\"\"\n        Return (self + other), that is string concatenation,\n        element-wise for a pair of array_likes of str or unicode.\n\n        See Also\n        --------\n        add\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 676, "code": "    def __radd__(self, other):\n        return add(other, self)", "documentation": "        \"\"\"\n        Return (other + self), that is string concatenation,\n        element-wise for a pair of array_likes of `bytes_` or `str_`.\n\n        See Also\n        --------\n        add\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 687, "code": "    def __mul__(self, i):\n        return asarray(multiply(self, i))", "documentation": "        \"\"\"\n        Return (self * i), that is string multiple concatenation,\n        element-wise.\n\n        See Also\n        --------\n        multiply\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 698, "code": "    def __rmul__(self, i):\n        return asarray(multiply(self, i))", "documentation": "        \"\"\"\n        Return (self * i), that is string multiple concatenation,\n        element-wise.\n\n        See Also\n        --------\n        multiply\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 709, "code": "    def __mod__(self, i):\n        return asarray(mod(self, i))", "documentation": "        \"\"\"\n        Return (self % i), that is pre-Python 2.6 string formatting\n        (interpolation), element-wise for a pair of array_likes of `bytes_`\n        or `str_`.\n\n        See Also\n        --------\n        mod\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 724, "code": "    def argsort(self, axis=-1, kind=None, order=None, *, stable=None):\n        return self.__array__().argsort(axis, kind, order, stable=stable)\n    argsort.__doc__ = ndarray.argsort.__doc__", "documentation": "        \"\"\"\n        Return the indices that sort the array lexicographically.\n\n        For full documentation see `numpy.argsort`, for which this method is\n        in fact merely a \"thin wrapper.\"\n\n        Examples\n        --------\n        >>> c = np.array(['a1b c', '1b ca', 'b ca1', 'Ca1b'], 'S5')\n        >>> c = c.view(np.char.chararray); c\n        chararray(['a1b c', '1b ca', 'b ca1', 'Ca1b'],\n              dtype='|S5')\n        >>> c[c.argsort()]\n        chararray(['1b ca', 'Ca1b', 'a1b c', 'b ca1'],\n              dtype='|S5')\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 745, "code": "    def capitalize(self):\n        return asarray(capitalize(self))", "documentation": "        \"\"\"\n        Return a copy of `self` with only the first character of each element\n        capitalized.\n\n        See Also\n        --------\n        char.capitalize\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 757, "code": "    def center(self, width, fillchar=' '):\n        return asarray(center(self, width, fillchar))", "documentation": "        \"\"\"\n        Return a copy of `self` with its elements centered in a\n        string of length `width`.\n\n        See Also\n        --------\n        center\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 768, "code": "    def count(self, sub, start=0, end=None):\n        return count(self, sub, start, end)", "documentation": "        \"\"\"\n        Returns an array with the number of non-overlapping occurrences of\n        substring `sub` in the range [`start`, `end`].\n\n        See Also\n        --------\n        char.count\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 780, "code": "    def decode(self, encoding=None, errors=None):\n        return decode(self, encoding, errors)", "documentation": "        \"\"\"\n        Calls ``bytes.decode`` element-wise.\n\n        See Also\n        --------\n        char.decode\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 791, "code": "    def encode(self, encoding=None, errors=None):\n        return encode(self, encoding, errors)", "documentation": "        \"\"\"\n        Calls :meth:`str.encode` element-wise.\n\n        See Also\n        --------\n        char.encode\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 802, "code": "    def endswith(self, suffix, start=0, end=None):\n        return endswith(self, suffix, start, end)", "documentation": "        \"\"\"\n        Returns a boolean array which is `True` where the string element\n        in `self` ends with `suffix`, otherwise `False`.\n\n        See Also\n        --------\n        char.endswith\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 814, "code": "    def expandtabs(self, tabsize=8):\n        return asarray(expandtabs(self, tabsize))", "documentation": "        \"\"\"\n        Return a copy of each string element where all tab characters are\n        replaced by one or more spaces.\n\n        See Also\n        --------\n        char.expandtabs\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 826, "code": "    def find(self, sub, start=0, end=None):\n        return find(self, sub, start, end)", "documentation": "        \"\"\"\n        For each element, return the lowest index in the string where\n        substring `sub` is found.\n\n        See Also\n        --------\n        char.find\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 838, "code": "    def index(self, sub, start=0, end=None):\n        return index(self, sub, start, end)", "documentation": "        \"\"\"\n        Like `find`, but raises :exc:`ValueError` when the substring is not\n        found.\n\n        See Also\n        --------\n        char.index\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 850, "code": "    def isalnum(self):\n        return isalnum(self)", "documentation": "        \"\"\"\n        Returns true for each element if all characters in the string\n        are alphanumeric and there is at least one character, false\n        otherwise.\n\n        See Also\n        --------\n        char.isalnum\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 863, "code": "    def isalpha(self):\n        return isalpha(self)", "documentation": "        \"\"\"\n        Returns true for each element if all characters in the string\n        are alphabetic and there is at least one character, false\n        otherwise.\n\n        See Also\n        --------\n        char.isalpha\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 876, "code": "    def isdigit(self):\n        return isdigit(self)", "documentation": "        \"\"\"\n        Returns true for each element if all characters in the string are\n        digits and there is at least one character, false otherwise.\n\n        See Also\n        --------\n        char.isdigit\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 888, "code": "    def islower(self):\n        return islower(self)", "documentation": "        \"\"\"\n        Returns true for each element if all cased characters in the\n        string are lowercase and there is at least one cased character,\n        false otherwise.\n\n        See Also\n        --------\n        char.islower\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 901, "code": "    def isspace(self):\n        return isspace(self)", "documentation": "        \"\"\"\n        Returns true for each element if there are only whitespace\n        characters in the string and there is at least one character,\n        false otherwise.\n\n        See Also\n        --------\n        char.isspace\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 914, "code": "    def istitle(self):\n        return istitle(self)", "documentation": "        \"\"\"\n        Returns true for each element if the element is a titlecased\n        string and there is at least one character, false otherwise.\n\n        See Also\n        --------\n        char.istitle\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 926, "code": "    def isupper(self):\n        return isupper(self)", "documentation": "        \"\"\"\n        Returns true for each element if all cased characters in the\n        string are uppercase and there is at least one character, false\n        otherwise.\n\n        See Also\n        --------\n        char.isupper\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 939, "code": "    def join(self, seq):\n        return join(self, seq)", "documentation": "        \"\"\"\n        Return a string which is the concatenation of the strings in the\n        sequence `seq`.\n\n        See Also\n        --------\n        char.join\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 951, "code": "    def ljust(self, width, fillchar=' '):\n        return asarray(ljust(self, width, fillchar))", "documentation": "        \"\"\"\n        Return an array with the elements of `self` left-justified in a\n        string of length `width`.\n\n        See Also\n        --------\n        char.ljust\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 963, "code": "    def lower(self):\n        return asarray(lower(self))", "documentation": "        \"\"\"\n        Return an array with the elements of `self` converted to\n        lowercase.\n\n        See Also\n        --------\n        char.lower\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 975, "code": "    def lstrip(self, chars=None):\n        return lstrip(self, chars)", "documentation": "        \"\"\"\n        For each element in `self`, return a copy with the leading characters\n        removed.\n\n        See Also\n        --------\n        char.lstrip\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 987, "code": "    def partition(self, sep):\n        return asarray(partition(self, sep))", "documentation": "        \"\"\"\n        Partition each element in `self` around `sep`.\n\n        See Also\n        --------\n        partition\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 997, "code": "    def replace(self, old, new, count=None):\n        return replace(self, old, new, count if count is not None else -1)", "documentation": "        \"\"\"\n        For each element in `self`, return a copy of the string with all\n        occurrences of substring `old` replaced by `new`.\n\n        See Also\n        --------\n        char.replace\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1009, "code": "    def rfind(self, sub, start=0, end=None):\n        return rfind(self, sub, start, end)", "documentation": "        \"\"\"\n        For each element in `self`, return the highest index in the string\n        where substring `sub` is found, such that `sub` is contained\n        within [`start`, `end`].\n\n        See Also\n        --------\n        char.rfind\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1022, "code": "    def rindex(self, sub, start=0, end=None):\n        return rindex(self, sub, start, end)", "documentation": "        \"\"\"\n        Like `rfind`, but raises :exc:`ValueError` when the substring `sub` is\n        not found.\n\n        See Also\n        --------\n        char.rindex\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1034, "code": "    def rjust(self, width, fillchar=' '):\n        return asarray(rjust(self, width, fillchar))", "documentation": "        \"\"\"\n        Return an array with the elements of `self`\n        right-justified in a string of length `width`.\n\n        See Also\n        --------\n        char.rjust\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1046, "code": "    def rpartition(self, sep):\n        return asarray(rpartition(self, sep))", "documentation": "        \"\"\"\n        Partition each element in `self` around `sep`.\n\n        See Also\n        --------\n        rpartition\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1056, "code": "    def rsplit(self, sep=None, maxsplit=None):\n        return rsplit(self, sep, maxsplit)", "documentation": "        \"\"\"\n        For each element in `self`, return a list of the words in\n        the string, using `sep` as the delimiter string.\n\n        See Also\n        --------\n        char.rsplit\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1068, "code": "    def rstrip(self, chars=None):\n        return rstrip(self, chars)", "documentation": "        \"\"\"\n        For each element in `self`, return a copy with the trailing\n        characters removed.\n\n        See Also\n        --------\n        char.rstrip\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1080, "code": "    def split(self, sep=None, maxsplit=None):\n        return split(self, sep, maxsplit)", "documentation": "        \"\"\"\n        For each element in `self`, return a list of the words in the\n        string, using `sep` as the delimiter string.\n\n        See Also\n        --------\n        char.split\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1092, "code": "    def splitlines(self, keepends=None):\n        return splitlines(self, keepends)", "documentation": "        \"\"\"\n        For each element in `self`, return a list of the lines in the\n        element, breaking at line boundaries.\n\n        See Also\n        --------\n        char.splitlines\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1104, "code": "    def startswith(self, prefix, start=0, end=None):\n        return startswith(self, prefix, start, end)", "documentation": "        \"\"\"\n        Returns a boolean array which is `True` where the string element\n        in `self` starts with `prefix`, otherwise `False`.\n\n        See Also\n        --------\n        char.startswith\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1116, "code": "    def strip(self, chars=None):\n        return strip(self, chars)", "documentation": "        \"\"\"\n        For each element in `self`, return a copy with the leading and\n        trailing characters removed.\n\n        See Also\n        --------\n        char.strip\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1128, "code": "    def swapcase(self):\n        return asarray(swapcase(self))", "documentation": "        \"\"\"\n        For each element in `self`, return a copy of the string with\n        uppercase characters converted to lowercase and vice versa.\n\n        See Also\n        --------\n        char.swapcase\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1140, "code": "    def title(self):\n        return asarray(title(self))", "documentation": "        \"\"\"\n        For each element in `self`, return a titlecased version of the\n        string: words start with uppercase characters, all remaining cased\n        characters are lowercase.\n\n        See Also\n        --------\n        char.title\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1153, "code": "    def translate(self, table, deletechars=None):\n        return asarray(translate(self, table, deletechars))", "documentation": "        \"\"\"\n        For each element in `self`, return a copy of the string where\n        all characters occurring in the optional argument\n        `deletechars` are removed, and the remaining characters have\n        been mapped through the given translation table.\n\n        See Also\n        --------\n        char.translate\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1167, "code": "    def upper(self):\n        return asarray(upper(self))", "documentation": "        \"\"\"\n        Return an array with the elements of `self` converted to\n        uppercase.\n\n        See Also\n        --------\n        char.upper\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1179, "code": "    def zfill(self, width):\n        return asarray(zfill(self, width))", "documentation": "        \"\"\"\n        Return the numeric string left-filled with zeros in a string of\n        length `width`.\n\n        See Also\n        --------\n        char.zfill\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1191, "code": "    def isnumeric(self):\n        return isnumeric(self)", "documentation": "        \"\"\"\n        For each element in `self`, return True if there are only\n        numeric characters in the element.\n\n        See Also\n        --------\n        char.isnumeric\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1203, "code": "    def isdecimal(self):\n        return isdecimal(self)\n@set_module(\"numpy.char\")", "documentation": "        \"\"\"\n        For each element in `self`, return True if there are only\n        decimal characters in the element.\n\n        See Also\n        --------\n        char.isdecimal\n\n        \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1217, "code": "def array(obj, itemsize=None, copy=True, unicode=None, order=None):\n    if isinstance(obj, (bytes, str)):\n        if unicode is None:\n            if isinstance(obj, str):\n                unicode = True\n            else:\n                unicode = False\n        if itemsize is None:\n            itemsize = len(obj)\n        shape = len(obj) // itemsize\n        return chararray(shape, itemsize=itemsize, unicode=unicode,", "documentation": "    \"\"\"\n    Create a `~numpy.char.chararray`.\n\n    .. note::\n       This class is provided for numarray backward-compatibility.\n       New code (not concerned with numarray compatibility) should use\n       arrays of type `bytes_` or `str_` and use the free functions\n       in :mod:`numpy.char` for fast vectorized string operations instead.\n\n    Versus a NumPy array of dtype `bytes_` or `str_`, this\n    class adds the following functionality:\n\n    1) values automatically have whitespace removed from the end\n       when indexed\n\n    2) comparison operators automatically remove whitespace from the\n       end when comparing values\n\n    3) vectorized string operations are provided as methods\n       (e.g. `chararray.endswith <numpy.char.chararray.endswith>`)\n       and infix operators (e.g. ``+, *, %``)\n\n    Parameters\n    ----------\n    obj : array of str or unicode-like\n\n    itemsize : int, optional\n        `itemsize` is the number of characters per scalar in the\n        resulting array.  If `itemsize` is None, and `obj` is an\n        object array or a Python list, the `itemsize` will be\n        automatically determined.  If `itemsize` is provided and `obj`\n        is of type str or unicode, then the `obj` string will be\n        chunked into `itemsize` pieces.\n\n    copy : bool, optional\n        If true (default), then the object is copied.  Otherwise, a copy\n        will only be made if ``__array__`` returns a copy, if obj is a\n        nested sequence, or if a copy is needed to satisfy any of the other\n        requirements (`itemsize`, unicode, `order`, etc.).\n\n    unicode : bool, optional\n        When true, the resulting `~numpy.char.chararray` can contain Unicode\n        characters, when false only 8-bit characters.  If unicode is\n        None and `obj` is one of the following:\n\n        - a `~numpy.char.chararray`,\n        - an ndarray of type :class:`str_` or :class:`bytes_`\n        - a Python :class:`str` or :class:`bytes` object,\n\n        then the unicode setting of the output array will be\n        automatically determined.\n\n    order : {'C', 'F', 'A'}, optional\n        Specify the order of the array.  If order is 'C' (default), then the\n        array will be in C-contiguous order (last-index varies the\n        fastest).  If order is 'F', then the returned array\n        will be in Fortran-contiguous order (first-index varies the\n        fastest).  If order is 'A', then the returned array may\n        be in any order (either C-, Fortran-contiguous, or even\n        discontiguous).\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> char_array = np.char.array(['hello', 'world', 'numpy','array'])\n    >>> char_array\n    chararray(['hello', 'world', 'numpy', 'array'], dtype='<U5')\n\n    \"\"\""}, {"filename": "numpy/_core/defchararray.py", "start_line": 1360, "code": "def asarray(obj, itemsize=None, unicode=None, order=None):\n    return array(obj, itemsize, copy=False,\n                 unicode=unicode, order=order)", "documentation": "    \"\"\"\n    Convert the input to a `~numpy.char.chararray`, copying the data only if\n    necessary.\n\n    Versus a NumPy array of dtype `bytes_` or `str_`, this\n    class adds the following functionality:\n\n    1) values automatically have whitespace removed from the end\n       when indexed\n\n    2) comparison operators automatically remove whitespace from the\n       end when comparing values\n\n    3) vectorized string operations are provided as methods\n       (e.g. `chararray.endswith <numpy.char.chararray.endswith>`)\n       and infix operators (e.g. ``+``, ``*``, ``%``)\n\n    Parameters\n    ----------\n    obj : array of str or unicode-like\n\n    itemsize : int, optional\n        `itemsize` is the number of characters per scalar in the\n        resulting array.  If `itemsize` is None, and `obj` is an\n        object array or a Python list, the `itemsize` will be\n        automatically determined.  If `itemsize` is provided and `obj`\n        is of type str or unicode, then the `obj` string will be\n        chunked into `itemsize` pieces.\n\n    unicode : bool, optional\n        When true, the resulting `~numpy.char.chararray` can contain Unicode\n        characters, when false only 8-bit characters.  If unicode is\n        None and `obj` is one of the following:\n\n        - a `~numpy.char.chararray`,\n        - an ndarray of type `str_` or `unicode_`\n        - a Python str or unicode object,\n\n        then the unicode setting of the output array will be\n        automatically determined.\n\n    order : {'C', 'F'}, optional\n        Specify the order of the array.  If order is 'C' (default), then the\n        array will be in C-contiguous order (last-index varies the\n        fastest).  If order is 'F', then the returned array\n        will be in Fortran-contiguous order (first-index varies the\n        fastest).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.char.asarray(['hello', 'world'])\n    chararray(['hello', 'world'], dtype='<U5')\n\n    \"\"\""}]}
{"repository": "numpy/numpy", "commit_sha": "4420e94a8f60715f5ab2829f16c04b875143f15c", "commit_message": "DEP: Deprecate numpy.fix in favor of numpy.trunc\n\nDeprecate `numpy.fix` as it provides identical functionality to\n`numpy.trunc` but is slower and is not part of the Array API standard.\n\n- Add DeprecationWarning to numpy.fix()\n- Update docstring with .. deprecated:: directive\n- Add tests to verify deprecation warning is emitted\n- Update existing tests to suppress deprecation warnings\n- Add release note about deprecation\n\nCloses #30096", "commit_date": "2026-01-12T22:35:07+00:00", "author": "skyvanguard", "file": "numpy/lib/_ufunclike_impl.py", "patch": "@@ -5,6 +5,8 @@\n \"\"\"\n __all__ = ['fix', 'isneginf', 'isposinf']\n \n+import warnings\n+\n import numpy._core.numeric as nx\n from numpy._core.overrides import array_function_dispatch\n \n@@ -18,6 +20,10 @@ def fix(x, out=None):\n     \"\"\"\n     Round to nearest integer towards zero.\n \n+    .. deprecated:: 2.3\n+        `numpy.fix` is deprecated. Use `numpy.trunc` instead,\n+        which is faster and follows the Array API standard.\n+\n     Round an array of floats element-wise to nearest integer towards zero.\n     The rounded values have the same data-type as the input.\n \n@@ -56,6 +62,13 @@ def fix(x, out=None):\n     array([ 2.,  2., -2., -2.])\n \n     \"\"\"\n+    # Deprecated in NumPy 2.3, 2025-01-12\n+    warnings.warn(\n+        \"numpy.fix is deprecated. Use numpy.trunc instead, \"\n+        \"which is faster and follows the Array API standard.\",\n+        DeprecationWarning,\n+        stacklevel=2,\n+    )\n     return nx.trunc(x, out=out)\n \n ", "before_segments": [{"filename": "numpy/lib/_ufunclike_impl.py", "start_line": 16, "code": "def fix(x, out=None):\n    return nx.trunc(x, out=out)\n@array_function_dispatch(_dispatcher, verify=False, module='numpy')", "documentation": "    \"\"\"\n    Round to nearest integer towards zero.\n\n    Round an array of floats element-wise to nearest integer towards zero.\n    The rounded values have the same data-type as the input.\n\n    Parameters\n    ----------\n    x : array_like\n        An array to be rounded\n    out : ndarray, optional\n        A location into which the result is stored. If provided, it must have\n        a shape that the input broadcasts to. If not provided or None, a\n        freshly-allocated array is returned.\n\n    Returns\n    -------\n    out : ndarray of floats\n        An array with the same dimensions and data-type as the input.\n        If second argument is not supplied then a new array is returned\n        with the rounded values.\n\n        If a second argument is supplied the result is stored there.\n        The return value ``out`` is then a reference to that array.\n\n    See Also\n    --------\n    rint, trunc, floor, ceil\n    around : Round to given number of decimals\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.fix(3.14)\n    3.0\n    >>> np.fix(3)\n    3\n    >>> np.fix([2.1, 2.9, -2.1, -2.9])\n    array([ 2.,  2., -2., -2.])\n\n    \"\"\""}, {"filename": "numpy/lib/_ufunclike_impl.py", "start_line": 62, "code": "def isposinf(x, out=None):\n    is_inf = nx.isinf(x)\n    try:\n        signbit = ~nx.signbit(x)\n    except TypeError as e:\n        dtype = nx.asanyarray(x).dtype\n        raise TypeError(f'This operation is not supported for {dtype} values '\n                        'because it would be ambiguous.') from e\n    else:\n        return nx.logical_and(is_inf, signbit, out)\n@array_function_dispatch(_dispatcher, verify=False, module='numpy')", "documentation": "    \"\"\"\n    Test element-wise for positive infinity, return result as bool array.\n\n    Parameters\n    ----------\n    x : array_like\n        The input array.\n    out : array_like, optional\n        A location into which the result is stored. If provided, it must have a\n        shape that the input broadcasts to. If not provided or None, a\n        freshly-allocated boolean array is returned.\n\n    Returns\n    -------\n    out : ndarray\n        A boolean array with the same dimensions as the input.\n        If second argument is not supplied then a boolean array is returned\n        with values True where the corresponding element of the input is\n        positive infinity and values False where the element of the input is\n        not positive infinity.\n\n        If a second argument is supplied the result is stored there. If the\n        type of that array is a numeric type the result is represented as zeros\n        and ones, if the type is boolean then as False and True.\n        The return value `out` is then a reference to that array.\n\n    See Also\n    --------\n    isinf, isneginf, isfinite, isnan\n\n    Notes\n    -----\n    NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic\n    (IEEE 754).\n\n    Errors result if the second argument is also supplied when x is a scalar\n    input, if first and second arguments have different shapes, or if the\n    first argument has complex values\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.isposinf(np.inf)\n    True\n    >>> np.isposinf(-np.inf)\n    False\n    >>> np.isposinf([-np.inf, 0., np.inf])\n    array([False, False,  True])\n\n    >>> x = np.array([-np.inf, 0., np.inf])\n    >>> y = np.array([2, 2, 2])\n    >>> np.isposinf(x, y)\n    array([0, 0, 1])\n    >>> y\n    array([0, 0, 1])\n\n    \"\"\""}, {"filename": "numpy/lib/_ufunclike_impl.py", "start_line": 132, "code": "def isneginf(x, out=None):\n    is_inf = nx.isinf(x)\n    try:\n        signbit = nx.signbit(x)\n    except TypeError as e:\n        dtype = nx.asanyarray(x).dtype\n        raise TypeError(f'This operation is not supported for {dtype} values '\n                        'because it would be ambiguous.') from e\n    else:\n        return nx.logical_and(is_inf, signbit, out)", "documentation": "    \"\"\"\n    Test element-wise for negative infinity, return result as bool array.\n\n    Parameters\n    ----------\n    x : array_like\n        The input array.\n    out : array_like, optional\n        A location into which the result is stored. If provided, it must have a\n        shape that the input broadcasts to. If not provided or None, a\n        freshly-allocated boolean array is returned.\n\n    Returns\n    -------\n    out : ndarray\n        A boolean array with the same dimensions as the input.\n        If second argument is not supplied then a numpy boolean array is\n        returned with values True where the corresponding element of the\n        input is negative infinity and values False where the element of\n        the input is not negative infinity.\n\n        If a second argument is supplied the result is stored there. If the\n        type of that array is a numeric type the result is represented as\n        zeros and ones, if the type is boolean then as False and True. The\n        return value `out` is then a reference to that array.\n\n    See Also\n    --------\n    isinf, isposinf, isnan, isfinite\n\n    Notes\n    -----\n    NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic\n    (IEEE 754).\n\n    Errors result if the second argument is also supplied when x is a scalar\n    input, if first and second arguments have different shapes, or if the\n    first argument has complex values.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.isneginf(-np.inf)\n    True\n    >>> np.isneginf(np.inf)\n    False\n    >>> np.isneginf([-np.inf, 0., np.inf])\n    array([ True, False, False])\n\n    >>> x = np.array([-np.inf, 0., np.inf])\n    >>> y = np.array([2, 2, 2])\n    >>> np.isneginf(x, y)\n    array([1, 0, 0])\n    >>> y\n    array([1, 0, 0])\n\n    \"\"\""}], "after_segments": [{"filename": "numpy/lib/_ufunclike_impl.py", "start_line": 18, "code": "def fix(x, out=None):\n    warnings.warn(\n        \"numpy.fix is deprecated. Use numpy.trunc instead, \"\n        \"which is faster and follows the Array API standard.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return nx.trunc(x, out=out)\n@array_function_dispatch(_dispatcher, verify=False, module='numpy')", "documentation": "    \"\"\"\n    Round to nearest integer towards zero.\n\n    .. deprecated:: 2.3\n        `numpy.fix` is deprecated. Use `numpy.trunc` instead,\n        which is faster and follows the Array API standard.\n\n    Round an array of floats element-wise to nearest integer towards zero.\n    The rounded values have the same data-type as the input.\n\n    Parameters\n    ----------\n    x : array_like\n        An array to be rounded\n    out : ndarray, optional\n        A location into which the result is stored. If provided, it must have\n        a shape that the input broadcasts to. If not provided or None, a\n        freshly-allocated array is returned.\n\n    Returns\n    -------\n    out : ndarray of floats\n        An array with the same dimensions and data-type as the input.\n        If second argument is not supplied then a new array is returned\n        with the rounded values.\n\n        If a second argument is supplied the result is stored there.\n        The return value ``out`` is then a reference to that array.\n\n    See Also\n    --------\n    rint, trunc, floor, ceil\n    around : Round to given number of decimals\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.fix(3.14)\n    3.0\n    >>> np.fix(3)\n    3\n    >>> np.fix([2.1, 2.9, -2.1, -2.9])\n    array([ 2.,  2., -2., -2.])\n\n    \"\"\""}, {"filename": "numpy/lib/_ufunclike_impl.py", "start_line": 75, "code": "def isposinf(x, out=None):\n    is_inf = nx.isinf(x)\n    try:\n        signbit = ~nx.signbit(x)\n    except TypeError as e:\n        dtype = nx.asanyarray(x).dtype\n        raise TypeError(f'This operation is not supported for {dtype} values '\n                        'because it would be ambiguous.') from e\n    else:\n        return nx.logical_and(is_inf, signbit, out)\n@array_function_dispatch(_dispatcher, verify=False, module='numpy')", "documentation": "    \"\"\"\n    Test element-wise for positive infinity, return result as bool array.\n\n    Parameters\n    ----------\n    x : array_like\n        The input array.\n    out : array_like, optional\n        A location into which the result is stored. If provided, it must have a\n        shape that the input broadcasts to. If not provided or None, a\n        freshly-allocated boolean array is returned.\n\n    Returns\n    -------\n    out : ndarray\n        A boolean array with the same dimensions as the input.\n        If second argument is not supplied then a boolean array is returned\n        with values True where the corresponding element of the input is\n        positive infinity and values False where the element of the input is\n        not positive infinity.\n\n        If a second argument is supplied the result is stored there. If the\n        type of that array is a numeric type the result is represented as zeros\n        and ones, if the type is boolean then as False and True.\n        The return value `out` is then a reference to that array.\n\n    See Also\n    --------\n    isinf, isneginf, isfinite, isnan\n\n    Notes\n    -----\n    NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic\n    (IEEE 754).\n\n    Errors result if the second argument is also supplied when x is a scalar\n    input, if first and second arguments have different shapes, or if the\n    first argument has complex values\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.isposinf(np.inf)\n    True\n    >>> np.isposinf(-np.inf)\n    False\n    >>> np.isposinf([-np.inf, 0., np.inf])\n    array([False, False,  True])\n\n    >>> x = np.array([-np.inf, 0., np.inf])\n    >>> y = np.array([2, 2, 2])\n    >>> np.isposinf(x, y)\n    array([0, 0, 1])\n    >>> y\n    array([0, 0, 1])\n\n    \"\"\""}, {"filename": "numpy/lib/_ufunclike_impl.py", "start_line": 145, "code": "def isneginf(x, out=None):\n    is_inf = nx.isinf(x)\n    try:\n        signbit = nx.signbit(x)\n    except TypeError as e:\n        dtype = nx.asanyarray(x).dtype\n        raise TypeError(f'This operation is not supported for {dtype} values '\n                        'because it would be ambiguous.') from e\n    else:\n        return nx.logical_and(is_inf, signbit, out)", "documentation": "    \"\"\"\n    Test element-wise for negative infinity, return result as bool array.\n\n    Parameters\n    ----------\n    x : array_like\n        The input array.\n    out : array_like, optional\n        A location into which the result is stored. If provided, it must have a\n        shape that the input broadcasts to. If not provided or None, a\n        freshly-allocated boolean array is returned.\n\n    Returns\n    -------\n    out : ndarray\n        A boolean array with the same dimensions as the input.\n        If second argument is not supplied then a numpy boolean array is\n        returned with values True where the corresponding element of the\n        input is negative infinity and values False where the element of\n        the input is not negative infinity.\n\n        If a second argument is supplied the result is stored there. If the\n        type of that array is a numeric type the result is represented as\n        zeros and ones, if the type is boolean then as False and True. The\n        return value `out` is then a reference to that array.\n\n    See Also\n    --------\n    isinf, isposinf, isnan, isfinite\n\n    Notes\n    -----\n    NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic\n    (IEEE 754).\n\n    Errors result if the second argument is also supplied when x is a scalar\n    input, if first and second arguments have different shapes, or if the\n    first argument has complex values.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.isneginf(-np.inf)\n    True\n    >>> np.isneginf(np.inf)\n    False\n    >>> np.isneginf([-np.inf, 0., np.inf])\n    array([ True, False, False])\n\n    >>> x = np.array([-np.inf, 0., np.inf])\n    >>> y = np.array([2, 2, 2])\n    >>> np.isneginf(x, y)\n    array([1, 0, 0])\n    >>> y\n    array([1, 0, 0])\n\n    \"\"\""}]}
{"repository": "numpy/numpy", "commit_sha": "4420e94a8f60715f5ab2829f16c04b875143f15c", "commit_message": "DEP: Deprecate numpy.fix in favor of numpy.trunc\n\nDeprecate `numpy.fix` as it provides identical functionality to\n`numpy.trunc` but is slower and is not part of the Array API standard.\n\n- Add DeprecationWarning to numpy.fix()\n- Update docstring with .. deprecated:: directive\n- Add tests to verify deprecation warning is emitted\n- Update existing tests to suppress deprecation warnings\n- Add release note about deprecation\n\nCloses #30096", "commit_date": "2026-01-12T22:35:07+00:00", "author": "skyvanguard", "file": "numpy/lib/tests/test_ufunclike.py", "patch": "@@ -1,3 +1,7 @@\n+import warnings\n+\n+import pytest\n+\n import numpy as np\n from numpy import fix, isneginf, isposinf\n from numpy.testing import assert_, assert_array_equal, assert_equal, assert_raises\n@@ -40,12 +44,14 @@ def test_fix(self):\n         out = np.zeros(a.shape, float)\n         tgt = np.array([[1., 1., 1., 1.], [-1., -1., -1., -1.]])\n \n-        res = fix(a)\n-        assert_equal(res, tgt)\n-        res = fix(a, out)\n-        assert_equal(res, tgt)\n-        assert_equal(out, tgt)\n-        assert_equal(fix(3.14), 3)\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"ignore\", DeprecationWarning)\n+            res = fix(a)\n+            assert_equal(res, tgt)\n+            res = fix(a, out)\n+            assert_equal(res, tgt)\n+            assert_equal(out, tgt)\n+            assert_equal(fix(3.14), 3)\n \n     def test_fix_with_subclass(self):\n         class MyArray(np.ndarray):\n@@ -67,17 +73,19 @@ def __array_finalize__(self, obj):\n \n         a = np.array([1.1, -1.1])\n         m = MyArray(a, metadata='foo')\n-        f = fix(m)\n-        assert_array_equal(f, np.array([1, -1]))\n-        assert_(isinstance(f, MyArray))\n-        assert_equal(f.metadata, 'foo')\n-\n-        # check 0d arrays don't decay to scalars\n-        m0d = m[0, ...]\n-        m0d.metadata = 'bar'\n-        f0d = fix(m0d)\n-        assert_(isinstance(f0d, MyArray))\n-        assert_equal(f0d.metadata, 'bar')\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"ignore\", DeprecationWarning)\n+            f = fix(m)\n+            assert_array_equal(f, np.array([1, -1]))\n+            assert_(isinstance(f, MyArray))\n+            assert_equal(f.metadata, 'foo')\n+\n+            # check 0d arrays don't decay to scalars\n+            m0d = m[0, ...]\n+            m0d.metadata = 'bar'\n+            f0d = fix(m0d)\n+            assert_(isinstance(f0d, MyArray))\n+            assert_equal(f0d.metadata, 'bar')\n \n     def test_scalar(self):\n         x = np.inf\n@@ -87,11 +95,32 @@ def test_scalar(self):\n         assert_equal(type(actual), type(expected))\n \n         x = -3.4\n-        actual = np.fix(x)\n-        expected = np.float64(-3.0)\n-        assert_equal(actual, expected)\n-        assert_equal(type(actual), type(expected))\n-\n-        out = np.array(0.0)\n-        actual = np.fix(x, out=out)\n-        assert_(actual is out)\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"ignore\", DeprecationWarning)\n+            actual = np.fix(x)\n+            expected = np.float64(-3.0)\n+            assert_equal(actual, expected)\n+            assert_equal(type(actual), type(expected))\n+\n+            out = np.array(0.0)\n+            actual = np.fix(x, out=out)\n+            assert_(actual is out)\n+\n+\n+class TestFixDeprecation:\n+    \"\"\"Test that numpy.fix emits a DeprecationWarning.\"\"\"\n+\n+    def test_fix_emits_deprecation_warning(self):\n+        a = np.array([1.5, 2.7, -1.5, -2.7])\n+        with pytest.warns(DeprecationWarning, match=\"numpy.fix is deprecated\"):\n+            fix(a)\n+\n+    def test_fix_scalar_emits_deprecation_warning(self):\n+        with pytest.warns(DeprecationWarning, match=\"numpy.fix is deprecated\"):\n+            fix(3.14)\n+\n+    def test_fix_with_out_emits_deprecation_warning(self):\n+        a = np.array([1.5, 2.7])\n+        out = np.zeros(a.shape)\n+        with pytest.warns(DeprecationWarning, match=\"numpy.fix is deprecated\"):\n+            fix(a, out=out)", "before_segments": [], "after_segments": [{"filename": "numpy/lib/tests/test_ufunclike.py", "start_line": 109, "code": "class TestFixDeprecation:", "documentation": "    \"\"\"Test that numpy.fix emits a DeprecationWarning.\"\"\""}]}
{"repository": "numpy/numpy", "commit_sha": "eba60dc3e61528953b2fa0a85d4d7ee009814797", "commit_message": "ENH: Reduce compute time for `tobytes` in non-contiguos paths (#30170)\n\n* Add optimal copy path for non-contiguos arrays in ToString C Impl.\n\n* Add tests for \tobytes new path\n\n* Add imports\n\n* fix comments\n\n* fix memory issues and add benchmarks\n\n* run ruff --fix\n\n* simplify function\n\n* minor fix\n\n* minor typos and move tests", "commit_date": "2025-11-12T11:48:33+00:00", "author": "Aadya Chinubhai", "file": "benchmarks/benchmarks/bench_core.py", "patch": "@@ -14,6 +14,7 @@ def setup(self):\n         self.l_view = [memoryview(a) for a in self.l]\n         self.l10x10 = np.ones((10, 10))\n         self.float64_dtype = np.dtype(np.float64)\n+        self.arr = np.arange(10000).reshape(100, 100)\n \n     def time_array_1(self):\n         np.array(1)\n@@ -48,6 +49,9 @@ def time_array_l_view(self):\n     def time_can_cast(self):\n         np.can_cast(self.l10x10, self.float64_dtype)\n \n+    def time_tobytes_noncontiguous(self):\n+        self.arr.T.tobytes()\n+\n     def time_can_cast_same_kind(self):\n         np.can_cast(self.l10x10, self.float64_dtype, casting=\"same_kind\")\n ", "before_segments": [], "after_segments": []}
{"repository": "numpy/numpy", "commit_sha": "eba60dc3e61528953b2fa0a85d4d7ee009814797", "commit_message": "ENH: Reduce compute time for `tobytes` in non-contiguos paths (#30170)\n\n* Add optimal copy path for non-contiguos arrays in ToString C Impl.\n\n* Add tests for \tobytes new path\n\n* Add imports\n\n* fix comments\n\n* fix memory issues and add benchmarks\n\n* run ruff --fix\n\n* simplify function\n\n* minor fix\n\n* minor typos and move tests", "commit_date": "2025-11-12T11:48:33+00:00", "author": "Aadya Chinubhai", "file": "numpy/_core/src/multiarray/convert.c", "patch": "@@ -335,11 +335,7 @@ NPY_NO_EXPORT PyObject *\n PyArray_ToString(PyArrayObject *self, NPY_ORDER order)\n {\n     npy_intp numbytes;\n-    npy_intp i;\n-    char *dptr;\n-    int elsize;\n     PyObject *ret;\n-    PyArrayIterObject *it;\n \n     if (order == NPY_ANYORDER)\n         order = PyArray_ISFORTRAN(self) ? NPY_FORTRANORDER : NPY_CORDER;\n@@ -354,41 +350,65 @@ PyArray_ToString(PyArrayObject *self, NPY_ORDER order)\n     numbytes = PyArray_NBYTES(self);\n     if ((PyArray_IS_C_CONTIGUOUS(self) && (order == NPY_CORDER))\n         || (PyArray_IS_F_CONTIGUOUS(self) && (order == NPY_FORTRANORDER))) {\n-        ret = PyBytes_FromStringAndSize(PyArray_DATA(self), (Py_ssize_t) numbytes);\n+        return PyBytes_FromStringAndSize(PyArray_DATA(self), (Py_ssize_t) numbytes);\n     }\n-    else {\n-        PyObject *new;\n-        if (order == NPY_FORTRANORDER) {\n-            /* iterators are always in C-order */\n-            new = PyArray_Transpose(self, NULL);\n-            if (new == NULL) {\n-                return NULL;\n-            }\n+    \n+    /* Avoid Ravel where possible for fewer copies. */\n+    if (!PyDataType_REFCHK(PyArray_DESCR(self)) && \n+        ((PyArray_DESCR(self)->flags & NPY_NEEDS_INIT) == 0)) {\n+        \n+        /* Allocate final Bytes Object */\n+        ret = PyBytes_FromStringAndSize(NULL, (Py_ssize_t) numbytes);\n+        if (ret == NULL) {\n+            return NULL;\n         }\n-        else {\n-            Py_INCREF(self);\n-            new = (PyObject *)self;\n+        \n+        /* Writable Buffer */\n+        char* dest = PyBytes_AS_STRING(ret);\n+\n+        int flags = NPY_ARRAY_WRITEABLE;\n+        if (order == NPY_FORTRANORDER) {\n+            flags |= NPY_ARRAY_F_CONTIGUOUS;\n         }\n-        it = (PyArrayIterObject *)PyArray_IterNew(new);\n-        Py_DECREF(new);\n-        if (it == NULL) {\n+\n+        Py_INCREF(PyArray_DESCR(self));\n+        /* Array view */\n+        PyArrayObject *dest_array = (PyArrayObject *)PyArray_NewFromDescr(\n+            &PyArray_Type,\n+            PyArray_DESCR(self),\n+            PyArray_NDIM(self),\n+            PyArray_DIMS(self),\n+            NULL, // strides\n+            dest,\n+            flags,\n+            NULL\n+        );\n+\n+        if (dest_array == NULL) {\n+            Py_DECREF(ret);\n             return NULL;\n         }\n-        ret = PyBytes_FromStringAndSize(NULL, (Py_ssize_t) numbytes);\n-        if (ret == NULL) {\n-            Py_DECREF(it);\n+        \n+        /* Copy directly from source to destination with proper ordering */\n+        if (PyArray_CopyInto(dest_array, self) < 0) {\n+            Py_DECREF(dest_array);\n+            Py_DECREF(ret);\n             return NULL;\n         }\n-        dptr = PyBytes_AS_STRING(ret);\n-        i = it->size;\n-        elsize = PyArray_ITEMSIZE(self);\n-        while (i--) {\n-            memcpy(dptr, it->dataptr, elsize);\n-            dptr += elsize;\n-            PyArray_ITER_NEXT(it);\n-        }\n-        Py_DECREF(it);\n+        \n+        Py_DECREF(dest_array);\n+        return ret;\n+\n+    }\n+\n+    /* Non-contiguous, Has References and/or Init Path.  */\n+    PyArrayObject *contig = (PyArrayObject *)PyArray_Ravel(self, order);\n+    if (contig == NULL) {\n+        return NULL;\n     }\n+    \n+    ret = PyBytes_FromStringAndSize(PyArray_DATA(contig), numbytes);\n+    Py_DECREF(contig);\n     return ret;\n }\n ", "before_segments": [], "after_segments": []}
{"repository": "numpy/numpy", "commit_sha": "eba60dc3e61528953b2fa0a85d4d7ee009814797", "commit_message": "ENH: Reduce compute time for `tobytes` in non-contiguos paths (#30170)\n\n* Add optimal copy path for non-contiguos arrays in ToString C Impl.\n\n* Add tests for \tobytes new path\n\n* Add imports\n\n* fix comments\n\n* fix memory issues and add benchmarks\n\n* run ruff --fix\n\n* simplify function\n\n* minor fix\n\n* minor typos and move tests", "commit_date": "2025-11-12T11:48:33+00:00", "author": "Aadya Chinubhai", "file": "numpy/_core/tests/test_multiarray.py", "patch": "@@ -3835,6 +3835,18 @@ class ArraySubclass(np.ndarray):\n         assert_(isinstance(a.ravel('A'), ArraySubclass))\n         assert_(isinstance(a.ravel('K'), ArraySubclass))\n \n+    @pytest.mark.parametrize(\"shape\", [(3, 224, 224), (8, 512, 512)])\n+    def test_tobytes_no_copy_fastpath(self, shape):\n+        # Test correctness of non-contiguous paths for `tobytes`\n+        rng = np.random.default_rng(0)\n+        arr = rng.standard_normal(shape, dtype=np.float32)\n+        noncontig = arr.transpose(1, 2, 0)\n+\n+        # correctness\n+        expected = np.ascontiguousarray(noncontig).tobytes()\n+        got = noncontig.tobytes()\n+        assert got == expected\n+\n     def test_swapaxes(self):\n         a = np.arange(1 * 2 * 3 * 4).reshape(1, 2, 3, 4).copy()\n         idx = np.indices(a.shape)\n@@ -10546,7 +10558,6 @@ def test_getfield():\n     pytest.raises(ValueError, a.getfield, 'uint8', 16)\n     pytest.raises(ValueError, a.getfield, 'uint64', 0)\n \n-\n class TestViewDtype:\n     \"\"\"\n     Verify that making a view of a non-contiguous array works as expected.", "before_segments": [{"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 72, "code": "def _aligned_zeros(shape, dtype=float, order=\"C\", align=None):\n    dtype = np.dtype(dtype)\n    if dtype == np.dtype(object):\n        if align is not None:\n            raise ValueError(\"object array alignment not supported\")\n        return np.zeros(shape, dtype=dtype, order=order)\n    if align is None:\n        align = dtype.alignment\n    if not hasattr(shape, '__len__'):\n        shape = (shape,)\n    size = functools.reduce(operator.mul, shape) * dtype.itemsize", "documentation": "    \"\"\"\n    Allocate a new ndarray with aligned memory.\n\n    The ndarray is guaranteed *not* aligned to twice the requested alignment.\n    Eg, if align=4, guarantees it is not aligned to 8. If align=None uses\n    dtype.alignment.\"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 667, "code": "        def inject_str(s):\n            set_printoptions(formatter={\"all\": lambda x: s})\n            try:\n                yield\n            finally:\n                set_printoptions()\n        a1d = np.array(['test'])\n        a0d = np.array('done')\n        with inject_str('bad'):\n            a1d[0] = a0d  # previously this would invoke __str__\n        assert_equal(a1d[0], 'done')", "documentation": "            \"\"\" replace ndarray.__str__ temporarily \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 963, "code": "class TestCreation:", "documentation": "    \"\"\"\n    Test the np.array constructor\n    \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 1131, "code": "    def test_non_sequence_sequence(self):", "documentation": "        \"\"\"Should not segfault.\n\n        Class Fail breaks the sequence protocol for new style classes, i.e.,\n        those derived from object. Class Map is a mapping type indicated by\n        raising a ValueError. At some point we may raise a warning instead\n        of an error in the Fail case.\n\n        \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 1977, "code": "    def test_pickle_empty(self):\n        arr = np.array([]).reshape(999999, 0)\n        pk_dmp = pickle.dumps(arr)\n        pk_load = pickle.loads(pk_dmp)\n        assert pk_load.size == 0\n    @pytest.mark.skipif(pickle.HIGHEST_PROTOCOL < 5,\n                        reason=\"requires pickle protocol 5\")", "documentation": "        \"\"\"Checking if an empty array pickled and un-pickled will not cause a\n        segmentation fault\"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 5653, "code": "class TestIO:", "documentation": "    \"\"\"Test tofile, fromfile, tobytes, and fromstring\"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 5944, "code": "    def decimal_sep_localization(self, request):\n        if request.param == \"period\":\n            yield\n        elif request.param == \"comma\":\n            with CommaDecimalPointLocale():\n                yield\n        else:\n            assert False, request.param", "documentation": "        \"\"\"\n        Including this fixture in a test will automatically\n        execute it with both types of decimal separator.\n\n        So::\n\n            def test_decimal(decimal_sep_localization):\n                pass\n\n        is equivalent to the following two tests::\n\n            def test_decimal_period_separator():\n                pass\n\n            def test_decimal_comma_separator():\n                with CommaDecimalPointLocale():\n                    pass\n        \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 7266, "code": "class MatmulCommon:\n    types = \"?bhilqBHILQefdgFDGO\"", "documentation": "    \"\"\"Common tests for '@' operator and numpy.matmul.\n\n    \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 8342, "code": "class TestNewBufferProtocol:", "documentation": "    \"\"\" Test PEP3118 buffers \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 8733, "code": "    def test_error_if_stored_buffer_info_is_corrupted(self, obj):\n        _multiarray_tests.corrupt_or_fix_bufferinfo(obj)\n        name = type(obj)\n        with pytest.raises(RuntimeError,\n                    match=f\".*{name} appears to be C subclassed\"):\n            memoryview(obj)\n        _multiarray_tests.corrupt_or_fix_bufferinfo(obj)", "documentation": "        \"\"\"\n        If a user extends a NumPy array before 1.20 and then runs it\n        on NumPy 1.20+. A C-subclassed array might in theory modify\n        the new buffer-info field. This checks that an error is raised\n        if this happens (for buffer export), an error is written on delete.\n        This is a sanity check to help users transition to safe code, it\n        may be deleted at any point.\n        \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 9741, "code": "class TestUnicodeEncoding:", "documentation": "    \"\"\"\n    Tests for encoding related bugs, such as UCS2 vs UCS4, round-tripping\n    issues, etc\n    \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 9746, "code": "    def test_round_trip(self):\n        arr = np.zeros(shape=(), dtype=\"U1\")\n        for i in range(1, sys.maxunicode + 1):\n            expected = chr(i)\n            arr[()] = expected\n            assert arr[()] == expected\n            assert arr.item() == expected", "documentation": "        \"\"\" Tests that GETITEM, SETITEM, and PyArray_Scalar roundtrip \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 10169, "code": "class TestArrayFinalize:", "documentation": "    \"\"\" Tests __array_finalize__ \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 10549, "code": "class TestViewDtype:", "documentation": "    \"\"\"\n    Verify that making a view of a non-contiguous array works as expected.\n    \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 10807, "code": "def test_insufficient_width():\n    with pytest.raises(ValueError):\n        np.binary_repr(10, width=2)\n    with pytest.raises(ValueError):\n        np.binary_repr(-5, width=2)", "documentation": "    \"\"\"\n    If a 'width' parameter is passed into ``binary_repr`` that is insufficient\n    to represent the number in base 2 (positive) or 2's complement (negative)\n    form, the function used to silently ignore the parameter and return a\n    representation using the minimal number of bits needed for the form in\n    question. Such behavior is now considered unsafe from a user perspective\n    and will raise an error.\n    \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 10827, "code": "class TestDevice:\n    @pytest.mark.parametrize(\"func, arg\", [\n        (np.arange, 5),\n        (np.empty_like, []),\n        (np.zeros, 5),\n        (np.empty, (5, 5)),\n        (np.asarray, []),\n        (np.asanyarray, []),\n    ])", "documentation": "    \"\"\"\n    Test arr.device attribute and arr.to_device() method.\n    \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 10871, "code": "def test_array_interface_excess_dimensions_raises():", "documentation": "    \"\"\"Regression test for gh-27949: ensure too many dims raises ValueError instead of segfault.\"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 10893, "code": "def test_array_dunder_array_preserves_dtype_on_none(dtype):\n    a = np.array([1], dtype=dtype)\n    b = a.__array__(None)\n    assert_array_equal(a, b, strict=True)\n@pytest.mark.skipif(sys.flags.optimize == 2, reason=\"Python running -OO\")\n@pytest.mark.skipif(IS_PYPY, reason=\"PyPy does not modify tp_doc\")", "documentation": "    \"\"\"\n    Regression test for: https://github.com/numpy/numpy/issues/27407\n    Ensure that __array__(None) returns an array of the same dtype.\n    \"\"\""}], "after_segments": [{"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 72, "code": "def _aligned_zeros(shape, dtype=float, order=\"C\", align=None):\n    dtype = np.dtype(dtype)\n    if dtype == np.dtype(object):\n        if align is not None:\n            raise ValueError(\"object array alignment not supported\")\n        return np.zeros(shape, dtype=dtype, order=order)\n    if align is None:\n        align = dtype.alignment\n    if not hasattr(shape, '__len__'):\n        shape = (shape,)\n    size = functools.reduce(operator.mul, shape) * dtype.itemsize", "documentation": "    \"\"\"\n    Allocate a new ndarray with aligned memory.\n\n    The ndarray is guaranteed *not* aligned to twice the requested alignment.\n    Eg, if align=4, guarantees it is not aligned to 8. If align=None uses\n    dtype.alignment.\"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 667, "code": "        def inject_str(s):\n            set_printoptions(formatter={\"all\": lambda x: s})\n            try:\n                yield\n            finally:\n                set_printoptions()\n        a1d = np.array(['test'])\n        a0d = np.array('done')\n        with inject_str('bad'):\n            a1d[0] = a0d  # previously this would invoke __str__\n        assert_equal(a1d[0], 'done')", "documentation": "            \"\"\" replace ndarray.__str__ temporarily \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 963, "code": "class TestCreation:", "documentation": "    \"\"\"\n    Test the np.array constructor\n    \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 1131, "code": "    def test_non_sequence_sequence(self):", "documentation": "        \"\"\"Should not segfault.\n\n        Class Fail breaks the sequence protocol for new style classes, i.e.,\n        those derived from object. Class Map is a mapping type indicated by\n        raising a ValueError. At some point we may raise a warning instead\n        of an error in the Fail case.\n\n        \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 1977, "code": "    def test_pickle_empty(self):\n        arr = np.array([]).reshape(999999, 0)\n        pk_dmp = pickle.dumps(arr)\n        pk_load = pickle.loads(pk_dmp)\n        assert pk_load.size == 0\n    @pytest.mark.skipif(pickle.HIGHEST_PROTOCOL < 5,\n                        reason=\"requires pickle protocol 5\")", "documentation": "        \"\"\"Checking if an empty array pickled and un-pickled will not cause a\n        segmentation fault\"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 5665, "code": "class TestIO:", "documentation": "    \"\"\"Test tofile, fromfile, tobytes, and fromstring\"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 5956, "code": "    def decimal_sep_localization(self, request):\n        if request.param == \"period\":\n            yield\n        elif request.param == \"comma\":\n            with CommaDecimalPointLocale():\n                yield\n        else:\n            assert False, request.param", "documentation": "        \"\"\"\n        Including this fixture in a test will automatically\n        execute it with both types of decimal separator.\n\n        So::\n\n            def test_decimal(decimal_sep_localization):\n                pass\n\n        is equivalent to the following two tests::\n\n            def test_decimal_period_separator():\n                pass\n\n            def test_decimal_comma_separator():\n                with CommaDecimalPointLocale():\n                    pass\n        \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 7278, "code": "class MatmulCommon:\n    types = \"?bhilqBHILQefdgFDGO\"", "documentation": "    \"\"\"Common tests for '@' operator and numpy.matmul.\n\n    \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 8354, "code": "class TestNewBufferProtocol:", "documentation": "    \"\"\" Test PEP3118 buffers \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 8745, "code": "    def test_error_if_stored_buffer_info_is_corrupted(self, obj):\n        _multiarray_tests.corrupt_or_fix_bufferinfo(obj)\n        name = type(obj)\n        with pytest.raises(RuntimeError,\n                    match=f\".*{name} appears to be C subclassed\"):\n            memoryview(obj)\n        _multiarray_tests.corrupt_or_fix_bufferinfo(obj)", "documentation": "        \"\"\"\n        If a user extends a NumPy array before 1.20 and then runs it\n        on NumPy 1.20+. A C-subclassed array might in theory modify\n        the new buffer-info field. This checks that an error is raised\n        if this happens (for buffer export), an error is written on delete.\n        This is a sanity check to help users transition to safe code, it\n        may be deleted at any point.\n        \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 9753, "code": "class TestUnicodeEncoding:", "documentation": "    \"\"\"\n    Tests for encoding related bugs, such as UCS2 vs UCS4, round-tripping\n    issues, etc\n    \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 9758, "code": "    def test_round_trip(self):\n        arr = np.zeros(shape=(), dtype=\"U1\")\n        for i in range(1, sys.maxunicode + 1):\n            expected = chr(i)\n            arr[()] = expected\n            assert arr[()] == expected\n            assert arr.item() == expected", "documentation": "        \"\"\" Tests that GETITEM, SETITEM, and PyArray_Scalar roundtrip \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 10181, "code": "class TestArrayFinalize:", "documentation": "    \"\"\" Tests __array_finalize__ \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 10560, "code": "class TestViewDtype:", "documentation": "    \"\"\"\n    Verify that making a view of a non-contiguous array works as expected.\n    \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 10818, "code": "def test_insufficient_width():\n    with pytest.raises(ValueError):\n        np.binary_repr(10, width=2)\n    with pytest.raises(ValueError):\n        np.binary_repr(-5, width=2)", "documentation": "    \"\"\"\n    If a 'width' parameter is passed into ``binary_repr`` that is insufficient\n    to represent the number in base 2 (positive) or 2's complement (negative)\n    form, the function used to silently ignore the parameter and return a\n    representation using the minimal number of bits needed for the form in\n    question. Such behavior is now considered unsafe from a user perspective\n    and will raise an error.\n    \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 10838, "code": "class TestDevice:\n    @pytest.mark.parametrize(\"func, arg\", [\n        (np.arange, 5),\n        (np.empty_like, []),\n        (np.zeros, 5),\n        (np.empty, (5, 5)),\n        (np.asarray, []),\n        (np.asanyarray, []),\n    ])", "documentation": "    \"\"\"\n    Test arr.device attribute and arr.to_device() method.\n    \"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 10882, "code": "def test_array_interface_excess_dimensions_raises():", "documentation": "    \"\"\"Regression test for gh-27949: ensure too many dims raises ValueError instead of segfault.\"\"\""}, {"filename": "numpy/_core/tests/test_multiarray.py", "start_line": 10904, "code": "def test_array_dunder_array_preserves_dtype_on_none(dtype):\n    a = np.array([1], dtype=dtype)\n    b = a.__array__(None)\n    assert_array_equal(a, b, strict=True)\n@pytest.mark.skipif(sys.flags.optimize == 2, reason=\"Python running -OO\")\n@pytest.mark.skipif(IS_PYPY, reason=\"PyPy does not modify tp_doc\")", "documentation": "    \"\"\"\n    Regression test for: https://github.com/numpy/numpy/issues/27407\n    Ensure that __array__(None) returns an array of the same dtype.\n    \"\"\""}]}
{"repository": "scipy/scipy", "commit_sha": "98307e9d953289d30edb1fad6799a86c46476558", "commit_message": "MAINT: stats.rankdata: consistently return floating point dtype (#24420)\n\n* MAINT: stats.rankdata: always output floating point dtype\n\n* MAINT: stats: avoid unnecessary dtype conversions\n\n* DOC: stats.rankdata: update documentation to reflect new output dtype\n\n* MAINT: stats.ks_2samp: update after change in rankdata dtype\n\n* TST: stats.mood: adjust tolerance to address failing test", "commit_date": "2026-02-03T07:12:12+00:00", "author": "Matt Haberland", "file": "scipy/stats/_correlation.py", "patch": "@@ -1,8 +1,7 @@\n import numpy as np\n import math\n from scipy import stats\n-from scipy._lib._array_api import (xp_capabilities, array_namespace, xp_promote,\n-                                   xp_result_type)\n+from scipy._lib._array_api import xp_capabilities, array_namespace, xp_promote\n from scipy.stats._stats_py import (_SimpleNormal, SignificanceResult, _get_pvalue,\n                                    _rankdata)\n from scipy.stats._axis_nan_policy import _axis_nan_policy_factory\n@@ -30,7 +29,6 @@ def _xi_statistic(x, y, y_continuous, xp):\n     # \" additionally define li to be the number of j such that Y(j) \u2265 Y(i)\"\n     # Could probably compute this from r, but that can be an enhancement\n     l = stats.rankdata(-y, method='max', axis=-1)\n-    r, l = xp.astype(r, x.dtype), xp.astype(l, x.dtype)\n \n     num = xp.sum(xp.abs(xp.diff(r, axis=-1)), axis=-1)\n     if y_continuous:  # [1] Eq. 1.1\n@@ -378,12 +376,8 @@ def spearmanrho(x, y, /, *, alternative='two-sided', method=None, axis=0):\n            [0.14526128, 0.        ]])\n \n     \"\"\"\n-    xp = array_namespace(x, y)\n-    dtype = xp_result_type(x, y, force_floating=True, xp=xp)\n     rx = stats.rankdata(x, axis=axis)\n     ry = stats.rankdata(y, axis=axis)\n-    rx = xp.astype(rx, dtype, copy=False)\n-    ry = xp.astype(ry, dtype, copy=False)\n     res = stats.pearsonr(rx, ry, method=method, alternative=alternative, axis=axis)\n     return SignificanceResult(res.statistic, res.pvalue)\n ", "before_segments": [], "after_segments": []}
{"repository": "scipy/scipy", "commit_sha": "98307e9d953289d30edb1fad6799a86c46476558", "commit_message": "MAINT: stats.rankdata: consistently return floating point dtype (#24420)\n\n* MAINT: stats.rankdata: always output floating point dtype\n\n* MAINT: stats: avoid unnecessary dtype conversions\n\n* DOC: stats.rankdata: update documentation to reflect new output dtype\n\n* MAINT: stats.ks_2samp: update after change in rankdata dtype\n\n* TST: stats.mood: adjust tolerance to address failing test", "commit_date": "2026-02-03T07:12:12+00:00", "author": "Matt Haberland", "file": "scipy/stats/_hypotests.py", "patch": "@@ -1784,7 +1784,6 @@ def cramervonmises_2samp(x, y, method='auto', *, axis=0):\n     # in case of ties, use midrank (see [1])\n     r = scipy.stats.rankdata(z, method='average', axis=-1)\n     dtype = xp_result_type(x, y, force_floating=True, xp=xp)\n-    r = xp.astype(r, dtype, copy=False)\n     rx = r[..., :nx]\n     ry = r[..., nx:]\n ", "before_segments": [{"filename": "scipy/stats/_hypotests.py", "start_line": 34, "code": "def epps_singleton_2samp(x, y, t=(0.4, 0.8), *, axis=0):\n    xp = array_namespace(x, y)\n    x, y = xp_promote(x, y, force_floating=True, xp=xp)\n    t = xp.asarray(t, dtype=x.dtype)\n    nx, ny = x.shape[-1], y.shape[-1]\n    if (nx < 5) or (ny < 5):  # only used by test_axis_nan_policy\n        raise ValueError('x and y should have at least 5 elements, but len(x) '\n                         f'= {nx} and len(y) = {ny}.')\n    n = nx + ny\n    if t.ndim > 1:\n        raise ValueError(f't must be 1d, but t.ndim equals {t.ndim}.')", "documentation": "    \"\"\"Compute the Epps-Singleton (ES) test statistic.\n\n    Test the null hypothesis that two samples have the same underlying\n    probability distribution.\n\n    Parameters\n    ----------\n    x, y : array-like\n        The two samples of observations to be tested. Input must not have more\n        than one dimension. Samples can have different lengths, but both\n        must have at least five observations.\n    t : array-like, optional\n        The points (t1, ..., tn) where the empirical characteristic function is\n        to be evaluated. It should be positive distinct numbers. The default\n        value (0.4, 0.8) is proposed in [1]_. Input must not have more than\n        one dimension.\n    axis : int or tuple of ints, default: 0\n        If an int or tuple of ints, the axis or axes of the input along which\n        to compute the statistic. The statistic of each axis-slice (e.g. row)\n        of the input will appear in a corresponding element of the output.\n        If ``None``, the input will be raveled before computing the statistic.\n\n    Returns\n    -------\n    statistic : float\n        The test statistic.\n    pvalue : float\n        The associated p-value based on the asymptotic chi2-distribution.\n\n    See Also\n    --------\n    ks_2samp, anderson_ksamp\n\n    Notes\n    -----\n    Testing whether two samples are generated by the same underlying\n    distribution is a classical question in statistics. A widely used test is\n    the Kolmogorov-Smirnov (KS) test which relies on the empirical\n    distribution function. Epps and Singleton introduce a test based on the\n    empirical characteristic function in [1]_.\n\n    One advantage of the ES test compared to the KS test is that is does\n    not assume a continuous distribution. In [1]_, the authors conclude\n    that the test also has a higher power than the KS test in many\n    examples. They recommend the use of the ES test for discrete samples as\n    well as continuous samples with at least 25 observations each, whereas\n    `anderson_ksamp` is recommended for smaller sample sizes in the\n    continuous case.\n\n    The p-value is computed from the asymptotic distribution of the test\n    statistic which follows a `chi2` distribution. If the sample size of both\n    `x` and `y` is below 25, the small sample correction proposed in [1]_ is\n    applied to the test statistic.\n\n    The default values of `t` are determined in [1]_ by considering\n    various distributions and finding good values that lead to a high power\n    of the test in general. Table III in [1]_ gives the optimal values for\n    the distributions tested in that study. The values of `t` are scaled by\n    the semi-interquartile range in the implementation, see [1]_.\n\n    References\n    ----------\n    .. [1] T. W. Epps and K. J. Singleton, \"An omnibus test for the two-sample\n       problem using the empirical characteristic function\", Journal of\n       Statistical Computation and Simulation 26, p. 177--203, 1986.\n\n    .. [2] S. J. Goerg and J. Kaiser, \"Nonparametric testing of distributions\n       - the Epps-Singleton two-sample test using the empirical characteristic\n       function\", The Stata Journal 9(3), p. 454--465, 2009.\n\n    \"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 395, "code": "def _psi1_mod(x, *, xp=None):\n    xp = array_namespace(x) if xp is None else xp", "documentation": "    \"\"\"\n    psi1 is defined in equation 1.10 in Cs\u00f6rg\u0151, S. and Faraway, J. (1996).\n    This implements a modified version by excluding the term V(x) / 12\n    (here: _cdf_cvm_inf(x) / 12) to avoid evaluating _cdf_cvm_inf(x)\n    twice in _cdf_cvm.\n\n    Implementation based on MAPLE code of Julian Faraway and R code of the\n    function pCvM in the package goftest (v1.1.1), permission granted\n    by Adrian Baddeley. Main difference in the implementation: the code\n    here keeps adding terms of the series until the terms are small enough.\n    \"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 456, "code": "def _cdf_cvm_inf(x, *, xp=None):\n    xp = array_namespace(x) if xp is None else xp\n    x = xp.asarray(x)", "documentation": "    \"\"\"\n    Calculate the cdf of the Cram\u00e9r-von Mises statistic (infinite sample size).\n\n    See equation 1.2 in Cs\u00f6rg\u0151, S. and Faraway, J. (1996).\n\n    Implementation based on MAPLE code of Julian Faraway and R code of the\n    function pCvM in the package goftest (v1.1.1), permission granted\n    by Adrian Baddeley. Main difference in the implementation: the code\n    here keeps adding terms of the series until the terms are small enough.\n\n    The function is not expected to be accurate for large values of x, say\n    x > 4, when the cdf is very close to 1.\n    \"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 495, "code": "def _cdf_cvm(x, n=None, *, xp=None):\n    xp = array_namespace(x) if xp is None else xp\n    x = xp.asarray(x)\n    if n is None:\n        y = _cdf_cvm_inf(x, xp=xp)\n    else:\n        y = xp.zeros_like(x, dtype=x.dtype)\n        sup = (1./(12*n) < x) & (x < n/3.)\n        y = xpx.at(y)[sup].set(_cdf_cvm_inf(x[sup], xp=xp) * (1 + 1./(12*n))\n                               + _psi1_mod(x[sup], xp=xp) / n)\n        y = xpx.at(y)[x >= n/3].set(1.)", "documentation": "    \"\"\"\n    Calculate the cdf of the Cram\u00e9r-von Mises statistic for a finite sample\n    size n. If N is None, use the asymptotic cdf (n=inf).\n\n    See equation 1.8 in Cs\u00f6rg\u0151, S. and Faraway, J. (1996) for finite samples,\n    1.2 for the asymptotic cdf.\n\n    The function is not expected to be accurate for large values of x, say\n    x > 2, when the cdf is very close to 1 and it might return values > 1\n    in that case, e.g. _cdf_cvm(2.0, 12) = 1.0000027556716846. Moreover, it\n    is not accurate for small values of n, especially close to the bounds of\n    the distribution's domain, [1/(12*n), n/3], where the value jumps to 0\n    and 1, respectively. These are limitations of the approximation by Cs\u00f6rg\u0151\n    and Faraway (1996) implemented in this function.\n    \"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 673, "code": "def _get_wilcoxon_distr(n):\n    c = np.ones(1, dtype=np.float64)\n    for k in range(1, n + 1):\n        prev_c = c\n        c = np.zeros(k * (k + 1) // 2 + 1, dtype=np.float64)\n        m = len(prev_c)\n        c[:m] = prev_c * 0.5\n        c[-m:] += prev_c * 0.5\n    return c", "documentation": "    \"\"\"\n    Distribution of probability of the Wilcoxon ranksum statistic r_plus (sum\n    of ranks of positive differences).\n    Returns an array with the probabilities of all the possible ranks\n    r = 0, ..., n*(n+1)/2\n    \"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 690, "code": "def _get_wilcoxon_distr2(n):\n    ai = np.arange(1, n+1)[:, None]\n    t = n*(n+1)/2\n    q = 2*t\n    j = np.arange(q)\n    theta = 2*np.pi/q*j\n    phi_sp = np.prod(np.cos(theta*ai), axis=0)\n    phi_s = np.exp(1j*theta*t) * phi_sp\n    p = np.real(ifft(phi_s))\n    res = np.zeros(int(t)+1)\n    res[:-1:] = p[::2]", "documentation": "    \"\"\"\n    Distribution of probability of the Wilcoxon ranksum statistic r_plus (sum\n    of ranks of positive differences).\n    Returns an array with the probabilities of all the possible ranks\n    r = 0, ..., n*(n+1)/2\n    This is a slower reference function\n    References\n    ----------\n    .. [1] 1. Harris T, Hardin JW. Exact Wilcoxon Signed-Rank and Wilcoxon\n        Mann-Whitney Ranksum Tests. The Stata Journal. 2013;13(2):337-343.\n    \"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 717, "code": "def _tau_b(A):\n    if A.shape[0] == 1 or A.shape[1] == 1:\n        return np.nan, np.nan\n    NA = A.sum()\n    PA = _P(A)\n    QA = _Q(A)\n    Sri2 = (A.sum(axis=1)**2).sum()\n    Scj2 = (A.sum(axis=0)**2).sum()\n    denominator = (NA**2 - Sri2)*(NA**2 - Scj2)\n    tau = (PA-QA)/(denominator)**0.5\n    numerator = 4*(_a_ij_Aij_Dij2(A) - (PA - QA)**2 / NA)", "documentation": "    \"\"\"Calculate Kendall's tau-b and p-value from contingency table.\"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 744, "code": "def _somers_d(A, alternative='two-sided'):\n    if A.shape[0] <= 1 or A.shape[1] <= 1:\n        return np.nan, np.nan\n    NA = A.sum()\n    NA2 = NA**2\n    PA = _P(A)\n    QA = _Q(A)\n    Sri2 = (A.sum(axis=1)**2).sum()\n    d = (PA - QA)/(NA2 - Sri2)\n    S = _a_ij_Aij_Dij2(A) - (PA-QA)**2/NA\n    with np.errstate(divide='ignore'):", "documentation": "    \"\"\"Calculate Somers' D and p-value from contingency table.\"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 963, "code": "def _all_partitions(nx, ny):\n    z = np.arange(nx+ny)\n    for c in combinations(z, nx):\n        x = np.array(c)\n        mask = np.ones(nx+ny, bool)\n        mask[x] = False\n        y = z[mask]\n        yield x, y", "documentation": "    \"\"\"\n    Partition a set of indices into two fixed-length sets in all possible ways\n\n    Partition a set of indices 0 ... nx + ny - 1 into two sets of length nx and\n    ny in all possible ways (ignoring order of elements).\n    \"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 979, "code": "def _compute_log_combinations(n):\n    gammaln_arr = gammaln(np.arange(n + 1) + 1)\n    return gammaln(n + 1) - gammaln_arr - gammaln_arr[::-1]\n@dataclass", "documentation": "    \"\"\"Compute all log combination of C(n, k).\"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 1567, "code": "def _pval_cvm_2samp_exact(s, m, n):\n    lcm = np.lcm(m, n)\n    a = lcm // m\n    b = lcm // n\n    mn = m * n\n    zeta = lcm ** 2 * (m + n) * (6 * s - mn * (4 * mn - 1)) // (6 * mn ** 2)\n    zeta_bound = lcm**2 * (m + n)  # bound elements in row 1\n    combinations = math.comb(m + n, m)  # sum of row 2\n    max_gs = max(zeta_bound, combinations)\n    dtype = np.min_scalar_type(max_gs)\n    gs = ([np.array([[0], [1]], dtype=dtype)]", "documentation": "    \"\"\"\n    Compute the exact p-value of the Cramer-von Mises two-sample test\n    for a given value s of the test statistic.\n    m and n are the sizes of the samples.\n\n    [1] Y. Xiao, A. Gordon, and A. Yakovlev, \"A C++ Program for\n        the Cram\u00e9r-Von Mises Two-Sample Test\", J. Stat. Soft.,\n        vol. 17, no. 8, pp. 1-15, Dec. 2006.\n    [2] T. W. Anderson \"On the Distribution of the Two-Sample Cramer-von Mises\n        Criterion,\" The Annals of Mathematical Statistics, Ann. Math. Statist.\n        33(3), 1148-1159, (September, 1962)\n    \"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 1808, "code": "class TukeyHSDResult:", "documentation": "    \"\"\"Result of `scipy.stats.tukey_hsd`.\n\n    Attributes\n    ----------\n    statistic : float ndarray\n        The computed statistic of the test for each comparison. The element\n        at index ``(i, j)`` is the statistic for the comparison between groups\n        ``i`` and ``j``.\n    pvalue : float ndarray\n        The associated p-value from the studentized range distribution. The\n        element at index ``(i, j)`` is the p-value for the comparison\n        between groups ``i`` and ``j``.\n\n    Notes\n    -----\n    The string representation of this object displays the most recently\n    calculated confidence interval, and if none have been previously\n    calculated, it will evaluate ``confidence_interval()``.\n\n    References\n    ----------\n    .. [1] NIST/SEMATECH e-Handbook of Statistical Methods, \"7.4.7.1. Tukey's\n           Method.\"\n           https://www.itl.nist.gov/div898/handbook/prc/section4/prc471.htm,\n           28 November 2020.\n    .. [2] P. A. Games and J. F. Howell, \"Pairwise Multiple Comparison Procedures\n           with Unequal N's and/or Variances: A Monte Carlo Study,\" Journal of\n           Educational Statistics, vol. 1, no. 2, pp. 113-125, Jun. 1976,\n           :doi:`10.3102/10769986001002113`.\n    \"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 1866, "code": "    def confidence_interval(self, confidence_level=.95):\n        if (self._ci is not None and self._ci_cl is not None and\n                confidence_level == self._ci_cl):\n            return self._ci\n        if not 0 < confidence_level < 1:\n            raise ValueError(\"Confidence level must be between 0 and 1.\")\n        params = (confidence_level, self._ntreatments, self._df)\n        srd = distributions.studentized_range.ppf(*params)\n        confidence_radius = srd * self._stand_err\n        upper_conf = self.statistic + confidence_radius\n        lower_conf = self.statistic - confidence_radius", "documentation": "        \"\"\"Compute the confidence interval for the specified confidence level.\n\n        Parameters\n        ----------\n        confidence_level : float, optional\n            Confidence level for the computed confidence interval\n            of the estimated proportion. Default is .95.\n\n        Returns\n        -------\n        ci : ``ConfidenceInterval`` object\n            The object has attributes ``low`` and ``high`` that hold the\n            lower and upper bounds of the confidence intervals for each\n            comparison. The high and low values are accessible for each\n            comparison at index ``(i, j)`` between groups ``i`` and ``j``.\n\n        References\n        ----------\n        .. [1] NIST/SEMATECH e-Handbook of Statistical Methods, \"7.4.7.1.\n               Tukey's Method.\"\n               https://www.itl.nist.gov/div898/handbook/prc/section4/prc471.htm,\n               28 November 2020.\n        .. [2] P. A. Games and J. F. Howell, \"Pairwise Multiple Comparison Procedures\n               with Unequal N's and/or Variances: A Monte Carlo Study,\" Journal of\n               Educational Statistics, vol. 1, no. 2, pp. 113-125, Jun. 1976,\n               :doi:`10.3102/10769986001002113`.\n\n        Examples\n        --------\n        >>> from scipy.stats import tukey_hsd\n        >>> group0 = [24.5, 23.5, 26.4, 27.1, 29.9]\n        >>> group1 = [28.4, 34.2, 29.5, 32.2, 30.1]\n        >>> group2 = [26.1, 28.3, 24.3, 26.2, 27.8]\n        >>> result = tukey_hsd(group0, group1, group2)\n        >>> ci = result.confidence_interval()\n        >>> ci.low\n        array([[-3.649159, -8.249159, -3.909159],\n               [ 0.950841, -3.649159,  0.690841],\n               [-3.389159, -7.989159, -3.649159]])\n        >>> ci.high\n        array([[ 3.649159, -0.950841,  3.389159],\n               [ 8.249159,  3.649159,  7.989159],\n               [ 3.909159, -0.690841,  3.649159]])\n        \"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 1956, "code": "def tukey_hsd(*args, equal_var=True):\n    args = _tukey_hsd_iv(args, equal_var)\n    ntreatments = len(args)\n    means = np.asarray([np.mean(arg) for arg in args])\n    nsamples_treatments = np.asarray([a.size for a in args])\n    nobs = np.sum(nsamples_treatments)\n    vars_ = np.asarray([np.var(arg, ddof=1) for arg in args])\n    if equal_var:\n        mse = (np.sum(vars_ * (nsamples_treatments - 1)) / (nobs - ntreatments))\n        if np.unique(nsamples_treatments).size == 1:\n            normalize = 2 / nsamples_treatments[0]", "documentation": "    \"\"\"Perform Tukey's HSD test for equality of means over multiple treatments.\n\n    Tukey's honestly significant difference (HSD) test performs pairwise\n    comparison of means for a set of samples. Whereas ANOVA (e.g. `f_oneway`)\n    assesses whether the true means underlying each sample are identical,\n    Tukey's HSD is a post hoc test used to compare the mean of each sample\n    to the mean of each other sample.\n\n    The null hypothesis is that the distributions underlying the samples all\n    have the same mean. The test statistic, which is computed for every\n    possible pairing of samples, is simply the difference between the sample\n    means. For each pair, the p-value is the probability under the null\n    hypothesis (and other assumptions; see notes) of observing such an extreme\n    value of the statistic, considering that many pairwise comparisons are\n    being performed. Confidence intervals for the difference between each pair\n    of means are also available.\n\n    Parameters\n    ----------\n    sample1, sample2, ... : array_like\n        The sample measurements for each group. There must be at least\n        two arguments.\n    equal_var : bool, optional\n        If True (default) and equal sample size, perform Tukey-HSD test [6].\n        If True and unequal sample size, perform Tukey-Kramer test [4]_.\n        If False, perform Games-Howell test [7]_, which does not assume equal variances.\n\n    Returns\n    -------\n    result : `~scipy.stats._result_classes.TukeyHSDResult` instance\n        The return value is an object with the following attributes:\n\n        statistic : float ndarray\n            The computed statistic of the test for each comparison. The element\n            at index ``(i, j)`` is the statistic for the comparison between\n            groups ``i`` and ``j``.\n        pvalue : float ndarray\n            The computed p-value of the test for each comparison. The element\n            at index ``(i, j)`` is the p-value for the comparison between\n            groups ``i`` and ``j``.\n\n        The object has the following methods:\n\n        confidence_interval(confidence_level=0.95):\n            Compute the confidence interval for the specified confidence level.\n\n    See Also\n    --------\n    dunnett : performs comparison of means against a control group.\n\n    Notes\n    -----\n    The use of this test relies on several assumptions.\n\n    1. The observations are independent within and among groups.\n    2. The observations within each group are normally distributed.\n    3. The distributions from which the samples are drawn have the same finite\n       variance.\n\n    The original formulation of the test was for samples of equal size drawn from\n    populations assumed to have equal variances [6]_. In case of unequal sample sizes,\n    the test uses the Tukey-Kramer method [4]_. When equal variances are not assumed\n    (``equal_var=False``), the test uses the Games-Howell method [7]_.\n\n    References\n    ----------\n    .. [1] NIST/SEMATECH e-Handbook of Statistical Methods, \"7.4.7.1. Tukey's\n           Method.\"\n           https://www.itl.nist.gov/div898/handbook/prc/section4/prc471.htm,\n           28 November 2020.\n    .. [2] Abdi, Herve & Williams, Lynne. (2021). \"Tukey's Honestly Significant\n           Difference (HSD) Test.\"\n           https://personal.utdallas.edu/~herve/abdi-HSD2010-pretty.pdf\n    .. [3] \"One-Way ANOVA Using SAS PROC ANOVA & PROC GLM.\" SAS\n           Tutorials, 2007.\n           https://www.stattutorials.com/SAS/TUTORIAL-PROC-GLM.htm\n    .. [4] Kramer, Clyde Young. \"Extension of Multiple Range Tests to Group\n           Means with Unequal Numbers of Replications.\" Biometrics, vol. 12,\n           no. 3, 1956, pp. 307-310. https://www.jstor.org/stable/3001469\n    .. [5] NIST/SEMATECH e-Handbook of Statistical Methods, \"7.4.3.3.\n           The ANOVA table and tests of hypotheses about means\"\n           https://www.itl.nist.gov/div898/handbook/prc/section4/prc433.htm,\n           2 June 2021.\n    .. [6] Tukey, John W. \"Comparing Individual Means in the Analysis of\n           Variance.\" Biometrics, vol. 5, no. 2, 1949, pp. 99-114.\n           https://www.jstor.org/stable/3001913\n    .. [7] P. A. Games and J. F. Howell, \"Pairwise Multiple Comparison Procedures\n           with Unequal N's and/or Variances: A Monte Carlo Study,\" Journal of\n           Educational Statistics, vol. 1, no. 2, pp. 113-125, Jun. 1976.\n           :doi:`10.3102/10769986001002113`.\n\n\n    Examples\n    --------\n    Here are some data comparing the time to relief of three brands of\n    headache medicine, reported in minutes. Data adapted from [3]_.\n\n    >>> import numpy as np\n    >>> from scipy.stats import tukey_hsd\n    >>> group0 = [24.5, 23.5, 26.4, 27.1, 29.9]\n    >>> group1 = [28.4, 34.2, 29.5, 32.2, 30.1]\n    >>> group2 = [26.1, 28.3, 24.3, 26.2, 27.8]\n\n    We would like to see if the means between any of the groups are\n    significantly different. First, visually examine a box and whisker plot.\n\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    >>> ax.boxplot([group0, group1, group2])\n    >>> ax.set_xticklabels([\"group0\", \"group1\", \"group2\"]) # doctest: +SKIP\n    >>> ax.set_ylabel(\"mean\") # doctest: +SKIP\n    >>> plt.show()\n\n    From the box and whisker plot, we can see overlap in the interquartile\n    ranges group 1 to group 2 and group 3, but we can apply the ``tukey_hsd``\n    test to determine if the difference between means is significant. We\n    set a significance level of .05 to reject the null hypothesis.\n\n    >>> res = tukey_hsd(group0, group1, group2)\n    >>> print(res)\n    Pairwise Group Comparisons (95.0% Confidence Interval)\n    Comparison  Statistic  p-value  Lower CI  Upper CI\n     (0 - 1)     -4.600     0.014    -8.249    -0.951\n     (0 - 2)     -0.260     0.980    -3.909     3.389\n     (1 - 0)      4.600     0.014     0.951     8.249\n     (1 - 2)      4.340     0.020     0.691     7.989\n     (2 - 0)      0.260     0.980    -3.389     3.909\n     (2 - 1)     -4.340     0.020    -7.989    -0.691\n\n    The null hypothesis is that each group has the same mean. The p-value for\n    comparisons between ``group0`` and ``group1`` as well as ``group1`` and\n    ``group2`` do not exceed .05, so we reject the null hypothesis that they\n    have the same means. The p-value of the comparison between ``group0``\n    and ``group2`` exceeds .05, so we accept the null hypothesis that there\n    is not a significant difference between their means.\n\n    We can also compute the confidence interval associated with our chosen\n    confidence level.\n\n    >>> group0 = [24.5, 23.5, 26.4, 27.1, 29.9]\n    >>> group1 = [28.4, 34.2, 29.5, 32.2, 30.1]\n    >>> group2 = [26.1, 28.3, 24.3, 26.2, 27.8]\n    >>> result = tukey_hsd(group0, group1, group2)\n    >>> conf = res.confidence_interval(confidence_level=.99)\n    >>> for ((i, j), l) in np.ndenumerate(conf.low):\n    ...     # filter out self comparisons\n    ...     if i != j:\n    ...         h = conf.high[i,j]\n    ...         print(f\"({i} - {j}) {l:>6.3f} {h:>6.3f}\")\n    (0 - 1) -9.480  0.280\n    (0 - 2) -5.140  4.620\n    (1 - 0) -0.280  9.480\n    (1 - 2) -0.540  9.220\n    (2 - 0) -4.620  5.140\n    (2 - 1) -9.220  0.540\n    \"\"\""}], "after_segments": [{"filename": "scipy/stats/_hypotests.py", "start_line": 34, "code": "def epps_singleton_2samp(x, y, t=(0.4, 0.8), *, axis=0):\n    xp = array_namespace(x, y)\n    x, y = xp_promote(x, y, force_floating=True, xp=xp)\n    t = xp.asarray(t, dtype=x.dtype)\n    nx, ny = x.shape[-1], y.shape[-1]\n    if (nx < 5) or (ny < 5):  # only used by test_axis_nan_policy\n        raise ValueError('x and y should have at least 5 elements, but len(x) '\n                         f'= {nx} and len(y) = {ny}.')\n    n = nx + ny\n    if t.ndim > 1:\n        raise ValueError(f't must be 1d, but t.ndim equals {t.ndim}.')", "documentation": "    \"\"\"Compute the Epps-Singleton (ES) test statistic.\n\n    Test the null hypothesis that two samples have the same underlying\n    probability distribution.\n\n    Parameters\n    ----------\n    x, y : array-like\n        The two samples of observations to be tested. Input must not have more\n        than one dimension. Samples can have different lengths, but both\n        must have at least five observations.\n    t : array-like, optional\n        The points (t1, ..., tn) where the empirical characteristic function is\n        to be evaluated. It should be positive distinct numbers. The default\n        value (0.4, 0.8) is proposed in [1]_. Input must not have more than\n        one dimension.\n    axis : int or tuple of ints, default: 0\n        If an int or tuple of ints, the axis or axes of the input along which\n        to compute the statistic. The statistic of each axis-slice (e.g. row)\n        of the input will appear in a corresponding element of the output.\n        If ``None``, the input will be raveled before computing the statistic.\n\n    Returns\n    -------\n    statistic : float\n        The test statistic.\n    pvalue : float\n        The associated p-value based on the asymptotic chi2-distribution.\n\n    See Also\n    --------\n    ks_2samp, anderson_ksamp\n\n    Notes\n    -----\n    Testing whether two samples are generated by the same underlying\n    distribution is a classical question in statistics. A widely used test is\n    the Kolmogorov-Smirnov (KS) test which relies on the empirical\n    distribution function. Epps and Singleton introduce a test based on the\n    empirical characteristic function in [1]_.\n\n    One advantage of the ES test compared to the KS test is that is does\n    not assume a continuous distribution. In [1]_, the authors conclude\n    that the test also has a higher power than the KS test in many\n    examples. They recommend the use of the ES test for discrete samples as\n    well as continuous samples with at least 25 observations each, whereas\n    `anderson_ksamp` is recommended for smaller sample sizes in the\n    continuous case.\n\n    The p-value is computed from the asymptotic distribution of the test\n    statistic which follows a `chi2` distribution. If the sample size of both\n    `x` and `y` is below 25, the small sample correction proposed in [1]_ is\n    applied to the test statistic.\n\n    The default values of `t` are determined in [1]_ by considering\n    various distributions and finding good values that lead to a high power\n    of the test in general. Table III in [1]_ gives the optimal values for\n    the distributions tested in that study. The values of `t` are scaled by\n    the semi-interquartile range in the implementation, see [1]_.\n\n    References\n    ----------\n    .. [1] T. W. Epps and K. J. Singleton, \"An omnibus test for the two-sample\n       problem using the empirical characteristic function\", Journal of\n       Statistical Computation and Simulation 26, p. 177--203, 1986.\n\n    .. [2] S. J. Goerg and J. Kaiser, \"Nonparametric testing of distributions\n       - the Epps-Singleton two-sample test using the empirical characteristic\n       function\", The Stata Journal 9(3), p. 454--465, 2009.\n\n    \"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 395, "code": "def _psi1_mod(x, *, xp=None):\n    xp = array_namespace(x) if xp is None else xp", "documentation": "    \"\"\"\n    psi1 is defined in equation 1.10 in Cs\u00f6rg\u0151, S. and Faraway, J. (1996).\n    This implements a modified version by excluding the term V(x) / 12\n    (here: _cdf_cvm_inf(x) / 12) to avoid evaluating _cdf_cvm_inf(x)\n    twice in _cdf_cvm.\n\n    Implementation based on MAPLE code of Julian Faraway and R code of the\n    function pCvM in the package goftest (v1.1.1), permission granted\n    by Adrian Baddeley. Main difference in the implementation: the code\n    here keeps adding terms of the series until the terms are small enough.\n    \"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 456, "code": "def _cdf_cvm_inf(x, *, xp=None):\n    xp = array_namespace(x) if xp is None else xp\n    x = xp.asarray(x)", "documentation": "    \"\"\"\n    Calculate the cdf of the Cram\u00e9r-von Mises statistic (infinite sample size).\n\n    See equation 1.2 in Cs\u00f6rg\u0151, S. and Faraway, J. (1996).\n\n    Implementation based on MAPLE code of Julian Faraway and R code of the\n    function pCvM in the package goftest (v1.1.1), permission granted\n    by Adrian Baddeley. Main difference in the implementation: the code\n    here keeps adding terms of the series until the terms are small enough.\n\n    The function is not expected to be accurate for large values of x, say\n    x > 4, when the cdf is very close to 1.\n    \"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 495, "code": "def _cdf_cvm(x, n=None, *, xp=None):\n    xp = array_namespace(x) if xp is None else xp\n    x = xp.asarray(x)\n    if n is None:\n        y = _cdf_cvm_inf(x, xp=xp)\n    else:\n        y = xp.zeros_like(x, dtype=x.dtype)\n        sup = (1./(12*n) < x) & (x < n/3.)\n        y = xpx.at(y)[sup].set(_cdf_cvm_inf(x[sup], xp=xp) * (1 + 1./(12*n))\n                               + _psi1_mod(x[sup], xp=xp) / n)\n        y = xpx.at(y)[x >= n/3].set(1.)", "documentation": "    \"\"\"\n    Calculate the cdf of the Cram\u00e9r-von Mises statistic for a finite sample\n    size n. If N is None, use the asymptotic cdf (n=inf).\n\n    See equation 1.8 in Cs\u00f6rg\u0151, S. and Faraway, J. (1996) for finite samples,\n    1.2 for the asymptotic cdf.\n\n    The function is not expected to be accurate for large values of x, say\n    x > 2, when the cdf is very close to 1 and it might return values > 1\n    in that case, e.g. _cdf_cvm(2.0, 12) = 1.0000027556716846. Moreover, it\n    is not accurate for small values of n, especially close to the bounds of\n    the distribution's domain, [1/(12*n), n/3], where the value jumps to 0\n    and 1, respectively. These are limitations of the approximation by Cs\u00f6rg\u0151\n    and Faraway (1996) implemented in this function.\n    \"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 673, "code": "def _get_wilcoxon_distr(n):\n    c = np.ones(1, dtype=np.float64)\n    for k in range(1, n + 1):\n        prev_c = c\n        c = np.zeros(k * (k + 1) // 2 + 1, dtype=np.float64)\n        m = len(prev_c)\n        c[:m] = prev_c * 0.5\n        c[-m:] += prev_c * 0.5\n    return c", "documentation": "    \"\"\"\n    Distribution of probability of the Wilcoxon ranksum statistic r_plus (sum\n    of ranks of positive differences).\n    Returns an array with the probabilities of all the possible ranks\n    r = 0, ..., n*(n+1)/2\n    \"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 690, "code": "def _get_wilcoxon_distr2(n):\n    ai = np.arange(1, n+1)[:, None]\n    t = n*(n+1)/2\n    q = 2*t\n    j = np.arange(q)\n    theta = 2*np.pi/q*j\n    phi_sp = np.prod(np.cos(theta*ai), axis=0)\n    phi_s = np.exp(1j*theta*t) * phi_sp\n    p = np.real(ifft(phi_s))\n    res = np.zeros(int(t)+1)\n    res[:-1:] = p[::2]", "documentation": "    \"\"\"\n    Distribution of probability of the Wilcoxon ranksum statistic r_plus (sum\n    of ranks of positive differences).\n    Returns an array with the probabilities of all the possible ranks\n    r = 0, ..., n*(n+1)/2\n    This is a slower reference function\n    References\n    ----------\n    .. [1] 1. Harris T, Hardin JW. Exact Wilcoxon Signed-Rank and Wilcoxon\n        Mann-Whitney Ranksum Tests. The Stata Journal. 2013;13(2):337-343.\n    \"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 717, "code": "def _tau_b(A):\n    if A.shape[0] == 1 or A.shape[1] == 1:\n        return np.nan, np.nan\n    NA = A.sum()\n    PA = _P(A)\n    QA = _Q(A)\n    Sri2 = (A.sum(axis=1)**2).sum()\n    Scj2 = (A.sum(axis=0)**2).sum()\n    denominator = (NA**2 - Sri2)*(NA**2 - Scj2)\n    tau = (PA-QA)/(denominator)**0.5\n    numerator = 4*(_a_ij_Aij_Dij2(A) - (PA - QA)**2 / NA)", "documentation": "    \"\"\"Calculate Kendall's tau-b and p-value from contingency table.\"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 744, "code": "def _somers_d(A, alternative='two-sided'):\n    if A.shape[0] <= 1 or A.shape[1] <= 1:\n        return np.nan, np.nan\n    NA = A.sum()\n    NA2 = NA**2\n    PA = _P(A)\n    QA = _Q(A)\n    Sri2 = (A.sum(axis=1)**2).sum()\n    d = (PA - QA)/(NA2 - Sri2)\n    S = _a_ij_Aij_Dij2(A) - (PA-QA)**2/NA\n    with np.errstate(divide='ignore'):", "documentation": "    \"\"\"Calculate Somers' D and p-value from contingency table.\"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 963, "code": "def _all_partitions(nx, ny):\n    z = np.arange(nx+ny)\n    for c in combinations(z, nx):\n        x = np.array(c)\n        mask = np.ones(nx+ny, bool)\n        mask[x] = False\n        y = z[mask]\n        yield x, y", "documentation": "    \"\"\"\n    Partition a set of indices into two fixed-length sets in all possible ways\n\n    Partition a set of indices 0 ... nx + ny - 1 into two sets of length nx and\n    ny in all possible ways (ignoring order of elements).\n    \"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 979, "code": "def _compute_log_combinations(n):\n    gammaln_arr = gammaln(np.arange(n + 1) + 1)\n    return gammaln(n + 1) - gammaln_arr - gammaln_arr[::-1]\n@dataclass", "documentation": "    \"\"\"Compute all log combination of C(n, k).\"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 1567, "code": "def _pval_cvm_2samp_exact(s, m, n):\n    lcm = np.lcm(m, n)\n    a = lcm // m\n    b = lcm // n\n    mn = m * n\n    zeta = lcm ** 2 * (m + n) * (6 * s - mn * (4 * mn - 1)) // (6 * mn ** 2)\n    zeta_bound = lcm**2 * (m + n)  # bound elements in row 1\n    combinations = math.comb(m + n, m)  # sum of row 2\n    max_gs = max(zeta_bound, combinations)\n    dtype = np.min_scalar_type(max_gs)\n    gs = ([np.array([[0], [1]], dtype=dtype)]", "documentation": "    \"\"\"\n    Compute the exact p-value of the Cramer-von Mises two-sample test\n    for a given value s of the test statistic.\n    m and n are the sizes of the samples.\n\n    [1] Y. Xiao, A. Gordon, and A. Yakovlev, \"A C++ Program for\n        the Cram\u00e9r-Von Mises Two-Sample Test\", J. Stat. Soft.,\n        vol. 17, no. 8, pp. 1-15, Dec. 2006.\n    [2] T. W. Anderson \"On the Distribution of the Two-Sample Cramer-von Mises\n        Criterion,\" The Annals of Mathematical Statistics, Ann. Math. Statist.\n        33(3), 1148-1159, (September, 1962)\n    \"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 1807, "code": "class TukeyHSDResult:", "documentation": "    \"\"\"Result of `scipy.stats.tukey_hsd`.\n\n    Attributes\n    ----------\n    statistic : float ndarray\n        The computed statistic of the test for each comparison. The element\n        at index ``(i, j)`` is the statistic for the comparison between groups\n        ``i`` and ``j``.\n    pvalue : float ndarray\n        The associated p-value from the studentized range distribution. The\n        element at index ``(i, j)`` is the p-value for the comparison\n        between groups ``i`` and ``j``.\n\n    Notes\n    -----\n    The string representation of this object displays the most recently\n    calculated confidence interval, and if none have been previously\n    calculated, it will evaluate ``confidence_interval()``.\n\n    References\n    ----------\n    .. [1] NIST/SEMATECH e-Handbook of Statistical Methods, \"7.4.7.1. Tukey's\n           Method.\"\n           https://www.itl.nist.gov/div898/handbook/prc/section4/prc471.htm,\n           28 November 2020.\n    .. [2] P. A. Games and J. F. Howell, \"Pairwise Multiple Comparison Procedures\n           with Unequal N's and/or Variances: A Monte Carlo Study,\" Journal of\n           Educational Statistics, vol. 1, no. 2, pp. 113-125, Jun. 1976,\n           :doi:`10.3102/10769986001002113`.\n    \"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 1865, "code": "    def confidence_interval(self, confidence_level=.95):\n        if (self._ci is not None and self._ci_cl is not None and\n                confidence_level == self._ci_cl):\n            return self._ci\n        if not 0 < confidence_level < 1:\n            raise ValueError(\"Confidence level must be between 0 and 1.\")\n        params = (confidence_level, self._ntreatments, self._df)\n        srd = distributions.studentized_range.ppf(*params)\n        confidence_radius = srd * self._stand_err\n        upper_conf = self.statistic + confidence_radius\n        lower_conf = self.statistic - confidence_radius", "documentation": "        \"\"\"Compute the confidence interval for the specified confidence level.\n\n        Parameters\n        ----------\n        confidence_level : float, optional\n            Confidence level for the computed confidence interval\n            of the estimated proportion. Default is .95.\n\n        Returns\n        -------\n        ci : ``ConfidenceInterval`` object\n            The object has attributes ``low`` and ``high`` that hold the\n            lower and upper bounds of the confidence intervals for each\n            comparison. The high and low values are accessible for each\n            comparison at index ``(i, j)`` between groups ``i`` and ``j``.\n\n        References\n        ----------\n        .. [1] NIST/SEMATECH e-Handbook of Statistical Methods, \"7.4.7.1.\n               Tukey's Method.\"\n               https://www.itl.nist.gov/div898/handbook/prc/section4/prc471.htm,\n               28 November 2020.\n        .. [2] P. A. Games and J. F. Howell, \"Pairwise Multiple Comparison Procedures\n               with Unequal N's and/or Variances: A Monte Carlo Study,\" Journal of\n               Educational Statistics, vol. 1, no. 2, pp. 113-125, Jun. 1976,\n               :doi:`10.3102/10769986001002113`.\n\n        Examples\n        --------\n        >>> from scipy.stats import tukey_hsd\n        >>> group0 = [24.5, 23.5, 26.4, 27.1, 29.9]\n        >>> group1 = [28.4, 34.2, 29.5, 32.2, 30.1]\n        >>> group2 = [26.1, 28.3, 24.3, 26.2, 27.8]\n        >>> result = tukey_hsd(group0, group1, group2)\n        >>> ci = result.confidence_interval()\n        >>> ci.low\n        array([[-3.649159, -8.249159, -3.909159],\n               [ 0.950841, -3.649159,  0.690841],\n               [-3.389159, -7.989159, -3.649159]])\n        >>> ci.high\n        array([[ 3.649159, -0.950841,  3.389159],\n               [ 8.249159,  3.649159,  7.989159],\n               [ 3.909159, -0.690841,  3.649159]])\n        \"\"\""}, {"filename": "scipy/stats/_hypotests.py", "start_line": 1955, "code": "def tukey_hsd(*args, equal_var=True):\n    args = _tukey_hsd_iv(args, equal_var)\n    ntreatments = len(args)\n    means = np.asarray([np.mean(arg) for arg in args])\n    nsamples_treatments = np.asarray([a.size for a in args])\n    nobs = np.sum(nsamples_treatments)\n    vars_ = np.asarray([np.var(arg, ddof=1) for arg in args])\n    if equal_var:\n        mse = (np.sum(vars_ * (nsamples_treatments - 1)) / (nobs - ntreatments))\n        if np.unique(nsamples_treatments).size == 1:\n            normalize = 2 / nsamples_treatments[0]", "documentation": "    \"\"\"Perform Tukey's HSD test for equality of means over multiple treatments.\n\n    Tukey's honestly significant difference (HSD) test performs pairwise\n    comparison of means for a set of samples. Whereas ANOVA (e.g. `f_oneway`)\n    assesses whether the true means underlying each sample are identical,\n    Tukey's HSD is a post hoc test used to compare the mean of each sample\n    to the mean of each other sample.\n\n    The null hypothesis is that the distributions underlying the samples all\n    have the same mean. The test statistic, which is computed for every\n    possible pairing of samples, is simply the difference between the sample\n    means. For each pair, the p-value is the probability under the null\n    hypothesis (and other assumptions; see notes) of observing such an extreme\n    value of the statistic, considering that many pairwise comparisons are\n    being performed. Confidence intervals for the difference between each pair\n    of means are also available.\n\n    Parameters\n    ----------\n    sample1, sample2, ... : array_like\n        The sample measurements for each group. There must be at least\n        two arguments.\n    equal_var : bool, optional\n        If True (default) and equal sample size, perform Tukey-HSD test [6].\n        If True and unequal sample size, perform Tukey-Kramer test [4]_.\n        If False, perform Games-Howell test [7]_, which does not assume equal variances.\n\n    Returns\n    -------\n    result : `~scipy.stats._result_classes.TukeyHSDResult` instance\n        The return value is an object with the following attributes:\n\n        statistic : float ndarray\n            The computed statistic of the test for each comparison. The element\n            at index ``(i, j)`` is the statistic for the comparison between\n            groups ``i`` and ``j``.\n        pvalue : float ndarray\n            The computed p-value of the test for each comparison. The element\n            at index ``(i, j)`` is the p-value for the comparison between\n            groups ``i`` and ``j``.\n\n        The object has the following methods:\n\n        confidence_interval(confidence_level=0.95):\n            Compute the confidence interval for the specified confidence level.\n\n    See Also\n    --------\n    dunnett : performs comparison of means against a control group.\n\n    Notes\n    -----\n    The use of this test relies on several assumptions.\n\n    1. The observations are independent within and among groups.\n    2. The observations within each group are normally distributed.\n    3. The distributions from which the samples are drawn have the same finite\n       variance.\n\n    The original formulation of the test was for samples of equal size drawn from\n    populations assumed to have equal variances [6]_. In case of unequal sample sizes,\n    the test uses the Tukey-Kramer method [4]_. When equal variances are not assumed\n    (``equal_var=False``), the test uses the Games-Howell method [7]_.\n\n    References\n    ----------\n    .. [1] NIST/SEMATECH e-Handbook of Statistical Methods, \"7.4.7.1. Tukey's\n           Method.\"\n           https://www.itl.nist.gov/div898/handbook/prc/section4/prc471.htm,\n           28 November 2020.\n    .. [2] Abdi, Herve & Williams, Lynne. (2021). \"Tukey's Honestly Significant\n           Difference (HSD) Test.\"\n           https://personal.utdallas.edu/~herve/abdi-HSD2010-pretty.pdf\n    .. [3] \"One-Way ANOVA Using SAS PROC ANOVA & PROC GLM.\" SAS\n           Tutorials, 2007.\n           https://www.stattutorials.com/SAS/TUTORIAL-PROC-GLM.htm\n    .. [4] Kramer, Clyde Young. \"Extension of Multiple Range Tests to Group\n           Means with Unequal Numbers of Replications.\" Biometrics, vol. 12,\n           no. 3, 1956, pp. 307-310. https://www.jstor.org/stable/3001469\n    .. [5] NIST/SEMATECH e-Handbook of Statistical Methods, \"7.4.3.3.\n           The ANOVA table and tests of hypotheses about means\"\n           https://www.itl.nist.gov/div898/handbook/prc/section4/prc433.htm,\n           2 June 2021.\n    .. [6] Tukey, John W. \"Comparing Individual Means in the Analysis of\n           Variance.\" Biometrics, vol. 5, no. 2, 1949, pp. 99-114.\n           https://www.jstor.org/stable/3001913\n    .. [7] P. A. Games and J. F. Howell, \"Pairwise Multiple Comparison Procedures\n           with Unequal N's and/or Variances: A Monte Carlo Study,\" Journal of\n           Educational Statistics, vol. 1, no. 2, pp. 113-125, Jun. 1976.\n           :doi:`10.3102/10769986001002113`.\n\n\n    Examples\n    --------\n    Here are some data comparing the time to relief of three brands of\n    headache medicine, reported in minutes. Data adapted from [3]_.\n\n    >>> import numpy as np\n    >>> from scipy.stats import tukey_hsd\n    >>> group0 = [24.5, 23.5, 26.4, 27.1, 29.9]\n    >>> group1 = [28.4, 34.2, 29.5, 32.2, 30.1]\n    >>> group2 = [26.1, 28.3, 24.3, 26.2, 27.8]\n\n    We would like to see if the means between any of the groups are\n    significantly different. First, visually examine a box and whisker plot.\n\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    >>> ax.boxplot([group0, group1, group2])\n    >>> ax.set_xticklabels([\"group0\", \"group1\", \"group2\"]) # doctest: +SKIP\n    >>> ax.set_ylabel(\"mean\") # doctest: +SKIP\n    >>> plt.show()\n\n    From the box and whisker plot, we can see overlap in the interquartile\n    ranges group 1 to group 2 and group 3, but we can apply the ``tukey_hsd``\n    test to determine if the difference between means is significant. We\n    set a significance level of .05 to reject the null hypothesis.\n\n    >>> res = tukey_hsd(group0, group1, group2)\n    >>> print(res)\n    Pairwise Group Comparisons (95.0% Confidence Interval)\n    Comparison  Statistic  p-value  Lower CI  Upper CI\n     (0 - 1)     -4.600     0.014    -8.249    -0.951\n     (0 - 2)     -0.260     0.980    -3.909     3.389\n     (1 - 0)      4.600     0.014     0.951     8.249\n     (1 - 2)      4.340     0.020     0.691     7.989\n     (2 - 0)      0.260     0.980    -3.389     3.909\n     (2 - 1)     -4.340     0.020    -7.989    -0.691\n\n    The null hypothesis is that each group has the same mean. The p-value for\n    comparisons between ``group0`` and ``group1`` as well as ``group1`` and\n    ``group2`` do not exceed .05, so we reject the null hypothesis that they\n    have the same means. The p-value of the comparison between ``group0``\n    and ``group2`` exceeds .05, so we accept the null hypothesis that there\n    is not a significant difference between their means.\n\n    We can also compute the confidence interval associated with our chosen\n    confidence level.\n\n    >>> group0 = [24.5, 23.5, 26.4, 27.1, 29.9]\n    >>> group1 = [28.4, 34.2, 29.5, 32.2, 30.1]\n    >>> group2 = [26.1, 28.3, 24.3, 26.2, 27.8]\n    >>> result = tukey_hsd(group0, group1, group2)\n    >>> conf = res.confidence_interval(confidence_level=.99)\n    >>> for ((i, j), l) in np.ndenumerate(conf.low):\n    ...     # filter out self comparisons\n    ...     if i != j:\n    ...         h = conf.high[i,j]\n    ...         print(f\"({i} - {j}) {l:>6.3f} {h:>6.3f}\")\n    (0 - 1) -9.480  0.280\n    (0 - 2) -5.140  4.620\n    (1 - 0) -0.280  9.480\n    (1 - 2) -0.540  9.220\n    (2 - 0) -4.620  5.140\n    (2 - 1) -9.220  0.540\n    \"\"\""}]}
{"repository": "scipy/scipy", "commit_sha": "98307e9d953289d30edb1fad6799a86c46476558", "commit_message": "MAINT: stats.rankdata: consistently return floating point dtype (#24420)\n\n* MAINT: stats.rankdata: always output floating point dtype\n\n* MAINT: stats: avoid unnecessary dtype conversions\n\n* DOC: stats.rankdata: update documentation to reflect new output dtype\n\n* MAINT: stats.ks_2samp: update after change in rankdata dtype\n\n* TST: stats.mood: adjust tolerance to address failing test", "commit_date": "2026-02-03T07:12:12+00:00", "author": "Matt Haberland", "file": "scipy/stats/_morestats.py", "patch": "@@ -3095,7 +3095,6 @@ def ansari(x, y, alternative='two-sided', *, axis=0):\n     N = m + n\n     xy = xp.concat([x, y], axis=-1)  # combine\n     rank, t = _stats_py._rankdata(xy, method='average', return_ties=True)\n-    rank, t = xp.astype(rank, dtype), xp.astype(t, dtype)\n     symrank = xp.minimum(rank, N - rank + 1)\n     AB = xp.sum(symrank[..., :n], axis=-1)\n     repeats = xp.any(t > 1)  # in theory we could branch for each slice separately\n@@ -3551,7 +3550,6 @@ def func(x):\n     Xij_Xibar = [xp.abs(sample - Xibar_) for sample, Xibar_ in zip(samples, Xibar)]\n     Xij_Xibar = xp.concat(Xij_Xibar, axis=-1)\n     ranks = stats.rankdata(Xij_Xibar, method='average', axis=-1)\n-    ranks = xp.astype(ranks, dtype)\n     a_Ni = special.ndtri(ranks / (2*(N + 1.0)) + 0.5)\n \n     # [3] Equation 2.1\n@@ -3615,7 +3613,6 @@ def sum_1(a, b):\n     xy = xp.concat((x, y), axis=-1)\n     i = xp.argsort(xy, stable=True, axis=-1)\n     _, a = _stats_py._rankdata(x, method='average', return_ties=True)\n-    a = xp.astype(a, phi.dtype)\n \n     zeros = xp.zeros(a.shape[:-1] + (n,), dtype=a.dtype)\n     a = xp.concat((a, zeros), axis=-1)\n@@ -3737,7 +3734,6 @@ def mood(x, y, axis=0, alternative=\"two-sided\"):\n     \"\"\"\n     xp = array_namespace(x, y)\n     x, y = xp_promote(x, y, force_floating=True, xp=xp)\n-    dtype = x.dtype\n \n     # _axis_nan_policy decorator ensures axis=-1\n     xy = xp.concat((x, y), axis=-1)\n@@ -3753,7 +3749,6 @@ def mood(x, y, axis=0, alternative=\"two-sided\"):\n     # determine if any of the samples contain ties\n     # `a` represents ties within `x`; `t` represents ties within `xy`\n     r, t = _stats_py._rankdata(xy, method='average', return_ties=True)\n-    r, t = xp.asarray(r, dtype=dtype), xp.asarray(t, dtype=dtype)\n \n     if is_lazy_array(t) or xp.any(t > 1):\n         z = _mood_statistic_with_ties(x, y, t, m, n, N, xp=xp)", "before_segments": [{"filename": "scipy/stats/_morestats.py", "start_line": 163, "code": "def mvsdist(data):\n    x = ravel(data)\n    n = len(x)\n    if n < 2:\n        raise ValueError(\"Need at least 2 data-points.\")\n    xbar = x.mean()\n    C = x.var()\n    if n > 1000:  # gaussian approximations for large n\n        mdist = distributions.norm(loc=xbar, scale=math.sqrt(C / n))\n        sdist = distributions.norm(loc=math.sqrt(C), scale=math.sqrt(C / (2. * n)))\n        vdist = distributions.norm(loc=C, scale=math.sqrt(2.0 / n) * C)", "documentation": "    \"\"\"\n    'Frozen' distributions for mean, variance, and standard deviation of data.\n\n    Parameters\n    ----------\n    data : array_like\n        Input array. Converted to 1-D using ravel.\n        Requires 2 or more data-points.\n\n    Returns\n    -------\n    mdist : \"frozen\" distribution object\n        Distribution object representing the mean of the data.\n    vdist : \"frozen\" distribution object\n        Distribution object representing the variance of the data.\n    sdist : \"frozen\" distribution object\n        Distribution object representing the standard deviation of the data.\n\n    See Also\n    --------\n    bayes_mvs\n\n    Notes\n    -----\n    The return values from ``bayes_mvs(data)`` is equivalent to\n    ``tuple((x.mean(), x.interval(0.90)) for x in mvsdist(data))``.\n\n    In other words, calling ``<dist>.mean()`` and ``<dist>.interval(0.90)``\n    on the three distribution objects returned from this function will give\n    the same results that are returned from `bayes_mvs`.\n\n    References\n    ----------\n    T.E. Oliphant, \"A Bayesian perspective on estimating mean, variance, and\n    standard-deviation from data\", https://scholarsarchive.byu.edu/facpub/278,\n    2006.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> data = [6, 9, 12, 7, 8, 8, 13]\n    >>> mean, var, std = stats.mvsdist(data)\n\n    We now have frozen distribution objects \"mean\", \"var\" and \"std\" that we can\n    examine:\n\n    >>> mean.mean()\n    9.0\n    >>> mean.interval(0.95)\n    (6.6120585482655692, 11.387941451734431)\n    >>> mean.std()\n    1.1952286093343936\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 406, "code": "def _calc_uniform_order_statistic_medians(n):\n    v = np.empty(n, dtype=np.float64)\n    v[-1] = 0.5**(1.0 / n)\n    v[0] = 1 - v[-1]\n    i = np.arange(2, n)\n    v[1:-1] = (i - 0.3175) / (n + 0.365)\n    return v", "documentation": "    \"\"\"Approximations of uniform order statistic medians.\n\n    Parameters\n    ----------\n    n : int\n        Sample size.\n\n    Returns\n    -------\n    v : 1d float array\n        Approximations of the order statistic medians.\n\n    References\n    ----------\n    .. [1] James J. Filliben, \"The Probability Plot Correlation Coefficient\n           Test for Normality\", Technometrics, Vol. 17, pp. 111-117, 1975.\n\n    Examples\n    --------\n    Order statistics of the uniform distribution on the unit interval\n    are marginally distributed according to beta distributions.\n    The expectations of these order statistic are evenly spaced across\n    the interval, but the distributions are skewed in a way that\n    pushes the medians slightly towards the endpoints of the unit interval:\n\n    >>> import numpy as np\n    >>> n = 4\n    >>> k = np.arange(1, n+1)\n    >>> from scipy.stats import beta\n    >>> a = k\n    >>> b = n-k+1\n    >>> beta.mean(a, b)\n    array([0.2, 0.4, 0.6, 0.8])\n    >>> beta.median(a, b)\n    array([0.15910358, 0.38572757, 0.61427243, 0.84089642])\n\n    The Filliben approximation uses the exact medians of the smallest\n    and greatest order statistics, and the remaining medians are approximated\n    by points spread evenly across a sub-interval of the unit interval:\n\n    >>> from scipy.stats._morestats import _calc_uniform_order_statistic_medians\n    >>> _calc_uniform_order_statistic_medians(n)\n    array([0.15910358, 0.38545246, 0.61454754, 0.84089642])\n\n    This plot shows the skewed distributions of the order statistics\n    of a sample of size four from a uniform distribution on the unit interval:\n\n    >>> import matplotlib.pyplot as plt\n    >>> x = np.linspace(0.0, 1.0, num=50, endpoint=True)\n    >>> pdfs = [beta.pdf(x, a[i], b[i]) for i in range(n)]\n    >>> plt.figure()\n    >>> plt.plot(x, pdfs[0], x, pdfs[1], x, pdfs[2], x, pdfs[3])\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 469, "code": "def _parse_dist_kw(dist, enforce_subclass=True):\n    if isinstance(dist, rv_generic):\n        pass\n    elif isinstance(dist, str):\n        try:\n            dist = getattr(distributions, dist)\n        except AttributeError as e:\n            raise ValueError(f\"{dist} is not a valid distribution name\") from e\n    elif enforce_subclass:\n        msg = (\"`dist` should be a stats.distributions instance or a string \"\n               \"with the name of such a distribution.\")", "documentation": "    \"\"\"Parse `dist` keyword.\n\n    Parameters\n    ----------\n    dist : str or stats.distributions instance.\n        Several functions take `dist` as a keyword, hence this utility\n        function.\n    enforce_subclass : bool, optional\n        If True (default), `dist` needs to be a\n        `_distn_infrastructure.rv_generic` instance.\n        It can sometimes be useful to set this keyword to False, if a function\n        wants to accept objects that just look somewhat like such an instance\n        (for example, they have a ``ppf`` method).\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 500, "code": "def _add_axis_labels_title(plot, xlabel, ylabel, title):\n    try:\n        if hasattr(plot, 'set_title'):\n            plot.set_title(title)\n            plot.set_xlabel(xlabel)\n            plot.set_ylabel(ylabel)\n        else:\n            plot.title(title)\n            plot.xlabel(xlabel)\n            plot.ylabel(ylabel)\n    except Exception:", "documentation": "    \"\"\"Helper function to add axes labels and a title to stats plots.\"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 520, "code": "def probplot(x, sparams=(), dist='norm', fit=True, plot=None, rvalue=False):\n    x = np.asarray(x)\n    if x.size == 0:\n        if fit:\n            return (x, x), (np.nan, np.nan, 0.0)\n        else:\n            return x, x\n    osm_uniform = _calc_uniform_order_statistic_medians(len(x))\n    dist = _parse_dist_kw(dist, enforce_subclass=False)\n    if sparams is None:\n        sparams = ()", "documentation": "    \"\"\"\n    Calculate quantiles for a probability plot, and optionally show the plot.\n\n    Generates a probability plot of sample data against the quantiles of a\n    specified theoretical distribution (the normal distribution by default).\n    `probplot` optionally calculates a best-fit line for the data and plots the\n    results using Matplotlib or a given plot function.\n\n    Parameters\n    ----------\n    x : array_like\n        Sample/response data from which `probplot` creates the plot.\n    sparams : tuple, optional\n        Distribution-specific shape parameters (shape parameters plus location\n        and scale).\n    dist : str or stats.distributions instance, optional\n        Distribution or distribution function name. The default is 'norm' for a\n        normal probability plot.  Objects that look enough like a\n        stats.distributions instance (i.e. they have a ``ppf`` method) are also\n        accepted.\n    fit : bool, optional\n        Fit a least-squares regression (best-fit) line to the sample data if\n        True (default).\n    plot : object, optional\n        If given, plots the quantiles.\n        If given and `fit` is True, also plots the least squares fit.\n        `plot` is an object that has to have methods \"plot\" and \"text\".\n        The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,\n        or a custom object with the same methods.\n        Default is None, which means that no plot is created.\n    rvalue : bool, optional\n        If `plot` is provided and `fit` is True, setting `rvalue` to True\n        includes the coefficient of determination on the plot.\n        Default is False.\n\n    Returns\n    -------\n    (osm, osr) : tuple of ndarrays\n        Tuple of theoretical quantiles (osm, or order statistic medians) and\n        ordered responses (osr).  `osr` is simply sorted input `x`.\n        For details on how `osm` is calculated see the Notes section.\n    (slope, intercept, r) : tuple of floats, optional\n        Tuple  containing the result of the least-squares fit, if that is\n        performed by `probplot`. `r` is the square root of the coefficient of\n        determination.  If ``fit=False`` and ``plot=None``, this tuple is not\n        returned.\n\n    Notes\n    -----\n    Even if `plot` is given, the figure is not shown or saved by `probplot`;\n    ``plt.show()`` or ``plt.savefig('figname.png')`` should be used after\n    calling `probplot`.\n\n    `probplot` generates a probability plot, which should not be confused with\n    a Q-Q or a P-P plot.  Statsmodels has more extensive functionality of this\n    type, see ``statsmodels.api.ProbPlot``.\n\n    The formula used for the theoretical quantiles (horizontal axis of the\n    probability plot) is Filliben's estimate::\n\n        quantiles = dist.ppf(val), for\n\n                0.5**(1/n),                  for i = n\n          val = (i - 0.3175) / (n + 0.365),  for i = 2, ..., n-1\n                1 - 0.5**(1/n),              for i = 1\n\n    where ``i`` indicates the i-th ordered value and ``n`` is the total number\n    of values.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n    >>> nsample = 100\n    >>> rng = np.random.default_rng()\n\n    A t distribution with small degrees of freedom:\n\n    >>> ax1 = plt.subplot(221)\n    >>> x = stats.t.rvs(3, size=nsample, random_state=rng)\n    >>> res = stats.probplot(x, plot=plt)\n\n    A t distribution with larger degrees of freedom:\n\n    >>> ax2 = plt.subplot(222)\n    >>> x = stats.t.rvs(25, size=nsample, random_state=rng)\n    >>> res = stats.probplot(x, plot=plt)\n\n    A mixture of two normal distributions with broadcasting:\n\n    >>> ax3 = plt.subplot(223)\n    >>> x = stats.norm.rvs(loc=[0,5], scale=[1,1.5],\n    ...                    size=(nsample//2,2), random_state=rng).ravel()\n    >>> res = stats.probplot(x, plot=plt)\n\n    A standard normal distribution:\n\n    >>> ax4 = plt.subplot(224)\n    >>> x = stats.norm.rvs(loc=0, scale=1, size=nsample, random_state=rng)\n    >>> res = stats.probplot(x, plot=plt)\n\n    Produce a new figure with a loggamma distribution, using the ``dist`` and\n    ``sparams`` keywords:\n\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111)\n    >>> x = stats.loggamma.rvs(c=2.5, size=500, random_state=rng)\n    >>> res = stats.probplot(x, dist=stats.loggamma, sparams=(2.5,), plot=ax)\n    >>> ax.set_title(\"Probplot for loggamma dist with shape parameter 2.5\")\n\n    Show the results with Matplotlib:\n\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 684, "code": "def ppcc_max(x, brack=(0.0, 1.0), dist='tukeylambda'):\n    dist = _parse_dist_kw(dist)\n    osm_uniform = _calc_uniform_order_statistic_medians(len(x))\n    osr = sort(x)", "documentation": "    \"\"\"Calculate the shape parameter that maximizes the PPCC.\n\n    The probability plot correlation coefficient (PPCC) plot can be used\n    to determine the optimal shape parameter for a one-parameter family\n    of distributions. ``ppcc_max`` returns the shape parameter that would\n    maximize the probability plot correlation coefficient for the given\n    data to a one-parameter family of distributions.\n\n    Parameters\n    ----------\n    x : array_like\n        Input array.\n    brack : tuple, optional\n        Triple (a,b,c) where (a<b<c). If bracket consists of two numbers (a, c)\n        then they are assumed to be a starting interval for a downhill bracket\n        search (see `scipy.optimize.brent`).\n    dist : str or stats.distributions instance, optional\n        Distribution or distribution function name.  Objects that look enough\n        like a stats.distributions instance (i.e. they have a ``ppf`` method)\n        are also accepted.  The default is ``'tukeylambda'``.\n\n    Returns\n    -------\n    shape_value : float\n        The shape parameter at which the probability plot correlation\n        coefficient reaches its max value.\n\n    See Also\n    --------\n    ppcc_plot, probplot, boxcox\n\n    Notes\n    -----\n    The brack keyword serves as a starting point which is useful in corner\n    cases. One can use a plot to obtain a rough visual estimate of the location\n    for the maximum to start the search near it.\n\n    References\n    ----------\n    .. [1] J.J. Filliben, \"The Probability Plot Correlation Coefficient Test\n           for Normality\", Technometrics, Vol. 17, pp. 111-117, 1975.\n    .. [2] Engineering Statistics Handbook, NIST/SEMATEC,\n           https://www.itl.nist.gov/div898/handbook/eda/section3/ppccplot.htm\n\n    Examples\n    --------\n    First we generate some random data from a Weibull distribution\n    with shape parameter 2.5:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n    >>> rng = np.random.default_rng()\n    >>> c = 2.5\n    >>> x = stats.weibull_min.rvs(c, scale=4, size=2000, random_state=rng)\n\n    Generate the PPCC plot for this data with the Weibull distribution.\n\n    >>> fig, ax = plt.subplots(figsize=(8, 6))\n    >>> res = stats.ppcc_plot(x, c/2, 2*c, dist='weibull_min', plot=ax)\n\n    We calculate the value where the shape should reach its maximum and a\n    red line is drawn there. The line should coincide with the highest\n    point in the PPCC graph.\n\n    >>> cmax = stats.ppcc_max(x, brack=(c/2, 2*c), dist='weibull_min')\n    >>> ax.axvline(cmax, color='r')\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 773, "code": "def ppcc_plot(x, a, b, dist='tukeylambda', plot=None, N=80):\n    if b <= a:\n        raise ValueError(\"`b` has to be larger than `a`.\")\n    svals = np.linspace(a, b, num=N)\n    ppcc = np.empty_like(svals)\n    for k, sval in enumerate(svals):\n        _, r2 = probplot(x, sval, dist=dist, fit=True)\n        ppcc[k] = r2[-1]\n    if plot is not None:\n        plot.plot(svals, ppcc, 'x')\n        _add_axis_labels_title(plot, xlabel='Shape Values',", "documentation": "    \"\"\"Calculate and optionally plot probability plot correlation coefficient.\n\n    The probability plot correlation coefficient (PPCC) plot can be used to\n    determine the optimal shape parameter for a one-parameter family of\n    distributions.  It cannot be used for distributions without shape\n    parameters\n    (like the normal distribution) or with multiple shape parameters.\n\n    By default a Tukey-Lambda distribution (`stats.tukeylambda`) is used. A\n    Tukey-Lambda PPCC plot interpolates from long-tailed to short-tailed\n    distributions via an approximately normal one, and is therefore\n    particularly useful in practice.\n\n    Parameters\n    ----------\n    x : array_like\n        Input array.\n    a, b : scalar\n        Lower and upper bounds of the shape parameter to use.\n    dist : str or stats.distributions instance, optional\n        Distribution or distribution function name.  Objects that look enough\n        like a stats.distributions instance (i.e. they have a ``ppf`` method)\n        are also accepted.  The default is ``'tukeylambda'``.\n    plot : object, optional\n        If given, plots PPCC against the shape parameter.\n        `plot` is an object that has to have methods \"plot\" and \"text\".\n        The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,\n        or a custom object with the same methods.\n        Default is None, which means that no plot is created.\n    N : int, optional\n        Number of points on the horizontal axis (equally distributed from\n        `a` to `b`).\n\n    Returns\n    -------\n    svals : ndarray\n        The shape values for which `ppcc` was calculated.\n    ppcc : ndarray\n        The calculated probability plot correlation coefficient values.\n\n    See Also\n    --------\n    ppcc_max, probplot, boxcox_normplot, tukeylambda\n\n    References\n    ----------\n    J.J. Filliben, \"The Probability Plot Correlation Coefficient Test for\n    Normality\", Technometrics, Vol. 17, pp. 111-117, 1975.\n\n    Examples\n    --------\n    First we generate some random data from a Weibull distribution\n    with shape parameter 2.5, and plot the histogram of the data:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n    >>> rng = np.random.default_rng()\n    >>> c = 2.5\n    >>> x = stats.weibull_min.rvs(c, scale=4, size=2000, random_state=rng)\n\n    Take a look at the histogram of the data.\n\n    >>> fig1, ax = plt.subplots(figsize=(9, 4))\n    >>> ax.hist(x, bins=50)\n    >>> ax.set_title('Histogram of x')\n    >>> plt.show()\n\n    Now we explore this data with a PPCC plot as well as the related\n    probability plot and Box-Cox normplot.  A red line is drawn where we\n    expect the PPCC value to be maximal (at the shape parameter ``c``\n    used above):\n\n    >>> fig2 = plt.figure(figsize=(12, 4))\n    >>> ax1 = fig2.add_subplot(1, 3, 1)\n    >>> ax2 = fig2.add_subplot(1, 3, 2)\n    >>> ax3 = fig2.add_subplot(1, 3, 3)\n    >>> res = stats.probplot(x, plot=ax1)\n    >>> res = stats.boxcox_normplot(x, -4, 4, plot=ax2)\n    >>> res = stats.ppcc_plot(x, c/2, 2*c, dist='weibull_min', plot=ax3)\n    >>> ax3.axvline(c, color='r')\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 1486, "code": "def _normplot(method, x, la, lb, plot=None, N=80):\n    if method == 'boxcox':\n        title = 'Box-Cox Normality Plot'\n        transform_func = boxcox\n    else:\n        title = 'Yeo-Johnson Normality Plot'\n        transform_func = yeojohnson\n    x = np.asarray(x)\n    if x.size == 0:\n        return x\n    if lb <= la:", "documentation": "    \"\"\"Compute parameters for a Box-Cox or Yeo-Johnson normality plot,\n    optionally show it.\n\n    See `boxcox_normplot` or `yeojohnson_normplot` for details.\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 1529, "code": "def boxcox_normplot(x, la, lb, plot=None, N=80):\n    return _normplot('boxcox', x, la, lb, plot, N)\n@xp_capabilities(np_only=True)", "documentation": "    \"\"\"Compute parameters for a Box-Cox normality plot, optionally show it.\n\n    A Box-Cox normality plot shows graphically what the best transformation\n    parameter is to use in `boxcox` to obtain a distribution that is close\n    to normal.\n\n    Parameters\n    ----------\n    x : array_like\n        Input array.\n    la, lb : scalar\n        The lower and upper bounds for the ``lmbda`` values to pass to `boxcox`\n        for Box-Cox transformations.  These are also the limits of the\n        horizontal axis of the plot if that is generated.\n    plot : object, optional\n        If given, plots the quantiles and least squares fit.\n        `plot` is an object that has to have methods \"plot\" and \"text\".\n        The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,\n        or a custom object with the same methods.\n        Default is None, which means that no plot is created.\n    N : int, optional\n        Number of points on the horizontal axis (equally distributed from\n        `la` to `lb`).\n\n    Returns\n    -------\n    lmbdas : ndarray\n        The ``lmbda`` values for which a Box-Cox transform was done.\n    ppcc : ndarray\n        Probability Plot Correlation Coefficient, as obtained from `probplot`\n        when fitting the Box-Cox transformed input `x` against a normal\n        distribution.\n\n    See Also\n    --------\n    probplot, boxcox, boxcox_normmax, boxcox_llf, ppcc_max\n\n    Notes\n    -----\n    Even if `plot` is given, the figure is not shown or saved by\n    `boxcox_normplot`; ``plt.show()`` or ``plt.savefig('figname.png')``\n    should be used after calling `probplot`.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n\n    Generate some non-normally distributed data, and create a Box-Cox plot:\n\n    >>> x = stats.loggamma.rvs(5, size=500) + 5\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111)\n    >>> prob = stats.boxcox_normplot(x, -20, 20, plot=ax)\n\n    Determine and plot the optimal ``lmbda`` to transform ``x`` and plot it in\n    the same plot:\n\n    >>> _, maxlog = stats.boxcox(x)\n    >>> ax.axvline(maxlog, color='r')\n\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 1709, "code": "def _yeojohnson_transform(x, lmbda, xp=None):\n    xp = array_namespace(x) if xp is None else xp\n    dtype = xp_result_type(x, lmbda, force_floating=True, xp=xp)\n    eps = xp.finfo(dtype).eps\n    out = xp.zeros_like(x, dtype=dtype)\n    pos = x >= 0  # binary mask\n    if is_jax(xp):\n        return xp.select(\n            [(abs(lmbda) < eps) & pos, (abs(lmbda - 2) < eps) & ~pos, pos],\n            [xp.log1p(x), -xp.log1p(-x), xp.expm1(lmbda * xp.log1p(x)) / lmbda],\n            -xp.expm1((2 - lmbda) * xp.log1p(-x)) / (2 - lmbda),", "documentation": "    \"\"\"Returns `x` transformed by the Yeo-Johnson power transform with given\n    parameter `lmbda`.\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 1905, "code": "def yeojohnson_normmax(x, brack=None, *, nan_policy='propagate'):", "documentation": "    \"\"\"Compute optimal Yeo-Johnson transform parameter.\n\n    Compute optimal Yeo-Johnson transform parameter for input data, using\n    maximum likelihood estimation.\n\n    Parameters\n    ----------\n    x : array_like\n        Input array.\n    brack : 2-tuple, optional\n        The starting interval for a downhill bracket search with\n        `optimize.brent`. Note that this is in most cases not critical; the\n        final result is allowed to be outside this bracket. If None,\n        `optimize.fminbound` is used with bounds that avoid overflow.\n    nan_policy : {'propagate', 'omit', 'raise'}\n        Defines how to handle input NaNs.\n\n        - ``propagate``: if a NaN is present in the input, the output will be NaN.\n        - ``omit``: NaNs will be omitted when performing the calculation.\n        - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n\n    Returns\n    -------\n    maxlog : float\n        The optimal transform parameter found.\n\n    See Also\n    --------\n    yeojohnson, yeojohnson_llf, yeojohnson_normplot\n\n    Notes\n    -----\n    .. versionadded:: 1.2.0\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n\n    Generate some data and determine optimal ``lmbda``\n\n    >>> rng = np.random.default_rng()\n    >>> x = stats.loggamma.rvs(5, size=30, random_state=rng) + 5\n    >>> lmax = stats.yeojohnson_normmax(x)\n\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111)\n    >>> prob = stats.yeojohnson_normplot(x, -10, 10, plot=ax)\n    >>> ax.axvline(lmax, color='r')\n\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 2009, "code": "def yeojohnson_normplot(x, la, lb, plot=None, N=80):\n    return _normplot('yeojohnson', x, la, lb, plot, N)\nShapiroResult = namedtuple('ShapiroResult', ('statistic', 'pvalue'))\n@xp_capabilities(np_only=True)\n@_axis_nan_policy_factory(ShapiroResult, n_samples=1, too_small=2, default_axis=None)", "documentation": "    \"\"\"Compute parameters for a Yeo-Johnson normality plot, optionally show it.\n\n    A Yeo-Johnson normality plot shows graphically what the best\n    transformation parameter is to use in `yeojohnson` to obtain a\n    distribution that is close to normal.\n\n    Parameters\n    ----------\n    x : array_like\n        Input array.\n    la, lb : scalar\n        The lower and upper bounds for the ``lmbda`` values to pass to\n        `yeojohnson` for Yeo-Johnson transformations. These are also the\n        limits of the horizontal axis of the plot if that is generated.\n    plot : object, optional\n        If given, plots the quantiles and least squares fit.\n        `plot` is an object that has to have methods \"plot\" and \"text\".\n        The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,\n        or a custom object with the same methods.\n        Default is None, which means that no plot is created.\n    N : int, optional\n        Number of points on the horizontal axis (equally distributed from\n        `la` to `lb`).\n\n    Returns\n    -------\n    lmbdas : ndarray\n        The ``lmbda`` values for which a Yeo-Johnson transform was done.\n    ppcc : ndarray\n        Probability Plot Correlation Coefficient, as obtained from `probplot`\n        when fitting the Box-Cox transformed input `x` against a normal\n        distribution.\n\n    See Also\n    --------\n    probplot, yeojohnson, yeojohnson_normmax, yeojohnson_llf, ppcc_max\n\n    Notes\n    -----\n    Even if `plot` is given, the figure is not shown or saved by\n    `boxcox_normplot`; ``plt.show()`` or ``plt.savefig('figname.png')``\n    should be used after calling `probplot`.\n\n    .. versionadded:: 1.2.0\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n\n    Generate some non-normally distributed data, and create a Yeo-Johnson plot:\n\n    >>> x = stats.loggamma.rvs(5, size=500) + 5\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111)\n    >>> prob = stats.yeojohnson_normplot(x, -20, 20, plot=ax)\n\n    Determine and plot the optimal ``lmbda`` to transform ``x`` and plot it in\n    the same plot:\n\n    >>> _, maxlog = stats.yeojohnson(x)\n    >>> ax.axvline(maxlog, color='r')\n\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 2287, "code": "def anderson(x, dist='norm', *, method=None):\n    dist = dist.lower()\n    if dist in {'extreme1', 'gumbel'}:\n        dist = 'gumbel_l'\n    dists = {'norm', 'expon', 'gumbel_l',\n             'gumbel_r', 'logistic', 'weibull_min'}\n    if dist not in dists:\n        raise ValueError(f\"Invalid distribution; dist must be in {dists}.\")\n    y = sort(x)\n    xbar = np.mean(x, axis=0)\n    N = len(y)", "documentation": "    \"\"\"Anderson-Darling test for data coming from a particular distribution.\n\n    The Anderson-Darling test tests the null hypothesis that a sample is\n    drawn from a population that follows a particular distribution.\n    For the Anderson-Darling test, the critical values depend on\n    which distribution is being tested against.  This function works\n    for normal, exponential, logistic, weibull_min, or Gumbel (Extreme Value\n    Type I) distributions.\n\n    Parameters\n    ----------\n    x : array_like\n        Array of sample data.\n    dist : {'norm', 'expon', 'logistic', 'gumbel', 'gumbel_l', 'gumbel_r', 'extreme1', 'weibull_min'}, optional\n        The type of distribution to test against.  The default is 'norm'.\n        The names 'extreme1', 'gumbel_l' and 'gumbel' are synonyms for the\n        same distribution.\n    method : str or instance of `MonteCarloMethod`\n        Defines the method used to compute the p-value.\n        If `method` is ``\"interpolated\"``, the p-value is interpolated from\n        pre-calculated tables.\n        If `method` is an instance of `MonteCarloMethod`, the p-value is computed using\n        `scipy.stats.monte_carlo_test` with the provided configuration options and other\n        appropriate settings.\n\n        .. versionadded:: 1.17.0\n            If `method` is not specified, `anderson` will emit a ``FutureWarning``\n            specifying that the user must opt into a p-value calculation method.\n            When `method` is specified, the object returned will include a ``pvalue``\n            attribute, but no ``critical_value``, ``significance_level``, or\n            ``fit_result`` attributes. Beginning in 1.19.0, these other attributes will\n            no longer be available, and a p-value will always be computed according to\n            one of the available `method` options.\n\n    Returns\n    -------\n    result : AndersonResult\n        If `method` is provided, this is an object with the following attributes:\n\n        statistic : float\n            The Anderson-Darling test statistic.\n        pvalue: float\n            The p-value corresponding with the test statistic, calculated according to\n            the specified `method`.\n\n        If `method` is unspecified, this is an object with the following attributes:\n\n        statistic : float\n            The Anderson-Darling test statistic.\n        critical_values : list\n            The critical values for this distribution.\n        significance_level : list\n            The significance levels for the corresponding critical values\n            in percents.  The function returns critical values for a\n            differing set of significance levels depending on the\n            distribution that is being tested against.\n        fit_result : `~scipy.stats._result_classes.FitResult`\n            An object containing the results of fitting the distribution to\n            the data.\n\n        .. deprecated:: 1.17.0\n            The tuple-unpacking behavior of the return object and attributes\n            ``critical_values``, ``significance_level``, and ``fit_result`` are\n            deprecated. Beginning in SciPy 1.19.0, these features will no longer be\n            available, and the object returned will have attributes ``statistic`` and\n            ``pvalue``.\n\n    See Also\n    --------\n    kstest : The Kolmogorov-Smirnov test for goodness-of-fit.\n\n    Notes\n    -----\n    Critical values provided when `method` is unspecified are for the following\n    significance levels:\n\n    normal/exponential\n        15%, 10%, 5%, 2.5%, 1%\n    logistic\n        25%, 10%, 5%, 2.5%, 1%, 0.5%\n    gumbel_l / gumbel_r\n        25%, 10%, 5%, 2.5%, 1%\n    weibull_min\n        50%, 25%, 15%, 10%, 5%, 2.5%, 1%, 0.5%\n\n    If the returned statistic is larger than these critical values then\n    for the corresponding significance level, the null hypothesis that\n    the data come from the chosen distribution can be rejected.\n    The returned statistic is referred to as 'A2' in the references.\n\n    For `weibull_min`, maximum likelihood estimation is known to be\n    challenging. If the test returns successfully, then the first order\n    conditions for a maximum likelihood estimate have been verified and\n    the critical values correspond relatively well to the significance levels,\n    provided that the sample is sufficiently large (>10 observations [7]).\n    However, for some data - especially data with no left tail - `anderson`\n    is likely to result in an error message. In this case, consider\n    performing a custom goodness of fit test using\n    `scipy.stats.monte_carlo_test`.\n\n    References\n    ----------\n    .. [1] https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm\n    .. [2] Stephens, M. A. (1974). EDF Statistics for Goodness of Fit and\n           Some Comparisons, Journal of the American Statistical Association,\n           Vol. 69, pp. 730-737.\n    .. [3] Stephens, M. A. (1976). Asymptotic Results for Goodness-of-Fit\n           Statistics with Unknown Parameters, Annals of Statistics, Vol. 4,\n           pp. 357-369.\n    .. [4] Stephens, M. A. (1977). Goodness of Fit for the Extreme Value\n           Distribution, Biometrika, Vol. 64, pp. 583-588.\n    .. [5] Stephens, M. A. (1977). Goodness of Fit with Special Reference\n           to Tests for Exponentiality , Technical Report No. 262,\n           Department of Statistics, Stanford University, Stanford, CA.\n    .. [6] Stephens, M. A. (1979). Tests of Fit for the Logistic Distribution\n           Based on the Empirical Distribution Function, Biometrika, Vol. 66,\n           pp. 591-595.\n    .. [7] Richard A. Lockhart and Michael A. Stephens \"Estimation and Tests of\n           Fit for the Three-Parameter Weibull Distribution\"\n           Journal of the Royal Statistical Society.Series B(Methodological)\n           Vol. 56, No. 3 (1994), pp. 491-500, Table 0.\n    .. [8] D'Agostino, Ralph B. (1986). \"Tests for the Normal Distribution\".\n           In: Goodness-of-Fit Techniques. Ed. by Ralph B. D'Agostino and\n           Michael A. Stephens. New York: Marcel Dekker, pp. 122-141. ISBN:\n           0-8247-7487-6.\n\n    Examples\n    --------\n    Test the null hypothesis that a random sample was drawn from a normal\n    distribution (with unspecified mean and standard deviation).\n\n    >>> import numpy as np\n    >>> from scipy.stats import anderson\n    >>> rng = np.random.default_rng(9781234521)\n    >>> data = rng.random(size=35)\n    >>> res = anderson(data, dist='norm', method='interpolate')\n    >>> res.statistic\n    np.float64(0.9887620209957291)\n    >>> res.pvalue\n    np.float64(0.012111200538380142)\n\n    The p-value is approximately 0.012,, so the null hypothesis may be rejected\n    at a significance level of 2.5%, but not at a significance level of 1%.\n\n    \"\"\" # numpy/numpydoc#87  # noqa: E501"}, {"filename": "scipy/stats/_morestats.py", "start_line": 2555, "code": "def _anderson_ksamp_continuous(samples, Z, Zstar, k, n, N):\n    A2kN = 0.\n    j = np.arange(1, N)\n    for i in arange(0, k):\n        s = np.sort(samples[i])\n        Mij = s.searchsorted(Z[:-1], side='right')\n        inner = (N*Mij - j*n[i])**2 / (j * (N - j))\n        A2kN += inner.sum() / n[i]\n    return A2kN / N", "documentation": "    \"\"\"Compute A2akN equation 3 of Scholz & Stephens.\n\n    Parameters\n    ----------\n    samples : sequence of 1-D array_like\n        Array of sample arrays.\n    Z : array_like\n        Sorted array of all observations.\n    Zstar : array_like\n        Sorted array of unique observations. Unused.\n    k : int\n        Number of samples.\n    n : array_like\n        Number of observations in each sample.\n    N : int\n        Total number of observations.\n\n    Returns\n    -------\n    A2KN : float\n        The A2KN statistics of Scholz and Stephens 1987.\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 2590, "code": "def _anderson_ksamp_midrank(samples, Z, Zstar, k, n, N):\n    A2akN = 0.\n    Z_ssorted_left = Z.searchsorted(Zstar, 'left')\n    if N == Zstar.size:\n        lj = 1.\n    else:\n        lj = Z.searchsorted(Zstar, 'right') - Z_ssorted_left\n    Bj = Z_ssorted_left + lj / 2.\n    for i in arange(0, k):\n        s = np.sort(samples[i])\n        s_ssorted_right = s.searchsorted(Zstar, side='right')", "documentation": "    \"\"\"Compute A2akN equation 7 of Scholz and Stephens.\n\n    Parameters\n    ----------\n    samples : sequence of 1-D array_like\n        Array of sample arrays.\n    Z : array_like\n        Sorted array of all observations.\n    Zstar : array_like\n        Sorted array of unique observations.\n    k : int\n        Number of samples.\n    n : array_like\n        Number of observations in each sample.\n    N : int\n        Total number of observations.\n\n    Returns\n    -------\n    A2aKN : float\n        The A2aKN statistics of Scholz and Stephens 1987.\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 2633, "code": "def _anderson_ksamp_right(samples, Z, Zstar, k, n, N):\n    A2kN = 0.\n    lj = Z.searchsorted(Zstar[:-1], 'right') - Z.searchsorted(Zstar[:-1],\n                                                              'left')\n    Bj = lj.cumsum()\n    for i in arange(0, k):\n        s = np.sort(samples[i])\n        Mij = s.searchsorted(Zstar[:-1], side='right')\n        inner = lj / float(N) * (N * Mij - Bj * n[i])**2 / (Bj * (N - Bj))\n        A2kN += inner.sum() / n[i]\n    return A2kN", "documentation": "    \"\"\"Compute A2akN equation 6 of Scholz & Stephens.\n\n    Parameters\n    ----------\n    samples : sequence of 1-D array_like\n        Array of sample arrays.\n    Z : array_like\n        Sorted array of all observations.\n    Zstar : array_like\n        Sorted array of unique observations.\n    k : int\n        Number of samples.\n    n : array_like\n        Number of observations in each sample.\n    N : int\n        Total number of observations.\n\n    Returns\n    -------\n    A2KN : float\n        The A2KN statistics of Scholz and Stephens 1987.\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 2676, "code": "def anderson_ksamp(samples, midrank=_NoValue, *, variant=_NoValue, method=None):\n    k = len(samples)\n    if (k < 2):\n        raise ValueError(\"anderson_ksamp needs at least two samples\")\n    samples = list(map(np.asarray, samples))\n    Z = np.sort(np.hstack(samples))\n    N = Z.size\n    Zstar = np.unique(Z)\n    if Zstar.size < 2:\n        raise ValueError(\"anderson_ksamp needs more than one distinct \"\n                         \"observation\")", "documentation": "    \"\"\"The Anderson-Darling test for k-samples.\n\n    The k-sample Anderson-Darling test is a modification of the\n    one-sample Anderson-Darling test. It tests the null hypothesis\n    that k-samples are drawn from the same population without having\n    to specify the distribution function of that population. The\n    critical values depend on the number of samples.\n\n    Parameters\n    ----------\n    samples : sequence of 1-D array_like\n        Array of sample data in arrays.\n    midrank : bool, optional\n        Variant of Anderson-Darling test which is computed. Default\n        (True) is the midrank test applicable to continuous and\n        discrete populations. If False, the right side empirical\n        distribution is used.\n\n        .. deprecated:: 1.17.0\n            Use parameter `variant` instead.\n    variant : {'midrank', 'right', 'continuous'}\n        Variant of Anderson-Darling test to be computed. ``'midrank'`` is applicable\n        to both continuous and discrete populations. ``'discrete'`` and ``'continuous'``\n        perform alternative versions of the test for discrete  and continuous\n        populations, respectively.\n        When `variant` is specified, the return object will not be unpackable as a\n        tuple, and only attributes ``statistic`` and ``pvalue`` will be present.\n    method : PermutationMethod, optional\n        Defines the method used to compute the p-value. If `method` is an\n        instance of `PermutationMethod`, the p-value is computed using\n        `scipy.stats.permutation_test` with the provided configuration options\n        and other appropriate settings. Otherwise, the p-value is interpolated\n        from tabulated values.\n\n    Returns\n    -------\n    res : Anderson_ksampResult\n        An object containing attributes:\n\n        statistic : float\n            Normalized k-sample Anderson-Darling test statistic.\n        critical_values : array\n            The critical values for significance levels 25%, 10%, 5%, 2.5%, 1%,\n            0.5%, 0.1%.\n\n            .. deprecated:: 1.17.0\n                 Present only when `variant` is unspecified.\n\n        pvalue : float\n            The approximate p-value of the test. If `method` is not\n            provided, the value is floored / capped at 0.1% / 25%.\n\n    Raises\n    ------\n    ValueError\n        If fewer than 2 samples are provided, a sample is empty, or no\n        distinct observations are in the samples.\n\n    See Also\n    --------\n    ks_2samp : 2 sample Kolmogorov-Smirnov test\n    anderson : 1 sample Anderson-Darling test\n\n    Notes\n    -----\n    [1]_ defines three versions of the k-sample Anderson-Darling test:\n    one for continuous distributions and two for discrete\n    distributions, in which ties between samples may occur. The\n    default of this routine is to compute the version based on the\n    midrank empirical distribution function. This test is applicable\n    to continuous and discrete data. If `variant` is set to ``'discrete'``, the\n    right side empirical distribution is used for a test for discrete\n    data; if `variant` is ``'continuous'``, the same test statistic and p-value are\n    computed for data with no ties, but with less computation. According to [1]_,\n    the two discrete test statistics differ only slightly if a few collisions due\n    to round-off errors occur in the test not adjusted for ties between samples.\n\n    The critical values corresponding to the significance levels from 0.01\n    to 0.25 are taken from [1]_. p-values are floored / capped\n    at 0.1% / 25%. Since the range of critical values might be extended in\n    future releases, it is recommended not to test ``p == 0.25``, but rather\n    ``p >= 0.25`` (analogously for the lower bound).\n\n    .. versionadded:: 0.14.0\n\n    References\n    ----------\n    .. [1] Scholz, F. W and Stephens, M. A. (1987), K-Sample\n           Anderson-Darling Tests, Journal of the American Statistical\n           Association, Vol. 82, pp. 918-924.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng(44925884305279435)\n    >>> res = stats.anderson_ksamp([rng.normal(size=50), rng.normal(loc=0.5, size=30)],\n    ...                            variant='midrank')\n    >>> res.statistic, res.pvalue\n    (3.4444310693448936, 0.013106682406720973)\n\n    The null hypothesis that the two random samples come from the same\n    distribution can be rejected at the 5% level because the returned\n    p-value is less than 0.05, but not at the 1% level.\n\n    >>> samples = [rng.normal(size=50), rng.normal(size=30),\n    ...            rng.normal(size=20)]\n    >>> res = stats.anderson_ksamp(samples, variant='continuous')\n    >>> res.statistic, res.pvalue\n    (-0.6309662273193832, 0.25)\n\n    As we might expect, the null hypothesis cannot be rejected here for three samples\n    from an identical distribution. The reported p-value (25%) has been capped at the\n    maximum value for which pre-computed p-values are available.\n\n    In such cases where the p-value is capped or when sample sizes are\n    small, a permutation test may be more accurate.\n\n    >>> method = stats.PermutationMethod(n_resamples=9999, random_state=rng)\n    >>> res = stats.anderson_ksamp(samples, variant='continuous', method=method)\n    >>> res.pvalue\n    0.699\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 2904, "code": "class _ABW:", "documentation": "    \"\"\"Distribution of Ansari-Bradley W-statistic under the null hypothesis.\"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 2910, "code": "    def __init__(self):\n        self.m = None\n        self.n = None\n        self.astart = None\n        self.total = None\n        self.freqs = None", "documentation": "        \"\"\"Minimal initializer.\"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 2918, "code": "    def _recalc(self, n, m):\n        if n != self.n or m != self.m:\n            self.n, self.m = n, m\n            astart, a1, _ = gscale(n, m)\n            self.astart = astart  # minimum value of statistic\n            self.freqs = a1.astype(np.float64)\n            self.total = self.freqs.sum()  # could calculate from m and n", "documentation": "        \"\"\"When necessary, recalculate exact distribution.\"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 2933, "code": "    def pmf(self, k, n, m):\n        self._recalc(n, m)\n        ind = np.floor(k - self.astart).astype(int)\n        return self.freqs[ind] / self.total", "documentation": "        \"\"\"Probability mass function.\"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 2941, "code": "    def cdf(self, k, n, m):\n        self._recalc(n, m)\n        ind = np.ceil(k - self.astart).astype(int)\n        return self.freqs[:ind+1].sum() / self.total", "documentation": "        \"\"\"Cumulative distribution function.\"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 2949, "code": "    def sf(self, k, n, m):\n        self._recalc(n, m)\n        ind = np.floor(k - self.astart).astype(int)\n        return self.freqs[ind:].sum() / self.total\n_abw_state = threading.local()\n@xp_capabilities(cpu_only=True, jax_jit=False,    # p-value is Cython\n                 skip_backends=[('dask.array', 'no rankdata')])\n@_axis_nan_policy_factory(AnsariResult, n_samples=2)", "documentation": "        \"\"\"Survival function.\"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 2967, "code": "def ansari(x, y, alternative='two-sided', *, axis=0):\n    xp = array_namespace(x, y)\n    dtype = xp_result_type(x, y, force_floating=True, xp=xp)\n    if alternative not in {'two-sided', 'greater', 'less'}:\n        raise ValueError(\"'alternative' must be 'two-sided',\"\n                         \" 'greater', or 'less'.\")\n    if not hasattr(_abw_state, 'a'):\n        _abw_state.a = _ABW()\n    n = x.shape[-1]\n    m = y.shape[-1]\n    if m < 1:  # needed by test_axis_nan_policy; not user-facing", "documentation": "    \"\"\"Perform the Ansari-Bradley test for equal scale parameters.\n\n    The Ansari-Bradley test ([1]_, [2]_) is a non-parametric test\n    for the equality of the scale parameter of the distributions\n    from which two samples were drawn. The null hypothesis states that\n    the ratio of the scale of the distribution underlying `x` to the scale\n    of the distribution underlying `y` is 1.\n\n    Parameters\n    ----------\n    x, y : array_like\n        Arrays of sample data.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n        The following options are available:\n\n        * 'two-sided': the ratio of scales is not equal to 1.\n        * 'less': the ratio of scales is less than 1.\n        * 'greater': the ratio of scales is greater than 1.\n\n        .. versionadded:: 1.7.0\n    axis : int or tuple of ints, default: 0\n        If an int or tuple of ints, the axis or axes of the input along which\n        to compute the statistic. The statistic of each axis-slice (e.g. row)\n        of the input will appear in a corresponding element of the output.\n        If ``None``, the input will be raveled before computing the statistic.\n\n    Returns\n    -------\n    statistic : float\n        The Ansari-Bradley test statistic.\n    pvalue : float\n        The p-value of the hypothesis test.\n\n    See Also\n    --------\n    fligner : A non-parametric test for the equality of k variances\n    mood : A non-parametric test for the equality of two scale parameters\n\n    Notes\n    -----\n    The p-value given is exact when the sample sizes are both less than\n    55 and there are no ties, otherwise a normal approximation for the\n    p-value is used.\n\n    References\n    ----------\n    .. [1] Ansari, A. R. and Bradley, R. A. (1960) Rank-sum tests for\n           dispersions, Annals of Mathematical Statistics, 31, 1174-1189.\n    .. [2] Sprent, Peter and N.C. Smeeton.  Applied nonparametric\n           statistical methods.  3rd ed. Chapman and Hall/CRC. 2001.\n           Section 5.8.2.\n    .. [3] Nathaniel E. Helwig \"Nonparametric Dispersion and Equality\n           Tests\" at http://users.stat.umn.edu/~helwig/notes/npde-Notes.pdf\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import ansari\n    >>> rng = np.random.default_rng()\n\n    For these examples, we'll create three random data sets.  The first\n    two, with sizes 35 and 25, are drawn from a normal distribution with\n    mean 0 and standard deviation 2.  The third data set has size 25 and\n    is drawn from a normal distribution with standard deviation 1.25.\n\n    >>> x1 = rng.normal(loc=0, scale=2, size=35)\n    >>> x2 = rng.normal(loc=0, scale=2, size=25)\n    >>> x3 = rng.normal(loc=0, scale=1.25, size=25)\n\n    First we apply `ansari` to `x1` and `x2`.  These samples are drawn\n    from the same distribution, so we expect the Ansari-Bradley test\n    should not lead us to conclude that the scales of the distributions\n    are different.\n\n    >>> ansari(x1, x2)\n    AnsariResult(statistic=541.0, pvalue=0.9762532927399098)\n\n    With a p-value close to 1, we cannot conclude that there is a\n    significant difference in the scales (as expected).\n\n    Now apply the test to `x1` and `x3`:\n\n    >>> ansari(x1, x3)\n    AnsariResult(statistic=425.0, pvalue=0.0003087020407974518)\n\n    The probability of observing such an extreme value of the statistic\n    under the null hypothesis of equal scales is only 0.03087%. We take this\n    as evidence against the null hypothesis in favor of the alternative:\n    the scales of the distributions from which the samples were drawn\n    are not equal.\n\n    We can use the `alternative` parameter to perform a one-tailed test.\n    In the above example, the scale of `x1` is greater than `x3` and so\n    the ratio of scales of `x1` and `x3` is greater than 1. This means\n    that the p-value when ``alternative='greater'`` should be near 0 and\n    hence we should be able to reject the null hypothesis:\n\n    >>> ansari(x1, x3, alternative='greater')\n    AnsariResult(statistic=425.0, pvalue=0.0001543510203987259)\n\n    As we can see, the p-value is indeed quite low. Use of\n    ``alternative='less'`` should thus yield a large p-value:\n\n    >>> ansari(x1, x3, alternative='less')\n    AnsariResult(statistic=425.0, pvalue=0.9998643258449039)\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 3647, "code": "def mood(x, y, axis=0, alternative=\"two-sided\"):\n    xp = array_namespace(x, y)\n    x, y = xp_promote(x, y, force_floating=True, xp=xp)\n    dtype = x.dtype\n    xy = xp.concat((x, y), axis=-1)\n    m = x.shape[-1]\n    n = y.shape[-1]\n    N = m + n\n    if m == 0 or n == 0 or N < 3:  # only needed for test_axis_nan_policy\n        NaN = _get_nan(x, y, xp=xp)\n        return SignificanceResult(NaN, NaN)", "documentation": "    \"\"\"Perform Mood's test for equal scale parameters.\n\n    Mood's two-sample test for scale parameters is a non-parametric\n    test for the null hypothesis that two samples are drawn from the\n    same distribution with the same scale parameter.\n\n    Parameters\n    ----------\n    x, y : array_like\n        Arrays of sample data. There must be at least three observations\n        total.\n    axis : int, optional\n        The axis along which the samples are tested.  `x` and `y` can be of\n        different length along `axis`.\n        If `axis` is None, `x` and `y` are flattened and the test is done on\n        all values in the flattened arrays.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n        The following options are available:\n\n        * 'two-sided': the scales of the distributions underlying `x` and `y`\n          are different.\n        * 'less': the scale of the distribution underlying `x` is less than\n          the scale of the distribution underlying `y`.\n        * 'greater': the scale of the distribution underlying `x` is greater\n          than the scale of the distribution underlying `y`.\n\n        .. versionadded:: 1.7.0\n\n    Returns\n    -------\n    res : SignificanceResult\n        An object containing attributes:\n\n        statistic : scalar or ndarray\n            The z-score for the hypothesis test.  For 1-D inputs a scalar is\n            returned.\n        pvalue : scalar ndarray\n            The p-value for the hypothesis test.\n\n    See Also\n    --------\n    fligner : A non-parametric test for the equality of k variances\n    ansari : A non-parametric test for the equality of 2 variances\n    bartlett : A parametric test for equality of k variances in normal samples\n    levene : A parametric test for equality of k variances\n\n    Notes\n    -----\n    The data are assumed to be drawn from probability distributions ``f(x)``\n    and ``f(x/s) / s`` respectively, for some probability density function f.\n    The null hypothesis is that ``s == 1``.\n\n    For multi-dimensional arrays, if the inputs are of shapes\n    ``(n0, n1, n2, n3)``  and ``(n0, m1, n2, n3)``, then if ``axis=1``, the\n    resulting z and p values will have shape ``(n0, n2, n3)``.  Note that\n    ``n1`` and ``m1`` don't have to be equal, but the other dimensions do.\n\n    References\n    ----------\n    [1] Mielke, Paul W. \"Note on Some Squared Rank Tests with Existing Ties.\"\n        Technometrics, vol. 9, no. 2, 1967, pp. 312-14. JSTOR,\n        :doi:`10.2307/1266427`. Accessed 18 May 2022.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> x2 = rng.standard_normal((2, 45, 6, 7))\n    >>> x1 = rng.standard_normal((2, 30, 6, 7))\n    >>> res = stats.mood(x1, x2, axis=1)\n    >>> res.pvalue.shape\n    (2, 6, 7)\n\n    Find the number of points where the difference in scale is not significant:\n\n    >>> (res.pvalue > 0.1).sum()\n    78\n\n    Perform the test with different scales:\n\n    >>> x1 = rng.standard_normal((2, 30))\n    >>> x2 = rng.standard_normal((2, 35)) * 10.0\n    >>> stats.mood(x1, x2, axis=1)\n    SignificanceResult(statistic=array([-5.76174136, -6.12650783]),\n                       pvalue=array([8.32505043e-09, 8.98287869e-10]))\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 4596, "code": "def directional_stats(samples, *, axis=0, normalize=True):\n    xp = array_namespace(samples)\n    samples = xp.asarray(samples)\n    if samples.ndim < 2:\n        raise ValueError(\"samples must at least be two-dimensional. \"\n                         f\"Instead samples has shape: {tuple(samples.shape)}\")\n    samples = xp.moveaxis(samples, axis, 0)\n    if is_marray(xp):\n        _xp = array_namespace(samples.mask)\n        mask = _xp.any(samples.mask, axis=-1, keepdims=True)\n        samples = xp.asarray(samples.data, mask=mask)", "documentation": "    \"\"\"\n    Computes sample statistics for directional data.\n\n    Computes the directional mean (also called the mean direction vector) and\n    mean resultant length of a sample of vectors.\n\n    The directional mean is a measure of \"preferred direction\" of vector data.\n    It is analogous to the sample mean, but it is for use when the length of\n    the data is irrelevant (e.g. unit vectors).\n\n    The mean resultant length is a value between 0 and 1 used to quantify the\n    dispersion of directional data: the smaller the mean resultant length, the\n    greater the dispersion. Several definitions of directional variance\n    involving the mean resultant length are given in [1]_ and [2]_.\n\n    Parameters\n    ----------\n    samples : array_like\n        Input array. Must be at least two-dimensional, and the last axis of the\n        input must correspond with the dimensionality of the vector space.\n        When the input is exactly two dimensional, this means that each row\n        of the data is a vector observation.\n    axis : int, default: 0\n        Axis along which the directional mean is computed.\n    normalize : bool, default: True\n        If True, normalize the input to ensure that each observation is a\n        unit vector. It the observations are already unit vectors, consider\n        setting this to False to avoid unnecessary computation.\n\n    Returns\n    -------\n    res : DirectionalStats\n        An object containing attributes:\n\n        mean_direction : ndarray\n            Directional mean.\n        mean_resultant_length : ndarray\n            The mean resultant length [1]_.\n\n    See Also\n    --------\n    circmean: circular mean; i.e. directional mean for 2D *angles*\n    circvar: circular variance; i.e. directional variance for 2D *angles*\n\n    Notes\n    -----\n    This uses a definition of directional mean from [1]_.\n    Assuming the observations are unit vectors, the calculation is as follows.\n\n    .. code-block:: python\n\n        mean = samples.mean(axis=0)\n        mean_resultant_length = np.linalg.norm(mean)\n        mean_direction = mean / mean_resultant_length\n\n    This definition is appropriate for *directional* data (i.e. vector data\n    for which the magnitude of each observation is irrelevant) but not\n    for *axial* data (i.e. vector data for which the magnitude and *sign* of\n    each observation is irrelevant).\n\n    Several definitions of directional variance involving the mean resultant\n    length ``R`` have been proposed, including ``1 - R`` [1]_, ``1 - R**2``\n    [2]_, and ``2 * (1 - R)`` [2]_. Rather than choosing one, this function\n    returns ``R`` as attribute `mean_resultant_length` so the user can compute\n    their preferred measure of dispersion.\n\n    References\n    ----------\n    .. [1] Mardia, Jupp. (2000). *Directional Statistics*\n       (p. 163). Wiley.\n\n    .. [2] https://en.wikipedia.org/wiki/Directional_statistics\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import directional_stats\n    >>> data = np.array([[3, 4],    # first observation, 2D vector space\n    ...                  [6, -8]])  # second observation\n    >>> dirstats = directional_stats(data)\n    >>> dirstats.mean_direction\n    array([1., 0.])\n\n    In contrast, the regular sample mean of the vectors would be influenced\n    by the magnitude of each observation. Furthermore, the result would not be\n    a unit vector.\n\n    >>> data.mean(axis=0)\n    array([4.5, -2.])\n\n    An exemplary use case for `directional_stats` is to find a *meaningful*\n    center for a set of observations on a sphere, e.g. geographical locations.\n\n    >>> data = np.array([[0.8660254, 0.5, 0.],\n    ...                  [0.8660254, -0.5, 0.]])\n    >>> dirstats = directional_stats(data)\n    >>> dirstats.mean_direction\n    array([1., 0., 0.])\n\n    The regular sample mean on the other hand yields a result which does not\n    lie on the surface of the sphere.\n\n    >>> data.mean(axis=0)\n    array([0.8660254, 0., 0.])\n\n    The function also returns the mean resultant length, which\n    can be used to calculate a directional variance. For example, using the\n    definition ``Var(z) = 1 - R`` from [2]_ where ``R`` is the\n    mean resultant length, we can calculate the directional variance of the\n    vectors in the above example as:\n\n    >>> 1 - dirstats.mean_resultant_length\n    0.13397459716167093\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 4736, "code": "def false_discovery_control(ps, *, axis=0, method='bh'):\n    xp = array_namespace(ps)\n    ps = xp.asarray(ps)\n    if not xp.isdtype(ps.dtype, (\"integral\", \"real floating\")):\n        raise ValueError(\"`ps` must contain only real numbers.\")\n    if is_lazy_array(ps):\n        ps = xp.where((ps < 0.) | (ps > 1.), xp.nan, ps)\n    else:\n        if not xp.all(ps == xp.clip(ps, 0., 1.)):\n            raise ValueError(\"All values in `ps` must lie between 0. and 1.\")\n    methods = {'bh', 'by'}", "documentation": "    \"\"\"Adjust p-values to control the false discovery rate.\n\n    The false discovery rate (FDR) is the expected proportion of rejected null\n    hypotheses that are actually true.\n    If the null hypothesis is rejected when the *adjusted* p-value falls below\n    a specified level, the false discovery rate is controlled at that level.\n\n    Parameters\n    ----------\n    ps : 1D array_like\n        The p-values to adjust. Elements must be real numbers between 0 and 1.\n    axis : int\n        The axis along which to perform the adjustment. The adjustment is\n        performed independently along each axis-slice. If `axis` is None, `ps`\n        is raveled before performing the adjustment.\n    method : {'bh', 'by'}\n        The false discovery rate control procedure to apply: ``'bh'`` is for\n        Benjamini-Hochberg [1]_ (Eq. 1), ``'by'`` is for Benjaminini-Yekutieli\n        [2]_ (Theorem 1.3). The latter is more conservative, but it is\n        guaranteed to control the FDR even when the p-values are not from\n        independent tests.\n\n    Returns\n    -------\n    ps_adusted : array_like\n        The adjusted p-values. If the null hypothesis is rejected where these\n        fall below a specified level, the false discovery rate is controlled\n        at that level.\n\n    See Also\n    --------\n    combine_pvalues\n    statsmodels.stats.multitest.multipletests\n\n    Notes\n    -----\n    In multiple hypothesis testing, false discovery control procedures tend to\n    offer higher power than familywise error rate control procedures (e.g.\n    Bonferroni correction [1]_).\n\n    If the p-values correspond with independent tests (or tests with\n    \"positive regression dependencies\" [2]_), rejecting null hypotheses\n    corresponding with Benjamini-Hochberg-adjusted p-values below :math:`q`\n    controls the false discovery rate at a level less than or equal to\n    :math:`q m_0 / m`, where :math:`m_0` is the number of true null hypotheses\n    and :math:`m` is the total number of null hypotheses tested. The same is\n    true even for dependent tests when the p-values are adjusted accorded to\n    the more conservative Benjaminini-Yekutieli procedure.\n\n    The adjusted p-values produced by this function are comparable to those\n    produced by the R function ``p.adjust`` and the statsmodels function\n    `statsmodels.stats.multitest.multipletests`. Please consider the latter\n    for more advanced methods of multiple comparison correction.\n\n    References\n    ----------\n    .. [1] Benjamini, Yoav, and Yosef Hochberg. \"Controlling the false\n           discovery rate: a practical and powerful approach to multiple\n           testing.\" Journal of the Royal statistical society: series B\n           (Methodological) 57.1 (1995): 289-300.\n\n    .. [2] Benjamini, Yoav, and Daniel Yekutieli. \"The control of the false\n           discovery rate in multiple testing under dependency.\" Annals of\n           statistics (2001): 1165-1188.\n\n    .. [3] TileStats. FDR - Benjamini-Hochberg explained - Youtube.\n           https://www.youtube.com/watch?v=rZKa4tW2NKs.\n\n    .. [4] Neuhaus, Karl-Ludwig, et al. \"Improved thrombolysis in acute\n           myocardial infarction with front-loaded administration of alteplase:\n           results of the rt-PA-APSAC patency study (TAPS).\" Journal of the\n           American College of Cardiology 19.5 (1992): 885-891.\n\n    Examples\n    --------\n    We follow the example from [1]_.\n\n        Thrombolysis with recombinant tissue-type plasminogen activator (rt-PA)\n        and anisoylated plasminogen streptokinase activator (APSAC) in\n        myocardial infarction has been proved to reduce mortality. [4]_\n        investigated the effects of a new front-loaded administration of rt-PA\n        versus those obtained with a standard regimen of APSAC, in a randomized\n        multicentre trial in 421 patients with acute myocardial infarction.\n\n    There were four families of hypotheses tested in the study, the last of\n    which was \"cardiac and other events after the start of thrombolitic\n    treatment\". FDR control may be desired in this family of hypotheses\n    because it would not be appropriate to conclude that the front-loaded\n    treatment is better if it is merely equivalent to the previous treatment.\n\n    The p-values corresponding with the 15 hypotheses in this family were\n\n    >>> ps = [0.0001, 0.0004, 0.0019, 0.0095, 0.0201, 0.0278, 0.0298, 0.0344,\n    ...       0.0459, 0.3240, 0.4262, 0.5719, 0.6528, 0.7590, 1.000]\n\n    If the chosen significance level is 0.05, we may be tempted to reject the\n    null hypotheses for the tests corresponding with the first nine p-values,\n    as the first nine p-values fall below the chosen significance level.\n    However, this would ignore the problem of \"multiplicity\": if we fail to\n    correct for the fact that multiple comparisons are being performed, we\n    are more likely to incorrectly reject true null hypotheses.\n\n    One approach to the multiplicity problem is to control the family-wise\n    error rate (FWER), that is, the rate at which the null hypothesis is\n    rejected when it is actually true. A common procedure of this kind is the\n    Bonferroni correction [1]_.  We begin by multiplying the p-values by the\n    number of hypotheses tested.\n\n    >>> import numpy as np\n    >>> np.array(ps) * len(ps)\n    array([1.5000e-03, 6.0000e-03, 2.8500e-02, 1.4250e-01, 3.0150e-01,\n           4.1700e-01, 4.4700e-01, 5.1600e-01, 6.8850e-01, 4.8600e+00,\n           6.3930e+00, 8.5785e+00, 9.7920e+00, 1.1385e+01, 1.5000e+01])\n\n    To control the FWER at 5%, we reject only the hypotheses corresponding\n    with adjusted p-values less than 0.05. In this case, only the hypotheses\n    corresponding with the first three p-values can be rejected. According to\n    [1]_, these three hypotheses concerned \"allergic reaction\" and \"two\n    different aspects of bleeding.\"\n\n    An alternative approach is to control the false discovery rate: the\n    expected fraction of rejected null hypotheses that are actually true. The\n    advantage of this approach is that it typically affords greater power: an\n    increased rate of rejecting the null hypothesis when it is indeed false. To\n    control the false discovery rate at 5%, we apply the Benjamini-Hochberg\n    p-value adjustment.\n\n    >>> from scipy import stats\n    >>> stats.false_discovery_control(ps)\n    array([0.0015    , 0.003     , 0.0095    , 0.035625  , 0.0603    ,\n           0.06385714, 0.06385714, 0.0645    , 0.0765    , 0.486     ,\n           0.58118182, 0.714875  , 0.75323077, 0.81321429, 1.        ])\n\n    Now, the first *four* adjusted p-values fall below 0.05, so we would reject\n    the null hypotheses corresponding with these *four* p-values. Rejection\n    of the fourth null hypothesis was particularly important to the original\n    study as it led to the conclusion that the new treatment had a\n    \"substantially lower in-hospital mortality rate.\"\n\n    For simplicity of exposition, the p-values in the example above were given in\n    sorted order, but this is not required; `false_discovery_control` returns\n    adjusted p-values in order corresponding with the input `ps`.\n\n    >>> stats.false_discovery_control([0.5, 0.6, 0.1, 0.001])\n    array([0.6  , 0.6  , 0.2  , 0.004])\n\n    \"\"\""}], "after_segments": [{"filename": "scipy/stats/_morestats.py", "start_line": 163, "code": "def mvsdist(data):\n    x = ravel(data)\n    n = len(x)\n    if n < 2:\n        raise ValueError(\"Need at least 2 data-points.\")\n    xbar = x.mean()\n    C = x.var()\n    if n > 1000:  # gaussian approximations for large n\n        mdist = distributions.norm(loc=xbar, scale=math.sqrt(C / n))\n        sdist = distributions.norm(loc=math.sqrt(C), scale=math.sqrt(C / (2. * n)))\n        vdist = distributions.norm(loc=C, scale=math.sqrt(2.0 / n) * C)", "documentation": "    \"\"\"\n    'Frozen' distributions for mean, variance, and standard deviation of data.\n\n    Parameters\n    ----------\n    data : array_like\n        Input array. Converted to 1-D using ravel.\n        Requires 2 or more data-points.\n\n    Returns\n    -------\n    mdist : \"frozen\" distribution object\n        Distribution object representing the mean of the data.\n    vdist : \"frozen\" distribution object\n        Distribution object representing the variance of the data.\n    sdist : \"frozen\" distribution object\n        Distribution object representing the standard deviation of the data.\n\n    See Also\n    --------\n    bayes_mvs\n\n    Notes\n    -----\n    The return values from ``bayes_mvs(data)`` is equivalent to\n    ``tuple((x.mean(), x.interval(0.90)) for x in mvsdist(data))``.\n\n    In other words, calling ``<dist>.mean()`` and ``<dist>.interval(0.90)``\n    on the three distribution objects returned from this function will give\n    the same results that are returned from `bayes_mvs`.\n\n    References\n    ----------\n    T.E. Oliphant, \"A Bayesian perspective on estimating mean, variance, and\n    standard-deviation from data\", https://scholarsarchive.byu.edu/facpub/278,\n    2006.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> data = [6, 9, 12, 7, 8, 8, 13]\n    >>> mean, var, std = stats.mvsdist(data)\n\n    We now have frozen distribution objects \"mean\", \"var\" and \"std\" that we can\n    examine:\n\n    >>> mean.mean()\n    9.0\n    >>> mean.interval(0.95)\n    (6.6120585482655692, 11.387941451734431)\n    >>> mean.std()\n    1.1952286093343936\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 406, "code": "def _calc_uniform_order_statistic_medians(n):\n    v = np.empty(n, dtype=np.float64)\n    v[-1] = 0.5**(1.0 / n)\n    v[0] = 1 - v[-1]\n    i = np.arange(2, n)\n    v[1:-1] = (i - 0.3175) / (n + 0.365)\n    return v", "documentation": "    \"\"\"Approximations of uniform order statistic medians.\n\n    Parameters\n    ----------\n    n : int\n        Sample size.\n\n    Returns\n    -------\n    v : 1d float array\n        Approximations of the order statistic medians.\n\n    References\n    ----------\n    .. [1] James J. Filliben, \"The Probability Plot Correlation Coefficient\n           Test for Normality\", Technometrics, Vol. 17, pp. 111-117, 1975.\n\n    Examples\n    --------\n    Order statistics of the uniform distribution on the unit interval\n    are marginally distributed according to beta distributions.\n    The expectations of these order statistic are evenly spaced across\n    the interval, but the distributions are skewed in a way that\n    pushes the medians slightly towards the endpoints of the unit interval:\n\n    >>> import numpy as np\n    >>> n = 4\n    >>> k = np.arange(1, n+1)\n    >>> from scipy.stats import beta\n    >>> a = k\n    >>> b = n-k+1\n    >>> beta.mean(a, b)\n    array([0.2, 0.4, 0.6, 0.8])\n    >>> beta.median(a, b)\n    array([0.15910358, 0.38572757, 0.61427243, 0.84089642])\n\n    The Filliben approximation uses the exact medians of the smallest\n    and greatest order statistics, and the remaining medians are approximated\n    by points spread evenly across a sub-interval of the unit interval:\n\n    >>> from scipy.stats._morestats import _calc_uniform_order_statistic_medians\n    >>> _calc_uniform_order_statistic_medians(n)\n    array([0.15910358, 0.38545246, 0.61454754, 0.84089642])\n\n    This plot shows the skewed distributions of the order statistics\n    of a sample of size four from a uniform distribution on the unit interval:\n\n    >>> import matplotlib.pyplot as plt\n    >>> x = np.linspace(0.0, 1.0, num=50, endpoint=True)\n    >>> pdfs = [beta.pdf(x, a[i], b[i]) for i in range(n)]\n    >>> plt.figure()\n    >>> plt.plot(x, pdfs[0], x, pdfs[1], x, pdfs[2], x, pdfs[3])\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 469, "code": "def _parse_dist_kw(dist, enforce_subclass=True):\n    if isinstance(dist, rv_generic):\n        pass\n    elif isinstance(dist, str):\n        try:\n            dist = getattr(distributions, dist)\n        except AttributeError as e:\n            raise ValueError(f\"{dist} is not a valid distribution name\") from e\n    elif enforce_subclass:\n        msg = (\"`dist` should be a stats.distributions instance or a string \"\n               \"with the name of such a distribution.\")", "documentation": "    \"\"\"Parse `dist` keyword.\n\n    Parameters\n    ----------\n    dist : str or stats.distributions instance.\n        Several functions take `dist` as a keyword, hence this utility\n        function.\n    enforce_subclass : bool, optional\n        If True (default), `dist` needs to be a\n        `_distn_infrastructure.rv_generic` instance.\n        It can sometimes be useful to set this keyword to False, if a function\n        wants to accept objects that just look somewhat like such an instance\n        (for example, they have a ``ppf`` method).\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 500, "code": "def _add_axis_labels_title(plot, xlabel, ylabel, title):\n    try:\n        if hasattr(plot, 'set_title'):\n            plot.set_title(title)\n            plot.set_xlabel(xlabel)\n            plot.set_ylabel(ylabel)\n        else:\n            plot.title(title)\n            plot.xlabel(xlabel)\n            plot.ylabel(ylabel)\n    except Exception:", "documentation": "    \"\"\"Helper function to add axes labels and a title to stats plots.\"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 520, "code": "def probplot(x, sparams=(), dist='norm', fit=True, plot=None, rvalue=False):\n    x = np.asarray(x)\n    if x.size == 0:\n        if fit:\n            return (x, x), (np.nan, np.nan, 0.0)\n        else:\n            return x, x\n    osm_uniform = _calc_uniform_order_statistic_medians(len(x))\n    dist = _parse_dist_kw(dist, enforce_subclass=False)\n    if sparams is None:\n        sparams = ()", "documentation": "    \"\"\"\n    Calculate quantiles for a probability plot, and optionally show the plot.\n\n    Generates a probability plot of sample data against the quantiles of a\n    specified theoretical distribution (the normal distribution by default).\n    `probplot` optionally calculates a best-fit line for the data and plots the\n    results using Matplotlib or a given plot function.\n\n    Parameters\n    ----------\n    x : array_like\n        Sample/response data from which `probplot` creates the plot.\n    sparams : tuple, optional\n        Distribution-specific shape parameters (shape parameters plus location\n        and scale).\n    dist : str or stats.distributions instance, optional\n        Distribution or distribution function name. The default is 'norm' for a\n        normal probability plot.  Objects that look enough like a\n        stats.distributions instance (i.e. they have a ``ppf`` method) are also\n        accepted.\n    fit : bool, optional\n        Fit a least-squares regression (best-fit) line to the sample data if\n        True (default).\n    plot : object, optional\n        If given, plots the quantiles.\n        If given and `fit` is True, also plots the least squares fit.\n        `plot` is an object that has to have methods \"plot\" and \"text\".\n        The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,\n        or a custom object with the same methods.\n        Default is None, which means that no plot is created.\n    rvalue : bool, optional\n        If `plot` is provided and `fit` is True, setting `rvalue` to True\n        includes the coefficient of determination on the plot.\n        Default is False.\n\n    Returns\n    -------\n    (osm, osr) : tuple of ndarrays\n        Tuple of theoretical quantiles (osm, or order statistic medians) and\n        ordered responses (osr).  `osr` is simply sorted input `x`.\n        For details on how `osm` is calculated see the Notes section.\n    (slope, intercept, r) : tuple of floats, optional\n        Tuple  containing the result of the least-squares fit, if that is\n        performed by `probplot`. `r` is the square root of the coefficient of\n        determination.  If ``fit=False`` and ``plot=None``, this tuple is not\n        returned.\n\n    Notes\n    -----\n    Even if `plot` is given, the figure is not shown or saved by `probplot`;\n    ``plt.show()`` or ``plt.savefig('figname.png')`` should be used after\n    calling `probplot`.\n\n    `probplot` generates a probability plot, which should not be confused with\n    a Q-Q or a P-P plot.  Statsmodels has more extensive functionality of this\n    type, see ``statsmodels.api.ProbPlot``.\n\n    The formula used for the theoretical quantiles (horizontal axis of the\n    probability plot) is Filliben's estimate::\n\n        quantiles = dist.ppf(val), for\n\n                0.5**(1/n),                  for i = n\n          val = (i - 0.3175) / (n + 0.365),  for i = 2, ..., n-1\n                1 - 0.5**(1/n),              for i = 1\n\n    where ``i`` indicates the i-th ordered value and ``n`` is the total number\n    of values.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n    >>> nsample = 100\n    >>> rng = np.random.default_rng()\n\n    A t distribution with small degrees of freedom:\n\n    >>> ax1 = plt.subplot(221)\n    >>> x = stats.t.rvs(3, size=nsample, random_state=rng)\n    >>> res = stats.probplot(x, plot=plt)\n\n    A t distribution with larger degrees of freedom:\n\n    >>> ax2 = plt.subplot(222)\n    >>> x = stats.t.rvs(25, size=nsample, random_state=rng)\n    >>> res = stats.probplot(x, plot=plt)\n\n    A mixture of two normal distributions with broadcasting:\n\n    >>> ax3 = plt.subplot(223)\n    >>> x = stats.norm.rvs(loc=[0,5], scale=[1,1.5],\n    ...                    size=(nsample//2,2), random_state=rng).ravel()\n    >>> res = stats.probplot(x, plot=plt)\n\n    A standard normal distribution:\n\n    >>> ax4 = plt.subplot(224)\n    >>> x = stats.norm.rvs(loc=0, scale=1, size=nsample, random_state=rng)\n    >>> res = stats.probplot(x, plot=plt)\n\n    Produce a new figure with a loggamma distribution, using the ``dist`` and\n    ``sparams`` keywords:\n\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111)\n    >>> x = stats.loggamma.rvs(c=2.5, size=500, random_state=rng)\n    >>> res = stats.probplot(x, dist=stats.loggamma, sparams=(2.5,), plot=ax)\n    >>> ax.set_title(\"Probplot for loggamma dist with shape parameter 2.5\")\n\n    Show the results with Matplotlib:\n\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 684, "code": "def ppcc_max(x, brack=(0.0, 1.0), dist='tukeylambda'):\n    dist = _parse_dist_kw(dist)\n    osm_uniform = _calc_uniform_order_statistic_medians(len(x))\n    osr = sort(x)", "documentation": "    \"\"\"Calculate the shape parameter that maximizes the PPCC.\n\n    The probability plot correlation coefficient (PPCC) plot can be used\n    to determine the optimal shape parameter for a one-parameter family\n    of distributions. ``ppcc_max`` returns the shape parameter that would\n    maximize the probability plot correlation coefficient for the given\n    data to a one-parameter family of distributions.\n\n    Parameters\n    ----------\n    x : array_like\n        Input array.\n    brack : tuple, optional\n        Triple (a,b,c) where (a<b<c). If bracket consists of two numbers (a, c)\n        then they are assumed to be a starting interval for a downhill bracket\n        search (see `scipy.optimize.brent`).\n    dist : str or stats.distributions instance, optional\n        Distribution or distribution function name.  Objects that look enough\n        like a stats.distributions instance (i.e. they have a ``ppf`` method)\n        are also accepted.  The default is ``'tukeylambda'``.\n\n    Returns\n    -------\n    shape_value : float\n        The shape parameter at which the probability plot correlation\n        coefficient reaches its max value.\n\n    See Also\n    --------\n    ppcc_plot, probplot, boxcox\n\n    Notes\n    -----\n    The brack keyword serves as a starting point which is useful in corner\n    cases. One can use a plot to obtain a rough visual estimate of the location\n    for the maximum to start the search near it.\n\n    References\n    ----------\n    .. [1] J.J. Filliben, \"The Probability Plot Correlation Coefficient Test\n           for Normality\", Technometrics, Vol. 17, pp. 111-117, 1975.\n    .. [2] Engineering Statistics Handbook, NIST/SEMATEC,\n           https://www.itl.nist.gov/div898/handbook/eda/section3/ppccplot.htm\n\n    Examples\n    --------\n    First we generate some random data from a Weibull distribution\n    with shape parameter 2.5:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n    >>> rng = np.random.default_rng()\n    >>> c = 2.5\n    >>> x = stats.weibull_min.rvs(c, scale=4, size=2000, random_state=rng)\n\n    Generate the PPCC plot for this data with the Weibull distribution.\n\n    >>> fig, ax = plt.subplots(figsize=(8, 6))\n    >>> res = stats.ppcc_plot(x, c/2, 2*c, dist='weibull_min', plot=ax)\n\n    We calculate the value where the shape should reach its maximum and a\n    red line is drawn there. The line should coincide with the highest\n    point in the PPCC graph.\n\n    >>> cmax = stats.ppcc_max(x, brack=(c/2, 2*c), dist='weibull_min')\n    >>> ax.axvline(cmax, color='r')\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 773, "code": "def ppcc_plot(x, a, b, dist='tukeylambda', plot=None, N=80):\n    if b <= a:\n        raise ValueError(\"`b` has to be larger than `a`.\")\n    svals = np.linspace(a, b, num=N)\n    ppcc = np.empty_like(svals)\n    for k, sval in enumerate(svals):\n        _, r2 = probplot(x, sval, dist=dist, fit=True)\n        ppcc[k] = r2[-1]\n    if plot is not None:\n        plot.plot(svals, ppcc, 'x')\n        _add_axis_labels_title(plot, xlabel='Shape Values',", "documentation": "    \"\"\"Calculate and optionally plot probability plot correlation coefficient.\n\n    The probability plot correlation coefficient (PPCC) plot can be used to\n    determine the optimal shape parameter for a one-parameter family of\n    distributions.  It cannot be used for distributions without shape\n    parameters\n    (like the normal distribution) or with multiple shape parameters.\n\n    By default a Tukey-Lambda distribution (`stats.tukeylambda`) is used. A\n    Tukey-Lambda PPCC plot interpolates from long-tailed to short-tailed\n    distributions via an approximately normal one, and is therefore\n    particularly useful in practice.\n\n    Parameters\n    ----------\n    x : array_like\n        Input array.\n    a, b : scalar\n        Lower and upper bounds of the shape parameter to use.\n    dist : str or stats.distributions instance, optional\n        Distribution or distribution function name.  Objects that look enough\n        like a stats.distributions instance (i.e. they have a ``ppf`` method)\n        are also accepted.  The default is ``'tukeylambda'``.\n    plot : object, optional\n        If given, plots PPCC against the shape parameter.\n        `plot` is an object that has to have methods \"plot\" and \"text\".\n        The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,\n        or a custom object with the same methods.\n        Default is None, which means that no plot is created.\n    N : int, optional\n        Number of points on the horizontal axis (equally distributed from\n        `a` to `b`).\n\n    Returns\n    -------\n    svals : ndarray\n        The shape values for which `ppcc` was calculated.\n    ppcc : ndarray\n        The calculated probability plot correlation coefficient values.\n\n    See Also\n    --------\n    ppcc_max, probplot, boxcox_normplot, tukeylambda\n\n    References\n    ----------\n    J.J. Filliben, \"The Probability Plot Correlation Coefficient Test for\n    Normality\", Technometrics, Vol. 17, pp. 111-117, 1975.\n\n    Examples\n    --------\n    First we generate some random data from a Weibull distribution\n    with shape parameter 2.5, and plot the histogram of the data:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n    >>> rng = np.random.default_rng()\n    >>> c = 2.5\n    >>> x = stats.weibull_min.rvs(c, scale=4, size=2000, random_state=rng)\n\n    Take a look at the histogram of the data.\n\n    >>> fig1, ax = plt.subplots(figsize=(9, 4))\n    >>> ax.hist(x, bins=50)\n    >>> ax.set_title('Histogram of x')\n    >>> plt.show()\n\n    Now we explore this data with a PPCC plot as well as the related\n    probability plot and Box-Cox normplot.  A red line is drawn where we\n    expect the PPCC value to be maximal (at the shape parameter ``c``\n    used above):\n\n    >>> fig2 = plt.figure(figsize=(12, 4))\n    >>> ax1 = fig2.add_subplot(1, 3, 1)\n    >>> ax2 = fig2.add_subplot(1, 3, 2)\n    >>> ax3 = fig2.add_subplot(1, 3, 3)\n    >>> res = stats.probplot(x, plot=ax1)\n    >>> res = stats.boxcox_normplot(x, -4, 4, plot=ax2)\n    >>> res = stats.ppcc_plot(x, c/2, 2*c, dist='weibull_min', plot=ax3)\n    >>> ax3.axvline(c, color='r')\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 1486, "code": "def _normplot(method, x, la, lb, plot=None, N=80):\n    if method == 'boxcox':\n        title = 'Box-Cox Normality Plot'\n        transform_func = boxcox\n    else:\n        title = 'Yeo-Johnson Normality Plot'\n        transform_func = yeojohnson\n    x = np.asarray(x)\n    if x.size == 0:\n        return x\n    if lb <= la:", "documentation": "    \"\"\"Compute parameters for a Box-Cox or Yeo-Johnson normality plot,\n    optionally show it.\n\n    See `boxcox_normplot` or `yeojohnson_normplot` for details.\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 1529, "code": "def boxcox_normplot(x, la, lb, plot=None, N=80):\n    return _normplot('boxcox', x, la, lb, plot, N)\n@xp_capabilities(np_only=True)", "documentation": "    \"\"\"Compute parameters for a Box-Cox normality plot, optionally show it.\n\n    A Box-Cox normality plot shows graphically what the best transformation\n    parameter is to use in `boxcox` to obtain a distribution that is close\n    to normal.\n\n    Parameters\n    ----------\n    x : array_like\n        Input array.\n    la, lb : scalar\n        The lower and upper bounds for the ``lmbda`` values to pass to `boxcox`\n        for Box-Cox transformations.  These are also the limits of the\n        horizontal axis of the plot if that is generated.\n    plot : object, optional\n        If given, plots the quantiles and least squares fit.\n        `plot` is an object that has to have methods \"plot\" and \"text\".\n        The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,\n        or a custom object with the same methods.\n        Default is None, which means that no plot is created.\n    N : int, optional\n        Number of points on the horizontal axis (equally distributed from\n        `la` to `lb`).\n\n    Returns\n    -------\n    lmbdas : ndarray\n        The ``lmbda`` values for which a Box-Cox transform was done.\n    ppcc : ndarray\n        Probability Plot Correlation Coefficient, as obtained from `probplot`\n        when fitting the Box-Cox transformed input `x` against a normal\n        distribution.\n\n    See Also\n    --------\n    probplot, boxcox, boxcox_normmax, boxcox_llf, ppcc_max\n\n    Notes\n    -----\n    Even if `plot` is given, the figure is not shown or saved by\n    `boxcox_normplot`; ``plt.show()`` or ``plt.savefig('figname.png')``\n    should be used after calling `probplot`.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n\n    Generate some non-normally distributed data, and create a Box-Cox plot:\n\n    >>> x = stats.loggamma.rvs(5, size=500) + 5\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111)\n    >>> prob = stats.boxcox_normplot(x, -20, 20, plot=ax)\n\n    Determine and plot the optimal ``lmbda`` to transform ``x`` and plot it in\n    the same plot:\n\n    >>> _, maxlog = stats.boxcox(x)\n    >>> ax.axvline(maxlog, color='r')\n\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 1709, "code": "def _yeojohnson_transform(x, lmbda, xp=None):\n    xp = array_namespace(x) if xp is None else xp\n    dtype = xp_result_type(x, lmbda, force_floating=True, xp=xp)\n    eps = xp.finfo(dtype).eps\n    out = xp.zeros_like(x, dtype=dtype)\n    pos = x >= 0  # binary mask\n    if is_jax(xp):\n        return xp.select(\n            [(abs(lmbda) < eps) & pos, (abs(lmbda - 2) < eps) & ~pos, pos],\n            [xp.log1p(x), -xp.log1p(-x), xp.expm1(lmbda * xp.log1p(x)) / lmbda],\n            -xp.expm1((2 - lmbda) * xp.log1p(-x)) / (2 - lmbda),", "documentation": "    \"\"\"Returns `x` transformed by the Yeo-Johnson power transform with given\n    parameter `lmbda`.\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 1905, "code": "def yeojohnson_normmax(x, brack=None, *, nan_policy='propagate'):", "documentation": "    \"\"\"Compute optimal Yeo-Johnson transform parameter.\n\n    Compute optimal Yeo-Johnson transform parameter for input data, using\n    maximum likelihood estimation.\n\n    Parameters\n    ----------\n    x : array_like\n        Input array.\n    brack : 2-tuple, optional\n        The starting interval for a downhill bracket search with\n        `optimize.brent`. Note that this is in most cases not critical; the\n        final result is allowed to be outside this bracket. If None,\n        `optimize.fminbound` is used with bounds that avoid overflow.\n    nan_policy : {'propagate', 'omit', 'raise'}\n        Defines how to handle input NaNs.\n\n        - ``propagate``: if a NaN is present in the input, the output will be NaN.\n        - ``omit``: NaNs will be omitted when performing the calculation.\n        - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n\n    Returns\n    -------\n    maxlog : float\n        The optimal transform parameter found.\n\n    See Also\n    --------\n    yeojohnson, yeojohnson_llf, yeojohnson_normplot\n\n    Notes\n    -----\n    .. versionadded:: 1.2.0\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n\n    Generate some data and determine optimal ``lmbda``\n\n    >>> rng = np.random.default_rng()\n    >>> x = stats.loggamma.rvs(5, size=30, random_state=rng) + 5\n    >>> lmax = stats.yeojohnson_normmax(x)\n\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111)\n    >>> prob = stats.yeojohnson_normplot(x, -10, 10, plot=ax)\n    >>> ax.axvline(lmax, color='r')\n\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 2009, "code": "def yeojohnson_normplot(x, la, lb, plot=None, N=80):\n    return _normplot('yeojohnson', x, la, lb, plot, N)\nShapiroResult = namedtuple('ShapiroResult', ('statistic', 'pvalue'))\n@xp_capabilities(np_only=True)\n@_axis_nan_policy_factory(ShapiroResult, n_samples=1, too_small=2, default_axis=None)", "documentation": "    \"\"\"Compute parameters for a Yeo-Johnson normality plot, optionally show it.\n\n    A Yeo-Johnson normality plot shows graphically what the best\n    transformation parameter is to use in `yeojohnson` to obtain a\n    distribution that is close to normal.\n\n    Parameters\n    ----------\n    x : array_like\n        Input array.\n    la, lb : scalar\n        The lower and upper bounds for the ``lmbda`` values to pass to\n        `yeojohnson` for Yeo-Johnson transformations. These are also the\n        limits of the horizontal axis of the plot if that is generated.\n    plot : object, optional\n        If given, plots the quantiles and least squares fit.\n        `plot` is an object that has to have methods \"plot\" and \"text\".\n        The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,\n        or a custom object with the same methods.\n        Default is None, which means that no plot is created.\n    N : int, optional\n        Number of points on the horizontal axis (equally distributed from\n        `la` to `lb`).\n\n    Returns\n    -------\n    lmbdas : ndarray\n        The ``lmbda`` values for which a Yeo-Johnson transform was done.\n    ppcc : ndarray\n        Probability Plot Correlation Coefficient, as obtained from `probplot`\n        when fitting the Box-Cox transformed input `x` against a normal\n        distribution.\n\n    See Also\n    --------\n    probplot, yeojohnson, yeojohnson_normmax, yeojohnson_llf, ppcc_max\n\n    Notes\n    -----\n    Even if `plot` is given, the figure is not shown or saved by\n    `boxcox_normplot`; ``plt.show()`` or ``plt.savefig('figname.png')``\n    should be used after calling `probplot`.\n\n    .. versionadded:: 1.2.0\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n\n    Generate some non-normally distributed data, and create a Yeo-Johnson plot:\n\n    >>> x = stats.loggamma.rvs(5, size=500) + 5\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111)\n    >>> prob = stats.yeojohnson_normplot(x, -20, 20, plot=ax)\n\n    Determine and plot the optimal ``lmbda`` to transform ``x`` and plot it in\n    the same plot:\n\n    >>> _, maxlog = stats.yeojohnson(x)\n    >>> ax.axvline(maxlog, color='r')\n\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 2287, "code": "def anderson(x, dist='norm', *, method=None):\n    dist = dist.lower()\n    if dist in {'extreme1', 'gumbel'}:\n        dist = 'gumbel_l'\n    dists = {'norm', 'expon', 'gumbel_l',\n             'gumbel_r', 'logistic', 'weibull_min'}\n    if dist not in dists:\n        raise ValueError(f\"Invalid distribution; dist must be in {dists}.\")\n    y = sort(x)\n    xbar = np.mean(x, axis=0)\n    N = len(y)", "documentation": "    \"\"\"Anderson-Darling test for data coming from a particular distribution.\n\n    The Anderson-Darling test tests the null hypothesis that a sample is\n    drawn from a population that follows a particular distribution.\n    For the Anderson-Darling test, the critical values depend on\n    which distribution is being tested against.  This function works\n    for normal, exponential, logistic, weibull_min, or Gumbel (Extreme Value\n    Type I) distributions.\n\n    Parameters\n    ----------\n    x : array_like\n        Array of sample data.\n    dist : {'norm', 'expon', 'logistic', 'gumbel', 'gumbel_l', 'gumbel_r', 'extreme1', 'weibull_min'}, optional\n        The type of distribution to test against.  The default is 'norm'.\n        The names 'extreme1', 'gumbel_l' and 'gumbel' are synonyms for the\n        same distribution.\n    method : str or instance of `MonteCarloMethod`\n        Defines the method used to compute the p-value.\n        If `method` is ``\"interpolated\"``, the p-value is interpolated from\n        pre-calculated tables.\n        If `method` is an instance of `MonteCarloMethod`, the p-value is computed using\n        `scipy.stats.monte_carlo_test` with the provided configuration options and other\n        appropriate settings.\n\n        .. versionadded:: 1.17.0\n            If `method` is not specified, `anderson` will emit a ``FutureWarning``\n            specifying that the user must opt into a p-value calculation method.\n            When `method` is specified, the object returned will include a ``pvalue``\n            attribute, but no ``critical_value``, ``significance_level``, or\n            ``fit_result`` attributes. Beginning in 1.19.0, these other attributes will\n            no longer be available, and a p-value will always be computed according to\n            one of the available `method` options.\n\n    Returns\n    -------\n    result : AndersonResult\n        If `method` is provided, this is an object with the following attributes:\n\n        statistic : float\n            The Anderson-Darling test statistic.\n        pvalue: float\n            The p-value corresponding with the test statistic, calculated according to\n            the specified `method`.\n\n        If `method` is unspecified, this is an object with the following attributes:\n\n        statistic : float\n            The Anderson-Darling test statistic.\n        critical_values : list\n            The critical values for this distribution.\n        significance_level : list\n            The significance levels for the corresponding critical values\n            in percents.  The function returns critical values for a\n            differing set of significance levels depending on the\n            distribution that is being tested against.\n        fit_result : `~scipy.stats._result_classes.FitResult`\n            An object containing the results of fitting the distribution to\n            the data.\n\n        .. deprecated:: 1.17.0\n            The tuple-unpacking behavior of the return object and attributes\n            ``critical_values``, ``significance_level``, and ``fit_result`` are\n            deprecated. Beginning in SciPy 1.19.0, these features will no longer be\n            available, and the object returned will have attributes ``statistic`` and\n            ``pvalue``.\n\n    See Also\n    --------\n    kstest : The Kolmogorov-Smirnov test for goodness-of-fit.\n\n    Notes\n    -----\n    Critical values provided when `method` is unspecified are for the following\n    significance levels:\n\n    normal/exponential\n        15%, 10%, 5%, 2.5%, 1%\n    logistic\n        25%, 10%, 5%, 2.5%, 1%, 0.5%\n    gumbel_l / gumbel_r\n        25%, 10%, 5%, 2.5%, 1%\n    weibull_min\n        50%, 25%, 15%, 10%, 5%, 2.5%, 1%, 0.5%\n\n    If the returned statistic is larger than these critical values then\n    for the corresponding significance level, the null hypothesis that\n    the data come from the chosen distribution can be rejected.\n    The returned statistic is referred to as 'A2' in the references.\n\n    For `weibull_min`, maximum likelihood estimation is known to be\n    challenging. If the test returns successfully, then the first order\n    conditions for a maximum likelihood estimate have been verified and\n    the critical values correspond relatively well to the significance levels,\n    provided that the sample is sufficiently large (>10 observations [7]).\n    However, for some data - especially data with no left tail - `anderson`\n    is likely to result in an error message. In this case, consider\n    performing a custom goodness of fit test using\n    `scipy.stats.monte_carlo_test`.\n\n    References\n    ----------\n    .. [1] https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm\n    .. [2] Stephens, M. A. (1974). EDF Statistics for Goodness of Fit and\n           Some Comparisons, Journal of the American Statistical Association,\n           Vol. 69, pp. 730-737.\n    .. [3] Stephens, M. A. (1976). Asymptotic Results for Goodness-of-Fit\n           Statistics with Unknown Parameters, Annals of Statistics, Vol. 4,\n           pp. 357-369.\n    .. [4] Stephens, M. A. (1977). Goodness of Fit for the Extreme Value\n           Distribution, Biometrika, Vol. 64, pp. 583-588.\n    .. [5] Stephens, M. A. (1977). Goodness of Fit with Special Reference\n           to Tests for Exponentiality , Technical Report No. 262,\n           Department of Statistics, Stanford University, Stanford, CA.\n    .. [6] Stephens, M. A. (1979). Tests of Fit for the Logistic Distribution\n           Based on the Empirical Distribution Function, Biometrika, Vol. 66,\n           pp. 591-595.\n    .. [7] Richard A. Lockhart and Michael A. Stephens \"Estimation and Tests of\n           Fit for the Three-Parameter Weibull Distribution\"\n           Journal of the Royal Statistical Society.Series B(Methodological)\n           Vol. 56, No. 3 (1994), pp. 491-500, Table 0.\n    .. [8] D'Agostino, Ralph B. (1986). \"Tests for the Normal Distribution\".\n           In: Goodness-of-Fit Techniques. Ed. by Ralph B. D'Agostino and\n           Michael A. Stephens. New York: Marcel Dekker, pp. 122-141. ISBN:\n           0-8247-7487-6.\n\n    Examples\n    --------\n    Test the null hypothesis that a random sample was drawn from a normal\n    distribution (with unspecified mean and standard deviation).\n\n    >>> import numpy as np\n    >>> from scipy.stats import anderson\n    >>> rng = np.random.default_rng(9781234521)\n    >>> data = rng.random(size=35)\n    >>> res = anderson(data, dist='norm', method='interpolate')\n    >>> res.statistic\n    np.float64(0.9887620209957291)\n    >>> res.pvalue\n    np.float64(0.012111200538380142)\n\n    The p-value is approximately 0.012,, so the null hypothesis may be rejected\n    at a significance level of 2.5%, but not at a significance level of 1%.\n\n    \"\"\" # numpy/numpydoc#87  # noqa: E501"}, {"filename": "scipy/stats/_morestats.py", "start_line": 2555, "code": "def _anderson_ksamp_continuous(samples, Z, Zstar, k, n, N):\n    A2kN = 0.\n    j = np.arange(1, N)\n    for i in arange(0, k):\n        s = np.sort(samples[i])\n        Mij = s.searchsorted(Z[:-1], side='right')\n        inner = (N*Mij - j*n[i])**2 / (j * (N - j))\n        A2kN += inner.sum() / n[i]\n    return A2kN / N", "documentation": "    \"\"\"Compute A2akN equation 3 of Scholz & Stephens.\n\n    Parameters\n    ----------\n    samples : sequence of 1-D array_like\n        Array of sample arrays.\n    Z : array_like\n        Sorted array of all observations.\n    Zstar : array_like\n        Sorted array of unique observations. Unused.\n    k : int\n        Number of samples.\n    n : array_like\n        Number of observations in each sample.\n    N : int\n        Total number of observations.\n\n    Returns\n    -------\n    A2KN : float\n        The A2KN statistics of Scholz and Stephens 1987.\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 2590, "code": "def _anderson_ksamp_midrank(samples, Z, Zstar, k, n, N):\n    A2akN = 0.\n    Z_ssorted_left = Z.searchsorted(Zstar, 'left')\n    if N == Zstar.size:\n        lj = 1.\n    else:\n        lj = Z.searchsorted(Zstar, 'right') - Z_ssorted_left\n    Bj = Z_ssorted_left + lj / 2.\n    for i in arange(0, k):\n        s = np.sort(samples[i])\n        s_ssorted_right = s.searchsorted(Zstar, side='right')", "documentation": "    \"\"\"Compute A2akN equation 7 of Scholz and Stephens.\n\n    Parameters\n    ----------\n    samples : sequence of 1-D array_like\n        Array of sample arrays.\n    Z : array_like\n        Sorted array of all observations.\n    Zstar : array_like\n        Sorted array of unique observations.\n    k : int\n        Number of samples.\n    n : array_like\n        Number of observations in each sample.\n    N : int\n        Total number of observations.\n\n    Returns\n    -------\n    A2aKN : float\n        The A2aKN statistics of Scholz and Stephens 1987.\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 2633, "code": "def _anderson_ksamp_right(samples, Z, Zstar, k, n, N):\n    A2kN = 0.\n    lj = Z.searchsorted(Zstar[:-1], 'right') - Z.searchsorted(Zstar[:-1],\n                                                              'left')\n    Bj = lj.cumsum()\n    for i in arange(0, k):\n        s = np.sort(samples[i])\n        Mij = s.searchsorted(Zstar[:-1], side='right')\n        inner = lj / float(N) * (N * Mij - Bj * n[i])**2 / (Bj * (N - Bj))\n        A2kN += inner.sum() / n[i]\n    return A2kN", "documentation": "    \"\"\"Compute A2akN equation 6 of Scholz & Stephens.\n\n    Parameters\n    ----------\n    samples : sequence of 1-D array_like\n        Array of sample arrays.\n    Z : array_like\n        Sorted array of all observations.\n    Zstar : array_like\n        Sorted array of unique observations.\n    k : int\n        Number of samples.\n    n : array_like\n        Number of observations in each sample.\n    N : int\n        Total number of observations.\n\n    Returns\n    -------\n    A2KN : float\n        The A2KN statistics of Scholz and Stephens 1987.\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 2676, "code": "def anderson_ksamp(samples, midrank=_NoValue, *, variant=_NoValue, method=None):\n    k = len(samples)\n    if (k < 2):\n        raise ValueError(\"anderson_ksamp needs at least two samples\")\n    samples = list(map(np.asarray, samples))\n    Z = np.sort(np.hstack(samples))\n    N = Z.size\n    Zstar = np.unique(Z)\n    if Zstar.size < 2:\n        raise ValueError(\"anderson_ksamp needs more than one distinct \"\n                         \"observation\")", "documentation": "    \"\"\"The Anderson-Darling test for k-samples.\n\n    The k-sample Anderson-Darling test is a modification of the\n    one-sample Anderson-Darling test. It tests the null hypothesis\n    that k-samples are drawn from the same population without having\n    to specify the distribution function of that population. The\n    critical values depend on the number of samples.\n\n    Parameters\n    ----------\n    samples : sequence of 1-D array_like\n        Array of sample data in arrays.\n    midrank : bool, optional\n        Variant of Anderson-Darling test which is computed. Default\n        (True) is the midrank test applicable to continuous and\n        discrete populations. If False, the right side empirical\n        distribution is used.\n\n        .. deprecated:: 1.17.0\n            Use parameter `variant` instead.\n    variant : {'midrank', 'right', 'continuous'}\n        Variant of Anderson-Darling test to be computed. ``'midrank'`` is applicable\n        to both continuous and discrete populations. ``'discrete'`` and ``'continuous'``\n        perform alternative versions of the test for discrete  and continuous\n        populations, respectively.\n        When `variant` is specified, the return object will not be unpackable as a\n        tuple, and only attributes ``statistic`` and ``pvalue`` will be present.\n    method : PermutationMethod, optional\n        Defines the method used to compute the p-value. If `method` is an\n        instance of `PermutationMethod`, the p-value is computed using\n        `scipy.stats.permutation_test` with the provided configuration options\n        and other appropriate settings. Otherwise, the p-value is interpolated\n        from tabulated values.\n\n    Returns\n    -------\n    res : Anderson_ksampResult\n        An object containing attributes:\n\n        statistic : float\n            Normalized k-sample Anderson-Darling test statistic.\n        critical_values : array\n            The critical values for significance levels 25%, 10%, 5%, 2.5%, 1%,\n            0.5%, 0.1%.\n\n            .. deprecated:: 1.17.0\n                 Present only when `variant` is unspecified.\n\n        pvalue : float\n            The approximate p-value of the test. If `method` is not\n            provided, the value is floored / capped at 0.1% / 25%.\n\n    Raises\n    ------\n    ValueError\n        If fewer than 2 samples are provided, a sample is empty, or no\n        distinct observations are in the samples.\n\n    See Also\n    --------\n    ks_2samp : 2 sample Kolmogorov-Smirnov test\n    anderson : 1 sample Anderson-Darling test\n\n    Notes\n    -----\n    [1]_ defines three versions of the k-sample Anderson-Darling test:\n    one for continuous distributions and two for discrete\n    distributions, in which ties between samples may occur. The\n    default of this routine is to compute the version based on the\n    midrank empirical distribution function. This test is applicable\n    to continuous and discrete data. If `variant` is set to ``'discrete'``, the\n    right side empirical distribution is used for a test for discrete\n    data; if `variant` is ``'continuous'``, the same test statistic and p-value are\n    computed for data with no ties, but with less computation. According to [1]_,\n    the two discrete test statistics differ only slightly if a few collisions due\n    to round-off errors occur in the test not adjusted for ties between samples.\n\n    The critical values corresponding to the significance levels from 0.01\n    to 0.25 are taken from [1]_. p-values are floored / capped\n    at 0.1% / 25%. Since the range of critical values might be extended in\n    future releases, it is recommended not to test ``p == 0.25``, but rather\n    ``p >= 0.25`` (analogously for the lower bound).\n\n    .. versionadded:: 0.14.0\n\n    References\n    ----------\n    .. [1] Scholz, F. W and Stephens, M. A. (1987), K-Sample\n           Anderson-Darling Tests, Journal of the American Statistical\n           Association, Vol. 82, pp. 918-924.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng(44925884305279435)\n    >>> res = stats.anderson_ksamp([rng.normal(size=50), rng.normal(loc=0.5, size=30)],\n    ...                            variant='midrank')\n    >>> res.statistic, res.pvalue\n    (3.4444310693448936, 0.013106682406720973)\n\n    The null hypothesis that the two random samples come from the same\n    distribution can be rejected at the 5% level because the returned\n    p-value is less than 0.05, but not at the 1% level.\n\n    >>> samples = [rng.normal(size=50), rng.normal(size=30),\n    ...            rng.normal(size=20)]\n    >>> res = stats.anderson_ksamp(samples, variant='continuous')\n    >>> res.statistic, res.pvalue\n    (-0.6309662273193832, 0.25)\n\n    As we might expect, the null hypothesis cannot be rejected here for three samples\n    from an identical distribution. The reported p-value (25%) has been capped at the\n    maximum value for which pre-computed p-values are available.\n\n    In such cases where the p-value is capped or when sample sizes are\n    small, a permutation test may be more accurate.\n\n    >>> method = stats.PermutationMethod(n_resamples=9999, random_state=rng)\n    >>> res = stats.anderson_ksamp(samples, variant='continuous', method=method)\n    >>> res.pvalue\n    0.699\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 2904, "code": "class _ABW:", "documentation": "    \"\"\"Distribution of Ansari-Bradley W-statistic under the null hypothesis.\"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 2910, "code": "    def __init__(self):\n        self.m = None\n        self.n = None\n        self.astart = None\n        self.total = None\n        self.freqs = None", "documentation": "        \"\"\"Minimal initializer.\"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 2918, "code": "    def _recalc(self, n, m):\n        if n != self.n or m != self.m:\n            self.n, self.m = n, m\n            astart, a1, _ = gscale(n, m)\n            self.astart = astart  # minimum value of statistic\n            self.freqs = a1.astype(np.float64)\n            self.total = self.freqs.sum()  # could calculate from m and n", "documentation": "        \"\"\"When necessary, recalculate exact distribution.\"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 2933, "code": "    def pmf(self, k, n, m):\n        self._recalc(n, m)\n        ind = np.floor(k - self.astart).astype(int)\n        return self.freqs[ind] / self.total", "documentation": "        \"\"\"Probability mass function.\"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 2941, "code": "    def cdf(self, k, n, m):\n        self._recalc(n, m)\n        ind = np.ceil(k - self.astart).astype(int)\n        return self.freqs[:ind+1].sum() / self.total", "documentation": "        \"\"\"Cumulative distribution function.\"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 2949, "code": "    def sf(self, k, n, m):\n        self._recalc(n, m)\n        ind = np.floor(k - self.astart).astype(int)\n        return self.freqs[ind:].sum() / self.total\n_abw_state = threading.local()\n@xp_capabilities(cpu_only=True, jax_jit=False,    # p-value is Cython\n                 skip_backends=[('dask.array', 'no rankdata')])\n@_axis_nan_policy_factory(AnsariResult, n_samples=2)", "documentation": "        \"\"\"Survival function.\"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 2967, "code": "def ansari(x, y, alternative='two-sided', *, axis=0):\n    xp = array_namespace(x, y)\n    dtype = xp_result_type(x, y, force_floating=True, xp=xp)\n    if alternative not in {'two-sided', 'greater', 'less'}:\n        raise ValueError(\"'alternative' must be 'two-sided',\"\n                         \" 'greater', or 'less'.\")\n    if not hasattr(_abw_state, 'a'):\n        _abw_state.a = _ABW()\n    n = x.shape[-1]\n    m = y.shape[-1]\n    if m < 1:  # needed by test_axis_nan_policy; not user-facing", "documentation": "    \"\"\"Perform the Ansari-Bradley test for equal scale parameters.\n\n    The Ansari-Bradley test ([1]_, [2]_) is a non-parametric test\n    for the equality of the scale parameter of the distributions\n    from which two samples were drawn. The null hypothesis states that\n    the ratio of the scale of the distribution underlying `x` to the scale\n    of the distribution underlying `y` is 1.\n\n    Parameters\n    ----------\n    x, y : array_like\n        Arrays of sample data.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n        The following options are available:\n\n        * 'two-sided': the ratio of scales is not equal to 1.\n        * 'less': the ratio of scales is less than 1.\n        * 'greater': the ratio of scales is greater than 1.\n\n        .. versionadded:: 1.7.0\n    axis : int or tuple of ints, default: 0\n        If an int or tuple of ints, the axis or axes of the input along which\n        to compute the statistic. The statistic of each axis-slice (e.g. row)\n        of the input will appear in a corresponding element of the output.\n        If ``None``, the input will be raveled before computing the statistic.\n\n    Returns\n    -------\n    statistic : float\n        The Ansari-Bradley test statistic.\n    pvalue : float\n        The p-value of the hypothesis test.\n\n    See Also\n    --------\n    fligner : A non-parametric test for the equality of k variances\n    mood : A non-parametric test for the equality of two scale parameters\n\n    Notes\n    -----\n    The p-value given is exact when the sample sizes are both less than\n    55 and there are no ties, otherwise a normal approximation for the\n    p-value is used.\n\n    References\n    ----------\n    .. [1] Ansari, A. R. and Bradley, R. A. (1960) Rank-sum tests for\n           dispersions, Annals of Mathematical Statistics, 31, 1174-1189.\n    .. [2] Sprent, Peter and N.C. Smeeton.  Applied nonparametric\n           statistical methods.  3rd ed. Chapman and Hall/CRC. 2001.\n           Section 5.8.2.\n    .. [3] Nathaniel E. Helwig \"Nonparametric Dispersion and Equality\n           Tests\" at http://users.stat.umn.edu/~helwig/notes/npde-Notes.pdf\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import ansari\n    >>> rng = np.random.default_rng()\n\n    For these examples, we'll create three random data sets.  The first\n    two, with sizes 35 and 25, are drawn from a normal distribution with\n    mean 0 and standard deviation 2.  The third data set has size 25 and\n    is drawn from a normal distribution with standard deviation 1.25.\n\n    >>> x1 = rng.normal(loc=0, scale=2, size=35)\n    >>> x2 = rng.normal(loc=0, scale=2, size=25)\n    >>> x3 = rng.normal(loc=0, scale=1.25, size=25)\n\n    First we apply `ansari` to `x1` and `x2`.  These samples are drawn\n    from the same distribution, so we expect the Ansari-Bradley test\n    should not lead us to conclude that the scales of the distributions\n    are different.\n\n    >>> ansari(x1, x2)\n    AnsariResult(statistic=541.0, pvalue=0.9762532927399098)\n\n    With a p-value close to 1, we cannot conclude that there is a\n    significant difference in the scales (as expected).\n\n    Now apply the test to `x1` and `x3`:\n\n    >>> ansari(x1, x3)\n    AnsariResult(statistic=425.0, pvalue=0.0003087020407974518)\n\n    The probability of observing such an extreme value of the statistic\n    under the null hypothesis of equal scales is only 0.03087%. We take this\n    as evidence against the null hypothesis in favor of the alternative:\n    the scales of the distributions from which the samples were drawn\n    are not equal.\n\n    We can use the `alternative` parameter to perform a one-tailed test.\n    In the above example, the scale of `x1` is greater than `x3` and so\n    the ratio of scales of `x1` and `x3` is greater than 1. This means\n    that the p-value when ``alternative='greater'`` should be near 0 and\n    hence we should be able to reject the null hypothesis:\n\n    >>> ansari(x1, x3, alternative='greater')\n    AnsariResult(statistic=425.0, pvalue=0.0001543510203987259)\n\n    As we can see, the p-value is indeed quite low. Use of\n    ``alternative='less'`` should thus yield a large p-value:\n\n    >>> ansari(x1, x3, alternative='less')\n    AnsariResult(statistic=425.0, pvalue=0.9998643258449039)\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 3644, "code": "def mood(x, y, axis=0, alternative=\"two-sided\"):\n    xp = array_namespace(x, y)\n    x, y = xp_promote(x, y, force_floating=True, xp=xp)\n    xy = xp.concat((x, y), axis=-1)\n    m = x.shape[-1]\n    n = y.shape[-1]\n    N = m + n\n    if m == 0 or n == 0 or N < 3:  # only needed for test_axis_nan_policy\n        NaN = _get_nan(x, y, xp=xp)\n        return SignificanceResult(NaN, NaN)\n    r, t = _stats_py._rankdata(xy, method='average', return_ties=True)", "documentation": "    \"\"\"Perform Mood's test for equal scale parameters.\n\n    Mood's two-sample test for scale parameters is a non-parametric\n    test for the null hypothesis that two samples are drawn from the\n    same distribution with the same scale parameter.\n\n    Parameters\n    ----------\n    x, y : array_like\n        Arrays of sample data. There must be at least three observations\n        total.\n    axis : int, optional\n        The axis along which the samples are tested.  `x` and `y` can be of\n        different length along `axis`.\n        If `axis` is None, `x` and `y` are flattened and the test is done on\n        all values in the flattened arrays.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n        The following options are available:\n\n        * 'two-sided': the scales of the distributions underlying `x` and `y`\n          are different.\n        * 'less': the scale of the distribution underlying `x` is less than\n          the scale of the distribution underlying `y`.\n        * 'greater': the scale of the distribution underlying `x` is greater\n          than the scale of the distribution underlying `y`.\n\n        .. versionadded:: 1.7.0\n\n    Returns\n    -------\n    res : SignificanceResult\n        An object containing attributes:\n\n        statistic : scalar or ndarray\n            The z-score for the hypothesis test.  For 1-D inputs a scalar is\n            returned.\n        pvalue : scalar ndarray\n            The p-value for the hypothesis test.\n\n    See Also\n    --------\n    fligner : A non-parametric test for the equality of k variances\n    ansari : A non-parametric test for the equality of 2 variances\n    bartlett : A parametric test for equality of k variances in normal samples\n    levene : A parametric test for equality of k variances\n\n    Notes\n    -----\n    The data are assumed to be drawn from probability distributions ``f(x)``\n    and ``f(x/s) / s`` respectively, for some probability density function f.\n    The null hypothesis is that ``s == 1``.\n\n    For multi-dimensional arrays, if the inputs are of shapes\n    ``(n0, n1, n2, n3)``  and ``(n0, m1, n2, n3)``, then if ``axis=1``, the\n    resulting z and p values will have shape ``(n0, n2, n3)``.  Note that\n    ``n1`` and ``m1`` don't have to be equal, but the other dimensions do.\n\n    References\n    ----------\n    [1] Mielke, Paul W. \"Note on Some Squared Rank Tests with Existing Ties.\"\n        Technometrics, vol. 9, no. 2, 1967, pp. 312-14. JSTOR,\n        :doi:`10.2307/1266427`. Accessed 18 May 2022.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> x2 = rng.standard_normal((2, 45, 6, 7))\n    >>> x1 = rng.standard_normal((2, 30, 6, 7))\n    >>> res = stats.mood(x1, x2, axis=1)\n    >>> res.pvalue.shape\n    (2, 6, 7)\n\n    Find the number of points where the difference in scale is not significant:\n\n    >>> (res.pvalue > 0.1).sum()\n    78\n\n    Perform the test with different scales:\n\n    >>> x1 = rng.standard_normal((2, 30))\n    >>> x2 = rng.standard_normal((2, 35)) * 10.0\n    >>> stats.mood(x1, x2, axis=1)\n    SignificanceResult(statistic=array([-5.76174136, -6.12650783]),\n                       pvalue=array([8.32505043e-09, 8.98287869e-10]))\n\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 4591, "code": "def directional_stats(samples, *, axis=0, normalize=True):\n    xp = array_namespace(samples)\n    samples = xp.asarray(samples)\n    if samples.ndim < 2:\n        raise ValueError(\"samples must at least be two-dimensional. \"\n                         f\"Instead samples has shape: {tuple(samples.shape)}\")\n    samples = xp.moveaxis(samples, axis, 0)\n    if is_marray(xp):\n        _xp = array_namespace(samples.mask)\n        mask = _xp.any(samples.mask, axis=-1, keepdims=True)\n        samples = xp.asarray(samples.data, mask=mask)", "documentation": "    \"\"\"\n    Computes sample statistics for directional data.\n\n    Computes the directional mean (also called the mean direction vector) and\n    mean resultant length of a sample of vectors.\n\n    The directional mean is a measure of \"preferred direction\" of vector data.\n    It is analogous to the sample mean, but it is for use when the length of\n    the data is irrelevant (e.g. unit vectors).\n\n    The mean resultant length is a value between 0 and 1 used to quantify the\n    dispersion of directional data: the smaller the mean resultant length, the\n    greater the dispersion. Several definitions of directional variance\n    involving the mean resultant length are given in [1]_ and [2]_.\n\n    Parameters\n    ----------\n    samples : array_like\n        Input array. Must be at least two-dimensional, and the last axis of the\n        input must correspond with the dimensionality of the vector space.\n        When the input is exactly two dimensional, this means that each row\n        of the data is a vector observation.\n    axis : int, default: 0\n        Axis along which the directional mean is computed.\n    normalize : bool, default: True\n        If True, normalize the input to ensure that each observation is a\n        unit vector. It the observations are already unit vectors, consider\n        setting this to False to avoid unnecessary computation.\n\n    Returns\n    -------\n    res : DirectionalStats\n        An object containing attributes:\n\n        mean_direction : ndarray\n            Directional mean.\n        mean_resultant_length : ndarray\n            The mean resultant length [1]_.\n\n    See Also\n    --------\n    circmean: circular mean; i.e. directional mean for 2D *angles*\n    circvar: circular variance; i.e. directional variance for 2D *angles*\n\n    Notes\n    -----\n    This uses a definition of directional mean from [1]_.\n    Assuming the observations are unit vectors, the calculation is as follows.\n\n    .. code-block:: python\n\n        mean = samples.mean(axis=0)\n        mean_resultant_length = np.linalg.norm(mean)\n        mean_direction = mean / mean_resultant_length\n\n    This definition is appropriate for *directional* data (i.e. vector data\n    for which the magnitude of each observation is irrelevant) but not\n    for *axial* data (i.e. vector data for which the magnitude and *sign* of\n    each observation is irrelevant).\n\n    Several definitions of directional variance involving the mean resultant\n    length ``R`` have been proposed, including ``1 - R`` [1]_, ``1 - R**2``\n    [2]_, and ``2 * (1 - R)`` [2]_. Rather than choosing one, this function\n    returns ``R`` as attribute `mean_resultant_length` so the user can compute\n    their preferred measure of dispersion.\n\n    References\n    ----------\n    .. [1] Mardia, Jupp. (2000). *Directional Statistics*\n       (p. 163). Wiley.\n\n    .. [2] https://en.wikipedia.org/wiki/Directional_statistics\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import directional_stats\n    >>> data = np.array([[3, 4],    # first observation, 2D vector space\n    ...                  [6, -8]])  # second observation\n    >>> dirstats = directional_stats(data)\n    >>> dirstats.mean_direction\n    array([1., 0.])\n\n    In contrast, the regular sample mean of the vectors would be influenced\n    by the magnitude of each observation. Furthermore, the result would not be\n    a unit vector.\n\n    >>> data.mean(axis=0)\n    array([4.5, -2.])\n\n    An exemplary use case for `directional_stats` is to find a *meaningful*\n    center for a set of observations on a sphere, e.g. geographical locations.\n\n    >>> data = np.array([[0.8660254, 0.5, 0.],\n    ...                  [0.8660254, -0.5, 0.]])\n    >>> dirstats = directional_stats(data)\n    >>> dirstats.mean_direction\n    array([1., 0., 0.])\n\n    The regular sample mean on the other hand yields a result which does not\n    lie on the surface of the sphere.\n\n    >>> data.mean(axis=0)\n    array([0.8660254, 0., 0.])\n\n    The function also returns the mean resultant length, which\n    can be used to calculate a directional variance. For example, using the\n    definition ``Var(z) = 1 - R`` from [2]_ where ``R`` is the\n    mean resultant length, we can calculate the directional variance of the\n    vectors in the above example as:\n\n    >>> 1 - dirstats.mean_resultant_length\n    0.13397459716167093\n    \"\"\""}, {"filename": "scipy/stats/_morestats.py", "start_line": 4731, "code": "def false_discovery_control(ps, *, axis=0, method='bh'):\n    xp = array_namespace(ps)\n    ps = xp.asarray(ps)\n    if not xp.isdtype(ps.dtype, (\"integral\", \"real floating\")):\n        raise ValueError(\"`ps` must contain only real numbers.\")\n    if is_lazy_array(ps):\n        ps = xp.where((ps < 0.) | (ps > 1.), xp.nan, ps)\n    else:\n        if not xp.all(ps == xp.clip(ps, 0., 1.)):\n            raise ValueError(\"All values in `ps` must lie between 0. and 1.\")\n    methods = {'bh', 'by'}", "documentation": "    \"\"\"Adjust p-values to control the false discovery rate.\n\n    The false discovery rate (FDR) is the expected proportion of rejected null\n    hypotheses that are actually true.\n    If the null hypothesis is rejected when the *adjusted* p-value falls below\n    a specified level, the false discovery rate is controlled at that level.\n\n    Parameters\n    ----------\n    ps : 1D array_like\n        The p-values to adjust. Elements must be real numbers between 0 and 1.\n    axis : int\n        The axis along which to perform the adjustment. The adjustment is\n        performed independently along each axis-slice. If `axis` is None, `ps`\n        is raveled before performing the adjustment.\n    method : {'bh', 'by'}\n        The false discovery rate control procedure to apply: ``'bh'`` is for\n        Benjamini-Hochberg [1]_ (Eq. 1), ``'by'`` is for Benjaminini-Yekutieli\n        [2]_ (Theorem 1.3). The latter is more conservative, but it is\n        guaranteed to control the FDR even when the p-values are not from\n        independent tests.\n\n    Returns\n    -------\n    ps_adusted : array_like\n        The adjusted p-values. If the null hypothesis is rejected where these\n        fall below a specified level, the false discovery rate is controlled\n        at that level.\n\n    See Also\n    --------\n    combine_pvalues\n    statsmodels.stats.multitest.multipletests\n\n    Notes\n    -----\n    In multiple hypothesis testing, false discovery control procedures tend to\n    offer higher power than familywise error rate control procedures (e.g.\n    Bonferroni correction [1]_).\n\n    If the p-values correspond with independent tests (or tests with\n    \"positive regression dependencies\" [2]_), rejecting null hypotheses\n    corresponding with Benjamini-Hochberg-adjusted p-values below :math:`q`\n    controls the false discovery rate at a level less than or equal to\n    :math:`q m_0 / m`, where :math:`m_0` is the number of true null hypotheses\n    and :math:`m` is the total number of null hypotheses tested. The same is\n    true even for dependent tests when the p-values are adjusted accorded to\n    the more conservative Benjaminini-Yekutieli procedure.\n\n    The adjusted p-values produced by this function are comparable to those\n    produced by the R function ``p.adjust`` and the statsmodels function\n    `statsmodels.stats.multitest.multipletests`. Please consider the latter\n    for more advanced methods of multiple comparison correction.\n\n    References\n    ----------\n    .. [1] Benjamini, Yoav, and Yosef Hochberg. \"Controlling the false\n           discovery rate: a practical and powerful approach to multiple\n           testing.\" Journal of the Royal statistical society: series B\n           (Methodological) 57.1 (1995): 289-300.\n\n    .. [2] Benjamini, Yoav, and Daniel Yekutieli. \"The control of the false\n           discovery rate in multiple testing under dependency.\" Annals of\n           statistics (2001): 1165-1188.\n\n    .. [3] TileStats. FDR - Benjamini-Hochberg explained - Youtube.\n           https://www.youtube.com/watch?v=rZKa4tW2NKs.\n\n    .. [4] Neuhaus, Karl-Ludwig, et al. \"Improved thrombolysis in acute\n           myocardial infarction with front-loaded administration of alteplase:\n           results of the rt-PA-APSAC patency study (TAPS).\" Journal of the\n           American College of Cardiology 19.5 (1992): 885-891.\n\n    Examples\n    --------\n    We follow the example from [1]_.\n\n        Thrombolysis with recombinant tissue-type plasminogen activator (rt-PA)\n        and anisoylated plasminogen streptokinase activator (APSAC) in\n        myocardial infarction has been proved to reduce mortality. [4]_\n        investigated the effects of a new front-loaded administration of rt-PA\n        versus those obtained with a standard regimen of APSAC, in a randomized\n        multicentre trial in 421 patients with acute myocardial infarction.\n\n    There were four families of hypotheses tested in the study, the last of\n    which was \"cardiac and other events after the start of thrombolitic\n    treatment\". FDR control may be desired in this family of hypotheses\n    because it would not be appropriate to conclude that the front-loaded\n    treatment is better if it is merely equivalent to the previous treatment.\n\n    The p-values corresponding with the 15 hypotheses in this family were\n\n    >>> ps = [0.0001, 0.0004, 0.0019, 0.0095, 0.0201, 0.0278, 0.0298, 0.0344,\n    ...       0.0459, 0.3240, 0.4262, 0.5719, 0.6528, 0.7590, 1.000]\n\n    If the chosen significance level is 0.05, we may be tempted to reject the\n    null hypotheses for the tests corresponding with the first nine p-values,\n    as the first nine p-values fall below the chosen significance level.\n    However, this would ignore the problem of \"multiplicity\": if we fail to\n    correct for the fact that multiple comparisons are being performed, we\n    are more likely to incorrectly reject true null hypotheses.\n\n    One approach to the multiplicity problem is to control the family-wise\n    error rate (FWER), that is, the rate at which the null hypothesis is\n    rejected when it is actually true. A common procedure of this kind is the\n    Bonferroni correction [1]_.  We begin by multiplying the p-values by the\n    number of hypotheses tested.\n\n    >>> import numpy as np\n    >>> np.array(ps) * len(ps)\n    array([1.5000e-03, 6.0000e-03, 2.8500e-02, 1.4250e-01, 3.0150e-01,\n           4.1700e-01, 4.4700e-01, 5.1600e-01, 6.8850e-01, 4.8600e+00,\n           6.3930e+00, 8.5785e+00, 9.7920e+00, 1.1385e+01, 1.5000e+01])\n\n    To control the FWER at 5%, we reject only the hypotheses corresponding\n    with adjusted p-values less than 0.05. In this case, only the hypotheses\n    corresponding with the first three p-values can be rejected. According to\n    [1]_, these three hypotheses concerned \"allergic reaction\" and \"two\n    different aspects of bleeding.\"\n\n    An alternative approach is to control the false discovery rate: the\n    expected fraction of rejected null hypotheses that are actually true. The\n    advantage of this approach is that it typically affords greater power: an\n    increased rate of rejecting the null hypothesis when it is indeed false. To\n    control the false discovery rate at 5%, we apply the Benjamini-Hochberg\n    p-value adjustment.\n\n    >>> from scipy import stats\n    >>> stats.false_discovery_control(ps)\n    array([0.0015    , 0.003     , 0.0095    , 0.035625  , 0.0603    ,\n           0.06385714, 0.06385714, 0.0645    , 0.0765    , 0.486     ,\n           0.58118182, 0.714875  , 0.75323077, 0.81321429, 1.        ])\n\n    Now, the first *four* adjusted p-values fall below 0.05, so we would reject\n    the null hypotheses corresponding with these *four* p-values. Rejection\n    of the fourth null hypothesis was particularly important to the original\n    study as it led to the conclusion that the new treatment had a\n    \"substantially lower in-hospital mortality rate.\"\n\n    For simplicity of exposition, the p-values in the example above were given in\n    sorted order, but this is not required; `false_discovery_control` returns\n    adjusted p-values in order corresponding with the input `ps`.\n\n    >>> stats.false_discovery_control([0.5, 0.6, 0.1, 0.001])\n    array([0.6  , 0.6  , 0.2  , 0.004])\n\n    \"\"\""}]}
{"repository": "scipy/scipy", "commit_sha": "98307e9d953289d30edb1fad6799a86c46476558", "commit_message": "MAINT: stats.rankdata: consistently return floating point dtype (#24420)\n\n* MAINT: stats.rankdata: always output floating point dtype\n\n* MAINT: stats: avoid unnecessary dtype conversions\n\n* DOC: stats.rankdata: update documentation to reflect new output dtype\n\n* MAINT: stats.ks_2samp: update after change in rankdata dtype\n\n* TST: stats.mood: adjust tolerance to address failing test", "commit_date": "2026-02-03T07:12:12+00:00", "author": "Matt Haberland", "file": "scipy/stats/_stats_py.py", "patch": "@@ -81,7 +81,6 @@\n     _length_nonmasked,\n     _share_masks,\n     xp_swapaxes,\n-    xp_default_dtype,\n     xp_device,\n )\n import scipy._external.array_api_extra as xpx\n@@ -8019,6 +8018,7 @@ def ks_2samp(data1, data2, alternative='two-sided', method='auto', *, axis=0):\n     # These counts are given by the differences between consecutive (\"min\" or \"max\")\n     # ranks corresponding with the observations in the (sorted) samples.\n     ranks, data_all = _rankdata(data_all, method='min', return_sorted=True)  # axis=-1\n+    ranks = xp.astype(ranks, xp.asarray(1).dtype)  # default int type\n     one = xp.ones((*ranks.shape[:-1], 1), dtype=ranks.dtype, device=xp_device(ranks))\n     cdf1_counts = xp.diff(ranks[..., :n1], prepend=one, append=n + one, axis=-1)\n     cdf2_counts = xp.diff(ranks[..., -n2:], prepend=one, append=n + one, axis=-1)\n@@ -8562,9 +8562,6 @@ def kruskal(*samples, nan_policy='propagate', axis=0):\n \n     alldata = xp.concat(samples, axis=-1)\n     ranked, t = _rankdata(alldata, method='average', return_ties=True)\n-    # should adjust output dtype of _rankdata\n-    ranked = xp.astype(ranked, alldata.dtype, copy=False)\n-    t = xp.astype(t, alldata.dtype, copy=False)\n     ties = 1 - xp.sum(t**3 - t, axis=-1) / (totaln**3 - totaln)  # tiecorrect(ranked)\n \n     # Compute sum^2/n for each group and sum\n@@ -8667,7 +8664,6 @@ def friedmanchisquare(*samples, axis=0):\n     # reducing statistic, so both axes 0 and -1 are consumed.\n     data = xp_swapaxes(xp.stack(samples), 0, -1)\n     data, t = _rankdata(data, method='average', return_ties=True)\n-    data, t = xp.asarray(data, dtype=dtype), xp.asarray(t, dtype=dtype)\n \n     # Handle ties\n     ties = xp.sum(t * (t*t - 1), axis=(0, -1))\n@@ -10044,7 +10040,7 @@ def rankdata(a, method='average', *, axis=None, nan_policy='propagate'):\n     -------\n     ranks : ndarray\n          An array of size equal to the size of `a`, containing rank\n-         scores.\n+         scores. The dtype is the result dtype of `a` and a Python float.\n \n     References\n     ----------\n@@ -10055,18 +10051,18 @@ def rankdata(a, method='average', *, axis=None, nan_policy='propagate'):\n     >>> import numpy as np\n     >>> from scipy.stats import rankdata\n     >>> rankdata([0, 2, 3, 2])\n-    array([ 1. ,  2.5,  4. ,  2.5])\n+    array([1. , 2.5, 4. , 2.5])\n     >>> rankdata([0, 2, 3, 2], method='min')\n-    array([ 1,  2,  4,  2])\n+    array([1., 2., 4., 2.])\n     >>> rankdata([0, 2, 3, 2], method='max')\n-    array([ 1,  3,  4,  3])\n+    array([1., 3., 4., 3.])\n     >>> rankdata([0, 2, 3, 2], method='dense')\n-    array([ 1,  2,  3,  2])\n+    array([1., 2., 3., 2.])\n     >>> rankdata([0, 2, 3, 2], method='ordinal')\n-    array([ 1,  2,  4,  3])\n-    >>> rankdata([[0, 2], [3, 2]]).reshape(2,2)\n+    array([1., 2., 4., 3.])\n+    >>> rankdata([[0, 2], [3, 2]]).reshape(2, 2)\n     array([[1. , 2.5],\n-          [4. , 2.5]])\n+           [4. , 2.5]])\n     >>> rankdata([[0, 2, 2], [3, 2, 5]], axis=1)\n     array([[1. , 2.5, 2.5],\n            [2. , 1. , 3. ]])\n@@ -10088,24 +10084,20 @@ def rankdata(a, method='average', *, axis=None, nan_policy='propagate'):\n         axis = -1\n \n     if xp_size(x) == 0:\n-        dtype = xp.asarray(1.).dtype if method == 'average' else xp.asarray(1).dtype\n-        return xp.empty(x.shape, dtype=dtype)\n+        dtype = xp_result_type(x, force_floating=True, xp=xp)\n+        return xp.empty_like(x, dtype=dtype)\n \n     contains_nan = _contains_nan(x, nan_policy)\n \n     x = xp_swapaxes(x, axis, -1, xp=xp)\n     ranks = _rankdata(x, method, xp=xp)\n \n-    # JIT won't allow use of `contains_nan` for control flow here, so we have to choose\n-    # whether to always or never run this block with JIT.\n-    # For now, *never* run it; otherwise, it would change dtype of `ranks`.\n-    # When gh-19889 is resolved, dtype will already be `float`, so *always* run it.\n-    # TODO then: broadcast `i_nan` to the shape of ranks before using `at.set`\n-    if not is_lazy_array(x) and contains_nan:\n-        default_float = xp_default_dtype(xp)\n+    # JIT won't allow use of `contains_nan` for control flow here, so we always have to\n+    # run this with JIT.\n+    if is_lazy_array(x) or contains_nan:\n         i_nan = (xp.isnan(x) if nan_policy == 'omit'\n-                 else xp.any(xp.isnan(x), axis=-1))\n-        ranks = xp.asarray(ranks, dtype=default_float)  # copy=False when implemented\n+                 else xp.any(xp.isnan(x), axis=-1, keepdims=True))\n+        i_nan = xp.broadcast_to(i_nan, ranks.shape)\n         ranks = xpx.at(ranks)[i_nan].set(xp.nan)\n \n     ranks = xp_swapaxes(ranks, axis, -1, xp=xp)\n@@ -10129,22 +10121,24 @@ def _order_ranks(ranks, j, *, xp):\n def _rankdata(x, method, return_sorted=False, return_ties=False, xp=None):\n     # Rank data `x` by desired `method`; `return_ties`/`return_sorted` data  if desired\n     xp = array_namespace(x) if xp is None else xp\n+    dtype = xp_result_type(x, force_floating=True, xp=xp)\n \n     if is_jax(xp):\n         import jax.scipy.stats as jax_stats\n         ranks = jax_stats.rankdata(x, method=method, axis=-1)\n+        ranks = xp.astype(ranks, dtype, copy=False)\n         out = [ranks]\n         y = xp.sort(x, axis=-1) if (return_ties or return_sorted) else None\n         if return_sorted:\n             out.append(y)\n         if return_ties:\n             max_ranks = jax_stats.rankdata(y, method='max', axis=-1)\n             t = xp.diff(max_ranks, axis=-1, prepend=0)\n+            t = xp.astype(t, dtype, copy=False)\n             out.append(t)\n         return out[0] if len(out) == 1 else tuple(out)\n \n     shape = x.shape\n-    dtype = xp.asarray(1.).dtype if method == 'average' else xp.asarray(1).dtype\n \n     # Get sort order\n     j = xp.argsort(x, axis=-1, stable=True)\n@@ -10169,10 +10163,10 @@ def _rankdata(x, method, return_sorted=False, return_ties=False, xp=None):\n     if method == 'min':\n         ranks = ordinal_ranks[i]\n     elif method == 'max':\n-        ranks = ordinal_ranks[i] + counts - 1\n+        ranks = ordinal_ranks[i] + xp.astype(counts, dtype) - 1\n     elif method == 'average':\n         # array API doesn't promote integers to floats\n-        ranks = ordinal_ranks[i] + (xp.asarray(counts, dtype=dtype) - 1)/2\n+        ranks = ordinal_ranks[i] + (xp.astype(counts, dtype) - 1)/2\n     elif method == 'dense':\n         ranks = xp.cumulative_sum(xp.astype(i, dtype, copy=False), axis=-1)[i]\n \n@@ -10203,8 +10197,8 @@ def _rankdata(x, method, return_sorted=False, return_ties=False, xp=None):\n         #   sorted order, so this does not unnecessarily reorder them.\n         # - One exception is `wilcoxon`, which needs the number of zeros. Zeros always\n         #   have the lowest rank, so it is easy to find them at the zeroth index.\n-        t = xp.zeros(shape, dtype=xp.float64)\n-        t = xpx.at(t)[i].set(xp.astype(counts, t.dtype, copy=False))\n+        t = xp.zeros(shape, dtype=dtype)\n+        t = xpx.at(t)[i].set(xp.astype(counts, dtype, copy=False))\n         out.append(t)\n \n     return out", "before_segments": [{"filename": "scipy/stats/_stats_py.py", "start_line": 603, "code": "def _put_val_to_limits(a, limits, inclusive, val=np.nan, xp=None):\n    xp = array_namespace(a) if xp is None else xp\n    mask = xp.zeros_like(a, dtype=xp.bool)\n    if limits is None:\n        return a, mask\n    lower_limit, upper_limit = limits\n    lower_include, upper_include = inclusive\n    if lower_limit is not None:\n        mask = mask | ((a < lower_limit) if lower_include else a <= lower_limit)\n    if upper_limit is not None:\n        mask = mask | ((a > upper_limit) if upper_include else a >= upper_limit)", "documentation": "    \"\"\"Replace elements outside limits with a value.\n\n    This is primarily a utility function.\n\n    Parameters\n    ----------\n    a : array\n    limits : (float or None, float or None)\n        A tuple consisting of the (lower limit, upper limit).  Elements in the\n        input array less than the lower limit or greater than the upper limit\n        will be replaced with `val`. None implies no limit.\n    inclusive : (bool, bool)\n        A tuple consisting of the (lower flag, upper flag).  These flags\n        determine whether values exactly equal to lower or upper are allowed.\n    val : float, default: NaN\n        The value with which extreme elements of the array are replaced.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 645, "code": "def tmean(a, limits=None, inclusive=(True, True), axis=None):\n    xp = array_namespace(a)\n    a, mask = _put_val_to_limits(a, limits, inclusive, val=0., xp=xp)\n    sum = xp.sum(a, axis=axis, dtype=a.dtype)\n    n = xp.sum(xp.asarray(~mask, dtype=a.dtype, device=xp_device(a)), axis=axis,\n               dtype=a.dtype)\n    mean = xpx.apply_where(n != 0, (sum, n), operator.truediv, fill_value=xp.nan)\n    return mean[()] if mean.ndim == 0 else mean\n@xp_capabilities()\n@_axis_nan_policy_factory(\n    lambda x: x, n_outputs=1, result_to_tuple=lambda x, _: (x,)", "documentation": "    \"\"\"Compute the trimmed mean.\n\n    This function finds the arithmetic mean of given values, ignoring values\n    outside the given `limits`.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    limits : None or (lower limit, upper limit), optional\n        Values in the input array less than the lower limit or greater than the\n        upper limit will be ignored.  When limits is None (default), then all\n        values are used.  Either of the limit values in the tuple can also be\n        None representing a half-open interval.\n    inclusive : (bool, bool), optional\n        A tuple consisting of the (lower flag, upper flag).  These flags\n        determine whether values exactly equal to the lower or upper limits\n        are included.  The default value is (True, True).\n    axis : int or None, optional\n        Axis along which to compute test. Default is None.\n\n    Returns\n    -------\n    tmean : ndarray\n        Trimmed mean.\n\n    See Also\n    --------\n    trim_mean : Returns mean after trimming a proportion from both tails.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tmean(x)\n    9.5\n    >>> stats.tmean(x, (3,17))\n    10.0\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 701, "code": "def tvar(a, limits=None, inclusive=(True, True), axis=0, ddof=1):\n    xp = array_namespace(a)\n    a, _ = _put_val_to_limits(a, limits, inclusive, xp=xp)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", SmallSampleWarning)\n        return _xp_var(a, correction=ddof, axis=axis, nan_policy='omit', xp=xp)\n@xp_capabilities()\n@_axis_nan_policy_factory(\n    lambda x: x, n_outputs=1, result_to_tuple=lambda x, _: (x,)\n)", "documentation": "    \"\"\"Compute the trimmed variance.\n\n    This function computes the sample variance of an array of values,\n    while ignoring values which are outside of given `limits`.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    limits : None or (lower limit, upper limit), optional\n        Values in the input array less than the lower limit or greater than the\n        upper limit will be ignored. When limits is None, then all values are\n        used. Either of the limit values in the tuple can also be None\n        representing a half-open interval.  The default value is None.\n    inclusive : (bool, bool), optional\n        A tuple consisting of the (lower flag, upper flag).  These flags\n        determine whether values exactly equal to the lower or upper limits\n        are included.  The default value is (True, True).\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over the\n        whole array `a`.\n    ddof : int, optional\n        Delta degrees of freedom.  Default is 1.\n\n    Returns\n    -------\n    tvar : float\n        Trimmed variance.\n\n    Notes\n    -----\n    `tvar` computes the unbiased sample variance, i.e. it uses a correction\n    factor ``n / (n - 1)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tvar(x)\n    35.0\n    >>> stats.tvar(x, (3,17))\n    20.0\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 761, "code": "def tmin(a, lowerlimit=None, axis=0, inclusive=True, nan_policy='propagate'):\n    xp = array_namespace(a)\n    max_ = xp.iinfo(a.dtype).max if xp.isdtype(a.dtype, 'integral') else xp.inf\n    a, mask = _put_val_to_limits(a, (lowerlimit, None), (inclusive, None),\n                                 val=max_, xp=xp)\n    res = xp.min(a, axis=axis)\n    invalid = xp.all(mask, axis=axis)  # All elements are below lowerlimit\n    if is_lazy_array(invalid) or xp.any(invalid):\n        res = xp_promote(res, force_floating=True, xp=xp)\n        res = xp.where(invalid, xp.nan, res)\n    return res[()] if res.ndim == 0 else res", "documentation": "    \"\"\"Compute the trimmed minimum.\n\n    This function finds the minimum value of an array `a` along the\n    specified axis, but only considering values greater than a specified\n    lower limit.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    lowerlimit : None or float, optional\n        Values in the input array less than the given limit will be ignored.\n        When lowerlimit is None, then all values are used. The default value\n        is None.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over the\n        whole array `a`.\n    inclusive : {True, False}, optional\n        This flag determines whether values exactly equal to the lower limit\n        are included.  The default value is True.\n\n    Returns\n    -------\n    tmin : float, int or ndarray\n        Trimmed minimum.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tmin(x)\n    0\n\n    >>> stats.tmin(x, 13)\n    13\n\n    >>> stats.tmin(x, 13, inclusive=False)\n    14\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 825, "code": "def tmax(a, upperlimit=None, axis=0, inclusive=True, nan_policy='propagate'):\n    xp = array_namespace(a)\n    min_ = xp.iinfo(a.dtype).min if xp.isdtype(a.dtype, 'integral') else -xp.inf\n    a, mask = _put_val_to_limits(a, (None, upperlimit), (None, inclusive),\n                                 val=min_, xp=xp)\n    res = xp.max(a, axis=axis)\n    invalid = xp.all(mask, axis=axis)  # All elements are above upperlimit\n    if is_lazy_array(invalid) or xp.any(invalid):\n        res = xp_promote(res, force_floating=True, xp=xp)\n        res = xp.where(invalid, xp.nan, res)\n    return res[()] if res.ndim == 0 else res", "documentation": "    \"\"\"Compute the trimmed maximum.\n\n    This function computes the maximum value of an array along a given axis,\n    while ignoring values larger than a specified upper limit.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    upperlimit : None or float, optional\n        Values in the input array greater than the given limit will be ignored.\n        When upperlimit is None, then all values are used. The default value\n        is None.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over the\n        whole array `a`.\n    inclusive : {True, False}, optional\n        This flag determines whether values exactly equal to the upper limit\n        are included.  The default value is True.\n\n    Returns\n    -------\n    tmax : float, int or ndarray\n        Trimmed maximum.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tmax(x)\n    19\n\n    >>> stats.tmax(x, 13)\n    13\n\n    >>> stats.tmax(x, 13, inclusive=False)\n    12\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 888, "code": "def tstd(a, limits=None, inclusive=(True, True), axis=0, ddof=1):\n    return tvar(a, limits, inclusive, axis, ddof, _no_deco=True)**0.5\n@xp_capabilities()\n@_axis_nan_policy_factory(\n    lambda x: x, n_outputs=1, result_to_tuple=lambda x, _: (x,)\n)", "documentation": "    \"\"\"Compute the trimmed sample standard deviation.\n\n    This function finds the sample standard deviation of given values,\n    ignoring values outside the given `limits`.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    limits : None or (lower limit, upper limit), optional\n        Values in the input array less than the lower limit or greater than the\n        upper limit will be ignored. When limits is None, then all values are\n        used. Either of the limit values in the tuple can also be None\n        representing a half-open interval.  The default value is None.\n    inclusive : (bool, bool), optional\n        A tuple consisting of the (lower flag, upper flag).  These flags\n        determine whether values exactly equal to the lower or upper limits\n        are included.  The default value is (True, True).\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over the\n        whole array `a`.\n    ddof : int, optional\n        Delta degrees of freedom.  Default is 1.\n\n    Returns\n    -------\n    tstd : float\n        Trimmed sample standard deviation.\n\n    Notes\n    -----\n    `tstd` computes the unbiased sample standard deviation, i.e. it uses a\n    correction factor ``n / (n - 1)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tstd(x)\n    5.9160797830996161\n    >>> stats.tstd(x, (3,17))\n    4.4721359549995796\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 941, "code": "def tsem(a, limits=None, inclusive=(True, True), axis=0, ddof=1):\n    xp = array_namespace(a)\n    a, _ = _put_val_to_limits(a, limits, inclusive, xp=xp)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", SmallSampleWarning)\n        sd = _xp_var(a, correction=ddof, axis=axis, nan_policy='omit', xp=xp)**0.5\n    not_nan = xp.astype(~xp.isnan(a), a.dtype)\n    n_obs = xp.sum(not_nan, axis=axis, dtype=sd.dtype)\n    return sd / n_obs**0.5", "documentation": "    \"\"\"Compute the trimmed standard error of the mean.\n\n    This function finds the standard error of the mean for given\n    values, ignoring values outside the given `limits`.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    limits : None or (lower limit, upper limit), optional\n        Values in the input array less than the lower limit or greater than the\n        upper limit will be ignored. When limits is None, then all values are\n        used. Either of the limit values in the tuple can also be None\n        representing a half-open interval.  The default value is None.\n    inclusive : (bool, bool), optional\n        A tuple consisting of the (lower flag, upper flag).  These flags\n        determine whether values exactly equal to the lower or upper limits\n        are included.  The default value is (True, True).\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over the\n        whole array `a`.\n    ddof : int, optional\n        Delta degrees of freedom.  Default is 1.\n\n    Returns\n    -------\n    tsem : float\n        Trimmed standard error of the mean.\n\n    Notes\n    -----\n    `tsem` uses unbiased sample standard deviation, i.e. it uses a\n    correction factor ``n / (n - 1)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tsem(x)\n    1.3228756555322954\n    >>> stats.tsem(x, (3,17))\n    1.1547005383792515\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 1192, "code": "def _moment(a, order, axis, *, mean=None, xp=None):\n    xp = array_namespace(a) if xp is None else xp\n    a = xp_promote(a, force_floating=True, xp=xp)\n    dtype = a.dtype\n    if xp_size(a) == 0:\n        return xp.mean(a, axis=axis)\n    if order == 0 or (order == 1 and mean is None):\n        shape = list(a.shape)\n        del shape[axis]\n        temp = (xp.ones(shape, dtype=dtype, device=xp_device(a)) if order == 0\n                else xp.zeros(shape, dtype=dtype, device=xp_device(a)))", "documentation": "    \"\"\"Vectorized calculation of raw moment about specified center\n\n    When `mean` is None, the mean is computed and used as the center;\n    otherwise, the provided value is used as the center.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 1365, "code": "def kurtosis(a, axis=0, fisher=True, bias=True, nan_policy='propagate'):\n    xp = array_namespace(a)\n    a, axis = _chk_asarray(a, axis, xp=xp)\n    n = _length_nonmasked(a, axis, xp=xp)\n    mean = xp.mean(a, axis=axis, keepdims=True)\n    mean_reduced = xp.squeeze(mean, axis=axis)  # needed later\n    m2 = _moment(a, 2, axis, mean=mean, xp=xp)\n    m4 = _moment(a, 4, axis, mean=mean, xp=xp)\n    with np.errstate(all='ignore'):\n        zero = m2 <= (xp.finfo(m2.dtype).eps * mean_reduced)**2\n        vals = xp.where(zero, xp.nan, m4 / m2**2.0)", "documentation": "    \"\"\"Compute the kurtosis (Fisher or Pearson) of a dataset.\n\n    Kurtosis is the fourth central moment divided by the square of the\n    variance. If Fisher's definition is used, then 3.0 is subtracted from\n    the result to give 0.0 for a normal distribution.\n\n    If bias is False then the kurtosis is calculated using k statistics to\n    eliminate bias coming from biased moment estimators\n\n    Use `kurtosistest` to see if result is close enough to normal.\n\n    Parameters\n    ----------\n    a : array\n        Data for which the kurtosis is calculated.\n    axis : int or None, optional\n        Axis along which the kurtosis is calculated. Default is 0.\n        If None, compute over the whole array `a`.\n    fisher : bool, optional\n        If True, Fisher's definition is used (normal ==> 0.0). If False,\n        Pearson's definition is used (normal ==> 3.0).\n    bias : bool, optional\n        If False, then the calculations are corrected for statistical bias.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan. 'propagate' returns nan,\n        'raise' throws an error, 'omit' performs the calculations ignoring nan\n        values. Default is 'propagate'.\n\n    Returns\n    -------\n    kurtosis : array\n        The kurtosis of values along an axis, returning NaN where all values\n        are equal.\n\n    References\n    ----------\n    .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n       Probability and Statistics Tables and Formulae. Chapman & Hall: New\n       York. 2000.\n\n    Examples\n    --------\n    In Fisher's definition, the kurtosis of the normal distribution is zero.\n    In the following example, the kurtosis is close to zero, because it was\n    calculated from the dataset, not from the continuous distribution.\n\n    >>> import numpy as np\n    >>> from scipy.stats import norm, kurtosis\n    >>> data = norm.rvs(size=1000, random_state=3)\n    >>> kurtosis(data)\n    -0.06928694200380558\n\n    The distribution with a higher kurtosis has a heavier tail.\n    The zero valued kurtosis of the normal distribution in Fisher's definition\n    can serve as a reference point.\n\n    >>> import matplotlib.pyplot as plt\n    >>> import scipy.stats as stats\n    >>> from scipy.stats import kurtosis\n\n    >>> x = np.linspace(-5, 5, 100)\n    >>> ax = plt.subplot()\n    >>> distnames = ['laplace', 'norm', 'uniform']\n\n    >>> for distname in distnames:\n    ...     if distname == 'uniform':\n    ...         dist = getattr(stats, distname)(loc=-2, scale=4)\n    ...     else:\n    ...         dist = getattr(stats, distname)\n    ...     data = dist.rvs(size=1000)\n    ...     kur = kurtosis(data, fisher=True)\n    ...     y = dist.pdf(x)\n    ...     ax.plot(x, y, label=\"{}, {}\".format(distname, round(kur, 3)))\n    ...     ax.legend()\n\n    The Laplace distribution has a heavier tail than the normal distribution.\n    The uniform distribution (which has negative kurtosis) has the thinnest\n    tail.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 1474, "code": "def describe(a, axis=0, ddof=1, bias=True, nan_policy='propagate'):\n    xp = array_namespace(a)\n    a, axis = _chk_asarray(a, axis, xp=xp)\n    contains_nan = _contains_nan(a, nan_policy)\n    if nan_policy == 'omit' and contains_nan:\n        a = ma.masked_invalid(a)\n        return mstats_basic.describe(a, axis, ddof, bias)\n    if xp_size(a) == 0:\n        raise ValueError(\"The input must not be empty.\")\n    n = xp.asarray(_length_nonmasked(a, axis, xp=xp), dtype=xp.int64,\n                   device=xp_device(a))", "documentation": "    \"\"\"Compute several descriptive statistics of the passed array.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n    axis : int or None, optional\n        Axis along which statistics are calculated. Default is 0.\n        If None, compute over the whole array `a`.\n    ddof : int, optional\n        Delta degrees of freedom (only for variance).  Default is 1.\n    bias : bool, optional\n        If False, then the skewness and kurtosis calculations are corrected\n        for statistical bias.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    nobs : int or ndarray of ints\n        Number of observations (length of data along `axis`).\n        When 'omit' is chosen as nan_policy, the length along each axis\n        slice is counted separately.\n    minmax: tuple of ndarrays or floats\n        Minimum and maximum value of `a` along the given axis.\n    mean : ndarray or float\n        Arithmetic mean of `a` along the given axis.\n    variance : ndarray or float\n        Unbiased variance of `a` along the given axis; denominator is number\n        of observations minus one.\n    skewness : ndarray or float\n        Skewness of `a` along the given axis, based on moment calculations\n        with denominator equal to the number of observations, i.e. no degrees\n        of freedom correction.\n    kurtosis : ndarray or float\n        Kurtosis (Fisher) of `a` along the given axis.  The kurtosis is\n        normalized so that it is zero for the normal distribution.  No\n        degrees of freedom are used.\n\n    Raises\n    ------\n    ValueError\n        If size of `a` is 0.\n\n    See Also\n    --------\n    skew, kurtosis\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> a = np.arange(10)\n    >>> stats.describe(a)\n    DescribeResult(nobs=10, minmax=(0, 9), mean=4.5,\n                   variance=9.166666666666666, skewness=0.0,\n                   kurtosis=-1.2242424242424244)\n    >>> b = [[1, 2], [3, 4]]\n    >>> stats.describe(b)\n    DescribeResult(nobs=2, minmax=(array([1, 2]), array([3, 4])),\n                   mean=array([2., 3.]), variance=array([2., 2.]),\n                   skewness=array([0., 0.]), kurtosis=array([-2., -2.]))\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 1576, "code": "def _get_pvalue(statistic, distribution, alternative, symmetric=True, xp=None):\n    xp = array_namespace(statistic) if xp is None else xp\n    if alternative == 'less':\n        pvalue = distribution.cdf(statistic)\n    elif alternative == 'greater':\n        pvalue = distribution.sf(statistic)\n    elif alternative == 'two-sided':\n        pvalue = 2 * (distribution.sf(xp.abs(statistic)) if symmetric\n                      else xp.minimum(distribution.cdf(statistic),\n                                      distribution.sf(statistic)))\n    else:", "documentation": "    \"\"\"Get p-value given the statistic, (continuous) distribution, and alternative\"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2103, "code": "def percentileofscore(a, score, kind='rank', nan_policy='propagate'):\n    a = np.asarray(a)\n    score = np.asarray(score)\n    if a.ndim != 1:\n        raise ValueError(\"`a` must be 1-dimensional.\")\n    n = len(a)\n    cna = _contains_nan(a, nan_policy)\n    cns = _contains_nan(score, nan_policy)\n    if cns:\n        score = ma.masked_where(np.isnan(score), score)\n    if cna:", "documentation": "    \"\"\"Compute the percentile rank of a score relative to a list of scores.\n\n    A `percentileofscore` of, for example, 80% means that 80% of the\n    scores in `a` are below the given score. In the case of gaps or\n    ties, the exact definition depends on the optional keyword, `kind`.\n\n    Parameters\n    ----------\n    a : array_like\n        A 1-D array to which `score` is compared.\n    score : float or array_like\n        A float score or array of scores for which to compute the percentile(s).\n    kind : {'rank', 'weak', 'strict', 'mean'}, optional\n        Specifies the interpretation of the resulting score.\n        The following options are available (default is 'rank'):\n\n        * 'rank': Average percentage ranking of score.  In case of multiple\n          matches, average the percentage rankings of all matching scores.\n        * 'weak': This kind corresponds to the definition of a cumulative\n          distribution function.  A percentileofscore of 80% means that 80%\n          of values are less than or equal to the provided score.\n        * 'strict': Similar to \"weak\", except that only values that are\n          strictly less than the given score are counted.\n        * 'mean': The average of the \"weak\" and \"strict\" scores, often used\n          in testing.  See https://en.wikipedia.org/wiki/Percentile_rank\n\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Specifies how to treat `nan` values in `a`.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan (for each value in `score`).\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    pcos : float or array-like\n        Percentile-position(s) of `score` (0-100) relative to `a`.\n\n    See Also\n    --------\n    numpy.percentile\n    scipy.stats.scoreatpercentile, scipy.stats.rankdata\n\n    Examples\n    --------\n    Three-quarters of the given values lie below a given score:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> stats.percentileofscore([1, 2, 3, 4], 3)\n    75.0\n\n    With multiple matches, note how the scores of the two matches, 0.6\n    and 0.8 respectively, are averaged:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3)\n    70.0\n\n    Only 2/5 values are strictly less than 3:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='strict')\n    40.0\n\n    But 4/5 values are less than or equal to 3:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='weak')\n    80.0\n\n    The average between the weak and the strict scores is:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='mean')\n    60.0\n\n    Score arrays (of any dimensionality) are supported:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], [2, 3])\n    array([40., 70.])\n\n    The inputs can be infinite:\n\n    >>> stats.percentileofscore([-np.inf, 0, 1, np.inf], [1, 2, np.inf])\n    array([75., 75., 100.])\n\n    If `a` is empty, then the resulting percentiles are all `nan`:\n\n    >>> stats.percentileofscore([], [1, 2])\n    array([nan, nan])\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2346, "code": "def cumfreq(a, numbins=10, defaultreallimits=None, weights=None):\n    h, l, b, e = _histogram(a, numbins, defaultreallimits, weights=weights)\n    cumhist = np.cumsum(h * 1, axis=0)\n    return CumfreqResult(cumhist, l, b, e)\nRelfreqResult = namedtuple('RelfreqResult',\n                           ('frequency', 'lowerlimit', 'binsize',\n                            'extrapoints'))\n@xp_capabilities(np_only=True)", "documentation": "    \"\"\"Return a cumulative frequency histogram, using the histogram function.\n\n    A cumulative histogram is a mapping that counts the cumulative number of\n    observations in all of the bins up to the specified bin.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    numbins : int, optional\n        The number of bins to use for the histogram. Default is 10.\n    defaultreallimits : tuple (lower, upper), optional\n        The lower and upper values for the range of the histogram.\n        If no value is given, a range slightly larger than the range of the\n        values in `a` is used. Specifically ``(a.min() - s, a.max() + s)``,\n        where ``s = (1/2)(a.max() - a.min()) / (numbins - 1)``.\n    weights : array_like, optional\n        The weights for each value in `a`. Default is None, which gives each\n        value a weight of 1.0\n\n    Returns\n    -------\n    cumcount : ndarray\n        Binned values of cumulative frequency.\n    lowerlimit : float\n        Lower real limit\n    binsize : float\n        Width of each bin.\n    extrapoints : int\n        Extra points.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> x = [1, 4, 2, 1, 3, 1]\n    >>> res = stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))\n    >>> res.cumcount\n    array([ 1.,  2.,  3.,  3.])\n    >>> res.extrapoints\n    3\n\n    Create a normal distribution with 1000 random values\n\n    >>> samples = stats.norm.rvs(size=1000, random_state=rng)\n\n    Calculate cumulative frequencies\n\n    >>> res = stats.cumfreq(samples, numbins=25)\n\n    Calculate space of values for x\n\n    >>> x = res.lowerlimit + np.linspace(0, res.binsize*res.cumcount.size,\n    ...                                  res.cumcount.size + 1)\n\n    Plot histogram and cumulative histogram\n\n    >>> fig = plt.figure(figsize=(10, 4))\n    >>> ax1 = fig.add_subplot(1, 2, 1)\n    >>> ax2 = fig.add_subplot(1, 2, 2)\n    >>> ax1.hist(samples, bins=25)\n    >>> ax1.set_title('Histogram')\n    >>> ax2.bar(x[:-1], res.cumcount, width=res.binsize, align='edge')\n    >>> ax2.set_title('Cumulative histogram')\n    >>> ax2.set_xlim([x.min(), x.max()])\n\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2429, "code": "def relfreq(a, numbins=10, defaultreallimits=None, weights=None):\n    a = np.asanyarray(a)\n    h, l, b, e = _histogram(a, numbins, defaultreallimits, weights=weights)\n    h = h / a.shape[0]\n    return RelfreqResult(h, l, b, e)\n@xp_capabilities(np_only=True)", "documentation": "    \"\"\"Return a relative frequency histogram, using the histogram function.\n\n    A relative frequency  histogram is a mapping of the number of\n    observations in each of the bins relative to the total of observations.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    numbins : int, optional\n        The number of bins to use for the histogram. Default is 10.\n    defaultreallimits : tuple (lower, upper), optional\n        The lower and upper values for the range of the histogram.\n        If no value is given, a range slightly larger than the range of the\n        values in a is used. Specifically ``(a.min() - s, a.max() + s)``,\n        where ``s = (1/2)(a.max() - a.min()) / (numbins - 1)``.\n    weights : array_like, optional\n        The weights for each value in `a`. Default is None, which gives each\n        value a weight of 1.0\n\n    Returns\n    -------\n    frequency : ndarray\n        Binned values of relative frequency.\n    lowerlimit : float\n        Lower real limit.\n    binsize : float\n        Width of each bin.\n    extrapoints : int\n        Extra points.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> a = np.array([2, 4, 1, 2, 3, 2])\n    >>> res = stats.relfreq(a, numbins=4)\n    >>> res.frequency\n    array([ 0.16666667, 0.5       , 0.16666667,  0.16666667])\n    >>> np.sum(res.frequency)  # relative frequencies should add up to 1\n    1.0\n\n    Create a normal distribution with 1000 random values\n\n    >>> samples = stats.norm.rvs(size=1000, random_state=rng)\n\n    Calculate relative frequencies\n\n    >>> res = stats.relfreq(samples, numbins=25)\n\n    Calculate space of values for x\n\n    >>> x = res.lowerlimit + np.linspace(0, res.binsize*res.frequency.size,\n    ...                                  res.frequency.size)\n\n    Plot relative frequency histogram\n\n    >>> fig = plt.figure(figsize=(5, 4))\n    >>> ax = fig.add_subplot(1, 1, 1)\n    >>> ax.bar(x, res.frequency, width=res.binsize)\n    >>> ax.set_title('Relative frequency histogram')\n    >>> ax.set_xlim([x.min(), x.max()])\n\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2510, "code": "def obrientransform(*samples):\n    TINY = np.sqrt(np.finfo(float).eps)\n    arrays = []\n    sLast = None\n    for sample in samples:\n        a = np.asarray(sample)\n        n = len(a)\n        mu = np.mean(a)\n        sq = (a - mu)**2\n        sumsq = sq.sum()\n        t = ((n - 1.5) * n * sq - 0.5 * sumsq) / ((n - 1) * (n - 2))", "documentation": "    \"\"\"Compute the O'Brien transform on input data (any number of arrays).\n\n    Used to test for homogeneity of variance prior to running one-way stats.\n    Each array in ``*samples`` is one level of a factor.\n    If `f_oneway` is run on the transformed data and found significant,\n    the variances are unequal.  From Maxwell and Delaney [1]_, p.112.\n\n    Parameters\n    ----------\n    sample1, sample2, ... : array_like\n        Any number of arrays.\n\n    Returns\n    -------\n    obrientransform : ndarray\n        Transformed data for use in an ANOVA.  The first dimension\n        of the result corresponds to the sequence of transformed\n        arrays.  If the arrays given are all 1-D of the same length,\n        the return value is a 2-D array; otherwise it is a 1-D array\n        of type object, with each element being an ndarray.\n\n    Raises\n    ------\n    ValueError\n        If the mean of the transformed data is not equal to the original\n        variance, indicating a lack of convergence in the O'Brien transform.\n\n    References\n    ----------\n    .. [1] S. E. Maxwell and H. D. Delaney, \"Designing Experiments and\n           Analyzing Data: A Model Comparison Perspective\", Wadsworth, 1990.\n\n    Examples\n    --------\n    We'll test the following data sets for differences in their variance.\n\n    >>> x = [10, 11, 13, 9, 7, 12, 12, 9, 10]\n    >>> y = [13, 21, 5, 10, 8, 14, 10, 12, 7, 15]\n\n    Apply the O'Brien transform to the data.\n\n    >>> from scipy.stats import obrientransform\n    >>> tx, ty = obrientransform(x, y)\n\n    Use `scipy.stats.f_oneway` to apply a one-way ANOVA test to the\n    transformed data.\n\n    >>> from scipy.stats import f_oneway\n    >>> F, p = f_oneway(tx, ty)\n    >>> p\n    0.1314139477040335\n\n    If we require that ``p < 0.05`` for significance, we cannot conclude\n    that the variances are different.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2603, "code": "def sem(a, axis=0, ddof=1, nan_policy='propagate'):\n    xp = array_namespace(a)\n    if axis is None:\n        a = xp.reshape(a, (-1,))\n        axis = 0\n    a = xpx.atleast_nd(xp.asarray(a), ndim=1, xp=xp)\n    n = _length_nonmasked(a, axis, xp=xp)\n    s = xp.std(a, axis=axis, correction=ddof) / n**0.5\n    return s", "documentation": "    \"\"\"Compute standard error of the mean.\n\n    Calculate the standard error of the mean (or standard error of\n    measurement) of the values in the input array.\n\n    Parameters\n    ----------\n    a : array_like\n        An array containing the values for which the standard error is\n        returned. Must contain at least two observations.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over\n        the whole array `a`.\n    ddof : int, optional\n        Delta degrees-of-freedom. How many degrees of freedom to adjust\n        for bias in limited samples relative to the population estimate\n        of variance. Defaults to 1.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    s : ndarray or float\n        The standard error of the mean in the sample(s), along the input axis.\n\n    Notes\n    -----\n    The default value for `ddof` is different to the default (0) used by other\n    ddof containing routines, such as np.std and np.nanstd.\n\n    Examples\n    --------\n    Find standard error along the first axis:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> a = np.arange(20).reshape(5,4)\n    >>> stats.sem(a)\n    array([ 2.8284,  2.8284,  2.8284,  2.8284])\n\n    Find standard error across the whole array, using n degrees of freedom:\n\n    >>> stats.sem(a, axis=None, ddof=0)\n    1.2893796958227628\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2665, "code": "def _isconst(x):\n    y = x[~np.isnan(x)]\n    if y.size == 0:\n        return np.array([True])\n    else:\n        return (y[0] == y).all(keepdims=True)\n@xp_capabilities()", "documentation": "    \"\"\"\n    Check if all values in x are the same.  nans are ignored.\n\n    x must be a 1d array.\n\n    The return value is a 1d array with length 1, so it can be used\n    in np.apply_along_axis.\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2682, "code": "def zscore(a, axis=0, ddof=0, nan_policy='propagate'):\n    return zmap(a, a, axis=axis, ddof=ddof, nan_policy=nan_policy)\n@xp_capabilities()", "documentation": "    \"\"\"\n    Compute the z score.\n\n    Compute the z score of each value in the sample, relative to the\n    sample mean and standard deviation.\n\n    Parameters\n    ----------\n    a : array_like\n        An array like object containing the sample data.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over\n        the whole array `a`.\n    ddof : int, optional\n        Degrees of freedom correction in the calculation of the\n        standard deviation. Default is 0.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan. 'propagate' returns nan,\n        'raise' throws an error, 'omit' performs the calculations ignoring nan\n        values. Default is 'propagate'.  Note that when the value is 'omit',\n        nans in the input also propagate to the output, but they do not affect\n        the z-scores computed for the non-nan values.\n\n    Returns\n    -------\n    zscore : array_like\n        The z-scores, standardized by mean and standard deviation of\n        input array `a`.\n\n    See Also\n    --------\n    numpy.mean : Arithmetic average\n    numpy.std : Arithmetic standard deviation\n    scipy.stats.gzscore : Geometric standard score\n\n    Notes\n    -----\n    This function preserves ndarray subclasses, and works also with\n    matrices and masked arrays (it uses `asanyarray` instead of\n    `asarray` for parameters).\n\n    References\n    ----------\n    .. [1] \"Standard score\", *Wikipedia*,\n           https://en.wikipedia.org/wiki/Standard_score.\n    .. [2] Huck, S. W., Cross, T. L., Clark, S. B, \"Overcoming misconceptions\n           about Z-scores\", Teaching Statistics, vol. 8, pp. 38-40, 1986\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> a = np.array([ 0.7972,  0.0767,  0.4383,  0.7866,  0.8091,\n    ...                0.1954,  0.6307,  0.6599,  0.1065,  0.0508])\n    >>> from scipy import stats\n    >>> stats.zscore(a)\n    array([ 1.1273, -1.247 , -0.0552,  1.0923,  1.1664, -0.8559,  0.5786,\n            0.6748, -1.1488, -1.3324])\n\n    Computing along a specified axis, using n-1 degrees of freedom\n    (``ddof=1``) to calculate the standard deviation:\n\n    >>> b = np.array([[ 0.3148,  0.0478,  0.6243,  0.4608],\n    ...               [ 0.7149,  0.0775,  0.6072,  0.9656],\n    ...               [ 0.6341,  0.1403,  0.9759,  0.4064],\n    ...               [ 0.5918,  0.6948,  0.904 ,  0.3721],\n    ...               [ 0.0921,  0.2481,  0.1188,  0.1366]])\n    >>> stats.zscore(b, axis=1, ddof=1)\n    array([[-0.19264823, -1.28415119,  1.07259584,  0.40420358],\n           [ 0.33048416, -1.37380874,  0.04251374,  1.00081084],\n           [ 0.26796377, -1.12598418,  1.23283094, -0.37481053],\n           [-0.22095197,  0.24468594,  1.19042819, -1.21416216],\n           [-0.82780366,  1.4457416 , -0.43867764, -0.1792603 ]])\n\n    An example with ``nan_policy='omit'``:\n\n    >>> x = np.array([[25.11, 30.10, np.nan, 32.02, 43.15],\n    ...               [14.95, 16.06, 121.25, 94.35, 29.81]])\n    >>> stats.zscore(x, axis=1, nan_policy='omit')\n    array([[-1.13490897, -0.37830299,         nan, -0.08718406,  1.60039602],\n           [-0.91611681, -0.89090508,  1.4983032 ,  0.88731639, -0.5785977 ]])\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2768, "code": "def gzscore(a, *, axis=0, ddof=0, nan_policy='propagate'):\n    xp = array_namespace(a)\n    a = xp_promote(a, force_floating=True, xp=xp)\n    log = ma.log if isinstance(a, ma.MaskedArray) else xp.log\n    return zscore(log(a), axis=axis, ddof=ddof, nan_policy=nan_policy)\n@xp_capabilities()", "documentation": "    \"\"\"\n    Compute the geometric standard score.\n\n    Compute the geometric z score of each strictly positive value in the\n    sample, relative to the geometric mean and standard deviation.\n    Mathematically the geometric z score can be evaluated as::\n\n        gzscore = log(a/gmu) / log(gsigma)\n\n    where ``gmu`` (resp. ``gsigma``) is the geometric mean (resp. standard\n    deviation).\n\n    Parameters\n    ----------\n    a : array_like\n        Sample data.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over\n        the whole array `a`.\n    ddof : int, optional\n        Degrees of freedom correction in the calculation of the\n        standard deviation. Default is 0.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan. 'propagate' returns nan,\n        'raise' throws an error, 'omit' performs the calculations ignoring nan\n        values. Default is 'propagate'.  Note that when the value is 'omit',\n        nans in the input also propagate to the output, but they do not affect\n        the geometric z scores computed for the non-nan values.\n\n    Returns\n    -------\n    gzscore : array_like\n        The geometric z scores, standardized by geometric mean and geometric\n        standard deviation of input array `a`.\n\n    See Also\n    --------\n    gmean : Geometric mean\n    gstd : Geometric standard deviation\n    zscore : Standard score\n\n    Notes\n    -----\n    This function preserves ndarray subclasses, and works also with\n    matrices and masked arrays (it uses ``asanyarray`` instead of\n    ``asarray`` for parameters).\n\n    .. versionadded:: 1.8\n\n    References\n    ----------\n    .. [1] \"Geometric standard score\", *Wikipedia*,\n           https://en.wikipedia.org/wiki/Geometric_standard_deviation#Geometric_standard_score.\n\n    Examples\n    --------\n    Draw samples from a log-normal distribution:\n\n    >>> import numpy as np\n    >>> from scipy.stats import zscore, gzscore\n    >>> import matplotlib.pyplot as plt\n\n    >>> rng = np.random.default_rng()\n    >>> mu, sigma = 3., 1.  # mean and standard deviation\n    >>> x = rng.lognormal(mu, sigma, size=500)\n\n    Display the histogram of the samples:\n\n    >>> fig, ax = plt.subplots()\n    >>> ax.hist(x, 50)\n    >>> plt.show()\n\n    Display the histogram of the samples standardized by the classical zscore.\n    Distribution is rescaled but its shape is unchanged.\n\n    >>> fig, ax = plt.subplots()\n    >>> ax.hist(zscore(x), 50)\n    >>> plt.show()\n\n    Demonstrate that the distribution of geometric zscores is rescaled and\n    quasinormal:\n\n    >>> fig, ax = plt.subplots()\n    >>> ax.hist(gzscore(x), 50)\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2863, "code": "def zmap(scores, compare, axis=0, ddof=0, nan_policy='propagate'):\n    like_zscore = (scores is compare)\n    xp = array_namespace(scores, compare)\n    scores, compare = xp_promote(scores, compare, force_floating=True, xp=xp)\n    with warnings.catch_warnings():\n        if like_zscore:  # zscore should not emit SmallSampleWarning\n            warnings.simplefilter('ignore', SmallSampleWarning)\n        mn = _xp_mean(compare, axis=axis, keepdims=True, nan_policy=nan_policy)\n        std = _xp_var(compare, axis=axis, correction=ddof,\n                      keepdims=True, nan_policy=nan_policy)**0.5\n    with np.errstate(invalid='ignore', divide='ignore'):", "documentation": "    \"\"\"\n    Calculate the relative z-scores.\n\n    Return an array of z-scores, i.e., scores that are standardized to\n    zero mean and unit variance, where mean and variance are calculated\n    from the comparison array.\n\n    Parameters\n    ----------\n    scores : array_like\n        The input for which z-scores are calculated.\n    compare : array_like\n        The input from which the mean and standard deviation of the\n        normalization are taken; assumed to have the same dimension as\n        `scores`.\n    axis : int or None, optional\n        Axis over which mean and variance of `compare` are calculated.\n        Default is 0. If None, compute over the whole array `scores`.\n    ddof : int, optional\n        Degrees of freedom correction in the calculation of the\n        standard deviation. Default is 0.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle the occurrence of nans in `compare`.\n        'propagate' returns nan, 'raise' raises an exception, 'omit'\n        performs the calculations ignoring nan values. Default is\n        'propagate'. Note that when the value is 'omit', nans in `scores`\n        also propagate to the output, but they do not affect the z-scores\n        computed for the non-nan values.\n\n    Returns\n    -------\n    zscore : array_like\n        Z-scores, in the same shape as `scores`.\n\n    Notes\n    -----\n    This function preserves ndarray subclasses, and works also with\n    matrices and masked arrays (it uses `asanyarray` instead of\n    `asarray` for parameters).\n\n    Examples\n    --------\n    >>> from scipy.stats import zmap\n    >>> a = [0.5, 2.0, 2.5, 3]\n    >>> b = [0, 1, 2, 3, 4]\n    >>> zmap(a, b)\n    array([-1.06066017,  0.        ,  0.35355339,  0.70710678])\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 3387, "code": "def sigmaclip(a, low=4., high=4., *, nan_policy='propagate'):\n    xp = array_namespace(a)\n    c = xp_ravel(xp.asarray(a))\n    contains_nan = _contains_nan(c, nan_policy, xp_omit_okay=True)\n    if contains_nan:\n        if nan_policy == 'propagate':\n            NaN = _get_nan(c, xp=xp)\n            clipped = xp.empty_like(c[0:0])\n            return SigmaclipResult(clipped, NaN, NaN)\n        elif nan_policy == 'omit':\n            c = c[~xp.isnan(c)]", "documentation": "    \"\"\"Perform iterative sigma-clipping of array elements.\n\n    Starting from the full sample, all elements outside the critical range are\n    removed, i.e. all elements of the input array `c` that satisfy either of\n    the following conditions::\n\n        c < mean(c) - std(c)*low\n        c > mean(c) + std(c)*high\n\n    The iteration continues with the updated sample until no\n    elements are outside the (updated) range.\n\n    Parameters\n    ----------\n    a : array_like\n        Data array, will be raveled if not 1-D.\n    low : float, optional\n        Lower bound factor of sigma clipping. Default is 4.\n    high : float, optional\n        Upper bound factor of sigma clipping. Default is 4.\n    nan_policy : {'propagate', 'omit', 'raise'}\n        Defines how to handle input NaNs.\n\n        - ``propagate``: if a NaN is present in the input, the clipped array will be\n          empty, and the upper and lower thresholds will be NaN.\n        - ``omit``: NaNs will be omitted when performing the calculation.\n        - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n\n    Returns\n    -------\n    clipped : ndarray\n        Input array with clipped elements removed.\n    lower : float\n        Lower threshold value use for clipping.\n    upper : float\n        Upper threshold value use for clipping.\n\n    Notes\n    -----\n    This function iteratively *removes* observations. Once observations are\n    removed, they are not re-added in subsequent iterations. Consequently,\n    although it is often the case that ``clipped`` is identical to\n    ``a[(a >= lower) & (a <= upper)]``, this property is not guaranteed to be\n    satisfied; ``clipped`` may have fewer elements.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import sigmaclip\n    >>> a = np.concatenate((np.linspace(9.5, 10.5, 31),\n    ...                     np.linspace(0, 20, 5)))\n    >>> fact = 1.5\n    >>> c, low, upp = sigmaclip(a, fact, fact)\n    >>> c\n    array([  9.96666667,  10.        ,  10.03333333,  10.        ])\n    >>> c.var(), c.std()\n    (0.00055555555555555165, 0.023570226039551501)\n    >>> low, c.mean() - fact*c.std(), c.min()\n    (9.9646446609406727, 9.9646446609406727, 9.9666666666666668)\n    >>> upp, c.mean() + fact*c.std(), c.max()\n    (10.035355339059327, 10.035355339059327, 10.033333333333333)\n\n    >>> a = np.concatenate((np.linspace(9.5, 10.5, 11),\n    ...                     np.linspace(-100, -50, 3)))\n    >>> c, low, upp = sigmaclip(a, 1.8, 1.8)\n    >>> (c == np.linspace(9.5, 10.5, 11)).all()\n    True\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 3482, "code": "def trimboth(a, proportiontocut, axis=0):\n    a = np.asarray(a)\n    if a.size == 0:\n        return a\n    if axis is None:\n        a = a.ravel()\n        axis = 0\n    nobs = a.shape[axis]\n    lowercut = int(proportiontocut * nobs)\n    uppercut = nobs - lowercut\n    if (lowercut >= uppercut):", "documentation": "    \"\"\"Slice off a proportion of items from both ends of an array.\n\n    Slice off the passed proportion of items from both ends of the passed\n    array (i.e., with `proportiontocut` = 0.1, slices leftmost 10% **and**\n    rightmost 10% of scores). The trimmed values are the lowest and\n    highest ones.\n    Slice off less if proportion results in a non-integer slice index (i.e.\n    conservatively slices off `proportiontocut`).\n\n    Parameters\n    ----------\n    a : array_like\n        Data to trim.\n    proportiontocut : float\n        Proportion (in range 0-1) of total data set to trim of each end.\n    axis : int or None, optional\n        Axis along which to trim data. Default is 0. If None, compute over\n        the whole array `a`.\n\n    Returns\n    -------\n    out : ndarray\n        Trimmed version of array `a`. The order of the trimmed content\n        is undefined.\n\n    See Also\n    --------\n    trim_mean\n\n    Examples\n    --------\n    Create an array of 10 values and trim 10% of those values from each end:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n    >>> stats.trimboth(a, 0.1)\n    array([1, 3, 2, 4, 5, 6, 7, 8])\n\n    Note that the elements of the input array are trimmed by value, but the\n    output array is not necessarily sorted.\n\n    The proportion to trim is rounded down to the nearest integer. For\n    instance, trimming 25% of the values from each end of an array of 10\n    values will return an array of 6 values:\n\n    >>> b = np.arange(10)\n    >>> stats.trimboth(b, 1/4).shape\n    (6,)\n\n    Multidimensional arrays can be trimmed along any axis or across the entire\n    array:\n\n    >>> c = [2, 4, 6, 8, 0, 1, 3, 5, 7, 9]\n    >>> d = np.array([a, b, c])\n    >>> stats.trimboth(d, 0.4, axis=0).shape\n    (1, 10)\n    >>> stats.trimboth(d, 0.4, axis=1).shape\n    (3, 2)\n    >>> stats.trimboth(d, 0.4, axis=None).shape\n    (6,)\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 3569, "code": "def trim1(a, proportiontocut, tail='right', axis=0):\n    a = np.asarray(a)\n    if axis is None:\n        a = a.ravel()\n        axis = 0\n    nobs = a.shape[axis]\n    if proportiontocut >= 1:\n        return []\n    if tail.lower() == 'right':\n        lowercut = 0\n        uppercut = nobs - int(proportiontocut * nobs)", "documentation": "    \"\"\"Slice off a proportion from ONE end of the passed array distribution.\n\n    If `proportiontocut` = 0.1, slices off 'leftmost' or 'rightmost'\n    10% of scores. The lowest or highest values are trimmed (depending on\n    the tail).\n    Slice off less if proportion results in a non-integer slice index\n    (i.e. conservatively slices off `proportiontocut` ).\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    proportiontocut : float\n        Fraction to cut off of 'left' or 'right' of distribution.\n    tail : {'left', 'right'}, optional\n        Defaults to 'right'.\n    axis : int or None, optional\n        Axis along which to trim data. Default is 0. If None, compute over\n        the whole array `a`.\n\n    Returns\n    -------\n    trim1 : ndarray\n        Trimmed version of array `a`. The order of the trimmed content is\n        undefined.\n\n    Examples\n    --------\n    Create an array of 10 values and trim 20% of its lowest values:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n    >>> stats.trim1(a, 0.2, 'left')\n    array([2, 4, 3, 5, 6, 7, 8, 9])\n\n    Note that the elements of the input array are trimmed by value, but the\n    output array is not necessarily sorted.\n\n    The proportion to trim is rounded down to the nearest integer. For\n    instance, trimming 25% of the values from an array of 10 values will\n    return an array of 8 values:\n\n    >>> b = np.arange(10)\n    >>> stats.trim1(b, 1/4).shape\n    (8,)\n\n    Multidimensional arrays can be trimmed along any axis or across the entire\n    array:\n\n    >>> c = [2, 4, 6, 8, 0, 1, 3, 5, 7, 9]\n    >>> d = np.array([a, b, c])\n    >>> stats.trim1(d, 0.8, axis=0).shape\n    (1, 10)\n    >>> stats.trim1(d, 0.8, axis=1).shape\n    (3, 2)\n    >>> stats.trim1(d, 0.8, axis=None).shape\n    (6,)\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 3658, "code": "def trim_mean(a, proportiontocut, axis=0):\n    xp = array_namespace(a)\n    a = xp.asarray(a)\n    if xp_size(a) == 0:\n        return _get_nan(a, xp=xp)\n    if axis is None:\n        a = xp_ravel(a)\n        axis = 0\n    nobs = a.shape[axis]\n    lowercut = int(proportiontocut * nobs)\n    uppercut = nobs - lowercut", "documentation": "    \"\"\"Return mean of array after trimming a specified fraction of extreme values.\n\n    Removes the specified proportion of elements from *each* end of the\n    sorted array, then computes the mean of the remaining elements.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    proportiontocut : float\n        Fraction of the most positive and most negative elements to remove.\n        When the specified proportion does not result in an integer number of\n        elements, the number of elements to trim is rounded down.\n    axis : int or None, default: 0\n        Axis along which the trimmed means are computed.\n        If None, compute over the raveled array.\n\n    Returns\n    -------\n    trim_mean : ndarray\n        Mean of trimmed array.\n\n    See Also\n    --------\n    trimboth : Remove a proportion of elements from each end of an array.\n    tmean : Compute the mean after trimming values outside specified limits.\n\n    Notes\n    -----\n    For 1-D array `a`, `trim_mean` is approximately equivalent to the following\n    calculation::\n\n        import numpy as np\n        a = np.sort(a)\n        m = int(proportiontocut * len(a))\n        np.mean(a[m: len(a) - m])\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = [1, 2, 3, 5]\n    >>> stats.trim_mean(x, 0.25)\n    2.5\n\n    When the specified proportion does not result in an integer number of\n    elements, the number of elements to trim is rounded down.\n\n    >>> stats.trim_mean(x, 0.24999) == np.mean(x)\n    True\n\n    Use `axis` to specify the axis along which the calculation is performed.\n\n    >>> x2 = [[1, 2, 3, 5],\n    ...       [10, 20, 30, 50]]\n    >>> stats.trim_mean(x2, 0.25)\n    array([ 5.5, 11. , 16.5, 27.5])\n    >>> stats.trim_mean(x2, 0.25, axis=1)\n    array([ 2.5, 25. ])\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 3773, "code": "def f_oneway(*samples, axis=0, equal_var=True):\n    xp = array_namespace(*samples)\n    samples = xp_promote(*samples, force_floating=True, xp=xp)\n    if len(samples) < 2:\n        raise TypeError('at least two inputs are required;'\n                        f' got {len(samples)}.')\n    num_groups = len(samples)\n    alldata = xp.concat(samples, axis=-1)\n    bign = _length_nonmasked(alldata, axis=-1, xp=xp)\n    if _f_oneway_is_too_small(samples):\n        NaN = _get_nan(*samples, xp=xp)", "documentation": "    \"\"\"Perform one-way ANOVA.\n\n    The one-way ANOVA tests the null hypothesis that two or more groups have\n    the same population mean.  The test is applied to samples from two or\n    more groups, possibly with differing sizes.\n\n    Parameters\n    ----------\n    sample1, sample2, ... : array_like\n        The sample measurements for each group.  There must be at least\n        two arguments.  If the arrays are multidimensional, then all the\n        dimensions of the array must be the same except for `axis`.\n    axis : int, optional\n        Axis of the input arrays along which the test is applied.\n        Default is 0.\n    equal_var : bool, optional\n        If True (default), perform a standard one-way ANOVA test that\n        assumes equal population variances [2]_.\n        If False, perform Welch's ANOVA test, which does not assume\n        equal population variances [4]_.\n\n        .. versionadded:: 1.16.0\n\n    Returns\n    -------\n    statistic : float\n        The computed F statistic of the test.\n    pvalue : float\n        The associated p-value from the F distribution.\n\n    Warns\n    -----\n    `~scipy.stats.ConstantInputWarning`\n        Emitted if all values within each of the input arrays are identical.\n        In this case the F statistic is either infinite or isn't defined,\n        so ``np.inf`` or ``np.nan`` is returned.\n\n    RuntimeWarning\n        Emitted if the length of any input array is 0, or if all the input\n        arrays have length 1.  ``np.nan`` is returned for the F statistic\n        and the p-value in these cases.\n\n    Notes\n    -----\n    The ANOVA test has important assumptions that must be satisfied in order\n    for the associated p-value to be valid.\n\n    1. The samples are independent.\n    2. Each sample is from a normally distributed population.\n    3. The population standard deviations of the groups are all equal.  This\n       property is known as homoscedasticity.\n\n    If these assumptions are not true for a given set of data, it may still\n    be possible to use the Kruskal-Wallis H-test (`scipy.stats.kruskal`) or\n    the Alexander-Govern test (`scipy.stats.alexandergovern`) although with\n    some loss of power.\n\n    The length of each group must be at least one, and there must be at\n    least one group with length greater than one.  If these conditions\n    are not satisfied, a warning is generated and (``np.nan``, ``np.nan``)\n    is returned.\n\n    If all values in each group are identical, and there exist at least two\n    groups with different values, the function generates a warning and\n    returns (``np.inf``, 0).\n\n    If all values in all groups are the same, function generates a warning\n    and returns (``np.nan``, ``np.nan``).\n\n    The algorithm is from Heiman [2]_, pp.394-7.\n\n    References\n    ----------\n    .. [1] R. Lowry, \"Concepts and Applications of Inferential Statistics\",\n           Chapter 14, 2014, http://vassarstats.net/textbook/\n\n    .. [2] G.W. Heiman, \"Understanding research methods and statistics: An\n           integrated introduction for psychology\", Houghton, Mifflin and\n           Company, 2001.\n\n    .. [3] J.H. McDonald, \"Handbook of Biological Statistics\",\n           One-way ANOVA, 2014.\n           http://www.biostathandbook.com/onewayanova.html\n\n    .. [4] B. L. Welch, \"On the Comparison of Several Mean Values:\n           An Alternative Approach\", Biometrika, vol. 38, no. 3/4,\n           pp. 330-336, 1951. :doi:`10.2307/2332579`.\n\n    .. [5] J.H. McDonald, R. Seed and R.K. Koehn, \"Allozymes and\n           morphometric characters of three species of Mytilus in\n           the Northern and Southern Hemispheres\",\n           Marine Biology, vol. 111, pp. 323-333, 1991.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import f_oneway\n\n    Here are some data [3]_ on a shell measurement (the length of the anterior\n    adductor muscle scar, standardized by dividing by length) in the mussel\n    Mytilus trossulus from five locations: Tillamook, Oregon; Newport, Oregon;\n    Petersburg, Alaska; Magadan, Russia; and Tvarminne, Finland, taken from a\n    much larger data set used in [5]_.\n\n    >>> tillamook = [0.0571, 0.0813, 0.0831, 0.0976, 0.0817, 0.0859, 0.0735,\n    ...              0.0659, 0.0923, 0.0836]\n    >>> newport = [0.0873, 0.0662, 0.0672, 0.0819, 0.0749, 0.0649, 0.0835,\n    ...            0.0725]\n    >>> petersburg = [0.0974, 0.1352, 0.0817, 0.1016, 0.0968, 0.1064, 0.105]\n    >>> magadan = [0.1033, 0.0915, 0.0781, 0.0685, 0.0677, 0.0697, 0.0764,\n    ...            0.0689]\n    >>> tvarminne = [0.0703, 0.1026, 0.0956, 0.0973, 0.1039, 0.1045]\n    >>> f_oneway(tillamook, newport, petersburg, magadan, tvarminne)\n    F_onewayResult(statistic=7.121019471642447, pvalue=0.0002812242314534544)\n\n    `f_oneway` accepts multidimensional input arrays.  When the inputs\n    are multidimensional and `axis` is not given, the test is performed\n    along the first axis of the input arrays.  For the following data, the\n    test is performed three times, once for each column.\n\n    >>> a = np.array([[9.87, 9.03, 6.81],\n    ...               [7.18, 8.35, 7.00],\n    ...               [8.39, 7.58, 7.68],\n    ...               [7.45, 6.33, 9.35],\n    ...               [6.41, 7.10, 9.33],\n    ...               [8.00, 8.24, 8.44]])\n    >>> b = np.array([[6.35, 7.30, 7.16],\n    ...               [6.65, 6.68, 7.63],\n    ...               [5.72, 7.73, 6.72],\n    ...               [7.01, 9.19, 7.41],\n    ...               [7.75, 7.87, 8.30],\n    ...               [6.90, 7.97, 6.97]])\n    >>> c = np.array([[3.31, 8.77, 1.01],\n    ...               [8.25, 3.24, 3.62],\n    ...               [6.32, 8.81, 5.19],\n    ...               [7.48, 8.83, 8.91],\n    ...               [8.59, 6.01, 6.07],\n    ...               [3.07, 9.72, 7.48]])\n    >>> F = f_oneway(a, b, c)\n    >>> F.statistic\n    array([1.75676344, 0.03701228, 3.76439349])\n    >>> F.pvalue\n    array([0.20630784, 0.96375203, 0.04733157])\n\n    Welch ANOVA will be performed if `equal_var` is False.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 4070, "code": "def alexandergovern(*samples, nan_policy='propagate', axis=0):\n    xp = array_namespace(*samples)\n    samples = _alexandergovern_input_validation(samples, nan_policy, axis, xp=xp)\n    lengths = [sample.shape[-1] for sample in samples]\n    means = xp.stack([_xp_mean(sample, axis=-1) for sample in samples])\n    se2 = [(_xp_var(sample, correction=1, axis=-1) / length)\n           for sample, length in zip(samples, lengths)]\n    standard_errors_squared = xp.stack(se2)\n    standard_errors = standard_errors_squared**0.5\n    eps = xp.finfo(standard_errors.dtype).eps\n    zero = standard_errors <= xp.abs(eps * means)", "documentation": "    \"\"\"Performs the Alexander Govern test.\n\n    The Alexander-Govern approximation tests the equality of k independent\n    means in the face of heterogeneity of variance. The test is applied to\n    samples from two or more groups, possibly with differing sizes.\n\n    Parameters\n    ----------\n    sample1, sample2, ... : array_like\n        The sample measurements for each group.  There must be at least\n        two samples, and each sample must contain at least two observations.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    res : AlexanderGovernResult\n        An object with attributes:\n\n        statistic : float\n            The computed A statistic of the test.\n        pvalue : float\n            The associated p-value from the chi-squared distribution.\n\n    Warns\n    -----\n    `~scipy.stats.ConstantInputWarning`\n        Raised if an input is a constant array.  The statistic is not defined\n        in this case, so ``np.nan`` is returned.\n\n    See Also\n    --------\n    f_oneway : one-way ANOVA\n\n    Notes\n    -----\n    The use of this test relies on several assumptions.\n\n    1. The samples are independent.\n    2. Each sample is from a normally distributed population.\n    3. Unlike `f_oneway`, this test does not assume on homoscedasticity,\n       instead relaxing the assumption of equal variances.\n\n    Input samples must be finite, one dimensional, and with size greater than\n    one.\n\n    References\n    ----------\n    .. [1] Alexander, Ralph A., and Diane M. Govern. \"A New and Simpler\n           Approximation for ANOVA under Variance Heterogeneity.\" Journal\n           of Educational Statistics, vol. 19, no. 2, 1994, pp. 91-101.\n           https://www.jstor.org/stable/1165140\n\n    Examples\n    --------\n    >>> from scipy.stats import alexandergovern\n\n    Here are some data on annual percentage rate of interest charged on\n    new car loans at nine of the largest banks in four American cities\n    taken from the National Institute of Standards and Technology's\n    ANOVA dataset.\n\n    We use `alexandergovern` to test the null hypothesis that all cities\n    have the same mean APR against the alternative that the cities do not\n    all have the same mean APR. We decide that a significance level of 5%\n    is required to reject the null hypothesis in favor of the alternative.\n\n    >>> atlanta = [13.75, 13.75, 13.5, 13.5, 13.0, 13.0, 13.0, 12.75, 12.5]\n    >>> chicago = [14.25, 13.0, 12.75, 12.5, 12.5, 12.4, 12.3, 11.9, 11.9]\n    >>> houston = [14.0, 14.0, 13.51, 13.5, 13.5, 13.25, 13.0, 12.5, 12.5]\n    >>> memphis = [15.0, 14.0, 13.75, 13.59, 13.25, 12.97, 12.5, 12.25,\n    ...           11.89]\n    >>> alexandergovern(atlanta, chicago, houston, memphis)\n    AlexanderGovernResult(statistic=4.65087071883494,\n                          pvalue=0.19922132490385214)\n\n    The p-value is 0.1992, indicating a nearly 20% chance of observing\n    such an extreme value of the test statistic under the null hypothesis.\n    This exceeds 5%, so we do not reject the null hypothesis in favor of\n    the alternative.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 4231, "code": "def _pearsonr_fisher_ci(r, n, confidence_level, alternative):\n    xp = array_namespace(r)\n    ones = xp.ones_like(r)\n    n = xp.asarray(n, dtype=r.dtype, device=xp_device(r))\n    confidence_level = xp.asarray(confidence_level, dtype=r.dtype, device=xp_device(r))\n    with np.errstate(divide='ignore', invalid='ignore'):\n        zr = xp.atanh(r)\n        se = xp.sqrt(1 / (n - 3))\n    if alternative == \"two-sided\":\n        h = special.ndtri(0.5 + confidence_level/2)\n        zlo = zr - h*se", "documentation": "    \"\"\"\n    Compute the confidence interval for Pearson's R.\n\n    Fisher's transformation is used to compute the confidence interval\n    (https://en.wikipedia.org/wiki/Fisher_transformation).\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 4279, "code": "def _pearsonr_bootstrap_ci(confidence_level, method, x, y, alternative, axis):", "documentation": "    \"\"\"\n    Compute the confidence interval for Pearson's R using the bootstrap.\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 4301, "code": "class PearsonRResult(PearsonRResultBase):", "documentation": "    \"\"\"\n    Result of `scipy.stats.pearsonr`\n\n    Attributes\n    ----------\n    statistic : float\n        Pearson product-moment correlation coefficient.\n    pvalue : float\n        The p-value associated with the chosen alternative.\n\n    Methods\n    -------\n    confidence_interval\n        Computes the confidence interval of the correlation\n        coefficient `statistic` for the given confidence level.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 4330, "code": "    def confidence_interval(self, confidence_level=0.95, method=None):\n        if isinstance(method, BootstrapMethod):\n            xp = array_namespace(self._x)\n            message = ('`method` must be `None` if `pearsonr` '\n                       'arguments were not NumPy arrays.')\n            if not is_numpy(xp):\n                raise ValueError(message)\n            ci = _pearsonr_bootstrap_ci(confidence_level, method, self._x, self._y,\n                                        self._alternative, self._axis)\n        elif method is None:\n            ci = _pearsonr_fisher_ci(self.statistic, self._n, confidence_level,", "documentation": "        \"\"\"\n        The confidence interval for the correlation coefficient.\n\n        Compute the confidence interval for the correlation coefficient\n        ``statistic`` with the given confidence level.\n\n        If `method` is not provided,\n        The confidence interval is computed using the Fisher transformation\n        F(r) = arctanh(r) [1]_.  When the sample pairs are drawn from a\n        bivariate normal distribution, F(r) approximately follows a normal\n        distribution with standard error ``1/sqrt(n - 3)``, where ``n`` is the\n        length of the original samples along the calculation axis. When\n        ``n <= 3``, this approximation does not yield a finite, real standard\n        error, so we define the confidence interval to be -1 to 1.\n\n        If `method` is an instance of `BootstrapMethod`, the confidence\n        interval is computed using `scipy.stats.bootstrap` with the provided\n        configuration options and other appropriate settings. In some cases,\n        confidence limits may be NaN due to a degenerate resample, and this is\n        typical for very small samples (~6 observations).\n\n        Parameters\n        ----------\n        confidence_level : float\n            The confidence level for the calculation of the correlation\n            coefficient confidence interval. Default is 0.95.\n\n        method : BootstrapMethod, optional\n            Defines the method used to compute the confidence interval. See\n            method description for details.\n\n            .. versionadded:: 1.11.0\n\n        Returns\n        -------\n        ci : namedtuple\n            The confidence interval is returned in a ``namedtuple`` with\n            fields `low` and `high`.\n\n        References\n        ----------\n        .. [1] \"Pearson correlation coefficient\", Wikipedia,\n               https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\n        \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 4815, "code": "def fisher_exact(table, alternative=None, *, method=None):\n    hypergeom = distributions.hypergeom\n    c = np.asarray(table, dtype=np.int64)\n    if not c.ndim == 2:\n        raise ValueError(\"The input `table` must have two dimensions.\")\n    if np.any(c < 0):\n        raise ValueError(\"All values in `table` must be nonnegative.\")\n    if not c.shape == (2, 2) or method is not None:\n        return _fisher_exact_rxc(c, alternative, method)\n    alternative = 'two-sided' if alternative is None else alternative\n    if 0 in c.sum(axis=0) or 0 in c.sum(axis=1):", "documentation": "    \"\"\"Perform a Fisher exact test on a contingency table.\n\n    For a 2x2 table,\n    the null hypothesis is that the true odds ratio of the populations\n    underlying the observations is one, and the observations were sampled\n    from these populations under a condition: the marginals of the\n    resulting table must equal those of the observed table.\n    The statistic is the unconditional maximum likelihood estimate of the odds\n    ratio, and the p-value is the probability under the null hypothesis of\n    obtaining a table at least as extreme as the one that was actually\n    observed.\n\n    For other table sizes, or if `method` is provided, the null hypothesis\n    is that the rows and columns of the tables have fixed sums and are\n    independent; i.e., the table was sampled from a `scipy.stats.random_table`\n    distribution with the observed marginals. The statistic is the\n    probability mass of this distribution evaluated at `table`, and the\n    p-value is the percentage of the population of tables with statistic at\n    least as extreme (small) as that of `table`. There is only one alternative\n    hypothesis available: the rows and columns are not independent.\n\n    There are other possible choices of statistic and two-sided\n    p-value definition associated with Fisher's exact test; please see the\n    Notes for more information.\n\n    Parameters\n    ----------\n    table : array_like of ints\n        A contingency table.  Elements must be non-negative integers.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis for 2x2 tables; unused for other\n        table sizes.\n        The following options are available (default is 'two-sided'):\n\n        * 'two-sided': the odds ratio of the underlying population is not one\n        * 'less': the odds ratio of the underlying population is less than one\n        * 'greater': the odds ratio of the underlying population is greater\n          than one\n\n        See the Notes for more details.\n\n    method : ResamplingMethod, optional\n        Defines the method used to compute the p-value.\n        If `method` is an instance of `PermutationMethod`/`MonteCarloMethod`,\n        the p-value is computed using\n        `scipy.stats.permutation_test`/`scipy.stats.monte_carlo_test` with the\n        provided configuration options and other appropriate settings.\n        Note that if `method` is an instance of `MonteCarloMethod`, the ``rvs``\n        attribute must be left unspecified; Monte Carlo samples are always drawn\n        using the ``rvs`` method of `scipy.stats.random_table`.\n        Otherwise, the p-value is computed as documented in the notes.\n\n        .. versionadded:: 1.15.0\n\n    Returns\n    -------\n    res : SignificanceResult\n        An object containing attributes:\n\n        statistic : float\n            For a 2x2 table with default `method`, this is the odds ratio - the\n            prior odds ratio not a posterior estimate. In all other cases, this\n            is the probability density of obtaining the observed table under the\n            null hypothesis of independence with marginals fixed.\n        pvalue : float\n            The probability under the null hypothesis of obtaining a\n            table at least as extreme as the one that was actually observed.\n\n    Raises\n    ------\n    ValueError\n        If `table` is not two-dimensional or has negative entries.\n\n    See Also\n    --------\n    chi2_contingency : Chi-square test of independence of variables in a\n        contingency table.  This can be used as an alternative to\n        `fisher_exact` when the numbers in the table are large.\n    contingency.odds_ratio : Compute the odds ratio (sample or conditional\n        MLE) for a 2x2 contingency table.\n    barnard_exact : Barnard's exact test, which is a more powerful alternative\n        than Fisher's exact test for 2x2 contingency tables.\n    boschloo_exact : Boschloo's exact test, which is a more powerful\n        alternative than Fisher's exact test for 2x2 contingency tables.\n    :ref:`hypothesis_fisher_exact` : Extended example\n\n    Notes\n    -----\n    *Null hypothesis and p-values*\n\n    The null hypothesis is that the true odds ratio of the populations\n    underlying the observations is one, and the observations were sampled at\n    random from these populations under a condition: the marginals of the\n    resulting table must equal those of the observed table. Equivalently,\n    the null hypothesis is that the input table is from the hypergeometric\n    distribution with parameters (as used in `hypergeom`)\n    ``M = a + b + c + d``, ``n = a + b`` and ``N = a + c``, where the\n    input table is ``[[a, b], [c, d]]``.  This distribution has support\n    ``max(0, N + n - M) <= x <= min(N, n)``, or, in terms of the values\n    in the input table, ``min(0, a - d) <= x <= a + min(b, c)``.  ``x``\n    can be interpreted as the upper-left element of a 2x2 table, so the\n    tables in the distribution have form::\n\n        [  x           n - x     ]\n        [N - x    M - (n + N) + x]\n\n    For example, if::\n\n        table = [6  2]\n                [1  4]\n\n    then the support is ``2 <= x <= 7``, and the tables in the distribution\n    are::\n\n        [2 6]   [3 5]   [4 4]   [5 3]   [6 2]  [7 1]\n        [5 0]   [4 1]   [3 2]   [2 3]   [1 4]  [0 5]\n\n    The probability of each table is given by the hypergeometric distribution\n    ``hypergeom.pmf(x, M, n, N)``.  For this example, these are (rounded to\n    three significant digits)::\n\n        x       2      3      4      5       6        7\n        p  0.0163  0.163  0.408  0.326  0.0816  0.00466\n\n    These can be computed with::\n\n        >>> import numpy as np\n        >>> from scipy.stats import hypergeom\n        >>> table = np.array([[6, 2], [1, 4]])\n        >>> M = table.sum()\n        >>> n = table[0].sum()\n        >>> N = table[:, 0].sum()\n        >>> start, end = hypergeom.support(M, n, N)\n        >>> hypergeom.pmf(np.arange(start, end+1), M, n, N)\n        array([0.01631702, 0.16317016, 0.40792541, 0.32634033, 0.08158508,\n               0.004662  ])\n\n    The two-sided p-value is the probability that, under the null hypothesis,\n    a random table would have a probability equal to or less than the\n    probability of the input table.  For our example, the probability of\n    the input table (where ``x = 6``) is 0.0816.  The x values where the\n    probability does not exceed this are 2, 6 and 7, so the two-sided p-value\n    is ``0.0163 + 0.0816 + 0.00466 ~= 0.10256``::\n\n        >>> from scipy.stats import fisher_exact\n        >>> res = fisher_exact(table, alternative='two-sided')\n        >>> res.pvalue\n        0.10256410256410257\n\n    The one-sided p-value for ``alternative='greater'`` is the probability\n    that a random table has ``x >= a``, which in our example is ``x >= 6``,\n    or ``0.0816 + 0.00466 ~= 0.08626``::\n\n        >>> res = fisher_exact(table, alternative='greater')\n        >>> res.pvalue\n        0.08624708624708627\n\n    This is equivalent to computing the survival function of the\n    distribution at ``x = 5`` (one less than ``x`` from the input table,\n    because we want to include the probability of ``x = 6`` in the sum)::\n\n        >>> hypergeom.sf(5, M, n, N)\n        0.08624708624708627\n\n    For ``alternative='less'``, the one-sided p-value is the probability\n    that a random table has ``x <= a``, (i.e. ``x <= 6`` in our example),\n    or ``0.0163 + 0.163 + 0.408 + 0.326 + 0.0816 ~= 0.9949``::\n\n        >>> res = fisher_exact(table, alternative='less')\n        >>> res.pvalue\n        0.9953379953379957\n\n    This is equivalent to computing the cumulative distribution function\n    of the distribution at ``x = 6``:\n\n        >>> hypergeom.cdf(6, M, n, N)\n        0.9953379953379957\n\n    *Odds ratio*\n\n    The calculated odds ratio is different from the value computed by the\n    R function ``fisher.test``.  This implementation returns the \"sample\"\n    or \"unconditional\" maximum likelihood estimate, while ``fisher.test``\n    in R uses the conditional maximum likelihood estimate.  To compute the\n    conditional maximum likelihood estimate of the odds ratio, use\n    `scipy.stats.contingency.odds_ratio`.\n\n    References\n    ----------\n    .. [1] Fisher, Sir Ronald A, \"The Design of Experiments:\n           Mathematics of a Lady Tasting Tea.\" ISBN 978-0-486-41151-4, 1935.\n    .. [2] \"Fisher's exact test\",\n           https://en.wikipedia.org/wiki/Fisher's_exact_test\n\n    Examples\n    --------\n\n    >>> from scipy.stats import fisher_exact\n    >>> res = fisher_exact([[8, 2], [1, 5]])\n    >>> res.statistic\n    20.0\n    >>> res.pvalue\n    0.034965034965034975\n\n    For tables with shape other than ``(2, 2)``, provide an instance of\n    `scipy.stats.MonteCarloMethod` or `scipy.stats.PermutationMethod` for the\n    `method` parameter:\n\n    >>> import numpy as np\n    >>> from scipy.stats import MonteCarloMethod\n    >>> rng = np.random.default_rng(4507195762371367)\n    >>> method = MonteCarloMethod(rng=rng)\n    >>> fisher_exact([[8, 2, 3], [1, 5, 4]], method=method)\n    SignificanceResult(statistic=np.float64(0.005782), pvalue=np.float64(0.0603))\n\n    For a more detailed example, see :ref:`hypothesis_fisher_exact`.\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 5981, "code": "class TtestResult(TtestResultBase):", "documentation": "    \"\"\"\n    Result of a t-test.\n\n    See the documentation of the particular t-test function for more\n    information about the definition of the statistic and meaning of\n    the confidence interval.\n\n    Attributes\n    ----------\n    statistic : float or array\n        The t-statistic of the sample.\n    pvalue : float or array\n        The p-value associated with the given alternative.\n    df : float or array\n        The number of degrees of freedom used in calculation of the\n        t-statistic; this is one less than the size of the sample\n        (``a.shape[axis]-1`` if there are no masked elements or omitted NaNs).\n\n    Methods\n    -------\n    confidence_interval\n        Computes a confidence interval around the population statistic\n        for the given confidence level.\n        The confidence interval is returned in a ``namedtuple`` with\n        fields `low` and `high`.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 6022, "code": "    def confidence_interval(self, confidence_level=0.95):\n        low, high = _t_confidence_interval(self.df, self._statistic_np,\n                                           confidence_level, self._alternative,\n                                           self._dtype, self._xp)\n        low = low * self._standard_error + self._estimate\n        high = high * self._standard_error + self._estimate\n        return ConfidenceInterval(low=low, high=high)", "documentation": "        \"\"\"\n        Parameters\n        ----------\n        confidence_level : float\n            The confidence level for the calculation of the population mean\n            confidence interval. Default is 0.95.\n\n        Returns\n        -------\n        ci : namedtuple\n            The confidence interval is returned in a ``namedtuple`` with\n            fields `low` and `high`.\n\n        \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 6068, "code": "def ttest_1samp(a, popmean, axis=0, nan_policy=\"propagate\", alternative=\"two-sided\"):\n    xp = array_namespace(a)\n    a, popmean = xp_promote(a, popmean, force_floating=True, xp=xp)\n    a, axis = _chk_asarray(a, axis, xp=xp)\n    n = _length_nonmasked(a, axis)\n    df = n - 1\n    if a.shape[axis] == 0:\n        NaN = _get_nan(a)\n        return TtestResult(NaN, NaN, df=NaN, alternative=NaN,\n                           standard_error=NaN, estimate=NaN)\n    mean = xp.mean(a, axis=axis)", "documentation": "    \"\"\"Calculate the T-test for the mean of ONE group of scores.\n\n    This is a test for the null hypothesis that the expected value\n    (mean) of a sample of independent observations `a` is equal to the given\n    population mean, `popmean`.\n\n    Parameters\n    ----------\n    a : array_like\n        Sample observations.\n    popmean : float or array_like\n        Expected value in null hypothesis. If array_like, then its length along\n        `axis` must equal 1, and it must otherwise be broadcastable with `a`.\n    axis : int or None, optional\n        Axis along which to compute test; default is 0. If None, compute over\n        the whole array `a`.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis.\n        The following options are available (default is 'two-sided'):\n\n        * 'two-sided': the mean of the underlying distribution of the sample\n          is different than the given population mean (`popmean`)\n        * 'less': the mean of the underlying distribution of the sample is\n          less than the given population mean (`popmean`)\n        * 'greater': the mean of the underlying distribution of the sample is\n          greater than the given population mean (`popmean`)\n\n    Returns\n    -------\n    result : `~scipy.stats._result_classes.TtestResult`\n        An object with the following attributes:\n\n        statistic : float or array\n            The t-statistic.\n        pvalue : float or array\n            The p-value associated with the given alternative.\n        df : float or array\n            The number of degrees of freedom used in calculation of the\n            t-statistic; this is one less than the size of the sample\n            (``a.shape[axis]``).\n\n            .. versionadded:: 1.10.0\n\n        The object also has the following method:\n\n        confidence_interval(confidence_level=0.95)\n            Computes a confidence interval around the population\n            mean for the given confidence level.\n            The confidence interval is returned in a ``namedtuple`` with\n            fields `low` and `high`.\n\n            .. versionadded:: 1.10.0\n\n    Notes\n    -----\n    The statistic is calculated as ``(np.mean(a) - popmean)/se``, where\n    ``se`` is the standard error. Therefore, the statistic will be positive\n    when the sample mean is greater than the population mean and negative when\n    the sample mean is less than the population mean.\n\n    Examples\n    --------\n    Suppose we wish to test the null hypothesis that the mean of a population\n    is equal to 0.5. We choose a confidence level of 99%; that is, we will\n    reject the null hypothesis in favor of the alternative if the p-value is\n    less than 0.01.\n\n    When testing random variates from the standard uniform distribution, which\n    has a mean of 0.5, we expect the data to be consistent with the null\n    hypothesis most of the time.\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> rvs = stats.uniform.rvs(size=50, random_state=rng)\n    >>> stats.ttest_1samp(rvs, popmean=0.5)\n    TtestResult(statistic=2.456308468440, pvalue=0.017628209047638, df=49)\n\n    As expected, the p-value of 0.017 is not below our threshold of 0.01, so\n    we cannot reject the null hypothesis.\n\n    When testing data from the standard *normal* distribution, which has a mean\n    of 0, we would expect the null hypothesis to be rejected.\n\n    >>> rvs = stats.norm.rvs(size=50, random_state=rng)\n    >>> stats.ttest_1samp(rvs, popmean=0.5)\n    TtestResult(statistic=-7.433605518875, pvalue=1.416760157221e-09, df=49)\n\n    Indeed, the p-value is lower than our threshold of 0.01, so we reject the\n    null hypothesis in favor of the default \"two-sided\" alternative: the mean\n    of the population is *not* equal to 0.5.\n\n    However, suppose we were to test the null hypothesis against the\n    one-sided alternative that the mean of the population is *greater* than\n    0.5. Since the mean of the standard normal is less than 0.5, we would not\n    expect the null hypothesis to be rejected.\n\n    >>> stats.ttest_1samp(rvs, popmean=0.5, alternative='greater')\n    TtestResult(statistic=-7.433605518875, pvalue=0.99999999929, df=49)\n\n    Unsurprisingly, with a p-value greater than our threshold, we would not\n    reject the null hypothesis.\n\n    Note that when working with a confidence level of 99%, a true null\n    hypothesis will be rejected approximately 1% of the time.\n\n    >>> rvs = stats.uniform.rvs(size=(100, 50), random_state=rng)\n    >>> res = stats.ttest_1samp(rvs, popmean=0.5, axis=1)\n    >>> np.sum(res.pvalue < 0.01)\n    1\n\n    Indeed, even though all 100 samples above were drawn from the standard\n    uniform distribution, which *does* have a population mean of 0.5, we would\n    mistakenly reject the null hypothesis for one of them.\n\n    `ttest_1samp` can also compute a confidence interval around the population\n    mean.\n\n    >>> rvs = stats.norm.rvs(size=50, random_state=rng)\n    >>> res = stats.ttest_1samp(rvs, popmean=0)\n    >>> ci = res.confidence_interval(confidence_level=0.95)\n    >>> ci\n    ConfidenceInterval(low=-0.3193887540880017, high=0.2898583388980972)\n\n    The bounds of the 95% confidence interval are the\n    minimum and maximum values of the parameter `popmean` for which the\n    p-value of the test would be 0.05.\n\n    >>> res = stats.ttest_1samp(rvs, popmean=ci.low)\n    >>> np.testing.assert_allclose(res.pvalue, 0.05)\n    >>> res = stats.ttest_1samp(rvs, popmean=ci.high)\n    >>> np.testing.assert_allclose(res.pvalue, 0.05)\n\n    Under certain assumptions about the population from which a sample\n    is drawn, the confidence interval with confidence level 95% is expected\n    to contain the true population mean in 95% of sample replications.\n\n    >>> rvs = stats.norm.rvs(size=(50, 1000), loc=1, random_state=rng)\n    >>> res = stats.ttest_1samp(rvs, popmean=0)\n    >>> ci = res.confidence_interval()\n    >>> contains_pop_mean = (ci.low < 1) & (ci.high > 1)\n    >>> contains_pop_mean.sum()\n    953\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 6822, "code": "def _ttest_trim_var_mean_len(a, trim, axis):\n    a = np.sort(a, axis=axis)\n    n = a.shape[axis]\n    g = int(n * trim)\n    v = _calculate_winsorized_variance(a, g, axis)\n    n -= 2 * g\n    m = trim_mean(a, trim, axis=axis)\n    return v, m, n", "documentation": "    \"\"\"Variance, mean, and length of winsorized input along specified axis\"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 6846, "code": "def _calculate_winsorized_variance(a, g, axis):\n    if g == 0:\n        return _var(a, ddof=1, axis=axis)\n    a_win = np.moveaxis(a, axis, -1)\n    nans_indices = np.any(np.isnan(a_win), axis=-1)\n    a_win[..., :g] = a_win[..., [g]]\n    a_win[..., -g:] = a_win[..., [-g - 1]]\n    var_win = np.asarray(_var(a_win, ddof=(2 * g + 1), axis=-1))\n    var_win[nans_indices] = np.nan\n    return var_win\n@xp_capabilities(cpu_only=True, exceptions=[\"cupy\", \"jax.numpy\"])", "documentation": "    \"\"\"Calculates g-times winsorized variance along specified axis\"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 6885, "code": "def ttest_rel(a, b, axis=0, nan_policy='propagate', alternative=\"two-sided\"):\n    return ttest_1samp(a - b, popmean=0., axis=axis, alternative=alternative,\n                       _no_deco=True)\n_power_div_lambda_names = {\n    \"pearson\": 1,\n    \"log-likelihood\": 0,\n    \"freeman-tukey\": -0.5,\n    \"mod-log-likelihood\": -1,\n    \"neyman\": -2,\n    \"cressie-read\": 2/3,\n}", "documentation": "    \"\"\"Calculate the t-test on TWO RELATED samples of scores, a and b.\n\n    This is a test for the null hypothesis that two related or\n    repeated samples have identical average (expected) values.\n\n    Parameters\n    ----------\n    a, b : array_like\n        The arrays must have the same shape.\n    axis : int or None, optional\n        Axis along which to compute test. If None, compute over the whole\n        arrays, `a`, and `b`.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis.\n        The following options are available (default is 'two-sided'):\n\n        * 'two-sided': the means of the distributions underlying the samples\n          are unequal.\n        * 'less': the mean of the distribution underlying the first sample\n          is less than the mean of the distribution underlying the second\n          sample.\n        * 'greater': the mean of the distribution underlying the first\n          sample is greater than the mean of the distribution underlying\n          the second sample.\n\n        .. versionadded:: 1.6.0\n\n    Returns\n    -------\n    result : `~scipy.stats._result_classes.TtestResult`\n        An object with the following attributes:\n\n        statistic : float or array\n            The t-statistic.\n        pvalue : float or array\n            The p-value associated with the given alternative.\n        df : float or array\n            The number of degrees of freedom used in calculation of the\n            t-statistic; this is one less than the size of the sample\n            (``a.shape[axis]``).\n\n            .. versionadded:: 1.10.0\n\n        The object also has the following method:\n\n        confidence_interval(confidence_level=0.95)\n            Computes a confidence interval around the difference in\n            population means for the given confidence level.\n            The confidence interval is returned in a ``namedtuple`` with\n            fields `low` and `high`.\n\n            .. versionadded:: 1.10.0\n\n    Notes\n    -----\n    Examples for use are scores of the same set of student in\n    different exams, or repeated sampling from the same units. The\n    test measures whether the average score differs significantly\n    across samples (e.g. exams). If we observe a large p-value, for\n    example greater than 0.05 or 0.1 then we cannot reject the null\n    hypothesis of identical average scores. If the p-value is smaller\n    than the threshold, e.g. 1%, 5% or 10%, then we reject the null\n    hypothesis of equal averages. Small p-values are associated with\n    large t-statistics.\n\n    The t-statistic is calculated as ``np.mean(a - b)/se``, where ``se`` is the\n    standard error. Therefore, the t-statistic will be positive when the sample\n    mean of ``a - b`` is greater than zero and negative when the sample mean of\n    ``a - b`` is less than zero.\n\n    References\n    ----------\n    https://en.wikipedia.org/wiki/T-test#Dependent_t-test_for_paired_samples\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n\n    >>> rvs1 = stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n    >>> rvs2 = (stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n    ...         + stats.norm.rvs(scale=0.2, size=500, random_state=rng))\n    >>> stats.ttest_rel(rvs1, rvs2)\n    TtestResult(statistic=-0.4549717054410304, pvalue=0.6493274702088672, df=499)\n    >>> rvs3 = (stats.norm.rvs(loc=8, scale=10, size=500, random_state=rng)\n    ...         + stats.norm.rvs(scale=0.2, size=500, random_state=rng))\n    >>> stats.ttest_rel(rvs1, rvs3)\n    TtestResult(statistic=-5.879467544540889, pvalue=7.540777129099917e-09, df=499)\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7011, "code": "def power_divergence(f_obs, f_exp=None, ddof=0, axis=0, lambda_=None):\n    return _power_divergence(f_obs, f_exp=f_exp, ddof=ddof, axis=axis, lambda_=lambda_)", "documentation": "    \"\"\"Cressie-Read power divergence statistic and goodness of fit test.\n\n    This function tests the null hypothesis that the categorical data\n    has the given frequencies, using the Cressie-Read power divergence\n    statistic.\n\n    Parameters\n    ----------\n    f_obs : array_like\n        Observed frequencies in each category.\n    f_exp : array_like, optional\n        Expected frequencies in each category.  By default the categories are\n        assumed to be equally likely.\n    ddof : int, optional\n        \"Delta degrees of freedom\": adjustment to the degrees of freedom\n        for the p-value.  The p-value is computed using a chi-squared\n        distribution with ``k - 1 - ddof`` degrees of freedom, where `k`\n        is the number of observed frequencies.  The default value of `ddof`\n        is 0.\n    axis : int or None, optional\n        The axis of the broadcast result of `f_obs` and `f_exp` along which to\n        apply the test.  If axis is None, all values in `f_obs` are treated\n        as a single data set.  Default is 0.\n    lambda_ : float or str, optional\n        The power in the Cressie-Read power divergence statistic.  The default\n        is 1.  For convenience, `lambda_` may be assigned one of the following\n        strings, in which case the corresponding numerical value is used:\n\n        * ``\"pearson\"`` (value 1)\n            Pearson's chi-squared statistic. In this case, the function is\n            equivalent to `chisquare`.\n        * ``\"log-likelihood\"`` (value 0)\n            Log-likelihood ratio. Also known as the G-test [3]_.\n        * ``\"freeman-tukey\"`` (value -1/2)\n            Freeman-Tukey statistic.\n        * ``\"mod-log-likelihood\"`` (value -1)\n            Modified log-likelihood ratio.\n        * ``\"neyman\"`` (value -2)\n            Neyman's statistic.\n        * ``\"cressie-read\"`` (value 2/3)\n            The power recommended in [5]_.\n\n    Returns\n    -------\n    res: Power_divergenceResult\n        An object containing attributes:\n\n        statistic : float or ndarray\n            The Cressie-Read power divergence test statistic.  The value is\n            a float if `axis` is None or if` `f_obs` and `f_exp` are 1-D.\n        pvalue : float or ndarray\n            The p-value of the test.  The value is a float if `ddof` and the\n            return value `stat` are scalars.\n\n    See Also\n    --------\n    chisquare\n\n    Notes\n    -----\n    This test is invalid when the observed or expected frequencies in each\n    category are too small.  A typical rule is that all of the observed\n    and expected frequencies should be at least 5.\n\n    Also, the sum of the observed and expected frequencies must be the same\n    for the test to be valid; `power_divergence` raises an error if the sums\n    do not agree within a relative tolerance of ``eps**0.5``, where ``eps``\n    is the precision of the input dtype.\n\n    When `lambda_` is less than zero, the formula for the statistic involves\n    dividing by `f_obs`, so a warning or error may be generated if any value\n    in `f_obs` is 0.\n\n    Similarly, a warning or error may be generated if any value in `f_exp` is\n    zero when `lambda_` >= 0.\n\n    The default degrees of freedom, k-1, are for the case when no parameters\n    of the distribution are estimated. If p parameters are estimated by\n    efficient maximum likelihood then the correct degrees of freedom are\n    k-1-p. If the parameters are estimated in a different way, then the\n    dof can be between k-1-p and k-1. However, it is also possible that\n    the asymptotic distribution is not a chisquare, in which case this\n    test is not appropriate.\n\n    References\n    ----------\n    .. [1] Lowry, Richard.  \"Concepts and Applications of Inferential\n           Statistics\". Chapter 8.\n           https://web.archive.org/web/20171015035606/http://faculty.vassar.edu/lowry/ch8pt1.html\n    .. [2] \"Chi-squared test\", https://en.wikipedia.org/wiki/Chi-squared_test\n    .. [3] \"G-test\", https://en.wikipedia.org/wiki/G-test\n    .. [4] Sokal, R. R. and Rohlf, F. J. \"Biometry: the principles and\n           practice of statistics in biological research\", New York: Freeman\n           (1981)\n    .. [5] Cressie, N. and Read, T. R. C., \"Multinomial Goodness-of-Fit\n           Tests\", J. Royal Stat. Soc. Series B, Vol. 46, No. 3 (1984),\n           pp. 440-464.\n\n    Examples\n    --------\n    (See `chisquare` for more examples.)\n\n    When just `f_obs` is given, it is assumed that the expected frequencies\n    are uniform and given by the mean of the observed frequencies.  Here we\n    perform a G-test (i.e. use the log-likelihood ratio statistic):\n\n    >>> import numpy as np\n    >>> from scipy.stats import power_divergence\n    >>> power_divergence([16, 18, 16, 14, 12, 12], lambda_='log-likelihood')\n    (2.006573162632538, 0.84823476779463769)\n\n    The expected frequencies can be given with the `f_exp` argument:\n\n    >>> power_divergence([16, 18, 16, 14, 12, 12],\n    ...                  f_exp=[16, 16, 16, 16, 16, 8],\n    ...                  lambda_='log-likelihood')\n    (3.3281031458963746, 0.6495419288047497)\n\n    When `f_obs` is 2-D, by default the test is applied to each column.\n\n    >>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T\n    >>> obs.shape\n    (6, 2)\n    >>> power_divergence(obs, lambda_=\"log-likelihood\")\n    (array([ 2.00657316,  6.77634498]), array([ 0.84823477,  0.23781225]))\n\n    By setting ``axis=None``, the test is applied to all data in the array,\n    which is equivalent to applying the test to the flattened array.\n\n    >>> power_divergence(obs, axis=None)\n    (23.31034482758621, 0.015975692534127565)\n    >>> power_divergence(obs.ravel())\n    (23.31034482758621, 0.015975692534127565)\n\n    `ddof` is the change to make to the default degrees of freedom.\n\n    >>> power_divergence([16, 18, 16, 14, 12, 12], ddof=1)\n    (2.0, 0.73575888234288467)\n\n    The calculation of the p-values is done by broadcasting the\n    test statistic with `ddof`.\n\n    >>> power_divergence([16, 18, 16, 14, 12, 12], ddof=[0,1,2])\n    (2.0, array([ 0.84914504,  0.73575888,  0.5724067 ]))\n\n    `f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has\n    shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting\n    `f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared\n    statistics, we must use ``axis=1``:\n\n    >>> power_divergence([16, 18, 16, 14, 12, 12],\n    ...                  f_exp=[[16, 16, 16, 16, 16, 8],\n    ...                         [8, 20, 20, 16, 12, 12]],\n    ...                  axis=1)\n    (array([ 3.5 ,  9.25]), array([ 0.62338763,  0.09949846]))\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7258, "code": "def chisquare(f_obs, f_exp=None, ddof=0, axis=0, *, sum_check=True):\n    return _power_divergence(f_obs, f_exp=f_exp, ddof=ddof, axis=axis,\n                             lambda_=\"pearson\", sum_check=sum_check)\nKstestResult = _make_tuple_bunch('KstestResult', ['statistic', 'pvalue'],\n                                 ['statistic_location', 'statistic_sign'])", "documentation": "    \"\"\"Perform Pearson's chi-squared test.\n\n    Pearson's chi-squared test [1]_ is a goodness-of-fit test for a multinomial\n    distribution with given probabilities; that is, it assesses the null hypothesis\n    that the observed frequencies (counts) are obtained by independent\n    sampling of *N* observations from a categorical distribution with given\n    expected frequencies.\n\n    Parameters\n    ----------\n    f_obs : array_like\n        Observed frequencies in each category.\n    f_exp : array_like, optional\n        Expected frequencies in each category. By default, the categories are\n        assumed to be equally likely.\n    ddof : int, optional\n        \"Delta degrees of freedom\": adjustment to the degrees of freedom\n        for the p-value.  The p-value is computed using a chi-squared\n        distribution with ``k - 1 - ddof`` degrees of freedom, where ``k``\n        is the number of categories.  The default value of `ddof` is 0.\n    axis : int or None, optional\n        The axis of the broadcast result of `f_obs` and `f_exp` along which to\n        apply the test.  If axis is None, all values in `f_obs` are treated\n        as a single data set.  Default is 0.\n    sum_check : bool, optional\n        Whether to perform a check that ``sum(f_obs) - sum(f_exp) == 0``. If True,\n        (default) raise an error (or, for lazy backends, return NaN) when the relative\n        difference exceeds the square root of the precision of the data type.\n        See Notes for rationale and possible exceptions.\n\n    Returns\n    -------\n    res: Power_divergenceResult\n        An object containing attributes:\n\n        statistic : float or ndarray\n            The chi-squared test statistic.  The value is a float if `axis` is\n            None or `f_obs` and `f_exp` are 1-D.\n        pvalue : float or ndarray\n            The p-value of the test.  The value is a float if `ddof` and the\n            result attribute `statistic` are scalars.\n\n    See Also\n    --------\n    scipy.stats.power_divergence\n    scipy.stats.fisher_exact : Fisher exact test on a 2x2 contingency table.\n    scipy.stats.barnard_exact : An unconditional exact test. An alternative\n        to chi-squared test for small sample sizes.\n    :ref:`hypothesis_chisquare` : Extended example\n\n    Notes\n    -----\n    This test is invalid when the observed or expected frequencies in each\n    category are too small.  A typical rule is that all of the observed\n    and expected frequencies should be at least 5. According to [2]_, the\n    total number of observations is recommended to be greater than 13,\n    otherwise exact tests (such as Barnard's Exact test) should be used\n    because they do not overreject.\n\n    The default degrees of freedom, k-1, are for the case when no parameters\n    of the distribution are estimated. If p parameters are estimated by\n    efficient maximum likelihood then the correct degrees of freedom are\n    k-1-p. If the parameters are estimated in a different way, then the\n    dof can be between k-1-p and k-1. However, it is also possible that\n    the asymptotic distribution is not chi-square, in which case this test\n    is not appropriate.\n\n    For Pearson's chi-squared test, the total observed and expected counts must match\n    for the p-value to accurately reflect the probability of observing such an extreme\n    value of the statistic under the null hypothesis.\n    This function may be used to perform other statistical tests that do not require\n    the total counts to be equal. For instance, to test the null hypothesis that\n    ``f_obs[i]`` is Poisson-distributed with expectation ``f_exp[i]``, set ``ddof=-1``\n    and ``sum_check=False``. This test follows from the fact that a Poisson random\n    variable with mean and variance ``f_exp[i]`` is approximately normal with the\n    same mean and variance; the chi-squared statistic standardizes, squares, and sums\n    the observations; and the sum of ``n`` squared standard normal variables follows\n    the chi-squared distribution with ``n`` degrees of freedom.\n\n    References\n    ----------\n    .. [1] \"Pearson's chi-squared test\".\n           *Wikipedia*. https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test\n    .. [2] Pearson, Karl. \"On the criterion that a given system of deviations from the probable\n           in the case of a correlated system of variables is such that it can be reasonably\n           supposed to have arisen from random sampling\", Philosophical Magazine. Series 5. 50\n           (1900), pp. 157-175.\n\n    Examples\n    --------\n    When only the mandatory `f_obs` argument is given, it is assumed that the\n    expected frequencies are uniform and given by the mean of the observed\n    frequencies:\n\n    >>> import numpy as np\n    >>> from scipy.stats import chisquare\n    >>> chisquare([16, 18, 16, 14, 12, 12])\n    Power_divergenceResult(statistic=2.0, pvalue=0.84914503608460956)\n\n    The optional `f_exp` argument gives the expected frequencies.\n\n    >>> chisquare([16, 18, 16, 14, 12, 12], f_exp=[16, 16, 16, 16, 16, 8])\n    Power_divergenceResult(statistic=3.5, pvalue=0.62338762774958223)\n\n    When `f_obs` is 2-D, by default the test is applied to each column.\n\n    >>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T\n    >>> obs.shape\n    (6, 2)\n    >>> chisquare(obs)\n    Power_divergenceResult(statistic=array([2.        , 6.66666667]), pvalue=array([0.84914504, 0.24663415]))\n\n    By setting ``axis=None``, the test is applied to all data in the array,\n    which is equivalent to applying the test to the flattened array.\n\n    >>> chisquare(obs, axis=None)\n    Power_divergenceResult(statistic=23.31034482758621, pvalue=0.015975692534127565)\n    >>> chisquare(obs.ravel())\n    Power_divergenceResult(statistic=23.310344827586206, pvalue=0.01597569253412758)\n\n    `ddof` is the change to make to the default degrees of freedom.\n\n    >>> chisquare([16, 18, 16, 14, 12, 12], ddof=1)\n    Power_divergenceResult(statistic=2.0, pvalue=0.7357588823428847)\n\n    The calculation of the p-values is done by broadcasting the\n    chi-squared statistic with `ddof`.\n\n    >>> chisquare([16, 18, 16, 14, 12, 12], ddof=[0, 1, 2])\n    Power_divergenceResult(statistic=2.0, pvalue=array([0.84914504, 0.73575888, 0.5724067 ]))\n\n    `f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has\n    shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting\n    `f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared\n    statistics, we use ``axis=1``:\n\n    >>> chisquare([16, 18, 16, 14, 12, 12],\n    ...           f_exp=[[16, 16, 16, 16, 16, 8], [8, 20, 20, 16, 12, 12]],\n    ...           axis=1)\n    Power_divergenceResult(statistic=array([3.5 , 9.25]), pvalue=array([0.62338763, 0.09949846]))\n\n    For a more detailed example, see :ref:`hypothesis_chisquare`.\n    \"\"\"  # noqa: E501"}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7410, "code": "def _compute_d(cdfvals, x, sign, xp=None):\n    xp = array_namespace(cdfvals, x) if xp is None else xp\n    n = cdfvals.shape[-1]\n    D = (xp.arange(1.0, n + 1, dtype=x.dtype) / n - cdfvals if sign == +1\n         else (cdfvals - xp.arange(0.0, n, dtype=x.dtype)/n))\n    amax = xp.argmax(D, axis=-1, keepdims=True)\n    loc_max = xp.squeeze(xp.take_along_axis(x, amax, axis=-1), axis=-1)\n    D = xp.squeeze(xp.take_along_axis(D, amax, axis=-1), axis=-1)\n    return D[()] if D.ndim == 0 else D, loc_max[()] if loc_max.ndim == 0 else loc_max", "documentation": "    \"\"\"Computes D+/D- as used in the Kolmogorov-Smirnov test.\n\n    Vectorized along the last axis.\n\n    Parameters\n    ----------\n    cdfvals : array_like\n        Sorted array of CDF values between 0 and 1\n    x: array_like\n        Sorted array of the stochastic variable itself\n    sign: int\n        Indicates whether to compute D+ (+1) or D- (-1).\n\n    Returns\n    -------\n    D : float or array\n        The maximum distance of the CDF values below/above (D+/D-) Uniform(0, 1).\n    loc_max : float or array\n        The location at which the maximum is reached.\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7458, "code": "def ks_1samp(x, cdf, args=(), alternative='two-sided', method='auto', *, axis=0):\n    xp = array_namespace(x)\n    mode = method\n    alternative = {'t': 'two-sided', 'g': 'greater', 'l': 'less'}.get(\n        alternative.lower()[0], alternative)\n    if alternative not in ['two-sided', 'greater', 'less']:\n        raise ValueError(f\"Unexpected value {alternative=}\")\n    N = x.shape[-1]\n    x = xp.sort(x, axis=-1)\n    x = xp_promote(x, force_floating=True, xp=xp)\n    cdfvals = cdf(x, *args)", "documentation": "    \"\"\"\n    Performs the one-sample Kolmogorov-Smirnov test for goodness of fit.\n\n    This test compares the underlying distribution F(x) of a sample\n    against a given continuous distribution G(x). See Notes for a description\n    of the available null and alternative hypotheses.\n\n    Parameters\n    ----------\n    x : array_like\n        a 1-D array of observations of iid random variables.\n    cdf : callable\n        callable used to calculate the cdf.\n    args : tuple, sequence, optional\n        Distribution parameters, used with `cdf`.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the null and alternative hypotheses. Default is 'two-sided'.\n        Please see explanations in the Notes below.\n    method : {'auto', 'exact', 'approx', 'asymp'}, optional\n        Defines the distribution used for calculating the p-value.\n        The following options are available (default is 'auto'):\n\n        * 'auto' : selects one of the other options.\n        * 'exact' : uses the exact distribution of test statistic.\n        * 'approx' : approximates the two-sided probability with twice\n          the one-sided probability\n        * 'asymp': uses asymptotic distribution of test statistic\n\n    axis : int or tuple of ints, default: 0\n        If an int or tuple of ints, the axis or axes of the input along which\n        to compute the statistic. The statistic of each axis-slice (e.g. row)\n        of the input will appear in a corresponding element of the output.\n        If ``None``, the input will be raveled before computing the statistic.\n\n    Returns\n    -------\n    res: KstestResult\n        An object containing attributes:\n\n        statistic : float\n            KS test statistic, either D+, D-, or D (the maximum of the two)\n        pvalue : float\n            One-tailed or two-tailed p-value.\n        statistic_location : float\n            Value of `x` corresponding with the KS statistic; i.e., the\n            distance between the empirical distribution function and the\n            hypothesized cumulative distribution function is measured at this\n            observation.\n        statistic_sign : int\n            +1 if the KS statistic is the maximum positive difference between\n            the empirical distribution function and the hypothesized cumulative\n            distribution function (D+); -1 if the KS statistic is the maximum\n            negative difference (D-).\n\n\n    See Also\n    --------\n    ks_2samp, kstest\n\n    Notes\n    -----\n    There are three options for the null and corresponding alternative\n    hypothesis that can be selected using the `alternative` parameter.\n\n    - `two-sided`: The null hypothesis is that the two distributions are\n      identical, F(x)=G(x) for all x; the alternative is that they are not\n      identical.\n\n    - `less`: The null hypothesis is that F(x) >= G(x) for all x; the\n      alternative is that F(x) < G(x) for at least one x.\n\n    - `greater`: The null hypothesis is that F(x) <= G(x) for all x; the\n      alternative is that F(x) > G(x) for at least one x.\n\n    Note that the alternative hypotheses describe the *CDFs* of the\n    underlying distributions, not the observed values. For example,\n    suppose x1 ~ F and x2 ~ G. If F(x) > G(x) for all x, the values in\n    x1 tend to be less than those in x2.\n\n    Examples\n    --------\n    Suppose we wish to test the null hypothesis that a sample is distributed\n    according to the standard normal.\n    We choose a confidence level of 95%; that is, we will reject the null\n    hypothesis in favor of the alternative if the p-value is less than 0.05.\n\n    When testing uniformly distributed data, we would expect the\n    null hypothesis to be rejected.\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> stats.ks_1samp(stats.uniform.rvs(size=100, random_state=rng),\n    ...                stats.norm.cdf)\n    KstestResult(statistic=0.5001899973268688,\n                 pvalue=1.1616392184763533e-23,\n                 statistic_location=0.00047625268963724654,\n                 statistic_sign=-1)\n\n    Indeed, the p-value is lower than our threshold of 0.05, so we reject the\n    null hypothesis in favor of the default \"two-sided\" alternative: the data\n    are *not* distributed according to the standard normal.\n\n    When testing random variates from the standard normal distribution, we\n    expect the data to be consistent with the null hypothesis most of the time.\n\n    >>> x = stats.norm.rvs(size=100, random_state=rng)\n    >>> stats.ks_1samp(x, stats.norm.cdf)\n    KstestResult(statistic=0.05345882212970396,\n                 pvalue=0.9227159037744717,\n                 statistic_location=-1.2451343873745018,\n                 statistic_sign=1)\n\n    As expected, the p-value of 0.92 is not below our threshold of 0.05, so\n    we cannot reject the null hypothesis.\n\n    Suppose, however, that the random variates are distributed according to\n    a normal distribution that is shifted toward greater values. In this case,\n    the cumulative density function (CDF) of the underlying distribution tends\n    to be *less* than the CDF of the standard normal. Therefore, we would\n    expect the null hypothesis to be rejected with ``alternative='less'``:\n\n    >>> x = stats.norm.rvs(size=100, loc=0.5, random_state=rng)\n    >>> stats.ks_1samp(x, stats.norm.cdf, alternative='less')\n    KstestResult(statistic=0.17482387821055168,\n                 pvalue=0.001913921057766743,\n                 statistic_location=0.3713830565352756,\n                 statistic_sign=-1)\n\n    and indeed, with p-value smaller than our threshold, we reject the null\n    hypothesis in favor of the alternative.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7652, "code": "def _compute_prob_outside_square(n, h):\n    P = 0.0\n    k = int(np.floor(n / h))\n    while k >= 0:\n        p1 = 1.0\n        for j in range(h):\n            p1 = (n - k * h - j) * p1 / (n + k * h + j + 1)\n        P = p1 * (1.0 - P)\n        k -= 1\n    return 2 * P", "documentation": "    \"\"\"\n    Compute the proportion of paths that pass outside the two diagonal lines.\n\n    Parameters\n    ----------\n    n : int\n        n > 0\n    h : int\n        0 <= h <= n\n\n    Returns\n    -------\n    p : float\n        The proportion of paths that pass outside the lines x-y = +/-h.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7690, "code": "def _count_paths_outside_method(m, n, g, h):\n    if m < n:\n        m, n = n, m\n    mg = m // g\n    ng = n // g\n    lxj = n + (mg-h)//mg\n    xj = [(h + mg * j + ng-1)//ng for j in range(lxj)]\n    if lxj == 0:\n        return special.binom(m + n, n)\n    B = np.zeros(lxj)\n    B[0] = 1", "documentation": "    \"\"\"Count the number of paths that pass outside the specified diagonal.\n\n    Parameters\n    ----------\n    m : int\n        m > 0\n    n : int\n        n > 0\n    g : int\n        g is greatest common divisor of m and n\n    h : int\n        0 <= h <= lcm(m,n)\n\n    Returns\n    -------\n    p : float\n        The number of paths that go low.\n        The calculation may overflow - check for a finite answer.\n\n    Notes\n    -----\n    Count the integer lattice paths from (0, 0) to (m, n), which at some\n    point (x, y) along the path, satisfy:\n      m*y <= n*x - h*g\n    The paths make steps of size +1 in either positive x or positive y\n    directions.\n\n    We generally follow Hodges' treatment of Drion/Gnedenko/Korolyuk.\n    Hodges, J.L. Jr.,\n    \"The Significance Probability of the Smirnov Two-Sample Test,\"\n    Arkiv fiur Matematik, 3, No. 43 (1958), 469-86.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7764, "code": "def _attempt_exact_2kssamp(n1, n2, g, d, alternative):\n    lcm = (n1 // g) * n2\n    h = int(np.round(d * lcm))\n    d = h * 1.0 / lcm\n    if h == 0:\n        return True, d, 1.0\n    saw_fp_error, prob = False, np.nan\n    try:\n        with np.errstate(invalid=\"raise\", over=\"raise\"):\n            if alternative == 'two-sided':\n                if n1 == n2:", "documentation": "    \"\"\"Attempts to compute the exact 2sample probability.\n\n    n1, n2 are the sample sizes\n    g is the gcd(n1, n2)\n    d is the computed max difference in ECDFs\n\n    Returns (success, d, probability)\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7818, "code": "def ks_2samp(data1, data2, alternative='two-sided', method='auto', *, axis=0):\n    mode = method\n    if mode not in ['auto', 'exact', 'asymp']:\n        raise ValueError(f'Invalid value for mode: {mode}')\n    alternative = {'t': 'two-sided', 'g': 'greater', 'l': 'less'}.get(\n        alternative.lower()[0], alternative)\n    if alternative not in ['two-sided', 'less', 'greater']:\n        raise ValueError(f'Invalid value for alternative: {alternative}')\n    MAX_AUTO_N = 10000  # 'auto' will attempt to be exact if n1,n2 <= MAX_AUTO_N\n    xp = array_namespace(data1, data2)\n    data1 = xp.sort(data1, axis=-1)", "documentation": "    \"\"\"\n    Performs the two-sample Kolmogorov-Smirnov test for goodness of fit.\n\n    This test compares the underlying continuous distributions F(x) and G(x)\n    of two independent samples.  See Notes for a description of the available\n    null and alternative hypotheses.\n\n    Parameters\n    ----------\n    data1, data2 : array_like, 1-Dimensional\n        Two arrays of sample observations assumed to be drawn from a continuous\n        distribution, sample sizes can be different.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the null and alternative hypotheses. Default is 'two-sided'.\n        Please see explanations in the Notes below.\n    method : {'auto', 'exact', 'asymp'}, optional\n        Defines the method used for calculating the p-value.\n        The following options are available (default is 'auto'):\n\n        * 'auto' : use 'exact' for small size arrays, 'asymp' for large\n        * 'exact' : use exact distribution of test statistic\n        * 'asymp' : use asymptotic distribution of test statistic\n\n    axis : int or tuple of ints, default: 0\n        If an int or tuple of ints, the axis or axes of the input along which\n        to compute the statistic. The statistic of each axis-slice (e.g. row)\n        of the input will appear in a corresponding element of the output.\n        If ``None``, the input will be raveled before computing the statistic.\n\n    Returns\n    -------\n    res: KstestResult\n        An object containing attributes:\n\n        statistic : float\n            KS test statistic.\n        pvalue : float\n            One-tailed or two-tailed p-value.\n        statistic_location : float\n            Value from `data1` or `data2` corresponding with the KS statistic;\n            i.e., the distance between the empirical distribution functions is\n            measured at this observation.\n        statistic_sign : int\n            +1 if the empirical distribution function of `data1` exceeds\n            the empirical distribution function of `data2` at\n            `statistic_location`, otherwise -1.\n\n    See Also\n    --------\n    kstest, ks_1samp, epps_singleton_2samp, anderson_ksamp\n\n    Notes\n    -----\n    There are three options for the null and corresponding alternative\n    hypothesis that can be selected using the `alternative` parameter.\n\n    - `less`: The null hypothesis is that F(x) >= G(x) for all x; the\n      alternative is that F(x) < G(x) for at least one x. The statistic\n      is the magnitude of the minimum (most negative) difference between the\n      empirical distribution functions of the samples.\n\n    - `greater`: The null hypothesis is that F(x) <= G(x) for all x; the\n      alternative is that F(x) > G(x) for at least one x. The statistic\n      is the maximum (most positive) difference between the empirical\n      distribution functions of the samples.\n\n    - `two-sided`: The null hypothesis is that the two distributions are\n      identical, F(x)=G(x) for all x; the alternative is that they are not\n      identical. The statistic is the maximum absolute difference between the\n      empirical distribution functions of the samples.\n\n    Note that the alternative hypotheses describe the *CDFs* of the\n    underlying distributions, not the observed values of the data. For example,\n    suppose x1 ~ F and x2 ~ G. If F(x) > G(x) for all x, the values in\n    x1 tend to be less than those in x2.\n\n    If the KS statistic is large, then the p-value will be small, and this may\n    be taken as evidence against the null hypothesis in favor of the\n    alternative.\n\n    If ``method='exact'``, `ks_2samp` attempts to compute an exact p-value,\n    that is, the probability under the null hypothesis of obtaining a test\n    statistic value as extreme as the value computed from the data.\n    If ``method='asymp'``, the asymptotic Kolmogorov-Smirnov distribution is\n    used to compute an approximate p-value.\n    If ``method='auto'``, an exact p-value computation is attempted if both\n    sample sizes are less than 10000; otherwise, the asymptotic method is used.\n    In any case, if an exact p-value calculation is attempted and fails, a\n    warning will be emitted, and the asymptotic p-value will be returned.\n\n    The 'two-sided' 'exact' computation computes the complementary probability\n    and then subtracts from 1.  As such, the minimum probability it can return\n    is about 1e-16.  While the algorithm itself is exact, numerical\n    errors may accumulate for large sample sizes.   It is most suited to\n    situations in which one of the sample sizes is only a few thousand.\n\n    We generally follow Hodges' treatment of Drion/Gnedenko/Korolyuk [1]_.\n\n    References\n    ----------\n    .. [1] Hodges, J.L. Jr.,  \"The Significance Probability of the Smirnov\n           Two-Sample Test,\" Arkiv fiur Matematik, 3, No. 43 (1958), 469-486.\n\n    Examples\n    --------\n    Suppose we wish to test the null hypothesis that two samples were drawn\n    from the same distribution.\n    We choose a confidence level of 95%; that is, we will reject the null\n    hypothesis in favor of the alternative if the p-value is less than 0.05.\n\n    If the first sample were drawn from a uniform distribution and the second\n    were drawn from the standard normal, we would expect the null hypothesis\n    to be rejected.\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> sample1 = stats.uniform.rvs(size=100, random_state=rng)\n    >>> sample2 = stats.norm.rvs(size=110, random_state=rng)\n    >>> stats.ks_2samp(sample1, sample2)\n    KstestResult(statistic=0.5454545454545454,\n                 pvalue=7.37417839555191e-15,\n                 statistic_location=-0.014071496412861274,\n                 statistic_sign=-1)\n\n\n    Indeed, the p-value is lower than our threshold of 0.05, so we reject the\n    null hypothesis in favor of the default \"two-sided\" alternative: the data\n    were *not* drawn from the same distribution.\n\n    When both samples are drawn from the same distribution, we expect the data\n    to be consistent with the null hypothesis most of the time.\n\n    >>> sample1 = stats.norm.rvs(size=105, random_state=rng)\n    >>> sample2 = stats.norm.rvs(size=95, random_state=rng)\n    >>> stats.ks_2samp(sample1, sample2)\n    KstestResult(statistic=0.10927318295739348,\n                 pvalue=0.5438289009927495,\n                 statistic_location=-0.1670157701848795,\n                 statistic_sign=-1)\n\n    As expected, the p-value of 0.54 is not below our threshold of 0.05, so\n    we cannot reject the null hypothesis.\n\n    Suppose, however, that the first sample were drawn from\n    a normal distribution shifted toward greater values. In this case,\n    the cumulative density function (CDF) of the underlying distribution tends\n    to be *less* than the CDF underlying the second sample. Therefore, we would\n    expect the null hypothesis to be rejected with ``alternative='less'``:\n\n    >>> sample1 = stats.norm.rvs(size=105, loc=0.5, random_state=rng)\n    >>> stats.ks_2samp(sample1, sample2, alternative='less')\n    KstestResult(statistic=0.4055137844611529,\n                 pvalue=3.5474563068855554e-08,\n                 statistic_location=-0.13249370614972575,\n                 statistic_sign=-1)\n\n    and indeed, with p-value smaller than our threshold, we reject the null\n    hypothesis in favor of the alternative.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 8148, "code": "def kstest(rvs, cdf, args=(), N=20, alternative='two-sided', method='auto'):\n    if alternative == 'two_sided':\n        alternative = 'two-sided'\n    if alternative not in ['two-sided', 'greater', 'less']:\n        raise ValueError(f\"Unexpected alternative: {alternative}\")\n    xvals, yvals, cdf = _parse_kstest_args(rvs, cdf, args, N)\n    if cdf:\n        return ks_1samp(xvals, cdf, args=args, alternative=alternative,\n                        method=method, _no_deco=True)\n    return ks_2samp(xvals, yvals, alternative=alternative, method=method,\n                    _no_deco=True)", "documentation": "    \"\"\"\n    Performs the (one-sample or two-sample) Kolmogorov-Smirnov test for\n    goodness of fit.\n\n    The one-sample test compares the underlying distribution F(x) of a sample\n    against a given distribution G(x). The two-sample test compares the\n    underlying distributions of two independent samples. Both tests are valid\n    only for continuous distributions.\n\n    Parameters\n    ----------\n    rvs : str, array_like, or callable\n        If an array, it should be a 1-D array of observations of random\n        variables.\n        If a callable, it should be a function to generate random variables;\n        it is required to have a keyword argument `size`.\n        If a string, it should be the name of a distribution in `scipy.stats`,\n        which will be used to generate random variables.\n    cdf : str, array_like or callable\n        If array_like, it should be a 1-D array of observations of random\n        variables, and the two-sample test is performed\n        (and rvs must be array_like).\n        If a callable, that callable is used to calculate the cdf.\n        If a string, it should be the name of a distribution in `scipy.stats`,\n        which will be used as the cdf function.\n    args : tuple, sequence, optional\n        Distribution parameters, used if `rvs` or `cdf` are strings or\n        callables.\n    N : int, optional\n        Sample size if `rvs` is string or callable.  Default is 20.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the null and alternative hypotheses. Default is 'two-sided'.\n        Please see explanations in the Notes below.\n    method : {'auto', 'exact', 'approx', 'asymp'}, optional\n        Defines the distribution used for calculating the p-value.\n        The following options are available (default is 'auto'):\n\n        * 'auto' : selects one of the other options.\n        * 'exact' : uses the exact distribution of test statistic.\n        * 'approx' : approximates the two-sided probability with twice the\n          one-sided probability\n        * 'asymp': uses asymptotic distribution of test statistic\n\n    Returns\n    -------\n    res: KstestResult\n        An object containing attributes:\n\n        statistic : float\n            KS test statistic, either D+, D-, or D (the maximum of the two)\n        pvalue : float\n            One-tailed or two-tailed p-value.\n        statistic_location : float\n            In a one-sample test, this is the value of `rvs`\n            corresponding with the KS statistic; i.e., the distance between\n            the empirical distribution function and the hypothesized cumulative\n            distribution function is measured at this observation.\n\n            In a two-sample test, this is the value from `rvs` or `cdf`\n            corresponding with the KS statistic; i.e., the distance between\n            the empirical distribution functions is measured at this\n            observation.\n        statistic_sign : int\n            In a one-sample test, this is +1 if the KS statistic is the\n            maximum positive difference between the empirical distribution\n            function and the hypothesized cumulative distribution function\n            (D+); it is -1 if the KS statistic is the maximum negative\n            difference (D-).\n\n            In a two-sample test, this is +1 if the empirical distribution\n            function of `rvs` exceeds the empirical distribution\n            function of `cdf` at `statistic_location`, otherwise -1.\n\n    See Also\n    --------\n    ks_1samp, ks_2samp\n\n    Notes\n    -----\n    There are three options for the null and corresponding alternative\n    hypothesis that can be selected using the `alternative` parameter.\n\n    - `two-sided`: The null hypothesis is that the two distributions are\n      identical, F(x)=G(x) for all x; the alternative is that they are not\n      identical.\n\n    - `less`: The null hypothesis is that F(x) >= G(x) for all x; the\n      alternative is that F(x) < G(x) for at least one x.\n\n    - `greater`: The null hypothesis is that F(x) <= G(x) for all x; the\n      alternative is that F(x) > G(x) for at least one x.\n\n    Note that the alternative hypotheses describe the *CDFs* of the\n    underlying distributions, not the observed values. For example,\n    suppose x1 ~ F and x2 ~ G. If F(x) > G(x) for all x, the values in\n    x1 tend to be less than those in x2.\n\n\n    Examples\n    --------\n    Suppose we wish to test the null hypothesis that a sample is distributed\n    according to the standard normal.\n    We choose a confidence level of 95%; that is, we will reject the null\n    hypothesis in favor of the alternative if the p-value is less than 0.05.\n\n    When testing uniformly distributed data, we would expect the\n    null hypothesis to be rejected.\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> stats.kstest(stats.uniform.rvs(size=100, random_state=rng),\n    ...              stats.norm.cdf)\n    KstestResult(statistic=0.5001899973268688,\n                 pvalue=1.1616392184763533e-23,\n                 statistic_location=0.00047625268963724654,\n                 statistic_sign=-1)\n\n    Indeed, the p-value is lower than our threshold of 0.05, so we reject the\n    null hypothesis in favor of the default \"two-sided\" alternative: the data\n    are *not* distributed according to the standard normal.\n\n    When testing random variates from the standard normal distribution, we\n    expect the data to be consistent with the null hypothesis most of the time.\n\n    >>> x = stats.norm.rvs(size=100, random_state=rng)\n    >>> stats.kstest(x, stats.norm.cdf)\n    KstestResult(statistic=0.05345882212970396,\n                 pvalue=0.9227159037744717,\n                 statistic_location=-1.2451343873745018,\n                 statistic_sign=1)\n\n\n    As expected, the p-value of 0.92 is not below our threshold of 0.05, so\n    we cannot reject the null hypothesis.\n\n    Suppose, however, that the random variates are distributed according to\n    a normal distribution that is shifted toward greater values. In this case,\n    the cumulative density function (CDF) of the underlying distribution tends\n    to be *less* than the CDF of the standard normal. Therefore, we would\n    expect the null hypothesis to be rejected with ``alternative='less'``:\n\n    >>> x = stats.norm.rvs(size=100, loc=0.5, random_state=rng)\n    >>> stats.kstest(x, stats.norm.cdf, alternative='less')\n    KstestResult(statistic=0.17482387821055168,\n                 pvalue=0.001913921057766743,\n                 statistic_location=0.3713830565352756,\n                 statistic_sign=-1)\n\n    and indeed, with p-value smaller than our threshold, we reject the null\n    hypothesis in favor of the alternative.\n\n    For convenience, the previous test can be performed using the name of the\n    distribution as the second argument.\n\n    >>> stats.kstest(x, \"norm\", alternative='less')\n    KstestResult(statistic=0.17482387821055168,\n                 pvalue=0.001913921057766743,\n                 statistic_location=0.3713830565352756,\n                 statistic_sign=-1)\n\n    The examples above have all been one-sample tests identical to those\n    performed by `ks_1samp`. Note that `kstest` can also perform two-sample\n    tests identical to those performed by `ks_2samp`. For example, when two\n    samples are drawn from the same distribution, we expect the data to be\n    consistent with the null hypothesis most of the time.\n\n    >>> sample1 = stats.laplace.rvs(size=105, random_state=rng)\n    >>> sample2 = stats.laplace.rvs(size=95, random_state=rng)\n    >>> stats.kstest(sample1, sample2)\n    KstestResult(statistic=0.11779448621553884,\n                 pvalue=0.4494256912629795,\n                 statistic_location=0.6138814275424155,\n                 statistic_sign=1)\n\n    As expected, the p-value of 0.45 is not below our threshold of 0.05, so\n    we cannot reject the null hypothesis.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 8342, "code": "def tiecorrect(rankvals):\n    arr = np.sort(rankvals)\n    idx = np.nonzero(np.r_[True, arr[1:] != arr[:-1], True])[0]\n    cnt = np.diff(idx).astype(np.float64)\n    size = np.float64(arr.size)\n    return 1.0 if size < 2 else 1.0 - (cnt**3 - cnt).sum() / (size**3 - size)\nRanksumsResult = namedtuple('RanksumsResult', ('statistic', 'pvalue'))\n@xp_capabilities(np_only=True)\n@_axis_nan_policy_factory(RanksumsResult, n_samples=2)", "documentation": "    \"\"\"Tie correction factor for Mann-Whitney U and Kruskal-Wallis H tests.\n\n    Parameters\n    ----------\n    rankvals : array_like\n        A 1-D sequence of ranks.  Typically this will be the array\n        returned by `~scipy.stats.rankdata`.\n\n    Returns\n    -------\n    factor : float\n        Correction factor for U or H.\n\n    See Also\n    --------\n    rankdata : Assign ranks to the data\n    mannwhitneyu : Mann-Whitney rank test\n    kruskal : Kruskal-Wallis H test\n\n    References\n    ----------\n    .. [1] Siegel, S. (1956) Nonparametric Statistics for the Behavioral\n           Sciences.  New York: McGraw-Hill.\n\n    Examples\n    --------\n    >>> from scipy.stats import tiecorrect, rankdata\n    >>> tiecorrect([1, 2.5, 2.5, 4])\n    0.9\n    >>> ranks = rankdata([1, 3, 2, 4, 5, 7, 2, 8, 4])\n    >>> ranks\n    array([ 1. ,  4. ,  2.5,  5.5,  7. ,  8. ,  2.5,  9. ,  5.5])\n    >>> tiecorrect(ranks)\n    0.9833333333333333\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 8392, "code": "def ranksums(x, y, alternative='two-sided'):\n    x, y = map(np.asarray, (x, y))\n    n1 = len(x)\n    n2 = len(y)\n    alldata = np.concatenate((x, y))\n    ranked = rankdata(alldata)\n    x = ranked[:n1]\n    s = np.sum(x, axis=0)\n    expected = n1 * (n1+n2+1) / 2.0\n    z = (s - expected) / np.sqrt(n1*n2*(n1+n2+1)/12.0)\n    pvalue = _get_pvalue(z, _SimpleNormal(), alternative, xp=np)", "documentation": "    \"\"\"Compute the Wilcoxon rank-sum statistic for two samples.\n\n    The Wilcoxon rank-sum test tests the null hypothesis that two sets\n    of measurements are drawn from the same distribution.  The alternative\n    hypothesis is that values in one sample are more likely to be\n    larger than the values in the other sample.\n\n    This test should be used to compare two samples from continuous\n    distributions.  It does not handle ties between measurements\n    in x and y.  For tie-handling and an optional continuity correction\n    see `scipy.stats.mannwhitneyu`.\n\n    Parameters\n    ----------\n    x, y : array_like\n        The data from the two samples.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n        The following options are available:\n\n        * 'two-sided': one of the distributions (underlying `x` or `y`) is\n          stochastically greater than the other.\n        * 'less': the distribution underlying `x` is stochastically less\n          than the distribution underlying `y`.\n        * 'greater': the distribution underlying `x` is stochastically greater\n          than the distribution underlying `y`.\n\n        .. versionadded:: 1.7.0\n\n    Returns\n    -------\n    statistic : float\n        The test statistic under the large-sample approximation that the\n        rank sum statistic is normally distributed.\n    pvalue : float\n        The p-value of the test.\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/Wilcoxon_rank-sum_test\n\n    Examples\n    --------\n    We can test the hypothesis that two independent unequal-sized samples are\n    drawn from the same distribution with computing the Wilcoxon rank-sum\n    statistic.\n\n    >>> import numpy as np\n    >>> from scipy.stats import ranksums\n    >>> rng = np.random.default_rng()\n    >>> sample1 = rng.uniform(-1, 1, 200)\n    >>> sample2 = rng.uniform(-0.5, 1.5, 300) # a shifted distribution\n    >>> ranksums(sample1, sample2)\n    RanksumsResult(statistic=-7.887059,\n                   pvalue=3.09390448e-15) # may vary\n    >>> ranksums(sample1, sample2, alternative='less')\n    RanksumsResult(statistic=-7.750585297581713,\n                   pvalue=4.573497606342543e-15) # may vary\n    >>> ranksums(sample1, sample2, alternative='greater')\n    RanksumsResult(statistic=-7.750585297581713,\n                   pvalue=0.9999999999999954) # may vary\n\n    The p-value of less than ``0.05`` indicates that this test rejects the\n    hypothesis at the 5% significance level.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 8478, "code": "def kruskal(*samples, nan_policy='propagate', axis=0):\n    xp = array_namespace(*samples)\n    samples = xp_promote(*samples, force_floating=True, xp=xp)\n    num_groups = len(samples)\n    if num_groups < 2:\n        raise ValueError(\"Need at least two groups in stats.kruskal()\")\n    n = [sample.shape[-1] for sample in samples]\n    totaln = sum(n)\n    if any(n) < 1:  # Only needed for `test_axis_nan_policy`\n        raise ValueError(\"Inputs must not be empty.\")\n    alldata = xp.concat(samples, axis=-1)", "documentation": "    \"\"\"Compute the Kruskal-Wallis H-test for independent samples.\n\n    The Kruskal-Wallis H-test tests the null hypothesis that the population\n    median of all of the groups are equal.  It is a non-parametric version of\n    ANOVA.  The test works on 2 or more independent samples, which may have\n    different sizes.  Note that rejecting the null hypothesis does not\n    indicate which of the groups differs.  Post hoc comparisons between\n    groups are required to determine which groups are different.\n\n    Parameters\n    ----------\n    sample1, sample2, ... : array_like\n       Two or more arrays with the sample measurements can be given as\n       arguments. Samples must be one-dimensional.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    axis : int or tuple of ints, default: 0\n        If an int or tuple of ints, the axis or axes of the input along which\n        to compute the statistic. The statistic of each axis-slice (e.g. row)\n        of the input will appear in a corresponding element of the output.\n        If ``None``, the input will be raveled before computing the statistic.\n\n    Returns\n    -------\n    statistic : float\n       The Kruskal-Wallis H statistic, corrected for ties.\n    pvalue : float\n       The p-value for the test using the assumption that H has a chi\n       square distribution. The p-value returned is the survival function of\n       the chi square distribution evaluated at H.\n\n    See Also\n    --------\n    f_oneway : 1-way ANOVA.\n    mannwhitneyu : Mann-Whitney rank test on two samples.\n    friedmanchisquare : Friedman test for repeated measurements.\n\n    Notes\n    -----\n    Due to the assumption that H has a chi square distribution, the number\n    of samples in each group must not be too small.  A typical rule is\n    that each sample must have at least 5 measurements.\n\n    References\n    ----------\n    .. [1] W. H. Kruskal & W. W. Wallis, \"Use of Ranks in\n       One-Criterion Variance Analysis\", Journal of the American Statistical\n       Association, Vol. 47, Issue 260, pp. 583-621, 1952.\n    .. [2] https://en.wikipedia.org/wiki/Kruskal-Wallis_one-way_analysis_of_variance\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> x = [1, 3, 5, 7, 9]\n    >>> y = [2, 4, 6, 8, 10]\n    >>> stats.kruskal(x, y)\n    KruskalResult(statistic=0.2727272727272734, pvalue=0.6015081344405895)\n\n    >>> x = [1, 1, 1]\n    >>> y = [2, 2, 2]\n    >>> z = [2, 2]\n    >>> stats.kruskal(x, y, z)\n    KruskalResult(statistic=7.0, pvalue=0.0301973834223185)\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 8589, "code": "def friedmanchisquare(*samples, axis=0):\n    k = len(samples)\n    if k < 3:\n        raise ValueError('At least 3 samples must be given '\n                         f'for Friedman test, got {k}.')\n    xp = array_namespace(*samples)\n    samples = xp_promote(*samples, force_floating=True, xp=xp)\n    dtype = samples[0].dtype\n    n = samples[0].shape[-1]\n    if n == 0:  # only for `test_axis_nan_policy`; user doesn't see this\n        raise ValueError(\"One or more sample arguments is too small.\")", "documentation": "    \"\"\"Compute the Friedman test for repeated samples.\n\n    The Friedman test tests the null hypothesis that repeated samples of\n    the same individuals have the same distribution.  It is often used\n    to test for consistency among samples obtained in different ways.\n    For example, if two sampling techniques are used on the same set of\n    individuals, the Friedman test can be used to determine if the two\n    sampling techniques are consistent.\n\n    Parameters\n    ----------\n    sample1, sample2, sample3... : array_like\n        Arrays of observations.  All of the arrays must have the same number\n        of elements.  At least three samples must be given.\n    axis : int or tuple of ints, default: 0\n        If an int or tuple of ints, the axis or axes of the input along which\n        to compute the statistic. The statistic of each axis-slice (e.g. row)\n        of the input will appear in a corresponding element of the output.\n        If ``None``, the input will be raveled before computing the statistic.\n\n    Returns\n    -------\n    statistic : float\n        The test statistic, correcting for ties.\n    pvalue : float\n        The associated p-value assuming that the test statistic has a chi\n        squared distribution.\n\n    See Also\n    --------\n    :ref:`hypothesis_friedmanchisquare` : Extended example\n\n    Notes\n    -----\n    Due to the assumption that the test statistic has a chi squared\n    distribution, the p-value is only reliable for n > 10 and more than\n    6 repeated samples.\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/Friedman_test\n    .. [2] Demsar, J. (2006). Statistical comparisons of classifiers over\n           multiple data sets. Journal of Machine Learning Research, 7, 1-30.\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> rng = np.random.default_rng(seed=18)\n    >>> x = rng.random((6, 10))\n    >>> from scipy.stats import friedmanchisquare\n    >>> res = friedmanchisquare(x[0], x[1], x[2], x[3], x[4], x[5])\n    >>> res.statistic, res.pvalue\n    (11.428571428571416, 0.043514520866727614)\n\n    The p-value is less than 0.05; however, as noted above, the results may not\n    be reliable since we have a small number of repeated samples.\n\n    For a more detailed example, see :ref:`hypothesis_friedmanchisquare`.\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 8830, "code": "def combine_pvalues(pvalues, method='fisher', weights=None, *, axis=0):\n    xp = array_namespace(pvalues, weights)\n    pvalues, weights = xp_promote(pvalues, weights, broadcast=True,\n                                  force_floating=True, xp=xp)\n    if xp_size(pvalues) == 0:\n        NaN = _get_nan(pvalues)\n        return SignificanceResult(NaN, NaN)\n    n = _length_nonmasked(pvalues, axis)\n    n = xp.asarray(n, dtype=pvalues.dtype, device=xp_device(pvalues))\n    if method == 'fisher':\n        statistic = -2 * xp.sum(xp.log(pvalues), axis=axis)", "documentation": "    \"\"\"\n    Combine p-values from independent tests that bear upon the same hypothesis.\n\n    These methods are intended only for combining p-values from hypothesis\n    tests based upon continuous distributions.\n\n    Each method assumes that under the null hypothesis, the p-values are\n    sampled independently and uniformly from the interval [0, 1]. A test\n    statistic (different for each method) is computed and a combined\n    p-value is calculated based upon the distribution of this test statistic\n    under the null hypothesis.\n\n    Parameters\n    ----------\n    pvalues : array_like\n        Array of p-values assumed to come from independent tests based on\n        continuous distributions.\n    method : {'fisher', 'pearson', 'tippett', 'stouffer', 'mudholkar_george'}\n\n        Name of method to use to combine p-values.\n\n        The available methods are (see Notes for details):\n\n        * 'fisher': Fisher's method (Fisher's combined probability test)\n        * 'pearson': Pearson's method\n        * 'mudholkar_george': Mudholkar's and George's method\n        * 'tippett': Tippett's method\n        * 'stouffer': Stouffer's Z-score method\n\n    weights : array_like, optional\n        Optional array of weights used only for Stouffer's Z-score method.\n        Ignored by other methods.\n\n    Returns\n    -------\n    res : SignificanceResult\n        An object containing attributes:\n\n        statistic : float\n            The statistic calculated by the specified method.\n        pvalue : float\n            The combined p-value.\n\n    Examples\n    --------\n    Suppose we wish to combine p-values from four independent tests\n    of the same null hypothesis using Fisher's method (default).\n\n    >>> from scipy.stats import combine_pvalues\n    >>> pvalues = [0.1, 0.05, 0.02, 0.3]\n    >>> combine_pvalues(pvalues)\n    SignificanceResult(statistic=20.828626352604235, pvalue=0.007616871850449092)\n\n    When the individual p-values carry different weights, consider Stouffer's\n    method.\n\n    >>> weights = [1, 2, 3, 4]\n    >>> res = combine_pvalues(pvalues, method='stouffer', weights=weights)\n    >>> res.pvalue\n    0.009578891494533616\n\n    Notes\n    -----\n    If this function is applied to tests with a discrete statistics such as\n    any rank test or contingency-table test, it will yield systematically\n    wrong results, e.g. Fisher's method will systematically overestimate the\n    p-value [1]_. This problem becomes less severe for large sample sizes\n    when the discrete distributions become approximately continuous.\n\n    The differences between the methods can be best illustrated by their\n    statistics and what aspects of a combination of p-values they emphasise\n    when considering significance [2]_. For example, methods emphasising large\n    p-values are more sensitive to strong false and true negatives; conversely\n    methods focussing on small p-values are sensitive to positives.\n\n    * The statistics of Fisher's method (also known as Fisher's combined\n      probability test) [3]_ is :math:`-2\\\\sum_i \\\\log(p_i)`, which is\n      equivalent (as a test statistics) to the product of individual p-values:\n      :math:`\\\\prod_i p_i`. Under the null hypothesis, this statistics follows\n      a :math:`\\\\chi^2` distribution. This method emphasises small p-values.\n    * Pearson's method uses :math:`-2\\\\sum_i\\\\log(1-p_i)`, which is equivalent\n      to :math:`\\\\prod_i \\\\frac{1}{1-p_i}` [2]_.\n      It thus emphasises large p-values.\n    * Mudholkar and George compromise between Fisher's and Pearson's method by\n      averaging their statistics [4]_. Their method emphasises extreme\n      p-values, both close to 1 and 0.\n    * Stouffer's method [5]_ uses Z-scores and the statistic:\n      :math:`\\\\sum_i \\\\Phi^{-1} (p_i)`, where :math:`\\\\Phi` is the CDF of the\n      standard normal distribution. The advantage of this method is that it is\n      straightforward to introduce weights, which can make Stouffer's method\n      more powerful than Fisher's method when the p-values are from studies\n      of different size [6]_ [7]_.\n    * Tippett's method uses the smallest p-value as a statistic.\n      (Mind that this minimum is not the combined p-value.)\n\n    Fisher's method may be extended to combine p-values from dependent tests\n    [8]_. Extensions such as Brown's method and Kost's method are not currently\n    implemented.\n\n    .. versionadded:: 0.15.0\n\n    References\n    ----------\n    .. [1] Kincaid, W. M., \"The Combination of Tests Based on Discrete\n           Distributions.\" Journal of the American Statistical Association 57,\n           no. 297 (1962), 10-19.\n    .. [2] Heard, N. and Rubin-Delanchey, P. \"Choosing between methods of\n           combining p-values.\"  Biometrika 105.1 (2018): 239-246.\n    .. [3] https://en.wikipedia.org/wiki/Fisher%27s_method\n    .. [4] George, E. O., and G. S. Mudholkar. \"On the convolution of logistic\n           random variables.\" Metrika 30.1 (1983): 1-13.\n    .. [5] https://en.wikipedia.org/wiki/Fisher%27s_method#Relation_to_Stouffer.27s_Z-score_method\n    .. [6] Whitlock, M. C. \"Combining probability from independent tests: the\n           weighted Z-method is superior to Fisher's approach.\" Journal of\n           Evolutionary Biology 18, no. 5 (2005): 1368-1373.\n    .. [7] Zaykin, Dmitri V. \"Optimally weighted Z-test is a powerful method\n           for combining probabilities in meta-analysis.\" Journal of\n           Evolutionary Biology 24, no. 8 (2011): 1836-1841.\n    .. [8] https://en.wikipedia.org/wiki/Extensions_of_Fisher%27s_method\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 9043, "code": "    def confidence_interval(self, confidence_level=0.95):\n        alternative = self._alternative\n        p = self._p\n        x = np.sort(self._x)\n        n = len(x)\n        bd = stats.binom(n, p)\n        if confidence_level <= 0 or confidence_level >= 1:\n            message = \"`confidence_level` must be a number between 0 and 1.\"\n            raise ValueError(message)\n        low_index = np.nan\n        high_index = np.nan", "documentation": "        \"\"\"\n        Compute the confidence interval of the quantile.\n\n        Parameters\n        ----------\n        confidence_level : float, default: 0.95\n            Confidence level for the computed confidence interval\n            of the quantile. Default is 0.95.\n\n        Returns\n        -------\n        ci : ``ConfidenceInterval`` object\n            The object has attributes ``low`` and ``high`` that hold the\n            lower and upper bounds of the confidence interval.\n\n        Examples\n        --------\n        >>> import numpy as np\n        >>> import scipy.stats as stats\n        >>> p = 0.75  # quantile of interest\n        >>> q = 0  # hypothesized value of the quantile\n        >>> x = np.exp(np.arange(0, 1.01, 0.01))\n        >>> res = stats.quantile_test(x, q=q, p=p, alternative='less')\n        >>> lb, ub = res.confidence_interval()\n        >>> lb, ub\n        (-inf, 2.293318740264183)\n        >>> res = stats.quantile_test(x, q=q, p=p, alternative='two-sided')\n        >>> lb, ub = res.confidence_interval(0.9)\n        >>> lb, ub\n        (1.9542373206359396, 2.293318740264183)\n        \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 9945, "code": "def _validate_distribution(values, weights):\n    values = np.asarray(values, dtype=float)\n    if len(values) == 0:\n        raise ValueError(\"Distribution can't be empty.\")\n    if weights is not None:\n        weights = np.asarray(weights, dtype=float)\n        if len(weights) != len(values):\n            raise ValueError('Value and weight array-likes for the same '\n                             'empirical distribution must be of the same size.')\n        if np.any(weights < 0):\n            raise ValueError('All weights must be non-negative.')", "documentation": "    \"\"\"\n    Validate the values and weights from a distribution input of `cdf_distance`\n    and return them as ndarray objects.\n\n    Parameters\n    ----------\n    values : array_like\n        Values observed in the (empirical) distribution.\n    weights : array_like\n        Weight for each value.\n\n    Returns\n    -------\n    values : ndarray\n        Values as ndarray.\n    weights : ndarray\n        Weights as ndarray.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 9990, "code": "def rankdata(a, method='average', *, axis=None, nan_policy='propagate'):\n    methods = ('average', 'min', 'max', 'dense', 'ordinal')\n    if method not in methods:\n        raise ValueError(f'unknown method \"{method}\"')\n    xp = array_namespace(a)\n    x = xp.asarray(a)\n    if axis is None:\n        x = xp_ravel(x)\n        axis = -1\n    if xp_size(x) == 0:\n        dtype = xp.asarray(1.).dtype if method == 'average' else xp.asarray(1).dtype", "documentation": "    \"\"\"Assign ranks to data, dealing with ties appropriately.\n\n    By default (``axis=None``), the data array is first flattened, and a flat\n    array of ranks is returned. Separately reshape the rank array to the\n    shape of the data array if desired (see Examples).\n\n    Ranks begin at 1.  The `method` argument controls how ranks are assigned\n    to equal values.  See [1]_ for further discussion of ranking methods.\n\n    Parameters\n    ----------\n    a : array_like\n        The array of values to be ranked.\n    method : {'average', 'min', 'max', 'dense', 'ordinal'}, optional\n        The method used to assign ranks to tied elements.\n        The following methods are available (default is 'average'):\n\n        * 'average': The average of the ranks that would have been assigned to\n          all the tied values is assigned to each value.\n        * 'min': The minimum of the ranks that would have been assigned to all\n          the tied values is assigned to each value.  (This is also\n          referred to as \"competition\" ranking.)\n        * 'max': The maximum of the ranks that would have been assigned to all\n          the tied values is assigned to each value.\n        * 'dense': Like 'min', but the rank of the next highest element is\n          assigned the rank immediately after those assigned to the tied\n          elements.\n        * 'ordinal': All values are given a distinct rank, corresponding to\n          the order that the values occur in `a`.\n\n    axis : {None, int}, optional\n        Axis along which to perform the ranking. If ``None``, the data array\n        is first flattened.\n    nan_policy : {'propagate', 'omit', 'raise'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': propagates nans through the rank calculation\n        * 'omit': performs the calculations ignoring nan values\n        * 'raise': raises an error\n\n        .. note::\n\n            When `nan_policy` is 'propagate', the output is an array of *all*\n            nans because ranks relative to nans in the input are undefined.\n            When `nan_policy` is 'omit', nans in `a` are ignored when ranking\n            the other values, and the corresponding locations of the output\n            are nan.\n\n        .. versionadded:: 1.10\n\n    Returns\n    -------\n    ranks : ndarray\n         An array of size equal to the size of `a`, containing rank\n         scores.\n\n    References\n    ----------\n    .. [1] \"Ranking\", https://en.wikipedia.org/wiki/Ranking\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import rankdata\n    >>> rankdata([0, 2, 3, 2])\n    array([ 1. ,  2.5,  4. ,  2.5])\n    >>> rankdata([0, 2, 3, 2], method='min')\n    array([ 1,  2,  4,  2])\n    >>> rankdata([0, 2, 3, 2], method='max')\n    array([ 1,  3,  4,  3])\n    >>> rankdata([0, 2, 3, 2], method='dense')\n    array([ 1,  2,  3,  2])\n    >>> rankdata([0, 2, 3, 2], method='ordinal')\n    array([ 1,  2,  4,  3])\n    >>> rankdata([[0, 2], [3, 2]]).reshape(2,2)\n    array([[1. , 2.5],\n          [4. , 2.5]])\n    >>> rankdata([[0, 2, 2], [3, 2, 5]], axis=1)\n    array([[1. , 2.5, 2.5],\n           [2. , 1. , 3. ]])\n    >>> rankdata([0, 2, 3, np.nan, -2, np.nan], nan_policy=\"propagate\")\n    array([nan, nan, nan, nan, nan, nan])\n    >>> rankdata([0, 2, 3, np.nan, -2, np.nan], nan_policy=\"omit\")\n    array([ 2.,  3.,  4., nan,  1., nan])\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 10539, "code": "def linregress(x, y, alternative='two-sided', *, axis=0):\n    xp = array_namespace(x, y)\n    x, y = xp_promote(x, y, force_floating=True, xp=xp)\n    TINY = 1.0e-20\n    n = x.shape[-1]\n    xmean = xp.mean(x, axis=-1, keepdims=True)\n    ymean = xp.mean(y, axis=-1, keepdims=True)\n    x_ = _demean(x, xmean, axis=-1, xp=xp)\n    y_ = _demean(y, ymean, axis=-1, xp=xp, precision_warning=False)\n    xmean = xp.squeeze(xmean, axis=-1)\n    ymean = xp.squeeze(ymean, axis=-1)", "documentation": "    \"\"\"\n    Calculate a linear least-squares regression for two sets of measurements.\n\n    Parameters\n    ----------\n    x, y : array_like\n        Two sets of measurements.  Both arrays should have the same length N.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n        The following options are available:\n\n        * 'two-sided': the slope of the regression line is nonzero\n        * 'less': the slope of the regression line is less than zero\n        * 'greater':  the slope of the regression line is greater than zero\n\n        .. versionadded:: 1.7.0\n    axis : int or None, default: 0\n        If an int, the axis of the input along which to compute the statistic.\n        The statistic of each axis-slice (e.g. row) of the input will appear in a\n        corresponding element of the output.\n        If ``None``, the input will be raveled before computing the statistic.\n\n    Returns\n    -------\n    result : ``LinregressResult`` instance\n        The return value is an object with the following attributes:\n\n        slope : float\n            Slope of the regression line.\n        intercept : float\n            Intercept of the regression line.\n        rvalue : float\n            The Pearson correlation coefficient. The square of ``rvalue``\n            is equal to the coefficient of determination.\n        pvalue : float\n            The p-value for a hypothesis test whose null hypothesis is\n            that the slope is zero, using Wald Test with t-distribution of\n            the test statistic. See `alternative` above for alternative\n            hypotheses.\n        stderr : float\n            Standard error of the estimated slope (gradient), under the\n            assumption of residual normality.\n        intercept_stderr : float\n            Standard error of the estimated intercept, under the assumption\n            of residual normality.\n\n    See Also\n    --------\n    scipy.optimize.curve_fit :\n        Use non-linear least squares to fit a function to data.\n    scipy.optimize.leastsq :\n        Minimize the sum of squares of a set of equations.\n\n    Notes\n    -----\n    For compatibility with older versions of SciPy, the return value acts\n    like a ``namedtuple`` of length 5, with fields ``slope``, ``intercept``,\n    ``rvalue``, ``pvalue`` and ``stderr``, so one can continue to write::\n\n        slope, intercept, r, p, se = linregress(x, y)\n\n    With that style, however, the standard error of the intercept is not\n    available.  To have access to all the computed values, including the\n    standard error of the intercept, use the return value as an object\n    with attributes, e.g.::\n\n        result = linregress(x, y)\n        print(result.intercept, result.intercept_stderr)\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n\n    Generate some data:\n\n    >>> x = rng.random(10)\n    >>> y = 1.6*x + rng.random(10)\n\n    Perform the linear regression:\n\n    >>> res = stats.linregress(x, y)\n\n    Coefficient of determination (R-squared):\n\n    >>> print(f\"R-squared: {res.rvalue**2:.6f}\")\n    R-squared: 0.717533\n\n    Plot the data along with the fitted line:\n\n    >>> plt.plot(x, y, 'o', label='original data')\n    >>> plt.plot(x, res.intercept + res.slope*x, 'r', label='fitted line')\n    >>> plt.legend()\n    >>> plt.show()\n\n    Calculate 95% confidence interval on slope and intercept:\n\n    >>> # Two-sided inverse Students t-distribution\n    >>> # p - probability, df - degrees of freedom\n    >>> from scipy.stats import t\n    >>> tinv = lambda p, df: abs(t.ppf(p/2, df))\n\n    >>> ts = tinv(0.05, len(x)-2)\n    >>> print(f\"slope (95%): {res.slope:.6f} +/- {ts*res.stderr:.6f}\")\n    slope (95%): 1.453392 +/- 0.743465\n    >>> print(f\"intercept (95%): {res.intercept:.6f}\"\n    ...       f\" +/- {ts*res.intercept_stderr:.6f}\")\n    intercept (95%): 0.616950 +/- 0.544475\n\n    \"\"\""}], "after_segments": [{"filename": "scipy/stats/_stats_py.py", "start_line": 602, "code": "def _put_val_to_limits(a, limits, inclusive, val=np.nan, xp=None):\n    xp = array_namespace(a) if xp is None else xp\n    mask = xp.zeros_like(a, dtype=xp.bool)\n    if limits is None:\n        return a, mask\n    lower_limit, upper_limit = limits\n    lower_include, upper_include = inclusive\n    if lower_limit is not None:\n        mask = mask | ((a < lower_limit) if lower_include else a <= lower_limit)\n    if upper_limit is not None:\n        mask = mask | ((a > upper_limit) if upper_include else a >= upper_limit)", "documentation": "    \"\"\"Replace elements outside limits with a value.\n\n    This is primarily a utility function.\n\n    Parameters\n    ----------\n    a : array\n    limits : (float or None, float or None)\n        A tuple consisting of the (lower limit, upper limit).  Elements in the\n        input array less than the lower limit or greater than the upper limit\n        will be replaced with `val`. None implies no limit.\n    inclusive : (bool, bool)\n        A tuple consisting of the (lower flag, upper flag).  These flags\n        determine whether values exactly equal to lower or upper are allowed.\n    val : float, default: NaN\n        The value with which extreme elements of the array are replaced.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 644, "code": "def tmean(a, limits=None, inclusive=(True, True), axis=None):\n    xp = array_namespace(a)\n    a, mask = _put_val_to_limits(a, limits, inclusive, val=0., xp=xp)\n    sum = xp.sum(a, axis=axis, dtype=a.dtype)\n    n = xp.sum(xp.asarray(~mask, dtype=a.dtype, device=xp_device(a)), axis=axis,\n               dtype=a.dtype)\n    mean = xpx.apply_where(n != 0, (sum, n), operator.truediv, fill_value=xp.nan)\n    return mean[()] if mean.ndim == 0 else mean\n@xp_capabilities()\n@_axis_nan_policy_factory(\n    lambda x: x, n_outputs=1, result_to_tuple=lambda x, _: (x,)", "documentation": "    \"\"\"Compute the trimmed mean.\n\n    This function finds the arithmetic mean of given values, ignoring values\n    outside the given `limits`.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    limits : None or (lower limit, upper limit), optional\n        Values in the input array less than the lower limit or greater than the\n        upper limit will be ignored.  When limits is None (default), then all\n        values are used.  Either of the limit values in the tuple can also be\n        None representing a half-open interval.\n    inclusive : (bool, bool), optional\n        A tuple consisting of the (lower flag, upper flag).  These flags\n        determine whether values exactly equal to the lower or upper limits\n        are included.  The default value is (True, True).\n    axis : int or None, optional\n        Axis along which to compute test. Default is None.\n\n    Returns\n    -------\n    tmean : ndarray\n        Trimmed mean.\n\n    See Also\n    --------\n    trim_mean : Returns mean after trimming a proportion from both tails.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tmean(x)\n    9.5\n    >>> stats.tmean(x, (3,17))\n    10.0\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 700, "code": "def tvar(a, limits=None, inclusive=(True, True), axis=0, ddof=1):\n    xp = array_namespace(a)\n    a, _ = _put_val_to_limits(a, limits, inclusive, xp=xp)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", SmallSampleWarning)\n        return _xp_var(a, correction=ddof, axis=axis, nan_policy='omit', xp=xp)\n@xp_capabilities()\n@_axis_nan_policy_factory(\n    lambda x: x, n_outputs=1, result_to_tuple=lambda x, _: (x,)\n)", "documentation": "    \"\"\"Compute the trimmed variance.\n\n    This function computes the sample variance of an array of values,\n    while ignoring values which are outside of given `limits`.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    limits : None or (lower limit, upper limit), optional\n        Values in the input array less than the lower limit or greater than the\n        upper limit will be ignored. When limits is None, then all values are\n        used. Either of the limit values in the tuple can also be None\n        representing a half-open interval.  The default value is None.\n    inclusive : (bool, bool), optional\n        A tuple consisting of the (lower flag, upper flag).  These flags\n        determine whether values exactly equal to the lower or upper limits\n        are included.  The default value is (True, True).\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over the\n        whole array `a`.\n    ddof : int, optional\n        Delta degrees of freedom.  Default is 1.\n\n    Returns\n    -------\n    tvar : float\n        Trimmed variance.\n\n    Notes\n    -----\n    `tvar` computes the unbiased sample variance, i.e. it uses a correction\n    factor ``n / (n - 1)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tvar(x)\n    35.0\n    >>> stats.tvar(x, (3,17))\n    20.0\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 760, "code": "def tmin(a, lowerlimit=None, axis=0, inclusive=True, nan_policy='propagate'):\n    xp = array_namespace(a)\n    max_ = xp.iinfo(a.dtype).max if xp.isdtype(a.dtype, 'integral') else xp.inf\n    a, mask = _put_val_to_limits(a, (lowerlimit, None), (inclusive, None),\n                                 val=max_, xp=xp)\n    res = xp.min(a, axis=axis)\n    invalid = xp.all(mask, axis=axis)  # All elements are below lowerlimit\n    if is_lazy_array(invalid) or xp.any(invalid):\n        res = xp_promote(res, force_floating=True, xp=xp)\n        res = xp.where(invalid, xp.nan, res)\n    return res[()] if res.ndim == 0 else res", "documentation": "    \"\"\"Compute the trimmed minimum.\n\n    This function finds the minimum value of an array `a` along the\n    specified axis, but only considering values greater than a specified\n    lower limit.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    lowerlimit : None or float, optional\n        Values in the input array less than the given limit will be ignored.\n        When lowerlimit is None, then all values are used. The default value\n        is None.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over the\n        whole array `a`.\n    inclusive : {True, False}, optional\n        This flag determines whether values exactly equal to the lower limit\n        are included.  The default value is True.\n\n    Returns\n    -------\n    tmin : float, int or ndarray\n        Trimmed minimum.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tmin(x)\n    0\n\n    >>> stats.tmin(x, 13)\n    13\n\n    >>> stats.tmin(x, 13, inclusive=False)\n    14\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 824, "code": "def tmax(a, upperlimit=None, axis=0, inclusive=True, nan_policy='propagate'):\n    xp = array_namespace(a)\n    min_ = xp.iinfo(a.dtype).min if xp.isdtype(a.dtype, 'integral') else -xp.inf\n    a, mask = _put_val_to_limits(a, (None, upperlimit), (None, inclusive),\n                                 val=min_, xp=xp)\n    res = xp.max(a, axis=axis)\n    invalid = xp.all(mask, axis=axis)  # All elements are above upperlimit\n    if is_lazy_array(invalid) or xp.any(invalid):\n        res = xp_promote(res, force_floating=True, xp=xp)\n        res = xp.where(invalid, xp.nan, res)\n    return res[()] if res.ndim == 0 else res", "documentation": "    \"\"\"Compute the trimmed maximum.\n\n    This function computes the maximum value of an array along a given axis,\n    while ignoring values larger than a specified upper limit.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    upperlimit : None or float, optional\n        Values in the input array greater than the given limit will be ignored.\n        When upperlimit is None, then all values are used. The default value\n        is None.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over the\n        whole array `a`.\n    inclusive : {True, False}, optional\n        This flag determines whether values exactly equal to the upper limit\n        are included.  The default value is True.\n\n    Returns\n    -------\n    tmax : float, int or ndarray\n        Trimmed maximum.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tmax(x)\n    19\n\n    >>> stats.tmax(x, 13)\n    13\n\n    >>> stats.tmax(x, 13, inclusive=False)\n    12\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 887, "code": "def tstd(a, limits=None, inclusive=(True, True), axis=0, ddof=1):\n    return tvar(a, limits, inclusive, axis, ddof, _no_deco=True)**0.5\n@xp_capabilities()\n@_axis_nan_policy_factory(\n    lambda x: x, n_outputs=1, result_to_tuple=lambda x, _: (x,)\n)", "documentation": "    \"\"\"Compute the trimmed sample standard deviation.\n\n    This function finds the sample standard deviation of given values,\n    ignoring values outside the given `limits`.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    limits : None or (lower limit, upper limit), optional\n        Values in the input array less than the lower limit or greater than the\n        upper limit will be ignored. When limits is None, then all values are\n        used. Either of the limit values in the tuple can also be None\n        representing a half-open interval.  The default value is None.\n    inclusive : (bool, bool), optional\n        A tuple consisting of the (lower flag, upper flag).  These flags\n        determine whether values exactly equal to the lower or upper limits\n        are included.  The default value is (True, True).\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over the\n        whole array `a`.\n    ddof : int, optional\n        Delta degrees of freedom.  Default is 1.\n\n    Returns\n    -------\n    tstd : float\n        Trimmed sample standard deviation.\n\n    Notes\n    -----\n    `tstd` computes the unbiased sample standard deviation, i.e. it uses a\n    correction factor ``n / (n - 1)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tstd(x)\n    5.9160797830996161\n    >>> stats.tstd(x, (3,17))\n    4.4721359549995796\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 940, "code": "def tsem(a, limits=None, inclusive=(True, True), axis=0, ddof=1):\n    xp = array_namespace(a)\n    a, _ = _put_val_to_limits(a, limits, inclusive, xp=xp)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", SmallSampleWarning)\n        sd = _xp_var(a, correction=ddof, axis=axis, nan_policy='omit', xp=xp)**0.5\n    not_nan = xp.astype(~xp.isnan(a), a.dtype)\n    n_obs = xp.sum(not_nan, axis=axis, dtype=sd.dtype)\n    return sd / n_obs**0.5", "documentation": "    \"\"\"Compute the trimmed standard error of the mean.\n\n    This function finds the standard error of the mean for given\n    values, ignoring values outside the given `limits`.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    limits : None or (lower limit, upper limit), optional\n        Values in the input array less than the lower limit or greater than the\n        upper limit will be ignored. When limits is None, then all values are\n        used. Either of the limit values in the tuple can also be None\n        representing a half-open interval.  The default value is None.\n    inclusive : (bool, bool), optional\n        A tuple consisting of the (lower flag, upper flag).  These flags\n        determine whether values exactly equal to the lower or upper limits\n        are included.  The default value is (True, True).\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over the\n        whole array `a`.\n    ddof : int, optional\n        Delta degrees of freedom.  Default is 1.\n\n    Returns\n    -------\n    tsem : float\n        Trimmed standard error of the mean.\n\n    Notes\n    -----\n    `tsem` uses unbiased sample standard deviation, i.e. it uses a\n    correction factor ``n / (n - 1)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tsem(x)\n    1.3228756555322954\n    >>> stats.tsem(x, (3,17))\n    1.1547005383792515\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 1191, "code": "def _moment(a, order, axis, *, mean=None, xp=None):\n    xp = array_namespace(a) if xp is None else xp\n    a = xp_promote(a, force_floating=True, xp=xp)\n    dtype = a.dtype\n    if xp_size(a) == 0:\n        return xp.mean(a, axis=axis)\n    if order == 0 or (order == 1 and mean is None):\n        shape = list(a.shape)\n        del shape[axis]\n        temp = (xp.ones(shape, dtype=dtype, device=xp_device(a)) if order == 0\n                else xp.zeros(shape, dtype=dtype, device=xp_device(a)))", "documentation": "    \"\"\"Vectorized calculation of raw moment about specified center\n\n    When `mean` is None, the mean is computed and used as the center;\n    otherwise, the provided value is used as the center.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 1364, "code": "def kurtosis(a, axis=0, fisher=True, bias=True, nan_policy='propagate'):\n    xp = array_namespace(a)\n    a, axis = _chk_asarray(a, axis, xp=xp)\n    n = _length_nonmasked(a, axis, xp=xp)\n    mean = xp.mean(a, axis=axis, keepdims=True)\n    mean_reduced = xp.squeeze(mean, axis=axis)  # needed later\n    m2 = _moment(a, 2, axis, mean=mean, xp=xp)\n    m4 = _moment(a, 4, axis, mean=mean, xp=xp)\n    with np.errstate(all='ignore'):\n        zero = m2 <= (xp.finfo(m2.dtype).eps * mean_reduced)**2\n        vals = xp.where(zero, xp.nan, m4 / m2**2.0)", "documentation": "    \"\"\"Compute the kurtosis (Fisher or Pearson) of a dataset.\n\n    Kurtosis is the fourth central moment divided by the square of the\n    variance. If Fisher's definition is used, then 3.0 is subtracted from\n    the result to give 0.0 for a normal distribution.\n\n    If bias is False then the kurtosis is calculated using k statistics to\n    eliminate bias coming from biased moment estimators\n\n    Use `kurtosistest` to see if result is close enough to normal.\n\n    Parameters\n    ----------\n    a : array\n        Data for which the kurtosis is calculated.\n    axis : int or None, optional\n        Axis along which the kurtosis is calculated. Default is 0.\n        If None, compute over the whole array `a`.\n    fisher : bool, optional\n        If True, Fisher's definition is used (normal ==> 0.0). If False,\n        Pearson's definition is used (normal ==> 3.0).\n    bias : bool, optional\n        If False, then the calculations are corrected for statistical bias.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan. 'propagate' returns nan,\n        'raise' throws an error, 'omit' performs the calculations ignoring nan\n        values. Default is 'propagate'.\n\n    Returns\n    -------\n    kurtosis : array\n        The kurtosis of values along an axis, returning NaN where all values\n        are equal.\n\n    References\n    ----------\n    .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n       Probability and Statistics Tables and Formulae. Chapman & Hall: New\n       York. 2000.\n\n    Examples\n    --------\n    In Fisher's definition, the kurtosis of the normal distribution is zero.\n    In the following example, the kurtosis is close to zero, because it was\n    calculated from the dataset, not from the continuous distribution.\n\n    >>> import numpy as np\n    >>> from scipy.stats import norm, kurtosis\n    >>> data = norm.rvs(size=1000, random_state=3)\n    >>> kurtosis(data)\n    -0.06928694200380558\n\n    The distribution with a higher kurtosis has a heavier tail.\n    The zero valued kurtosis of the normal distribution in Fisher's definition\n    can serve as a reference point.\n\n    >>> import matplotlib.pyplot as plt\n    >>> import scipy.stats as stats\n    >>> from scipy.stats import kurtosis\n\n    >>> x = np.linspace(-5, 5, 100)\n    >>> ax = plt.subplot()\n    >>> distnames = ['laplace', 'norm', 'uniform']\n\n    >>> for distname in distnames:\n    ...     if distname == 'uniform':\n    ...         dist = getattr(stats, distname)(loc=-2, scale=4)\n    ...     else:\n    ...         dist = getattr(stats, distname)\n    ...     data = dist.rvs(size=1000)\n    ...     kur = kurtosis(data, fisher=True)\n    ...     y = dist.pdf(x)\n    ...     ax.plot(x, y, label=\"{}, {}\".format(distname, round(kur, 3)))\n    ...     ax.legend()\n\n    The Laplace distribution has a heavier tail than the normal distribution.\n    The uniform distribution (which has negative kurtosis) has the thinnest\n    tail.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 1473, "code": "def describe(a, axis=0, ddof=1, bias=True, nan_policy='propagate'):\n    xp = array_namespace(a)\n    a, axis = _chk_asarray(a, axis, xp=xp)\n    contains_nan = _contains_nan(a, nan_policy)\n    if nan_policy == 'omit' and contains_nan:\n        a = ma.masked_invalid(a)\n        return mstats_basic.describe(a, axis, ddof, bias)\n    if xp_size(a) == 0:\n        raise ValueError(\"The input must not be empty.\")\n    n = xp.asarray(_length_nonmasked(a, axis, xp=xp), dtype=xp.int64,\n                   device=xp_device(a))", "documentation": "    \"\"\"Compute several descriptive statistics of the passed array.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n    axis : int or None, optional\n        Axis along which statistics are calculated. Default is 0.\n        If None, compute over the whole array `a`.\n    ddof : int, optional\n        Delta degrees of freedom (only for variance).  Default is 1.\n    bias : bool, optional\n        If False, then the skewness and kurtosis calculations are corrected\n        for statistical bias.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    nobs : int or ndarray of ints\n        Number of observations (length of data along `axis`).\n        When 'omit' is chosen as nan_policy, the length along each axis\n        slice is counted separately.\n    minmax: tuple of ndarrays or floats\n        Minimum and maximum value of `a` along the given axis.\n    mean : ndarray or float\n        Arithmetic mean of `a` along the given axis.\n    variance : ndarray or float\n        Unbiased variance of `a` along the given axis; denominator is number\n        of observations minus one.\n    skewness : ndarray or float\n        Skewness of `a` along the given axis, based on moment calculations\n        with denominator equal to the number of observations, i.e. no degrees\n        of freedom correction.\n    kurtosis : ndarray or float\n        Kurtosis (Fisher) of `a` along the given axis.  The kurtosis is\n        normalized so that it is zero for the normal distribution.  No\n        degrees of freedom are used.\n\n    Raises\n    ------\n    ValueError\n        If size of `a` is 0.\n\n    See Also\n    --------\n    skew, kurtosis\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> a = np.arange(10)\n    >>> stats.describe(a)\n    DescribeResult(nobs=10, minmax=(0, 9), mean=4.5,\n                   variance=9.166666666666666, skewness=0.0,\n                   kurtosis=-1.2242424242424244)\n    >>> b = [[1, 2], [3, 4]]\n    >>> stats.describe(b)\n    DescribeResult(nobs=2, minmax=(array([1, 2]), array([3, 4])),\n                   mean=array([2., 3.]), variance=array([2., 2.]),\n                   skewness=array([0., 0.]), kurtosis=array([-2., -2.]))\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 1575, "code": "def _get_pvalue(statistic, distribution, alternative, symmetric=True, xp=None):\n    xp = array_namespace(statistic) if xp is None else xp\n    if alternative == 'less':\n        pvalue = distribution.cdf(statistic)\n    elif alternative == 'greater':\n        pvalue = distribution.sf(statistic)\n    elif alternative == 'two-sided':\n        pvalue = 2 * (distribution.sf(xp.abs(statistic)) if symmetric\n                      else xp.minimum(distribution.cdf(statistic),\n                                      distribution.sf(statistic)))\n    else:", "documentation": "    \"\"\"Get p-value given the statistic, (continuous) distribution, and alternative\"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2102, "code": "def percentileofscore(a, score, kind='rank', nan_policy='propagate'):\n    a = np.asarray(a)\n    score = np.asarray(score)\n    if a.ndim != 1:\n        raise ValueError(\"`a` must be 1-dimensional.\")\n    n = len(a)\n    cna = _contains_nan(a, nan_policy)\n    cns = _contains_nan(score, nan_policy)\n    if cns:\n        score = ma.masked_where(np.isnan(score), score)\n    if cna:", "documentation": "    \"\"\"Compute the percentile rank of a score relative to a list of scores.\n\n    A `percentileofscore` of, for example, 80% means that 80% of the\n    scores in `a` are below the given score. In the case of gaps or\n    ties, the exact definition depends on the optional keyword, `kind`.\n\n    Parameters\n    ----------\n    a : array_like\n        A 1-D array to which `score` is compared.\n    score : float or array_like\n        A float score or array of scores for which to compute the percentile(s).\n    kind : {'rank', 'weak', 'strict', 'mean'}, optional\n        Specifies the interpretation of the resulting score.\n        The following options are available (default is 'rank'):\n\n        * 'rank': Average percentage ranking of score.  In case of multiple\n          matches, average the percentage rankings of all matching scores.\n        * 'weak': This kind corresponds to the definition of a cumulative\n          distribution function.  A percentileofscore of 80% means that 80%\n          of values are less than or equal to the provided score.\n        * 'strict': Similar to \"weak\", except that only values that are\n          strictly less than the given score are counted.\n        * 'mean': The average of the \"weak\" and \"strict\" scores, often used\n          in testing.  See https://en.wikipedia.org/wiki/Percentile_rank\n\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Specifies how to treat `nan` values in `a`.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan (for each value in `score`).\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    pcos : float or array-like\n        Percentile-position(s) of `score` (0-100) relative to `a`.\n\n    See Also\n    --------\n    numpy.percentile\n    scipy.stats.scoreatpercentile, scipy.stats.rankdata\n\n    Examples\n    --------\n    Three-quarters of the given values lie below a given score:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> stats.percentileofscore([1, 2, 3, 4], 3)\n    75.0\n\n    With multiple matches, note how the scores of the two matches, 0.6\n    and 0.8 respectively, are averaged:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3)\n    70.0\n\n    Only 2/5 values are strictly less than 3:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='strict')\n    40.0\n\n    But 4/5 values are less than or equal to 3:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='weak')\n    80.0\n\n    The average between the weak and the strict scores is:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='mean')\n    60.0\n\n    Score arrays (of any dimensionality) are supported:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], [2, 3])\n    array([40., 70.])\n\n    The inputs can be infinite:\n\n    >>> stats.percentileofscore([-np.inf, 0, 1, np.inf], [1, 2, np.inf])\n    array([75., 75., 100.])\n\n    If `a` is empty, then the resulting percentiles are all `nan`:\n\n    >>> stats.percentileofscore([], [1, 2])\n    array([nan, nan])\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2345, "code": "def cumfreq(a, numbins=10, defaultreallimits=None, weights=None):\n    h, l, b, e = _histogram(a, numbins, defaultreallimits, weights=weights)\n    cumhist = np.cumsum(h * 1, axis=0)\n    return CumfreqResult(cumhist, l, b, e)\nRelfreqResult = namedtuple('RelfreqResult',\n                           ('frequency', 'lowerlimit', 'binsize',\n                            'extrapoints'))\n@xp_capabilities(np_only=True)", "documentation": "    \"\"\"Return a cumulative frequency histogram, using the histogram function.\n\n    A cumulative histogram is a mapping that counts the cumulative number of\n    observations in all of the bins up to the specified bin.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    numbins : int, optional\n        The number of bins to use for the histogram. Default is 10.\n    defaultreallimits : tuple (lower, upper), optional\n        The lower and upper values for the range of the histogram.\n        If no value is given, a range slightly larger than the range of the\n        values in `a` is used. Specifically ``(a.min() - s, a.max() + s)``,\n        where ``s = (1/2)(a.max() - a.min()) / (numbins - 1)``.\n    weights : array_like, optional\n        The weights for each value in `a`. Default is None, which gives each\n        value a weight of 1.0\n\n    Returns\n    -------\n    cumcount : ndarray\n        Binned values of cumulative frequency.\n    lowerlimit : float\n        Lower real limit\n    binsize : float\n        Width of each bin.\n    extrapoints : int\n        Extra points.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> x = [1, 4, 2, 1, 3, 1]\n    >>> res = stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))\n    >>> res.cumcount\n    array([ 1.,  2.,  3.,  3.])\n    >>> res.extrapoints\n    3\n\n    Create a normal distribution with 1000 random values\n\n    >>> samples = stats.norm.rvs(size=1000, random_state=rng)\n\n    Calculate cumulative frequencies\n\n    >>> res = stats.cumfreq(samples, numbins=25)\n\n    Calculate space of values for x\n\n    >>> x = res.lowerlimit + np.linspace(0, res.binsize*res.cumcount.size,\n    ...                                  res.cumcount.size + 1)\n\n    Plot histogram and cumulative histogram\n\n    >>> fig = plt.figure(figsize=(10, 4))\n    >>> ax1 = fig.add_subplot(1, 2, 1)\n    >>> ax2 = fig.add_subplot(1, 2, 2)\n    >>> ax1.hist(samples, bins=25)\n    >>> ax1.set_title('Histogram')\n    >>> ax2.bar(x[:-1], res.cumcount, width=res.binsize, align='edge')\n    >>> ax2.set_title('Cumulative histogram')\n    >>> ax2.set_xlim([x.min(), x.max()])\n\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2428, "code": "def relfreq(a, numbins=10, defaultreallimits=None, weights=None):\n    a = np.asanyarray(a)\n    h, l, b, e = _histogram(a, numbins, defaultreallimits, weights=weights)\n    h = h / a.shape[0]\n    return RelfreqResult(h, l, b, e)\n@xp_capabilities(np_only=True)", "documentation": "    \"\"\"Return a relative frequency histogram, using the histogram function.\n\n    A relative frequency  histogram is a mapping of the number of\n    observations in each of the bins relative to the total of observations.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    numbins : int, optional\n        The number of bins to use for the histogram. Default is 10.\n    defaultreallimits : tuple (lower, upper), optional\n        The lower and upper values for the range of the histogram.\n        If no value is given, a range slightly larger than the range of the\n        values in a is used. Specifically ``(a.min() - s, a.max() + s)``,\n        where ``s = (1/2)(a.max() - a.min()) / (numbins - 1)``.\n    weights : array_like, optional\n        The weights for each value in `a`. Default is None, which gives each\n        value a weight of 1.0\n\n    Returns\n    -------\n    frequency : ndarray\n        Binned values of relative frequency.\n    lowerlimit : float\n        Lower real limit.\n    binsize : float\n        Width of each bin.\n    extrapoints : int\n        Extra points.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> a = np.array([2, 4, 1, 2, 3, 2])\n    >>> res = stats.relfreq(a, numbins=4)\n    >>> res.frequency\n    array([ 0.16666667, 0.5       , 0.16666667,  0.16666667])\n    >>> np.sum(res.frequency)  # relative frequencies should add up to 1\n    1.0\n\n    Create a normal distribution with 1000 random values\n\n    >>> samples = stats.norm.rvs(size=1000, random_state=rng)\n\n    Calculate relative frequencies\n\n    >>> res = stats.relfreq(samples, numbins=25)\n\n    Calculate space of values for x\n\n    >>> x = res.lowerlimit + np.linspace(0, res.binsize*res.frequency.size,\n    ...                                  res.frequency.size)\n\n    Plot relative frequency histogram\n\n    >>> fig = plt.figure(figsize=(5, 4))\n    >>> ax = fig.add_subplot(1, 1, 1)\n    >>> ax.bar(x, res.frequency, width=res.binsize)\n    >>> ax.set_title('Relative frequency histogram')\n    >>> ax.set_xlim([x.min(), x.max()])\n\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2509, "code": "def obrientransform(*samples):\n    TINY = np.sqrt(np.finfo(float).eps)\n    arrays = []\n    sLast = None\n    for sample in samples:\n        a = np.asarray(sample)\n        n = len(a)\n        mu = np.mean(a)\n        sq = (a - mu)**2\n        sumsq = sq.sum()\n        t = ((n - 1.5) * n * sq - 0.5 * sumsq) / ((n - 1) * (n - 2))", "documentation": "    \"\"\"Compute the O'Brien transform on input data (any number of arrays).\n\n    Used to test for homogeneity of variance prior to running one-way stats.\n    Each array in ``*samples`` is one level of a factor.\n    If `f_oneway` is run on the transformed data and found significant,\n    the variances are unequal.  From Maxwell and Delaney [1]_, p.112.\n\n    Parameters\n    ----------\n    sample1, sample2, ... : array_like\n        Any number of arrays.\n\n    Returns\n    -------\n    obrientransform : ndarray\n        Transformed data for use in an ANOVA.  The first dimension\n        of the result corresponds to the sequence of transformed\n        arrays.  If the arrays given are all 1-D of the same length,\n        the return value is a 2-D array; otherwise it is a 1-D array\n        of type object, with each element being an ndarray.\n\n    Raises\n    ------\n    ValueError\n        If the mean of the transformed data is not equal to the original\n        variance, indicating a lack of convergence in the O'Brien transform.\n\n    References\n    ----------\n    .. [1] S. E. Maxwell and H. D. Delaney, \"Designing Experiments and\n           Analyzing Data: A Model Comparison Perspective\", Wadsworth, 1990.\n\n    Examples\n    --------\n    We'll test the following data sets for differences in their variance.\n\n    >>> x = [10, 11, 13, 9, 7, 12, 12, 9, 10]\n    >>> y = [13, 21, 5, 10, 8, 14, 10, 12, 7, 15]\n\n    Apply the O'Brien transform to the data.\n\n    >>> from scipy.stats import obrientransform\n    >>> tx, ty = obrientransform(x, y)\n\n    Use `scipy.stats.f_oneway` to apply a one-way ANOVA test to the\n    transformed data.\n\n    >>> from scipy.stats import f_oneway\n    >>> F, p = f_oneway(tx, ty)\n    >>> p\n    0.1314139477040335\n\n    If we require that ``p < 0.05`` for significance, we cannot conclude\n    that the variances are different.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2602, "code": "def sem(a, axis=0, ddof=1, nan_policy='propagate'):\n    xp = array_namespace(a)\n    if axis is None:\n        a = xp.reshape(a, (-1,))\n        axis = 0\n    a = xpx.atleast_nd(xp.asarray(a), ndim=1, xp=xp)\n    n = _length_nonmasked(a, axis, xp=xp)\n    s = xp.std(a, axis=axis, correction=ddof) / n**0.5\n    return s", "documentation": "    \"\"\"Compute standard error of the mean.\n\n    Calculate the standard error of the mean (or standard error of\n    measurement) of the values in the input array.\n\n    Parameters\n    ----------\n    a : array_like\n        An array containing the values for which the standard error is\n        returned. Must contain at least two observations.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over\n        the whole array `a`.\n    ddof : int, optional\n        Delta degrees-of-freedom. How many degrees of freedom to adjust\n        for bias in limited samples relative to the population estimate\n        of variance. Defaults to 1.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    s : ndarray or float\n        The standard error of the mean in the sample(s), along the input axis.\n\n    Notes\n    -----\n    The default value for `ddof` is different to the default (0) used by other\n    ddof containing routines, such as np.std and np.nanstd.\n\n    Examples\n    --------\n    Find standard error along the first axis:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> a = np.arange(20).reshape(5,4)\n    >>> stats.sem(a)\n    array([ 2.8284,  2.8284,  2.8284,  2.8284])\n\n    Find standard error across the whole array, using n degrees of freedom:\n\n    >>> stats.sem(a, axis=None, ddof=0)\n    1.2893796958227628\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2664, "code": "def _isconst(x):\n    y = x[~np.isnan(x)]\n    if y.size == 0:\n        return np.array([True])\n    else:\n        return (y[0] == y).all(keepdims=True)\n@xp_capabilities()", "documentation": "    \"\"\"\n    Check if all values in x are the same.  nans are ignored.\n\n    x must be a 1d array.\n\n    The return value is a 1d array with length 1, so it can be used\n    in np.apply_along_axis.\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2681, "code": "def zscore(a, axis=0, ddof=0, nan_policy='propagate'):\n    return zmap(a, a, axis=axis, ddof=ddof, nan_policy=nan_policy)\n@xp_capabilities()", "documentation": "    \"\"\"\n    Compute the z score.\n\n    Compute the z score of each value in the sample, relative to the\n    sample mean and standard deviation.\n\n    Parameters\n    ----------\n    a : array_like\n        An array like object containing the sample data.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over\n        the whole array `a`.\n    ddof : int, optional\n        Degrees of freedom correction in the calculation of the\n        standard deviation. Default is 0.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan. 'propagate' returns nan,\n        'raise' throws an error, 'omit' performs the calculations ignoring nan\n        values. Default is 'propagate'.  Note that when the value is 'omit',\n        nans in the input also propagate to the output, but they do not affect\n        the z-scores computed for the non-nan values.\n\n    Returns\n    -------\n    zscore : array_like\n        The z-scores, standardized by mean and standard deviation of\n        input array `a`.\n\n    See Also\n    --------\n    numpy.mean : Arithmetic average\n    numpy.std : Arithmetic standard deviation\n    scipy.stats.gzscore : Geometric standard score\n\n    Notes\n    -----\n    This function preserves ndarray subclasses, and works also with\n    matrices and masked arrays (it uses `asanyarray` instead of\n    `asarray` for parameters).\n\n    References\n    ----------\n    .. [1] \"Standard score\", *Wikipedia*,\n           https://en.wikipedia.org/wiki/Standard_score.\n    .. [2] Huck, S. W., Cross, T. L., Clark, S. B, \"Overcoming misconceptions\n           about Z-scores\", Teaching Statistics, vol. 8, pp. 38-40, 1986\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> a = np.array([ 0.7972,  0.0767,  0.4383,  0.7866,  0.8091,\n    ...                0.1954,  0.6307,  0.6599,  0.1065,  0.0508])\n    >>> from scipy import stats\n    >>> stats.zscore(a)\n    array([ 1.1273, -1.247 , -0.0552,  1.0923,  1.1664, -0.8559,  0.5786,\n            0.6748, -1.1488, -1.3324])\n\n    Computing along a specified axis, using n-1 degrees of freedom\n    (``ddof=1``) to calculate the standard deviation:\n\n    >>> b = np.array([[ 0.3148,  0.0478,  0.6243,  0.4608],\n    ...               [ 0.7149,  0.0775,  0.6072,  0.9656],\n    ...               [ 0.6341,  0.1403,  0.9759,  0.4064],\n    ...               [ 0.5918,  0.6948,  0.904 ,  0.3721],\n    ...               [ 0.0921,  0.2481,  0.1188,  0.1366]])\n    >>> stats.zscore(b, axis=1, ddof=1)\n    array([[-0.19264823, -1.28415119,  1.07259584,  0.40420358],\n           [ 0.33048416, -1.37380874,  0.04251374,  1.00081084],\n           [ 0.26796377, -1.12598418,  1.23283094, -0.37481053],\n           [-0.22095197,  0.24468594,  1.19042819, -1.21416216],\n           [-0.82780366,  1.4457416 , -0.43867764, -0.1792603 ]])\n\n    An example with ``nan_policy='omit'``:\n\n    >>> x = np.array([[25.11, 30.10, np.nan, 32.02, 43.15],\n    ...               [14.95, 16.06, 121.25, 94.35, 29.81]])\n    >>> stats.zscore(x, axis=1, nan_policy='omit')\n    array([[-1.13490897, -0.37830299,         nan, -0.08718406,  1.60039602],\n           [-0.91611681, -0.89090508,  1.4983032 ,  0.88731639, -0.5785977 ]])\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2767, "code": "def gzscore(a, *, axis=0, ddof=0, nan_policy='propagate'):\n    xp = array_namespace(a)\n    a = xp_promote(a, force_floating=True, xp=xp)\n    log = ma.log if isinstance(a, ma.MaskedArray) else xp.log\n    return zscore(log(a), axis=axis, ddof=ddof, nan_policy=nan_policy)\n@xp_capabilities()", "documentation": "    \"\"\"\n    Compute the geometric standard score.\n\n    Compute the geometric z score of each strictly positive value in the\n    sample, relative to the geometric mean and standard deviation.\n    Mathematically the geometric z score can be evaluated as::\n\n        gzscore = log(a/gmu) / log(gsigma)\n\n    where ``gmu`` (resp. ``gsigma``) is the geometric mean (resp. standard\n    deviation).\n\n    Parameters\n    ----------\n    a : array_like\n        Sample data.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over\n        the whole array `a`.\n    ddof : int, optional\n        Degrees of freedom correction in the calculation of the\n        standard deviation. Default is 0.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan. 'propagate' returns nan,\n        'raise' throws an error, 'omit' performs the calculations ignoring nan\n        values. Default is 'propagate'.  Note that when the value is 'omit',\n        nans in the input also propagate to the output, but they do not affect\n        the geometric z scores computed for the non-nan values.\n\n    Returns\n    -------\n    gzscore : array_like\n        The geometric z scores, standardized by geometric mean and geometric\n        standard deviation of input array `a`.\n\n    See Also\n    --------\n    gmean : Geometric mean\n    gstd : Geometric standard deviation\n    zscore : Standard score\n\n    Notes\n    -----\n    This function preserves ndarray subclasses, and works also with\n    matrices and masked arrays (it uses ``asanyarray`` instead of\n    ``asarray`` for parameters).\n\n    .. versionadded:: 1.8\n\n    References\n    ----------\n    .. [1] \"Geometric standard score\", *Wikipedia*,\n           https://en.wikipedia.org/wiki/Geometric_standard_deviation#Geometric_standard_score.\n\n    Examples\n    --------\n    Draw samples from a log-normal distribution:\n\n    >>> import numpy as np\n    >>> from scipy.stats import zscore, gzscore\n    >>> import matplotlib.pyplot as plt\n\n    >>> rng = np.random.default_rng()\n    >>> mu, sigma = 3., 1.  # mean and standard deviation\n    >>> x = rng.lognormal(mu, sigma, size=500)\n\n    Display the histogram of the samples:\n\n    >>> fig, ax = plt.subplots()\n    >>> ax.hist(x, 50)\n    >>> plt.show()\n\n    Display the histogram of the samples standardized by the classical zscore.\n    Distribution is rescaled but its shape is unchanged.\n\n    >>> fig, ax = plt.subplots()\n    >>> ax.hist(zscore(x), 50)\n    >>> plt.show()\n\n    Demonstrate that the distribution of geometric zscores is rescaled and\n    quasinormal:\n\n    >>> fig, ax = plt.subplots()\n    >>> ax.hist(gzscore(x), 50)\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2862, "code": "def zmap(scores, compare, axis=0, ddof=0, nan_policy='propagate'):\n    like_zscore = (scores is compare)\n    xp = array_namespace(scores, compare)\n    scores, compare = xp_promote(scores, compare, force_floating=True, xp=xp)\n    with warnings.catch_warnings():\n        if like_zscore:  # zscore should not emit SmallSampleWarning\n            warnings.simplefilter('ignore', SmallSampleWarning)\n        mn = _xp_mean(compare, axis=axis, keepdims=True, nan_policy=nan_policy)\n        std = _xp_var(compare, axis=axis, correction=ddof,\n                      keepdims=True, nan_policy=nan_policy)**0.5\n    with np.errstate(invalid='ignore', divide='ignore'):", "documentation": "    \"\"\"\n    Calculate the relative z-scores.\n\n    Return an array of z-scores, i.e., scores that are standardized to\n    zero mean and unit variance, where mean and variance are calculated\n    from the comparison array.\n\n    Parameters\n    ----------\n    scores : array_like\n        The input for which z-scores are calculated.\n    compare : array_like\n        The input from which the mean and standard deviation of the\n        normalization are taken; assumed to have the same dimension as\n        `scores`.\n    axis : int or None, optional\n        Axis over which mean and variance of `compare` are calculated.\n        Default is 0. If None, compute over the whole array `scores`.\n    ddof : int, optional\n        Degrees of freedom correction in the calculation of the\n        standard deviation. Default is 0.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle the occurrence of nans in `compare`.\n        'propagate' returns nan, 'raise' raises an exception, 'omit'\n        performs the calculations ignoring nan values. Default is\n        'propagate'. Note that when the value is 'omit', nans in `scores`\n        also propagate to the output, but they do not affect the z-scores\n        computed for the non-nan values.\n\n    Returns\n    -------\n    zscore : array_like\n        Z-scores, in the same shape as `scores`.\n\n    Notes\n    -----\n    This function preserves ndarray subclasses, and works also with\n    matrices and masked arrays (it uses `asanyarray` instead of\n    `asarray` for parameters).\n\n    Examples\n    --------\n    >>> from scipy.stats import zmap\n    >>> a = [0.5, 2.0, 2.5, 3]\n    >>> b = [0, 1, 2, 3, 4]\n    >>> zmap(a, b)\n    array([-1.06066017,  0.        ,  0.35355339,  0.70710678])\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 3386, "code": "def sigmaclip(a, low=4., high=4., *, nan_policy='propagate'):\n    xp = array_namespace(a)\n    c = xp_ravel(xp.asarray(a))\n    contains_nan = _contains_nan(c, nan_policy, xp_omit_okay=True)\n    if contains_nan:\n        if nan_policy == 'propagate':\n            NaN = _get_nan(c, xp=xp)\n            clipped = xp.empty_like(c[0:0])\n            return SigmaclipResult(clipped, NaN, NaN)\n        elif nan_policy == 'omit':\n            c = c[~xp.isnan(c)]", "documentation": "    \"\"\"Perform iterative sigma-clipping of array elements.\n\n    Starting from the full sample, all elements outside the critical range are\n    removed, i.e. all elements of the input array `c` that satisfy either of\n    the following conditions::\n\n        c < mean(c) - std(c)*low\n        c > mean(c) + std(c)*high\n\n    The iteration continues with the updated sample until no\n    elements are outside the (updated) range.\n\n    Parameters\n    ----------\n    a : array_like\n        Data array, will be raveled if not 1-D.\n    low : float, optional\n        Lower bound factor of sigma clipping. Default is 4.\n    high : float, optional\n        Upper bound factor of sigma clipping. Default is 4.\n    nan_policy : {'propagate', 'omit', 'raise'}\n        Defines how to handle input NaNs.\n\n        - ``propagate``: if a NaN is present in the input, the clipped array will be\n          empty, and the upper and lower thresholds will be NaN.\n        - ``omit``: NaNs will be omitted when performing the calculation.\n        - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n\n    Returns\n    -------\n    clipped : ndarray\n        Input array with clipped elements removed.\n    lower : float\n        Lower threshold value use for clipping.\n    upper : float\n        Upper threshold value use for clipping.\n\n    Notes\n    -----\n    This function iteratively *removes* observations. Once observations are\n    removed, they are not re-added in subsequent iterations. Consequently,\n    although it is often the case that ``clipped`` is identical to\n    ``a[(a >= lower) & (a <= upper)]``, this property is not guaranteed to be\n    satisfied; ``clipped`` may have fewer elements.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import sigmaclip\n    >>> a = np.concatenate((np.linspace(9.5, 10.5, 31),\n    ...                     np.linspace(0, 20, 5)))\n    >>> fact = 1.5\n    >>> c, low, upp = sigmaclip(a, fact, fact)\n    >>> c\n    array([  9.96666667,  10.        ,  10.03333333,  10.        ])\n    >>> c.var(), c.std()\n    (0.00055555555555555165, 0.023570226039551501)\n    >>> low, c.mean() - fact*c.std(), c.min()\n    (9.9646446609406727, 9.9646446609406727, 9.9666666666666668)\n    >>> upp, c.mean() + fact*c.std(), c.max()\n    (10.035355339059327, 10.035355339059327, 10.033333333333333)\n\n    >>> a = np.concatenate((np.linspace(9.5, 10.5, 11),\n    ...                     np.linspace(-100, -50, 3)))\n    >>> c, low, upp = sigmaclip(a, 1.8, 1.8)\n    >>> (c == np.linspace(9.5, 10.5, 11)).all()\n    True\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 3481, "code": "def trimboth(a, proportiontocut, axis=0):\n    a = np.asarray(a)\n    if a.size == 0:\n        return a\n    if axis is None:\n        a = a.ravel()\n        axis = 0\n    nobs = a.shape[axis]\n    lowercut = int(proportiontocut * nobs)\n    uppercut = nobs - lowercut\n    if (lowercut >= uppercut):", "documentation": "    \"\"\"Slice off a proportion of items from both ends of an array.\n\n    Slice off the passed proportion of items from both ends of the passed\n    array (i.e., with `proportiontocut` = 0.1, slices leftmost 10% **and**\n    rightmost 10% of scores). The trimmed values are the lowest and\n    highest ones.\n    Slice off less if proportion results in a non-integer slice index (i.e.\n    conservatively slices off `proportiontocut`).\n\n    Parameters\n    ----------\n    a : array_like\n        Data to trim.\n    proportiontocut : float\n        Proportion (in range 0-1) of total data set to trim of each end.\n    axis : int or None, optional\n        Axis along which to trim data. Default is 0. If None, compute over\n        the whole array `a`.\n\n    Returns\n    -------\n    out : ndarray\n        Trimmed version of array `a`. The order of the trimmed content\n        is undefined.\n\n    See Also\n    --------\n    trim_mean\n\n    Examples\n    --------\n    Create an array of 10 values and trim 10% of those values from each end:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n    >>> stats.trimboth(a, 0.1)\n    array([1, 3, 2, 4, 5, 6, 7, 8])\n\n    Note that the elements of the input array are trimmed by value, but the\n    output array is not necessarily sorted.\n\n    The proportion to trim is rounded down to the nearest integer. For\n    instance, trimming 25% of the values from each end of an array of 10\n    values will return an array of 6 values:\n\n    >>> b = np.arange(10)\n    >>> stats.trimboth(b, 1/4).shape\n    (6,)\n\n    Multidimensional arrays can be trimmed along any axis or across the entire\n    array:\n\n    >>> c = [2, 4, 6, 8, 0, 1, 3, 5, 7, 9]\n    >>> d = np.array([a, b, c])\n    >>> stats.trimboth(d, 0.4, axis=0).shape\n    (1, 10)\n    >>> stats.trimboth(d, 0.4, axis=1).shape\n    (3, 2)\n    >>> stats.trimboth(d, 0.4, axis=None).shape\n    (6,)\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 3568, "code": "def trim1(a, proportiontocut, tail='right', axis=0):\n    a = np.asarray(a)\n    if axis is None:\n        a = a.ravel()\n        axis = 0\n    nobs = a.shape[axis]\n    if proportiontocut >= 1:\n        return []\n    if tail.lower() == 'right':\n        lowercut = 0\n        uppercut = nobs - int(proportiontocut * nobs)", "documentation": "    \"\"\"Slice off a proportion from ONE end of the passed array distribution.\n\n    If `proportiontocut` = 0.1, slices off 'leftmost' or 'rightmost'\n    10% of scores. The lowest or highest values are trimmed (depending on\n    the tail).\n    Slice off less if proportion results in a non-integer slice index\n    (i.e. conservatively slices off `proportiontocut` ).\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    proportiontocut : float\n        Fraction to cut off of 'left' or 'right' of distribution.\n    tail : {'left', 'right'}, optional\n        Defaults to 'right'.\n    axis : int or None, optional\n        Axis along which to trim data. Default is 0. If None, compute over\n        the whole array `a`.\n\n    Returns\n    -------\n    trim1 : ndarray\n        Trimmed version of array `a`. The order of the trimmed content is\n        undefined.\n\n    Examples\n    --------\n    Create an array of 10 values and trim 20% of its lowest values:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n    >>> stats.trim1(a, 0.2, 'left')\n    array([2, 4, 3, 5, 6, 7, 8, 9])\n\n    Note that the elements of the input array are trimmed by value, but the\n    output array is not necessarily sorted.\n\n    The proportion to trim is rounded down to the nearest integer. For\n    instance, trimming 25% of the values from an array of 10 values will\n    return an array of 8 values:\n\n    >>> b = np.arange(10)\n    >>> stats.trim1(b, 1/4).shape\n    (8,)\n\n    Multidimensional arrays can be trimmed along any axis or across the entire\n    array:\n\n    >>> c = [2, 4, 6, 8, 0, 1, 3, 5, 7, 9]\n    >>> d = np.array([a, b, c])\n    >>> stats.trim1(d, 0.8, axis=0).shape\n    (1, 10)\n    >>> stats.trim1(d, 0.8, axis=1).shape\n    (3, 2)\n    >>> stats.trim1(d, 0.8, axis=None).shape\n    (6,)\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 3657, "code": "def trim_mean(a, proportiontocut, axis=0):\n    xp = array_namespace(a)\n    a = xp.asarray(a)\n    if xp_size(a) == 0:\n        return _get_nan(a, xp=xp)\n    if axis is None:\n        a = xp_ravel(a)\n        axis = 0\n    nobs = a.shape[axis]\n    lowercut = int(proportiontocut * nobs)\n    uppercut = nobs - lowercut", "documentation": "    \"\"\"Return mean of array after trimming a specified fraction of extreme values.\n\n    Removes the specified proportion of elements from *each* end of the\n    sorted array, then computes the mean of the remaining elements.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    proportiontocut : float\n        Fraction of the most positive and most negative elements to remove.\n        When the specified proportion does not result in an integer number of\n        elements, the number of elements to trim is rounded down.\n    axis : int or None, default: 0\n        Axis along which the trimmed means are computed.\n        If None, compute over the raveled array.\n\n    Returns\n    -------\n    trim_mean : ndarray\n        Mean of trimmed array.\n\n    See Also\n    --------\n    trimboth : Remove a proportion of elements from each end of an array.\n    tmean : Compute the mean after trimming values outside specified limits.\n\n    Notes\n    -----\n    For 1-D array `a`, `trim_mean` is approximately equivalent to the following\n    calculation::\n\n        import numpy as np\n        a = np.sort(a)\n        m = int(proportiontocut * len(a))\n        np.mean(a[m: len(a) - m])\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = [1, 2, 3, 5]\n    >>> stats.trim_mean(x, 0.25)\n    2.5\n\n    When the specified proportion does not result in an integer number of\n    elements, the number of elements to trim is rounded down.\n\n    >>> stats.trim_mean(x, 0.24999) == np.mean(x)\n    True\n\n    Use `axis` to specify the axis along which the calculation is performed.\n\n    >>> x2 = [[1, 2, 3, 5],\n    ...       [10, 20, 30, 50]]\n    >>> stats.trim_mean(x2, 0.25)\n    array([ 5.5, 11. , 16.5, 27.5])\n    >>> stats.trim_mean(x2, 0.25, axis=1)\n    array([ 2.5, 25. ])\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 3772, "code": "def f_oneway(*samples, axis=0, equal_var=True):\n    xp = array_namespace(*samples)\n    samples = xp_promote(*samples, force_floating=True, xp=xp)\n    if len(samples) < 2:\n        raise TypeError('at least two inputs are required;'\n                        f' got {len(samples)}.')\n    num_groups = len(samples)\n    alldata = xp.concat(samples, axis=-1)\n    bign = _length_nonmasked(alldata, axis=-1, xp=xp)\n    if _f_oneway_is_too_small(samples):\n        NaN = _get_nan(*samples, xp=xp)", "documentation": "    \"\"\"Perform one-way ANOVA.\n\n    The one-way ANOVA tests the null hypothesis that two or more groups have\n    the same population mean.  The test is applied to samples from two or\n    more groups, possibly with differing sizes.\n\n    Parameters\n    ----------\n    sample1, sample2, ... : array_like\n        The sample measurements for each group.  There must be at least\n        two arguments.  If the arrays are multidimensional, then all the\n        dimensions of the array must be the same except for `axis`.\n    axis : int, optional\n        Axis of the input arrays along which the test is applied.\n        Default is 0.\n    equal_var : bool, optional\n        If True (default), perform a standard one-way ANOVA test that\n        assumes equal population variances [2]_.\n        If False, perform Welch's ANOVA test, which does not assume\n        equal population variances [4]_.\n\n        .. versionadded:: 1.16.0\n\n    Returns\n    -------\n    statistic : float\n        The computed F statistic of the test.\n    pvalue : float\n        The associated p-value from the F distribution.\n\n    Warns\n    -----\n    `~scipy.stats.ConstantInputWarning`\n        Emitted if all values within each of the input arrays are identical.\n        In this case the F statistic is either infinite or isn't defined,\n        so ``np.inf`` or ``np.nan`` is returned.\n\n    RuntimeWarning\n        Emitted if the length of any input array is 0, or if all the input\n        arrays have length 1.  ``np.nan`` is returned for the F statistic\n        and the p-value in these cases.\n\n    Notes\n    -----\n    The ANOVA test has important assumptions that must be satisfied in order\n    for the associated p-value to be valid.\n\n    1. The samples are independent.\n    2. Each sample is from a normally distributed population.\n    3. The population standard deviations of the groups are all equal.  This\n       property is known as homoscedasticity.\n\n    If these assumptions are not true for a given set of data, it may still\n    be possible to use the Kruskal-Wallis H-test (`scipy.stats.kruskal`) or\n    the Alexander-Govern test (`scipy.stats.alexandergovern`) although with\n    some loss of power.\n\n    The length of each group must be at least one, and there must be at\n    least one group with length greater than one.  If these conditions\n    are not satisfied, a warning is generated and (``np.nan``, ``np.nan``)\n    is returned.\n\n    If all values in each group are identical, and there exist at least two\n    groups with different values, the function generates a warning and\n    returns (``np.inf``, 0).\n\n    If all values in all groups are the same, function generates a warning\n    and returns (``np.nan``, ``np.nan``).\n\n    The algorithm is from Heiman [2]_, pp.394-7.\n\n    References\n    ----------\n    .. [1] R. Lowry, \"Concepts and Applications of Inferential Statistics\",\n           Chapter 14, 2014, http://vassarstats.net/textbook/\n\n    .. [2] G.W. Heiman, \"Understanding research methods and statistics: An\n           integrated introduction for psychology\", Houghton, Mifflin and\n           Company, 2001.\n\n    .. [3] J.H. McDonald, \"Handbook of Biological Statistics\",\n           One-way ANOVA, 2014.\n           http://www.biostathandbook.com/onewayanova.html\n\n    .. [4] B. L. Welch, \"On the Comparison of Several Mean Values:\n           An Alternative Approach\", Biometrika, vol. 38, no. 3/4,\n           pp. 330-336, 1951. :doi:`10.2307/2332579`.\n\n    .. [5] J.H. McDonald, R. Seed and R.K. Koehn, \"Allozymes and\n           morphometric characters of three species of Mytilus in\n           the Northern and Southern Hemispheres\",\n           Marine Biology, vol. 111, pp. 323-333, 1991.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import f_oneway\n\n    Here are some data [3]_ on a shell measurement (the length of the anterior\n    adductor muscle scar, standardized by dividing by length) in the mussel\n    Mytilus trossulus from five locations: Tillamook, Oregon; Newport, Oregon;\n    Petersburg, Alaska; Magadan, Russia; and Tvarminne, Finland, taken from a\n    much larger data set used in [5]_.\n\n    >>> tillamook = [0.0571, 0.0813, 0.0831, 0.0976, 0.0817, 0.0859, 0.0735,\n    ...              0.0659, 0.0923, 0.0836]\n    >>> newport = [0.0873, 0.0662, 0.0672, 0.0819, 0.0749, 0.0649, 0.0835,\n    ...            0.0725]\n    >>> petersburg = [0.0974, 0.1352, 0.0817, 0.1016, 0.0968, 0.1064, 0.105]\n    >>> magadan = [0.1033, 0.0915, 0.0781, 0.0685, 0.0677, 0.0697, 0.0764,\n    ...            0.0689]\n    >>> tvarminne = [0.0703, 0.1026, 0.0956, 0.0973, 0.1039, 0.1045]\n    >>> f_oneway(tillamook, newport, petersburg, magadan, tvarminne)\n    F_onewayResult(statistic=7.121019471642447, pvalue=0.0002812242314534544)\n\n    `f_oneway` accepts multidimensional input arrays.  When the inputs\n    are multidimensional and `axis` is not given, the test is performed\n    along the first axis of the input arrays.  For the following data, the\n    test is performed three times, once for each column.\n\n    >>> a = np.array([[9.87, 9.03, 6.81],\n    ...               [7.18, 8.35, 7.00],\n    ...               [8.39, 7.58, 7.68],\n    ...               [7.45, 6.33, 9.35],\n    ...               [6.41, 7.10, 9.33],\n    ...               [8.00, 8.24, 8.44]])\n    >>> b = np.array([[6.35, 7.30, 7.16],\n    ...               [6.65, 6.68, 7.63],\n    ...               [5.72, 7.73, 6.72],\n    ...               [7.01, 9.19, 7.41],\n    ...               [7.75, 7.87, 8.30],\n    ...               [6.90, 7.97, 6.97]])\n    >>> c = np.array([[3.31, 8.77, 1.01],\n    ...               [8.25, 3.24, 3.62],\n    ...               [6.32, 8.81, 5.19],\n    ...               [7.48, 8.83, 8.91],\n    ...               [8.59, 6.01, 6.07],\n    ...               [3.07, 9.72, 7.48]])\n    >>> F = f_oneway(a, b, c)\n    >>> F.statistic\n    array([1.75676344, 0.03701228, 3.76439349])\n    >>> F.pvalue\n    array([0.20630784, 0.96375203, 0.04733157])\n\n    Welch ANOVA will be performed if `equal_var` is False.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 4069, "code": "def alexandergovern(*samples, nan_policy='propagate', axis=0):\n    xp = array_namespace(*samples)\n    samples = _alexandergovern_input_validation(samples, nan_policy, axis, xp=xp)\n    lengths = [sample.shape[-1] for sample in samples]\n    means = xp.stack([_xp_mean(sample, axis=-1) for sample in samples])\n    se2 = [(_xp_var(sample, correction=1, axis=-1) / length)\n           for sample, length in zip(samples, lengths)]\n    standard_errors_squared = xp.stack(se2)\n    standard_errors = standard_errors_squared**0.5\n    eps = xp.finfo(standard_errors.dtype).eps\n    zero = standard_errors <= xp.abs(eps * means)", "documentation": "    \"\"\"Performs the Alexander Govern test.\n\n    The Alexander-Govern approximation tests the equality of k independent\n    means in the face of heterogeneity of variance. The test is applied to\n    samples from two or more groups, possibly with differing sizes.\n\n    Parameters\n    ----------\n    sample1, sample2, ... : array_like\n        The sample measurements for each group.  There must be at least\n        two samples, and each sample must contain at least two observations.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    res : AlexanderGovernResult\n        An object with attributes:\n\n        statistic : float\n            The computed A statistic of the test.\n        pvalue : float\n            The associated p-value from the chi-squared distribution.\n\n    Warns\n    -----\n    `~scipy.stats.ConstantInputWarning`\n        Raised if an input is a constant array.  The statistic is not defined\n        in this case, so ``np.nan`` is returned.\n\n    See Also\n    --------\n    f_oneway : one-way ANOVA\n\n    Notes\n    -----\n    The use of this test relies on several assumptions.\n\n    1. The samples are independent.\n    2. Each sample is from a normally distributed population.\n    3. Unlike `f_oneway`, this test does not assume on homoscedasticity,\n       instead relaxing the assumption of equal variances.\n\n    Input samples must be finite, one dimensional, and with size greater than\n    one.\n\n    References\n    ----------\n    .. [1] Alexander, Ralph A., and Diane M. Govern. \"A New and Simpler\n           Approximation for ANOVA under Variance Heterogeneity.\" Journal\n           of Educational Statistics, vol. 19, no. 2, 1994, pp. 91-101.\n           https://www.jstor.org/stable/1165140\n\n    Examples\n    --------\n    >>> from scipy.stats import alexandergovern\n\n    Here are some data on annual percentage rate of interest charged on\n    new car loans at nine of the largest banks in four American cities\n    taken from the National Institute of Standards and Technology's\n    ANOVA dataset.\n\n    We use `alexandergovern` to test the null hypothesis that all cities\n    have the same mean APR against the alternative that the cities do not\n    all have the same mean APR. We decide that a significance level of 5%\n    is required to reject the null hypothesis in favor of the alternative.\n\n    >>> atlanta = [13.75, 13.75, 13.5, 13.5, 13.0, 13.0, 13.0, 12.75, 12.5]\n    >>> chicago = [14.25, 13.0, 12.75, 12.5, 12.5, 12.4, 12.3, 11.9, 11.9]\n    >>> houston = [14.0, 14.0, 13.51, 13.5, 13.5, 13.25, 13.0, 12.5, 12.5]\n    >>> memphis = [15.0, 14.0, 13.75, 13.59, 13.25, 12.97, 12.5, 12.25,\n    ...           11.89]\n    >>> alexandergovern(atlanta, chicago, houston, memphis)\n    AlexanderGovernResult(statistic=4.65087071883494,\n                          pvalue=0.19922132490385214)\n\n    The p-value is 0.1992, indicating a nearly 20% chance of observing\n    such an extreme value of the test statistic under the null hypothesis.\n    This exceeds 5%, so we do not reject the null hypothesis in favor of\n    the alternative.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 4230, "code": "def _pearsonr_fisher_ci(r, n, confidence_level, alternative):\n    xp = array_namespace(r)\n    ones = xp.ones_like(r)\n    n = xp.asarray(n, dtype=r.dtype, device=xp_device(r))\n    confidence_level = xp.asarray(confidence_level, dtype=r.dtype, device=xp_device(r))\n    with np.errstate(divide='ignore', invalid='ignore'):\n        zr = xp.atanh(r)\n        se = xp.sqrt(1 / (n - 3))\n    if alternative == \"two-sided\":\n        h = special.ndtri(0.5 + confidence_level/2)\n        zlo = zr - h*se", "documentation": "    \"\"\"\n    Compute the confidence interval for Pearson's R.\n\n    Fisher's transformation is used to compute the confidence interval\n    (https://en.wikipedia.org/wiki/Fisher_transformation).\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 4278, "code": "def _pearsonr_bootstrap_ci(confidence_level, method, x, y, alternative, axis):", "documentation": "    \"\"\"\n    Compute the confidence interval for Pearson's R using the bootstrap.\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 4300, "code": "class PearsonRResult(PearsonRResultBase):", "documentation": "    \"\"\"\n    Result of `scipy.stats.pearsonr`\n\n    Attributes\n    ----------\n    statistic : float\n        Pearson product-moment correlation coefficient.\n    pvalue : float\n        The p-value associated with the chosen alternative.\n\n    Methods\n    -------\n    confidence_interval\n        Computes the confidence interval of the correlation\n        coefficient `statistic` for the given confidence level.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 4329, "code": "    def confidence_interval(self, confidence_level=0.95, method=None):\n        if isinstance(method, BootstrapMethod):\n            xp = array_namespace(self._x)\n            message = ('`method` must be `None` if `pearsonr` '\n                       'arguments were not NumPy arrays.')\n            if not is_numpy(xp):\n                raise ValueError(message)\n            ci = _pearsonr_bootstrap_ci(confidence_level, method, self._x, self._y,\n                                        self._alternative, self._axis)\n        elif method is None:\n            ci = _pearsonr_fisher_ci(self.statistic, self._n, confidence_level,", "documentation": "        \"\"\"\n        The confidence interval for the correlation coefficient.\n\n        Compute the confidence interval for the correlation coefficient\n        ``statistic`` with the given confidence level.\n\n        If `method` is not provided,\n        The confidence interval is computed using the Fisher transformation\n        F(r) = arctanh(r) [1]_.  When the sample pairs are drawn from a\n        bivariate normal distribution, F(r) approximately follows a normal\n        distribution with standard error ``1/sqrt(n - 3)``, where ``n`` is the\n        length of the original samples along the calculation axis. When\n        ``n <= 3``, this approximation does not yield a finite, real standard\n        error, so we define the confidence interval to be -1 to 1.\n\n        If `method` is an instance of `BootstrapMethod`, the confidence\n        interval is computed using `scipy.stats.bootstrap` with the provided\n        configuration options and other appropriate settings. In some cases,\n        confidence limits may be NaN due to a degenerate resample, and this is\n        typical for very small samples (~6 observations).\n\n        Parameters\n        ----------\n        confidence_level : float\n            The confidence level for the calculation of the correlation\n            coefficient confidence interval. Default is 0.95.\n\n        method : BootstrapMethod, optional\n            Defines the method used to compute the confidence interval. See\n            method description for details.\n\n            .. versionadded:: 1.11.0\n\n        Returns\n        -------\n        ci : namedtuple\n            The confidence interval is returned in a ``namedtuple`` with\n            fields `low` and `high`.\n\n        References\n        ----------\n        .. [1] \"Pearson correlation coefficient\", Wikipedia,\n               https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\n        \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 4814, "code": "def fisher_exact(table, alternative=None, *, method=None):\n    hypergeom = distributions.hypergeom\n    c = np.asarray(table, dtype=np.int64)\n    if not c.ndim == 2:\n        raise ValueError(\"The input `table` must have two dimensions.\")\n    if np.any(c < 0):\n        raise ValueError(\"All values in `table` must be nonnegative.\")\n    if not c.shape == (2, 2) or method is not None:\n        return _fisher_exact_rxc(c, alternative, method)\n    alternative = 'two-sided' if alternative is None else alternative\n    if 0 in c.sum(axis=0) or 0 in c.sum(axis=1):", "documentation": "    \"\"\"Perform a Fisher exact test on a contingency table.\n\n    For a 2x2 table,\n    the null hypothesis is that the true odds ratio of the populations\n    underlying the observations is one, and the observations were sampled\n    from these populations under a condition: the marginals of the\n    resulting table must equal those of the observed table.\n    The statistic is the unconditional maximum likelihood estimate of the odds\n    ratio, and the p-value is the probability under the null hypothesis of\n    obtaining a table at least as extreme as the one that was actually\n    observed.\n\n    For other table sizes, or if `method` is provided, the null hypothesis\n    is that the rows and columns of the tables have fixed sums and are\n    independent; i.e., the table was sampled from a `scipy.stats.random_table`\n    distribution with the observed marginals. The statistic is the\n    probability mass of this distribution evaluated at `table`, and the\n    p-value is the percentage of the population of tables with statistic at\n    least as extreme (small) as that of `table`. There is only one alternative\n    hypothesis available: the rows and columns are not independent.\n\n    There are other possible choices of statistic and two-sided\n    p-value definition associated with Fisher's exact test; please see the\n    Notes for more information.\n\n    Parameters\n    ----------\n    table : array_like of ints\n        A contingency table.  Elements must be non-negative integers.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis for 2x2 tables; unused for other\n        table sizes.\n        The following options are available (default is 'two-sided'):\n\n        * 'two-sided': the odds ratio of the underlying population is not one\n        * 'less': the odds ratio of the underlying population is less than one\n        * 'greater': the odds ratio of the underlying population is greater\n          than one\n\n        See the Notes for more details.\n\n    method : ResamplingMethod, optional\n        Defines the method used to compute the p-value.\n        If `method` is an instance of `PermutationMethod`/`MonteCarloMethod`,\n        the p-value is computed using\n        `scipy.stats.permutation_test`/`scipy.stats.monte_carlo_test` with the\n        provided configuration options and other appropriate settings.\n        Note that if `method` is an instance of `MonteCarloMethod`, the ``rvs``\n        attribute must be left unspecified; Monte Carlo samples are always drawn\n        using the ``rvs`` method of `scipy.stats.random_table`.\n        Otherwise, the p-value is computed as documented in the notes.\n\n        .. versionadded:: 1.15.0\n\n    Returns\n    -------\n    res : SignificanceResult\n        An object containing attributes:\n\n        statistic : float\n            For a 2x2 table with default `method`, this is the odds ratio - the\n            prior odds ratio not a posterior estimate. In all other cases, this\n            is the probability density of obtaining the observed table under the\n            null hypothesis of independence with marginals fixed.\n        pvalue : float\n            The probability under the null hypothesis of obtaining a\n            table at least as extreme as the one that was actually observed.\n\n    Raises\n    ------\n    ValueError\n        If `table` is not two-dimensional or has negative entries.\n\n    See Also\n    --------\n    chi2_contingency : Chi-square test of independence of variables in a\n        contingency table.  This can be used as an alternative to\n        `fisher_exact` when the numbers in the table are large.\n    contingency.odds_ratio : Compute the odds ratio (sample or conditional\n        MLE) for a 2x2 contingency table.\n    barnard_exact : Barnard's exact test, which is a more powerful alternative\n        than Fisher's exact test for 2x2 contingency tables.\n    boschloo_exact : Boschloo's exact test, which is a more powerful\n        alternative than Fisher's exact test for 2x2 contingency tables.\n    :ref:`hypothesis_fisher_exact` : Extended example\n\n    Notes\n    -----\n    *Null hypothesis and p-values*\n\n    The null hypothesis is that the true odds ratio of the populations\n    underlying the observations is one, and the observations were sampled at\n    random from these populations under a condition: the marginals of the\n    resulting table must equal those of the observed table. Equivalently,\n    the null hypothesis is that the input table is from the hypergeometric\n    distribution with parameters (as used in `hypergeom`)\n    ``M = a + b + c + d``, ``n = a + b`` and ``N = a + c``, where the\n    input table is ``[[a, b], [c, d]]``.  This distribution has support\n    ``max(0, N + n - M) <= x <= min(N, n)``, or, in terms of the values\n    in the input table, ``min(0, a - d) <= x <= a + min(b, c)``.  ``x``\n    can be interpreted as the upper-left element of a 2x2 table, so the\n    tables in the distribution have form::\n\n        [  x           n - x     ]\n        [N - x    M - (n + N) + x]\n\n    For example, if::\n\n        table = [6  2]\n                [1  4]\n\n    then the support is ``2 <= x <= 7``, and the tables in the distribution\n    are::\n\n        [2 6]   [3 5]   [4 4]   [5 3]   [6 2]  [7 1]\n        [5 0]   [4 1]   [3 2]   [2 3]   [1 4]  [0 5]\n\n    The probability of each table is given by the hypergeometric distribution\n    ``hypergeom.pmf(x, M, n, N)``.  For this example, these are (rounded to\n    three significant digits)::\n\n        x       2      3      4      5       6        7\n        p  0.0163  0.163  0.408  0.326  0.0816  0.00466\n\n    These can be computed with::\n\n        >>> import numpy as np\n        >>> from scipy.stats import hypergeom\n        >>> table = np.array([[6, 2], [1, 4]])\n        >>> M = table.sum()\n        >>> n = table[0].sum()\n        >>> N = table[:, 0].sum()\n        >>> start, end = hypergeom.support(M, n, N)\n        >>> hypergeom.pmf(np.arange(start, end+1), M, n, N)\n        array([0.01631702, 0.16317016, 0.40792541, 0.32634033, 0.08158508,\n               0.004662  ])\n\n    The two-sided p-value is the probability that, under the null hypothesis,\n    a random table would have a probability equal to or less than the\n    probability of the input table.  For our example, the probability of\n    the input table (where ``x = 6``) is 0.0816.  The x values where the\n    probability does not exceed this are 2, 6 and 7, so the two-sided p-value\n    is ``0.0163 + 0.0816 + 0.00466 ~= 0.10256``::\n\n        >>> from scipy.stats import fisher_exact\n        >>> res = fisher_exact(table, alternative='two-sided')\n        >>> res.pvalue\n        0.10256410256410257\n\n    The one-sided p-value for ``alternative='greater'`` is the probability\n    that a random table has ``x >= a``, which in our example is ``x >= 6``,\n    or ``0.0816 + 0.00466 ~= 0.08626``::\n\n        >>> res = fisher_exact(table, alternative='greater')\n        >>> res.pvalue\n        0.08624708624708627\n\n    This is equivalent to computing the survival function of the\n    distribution at ``x = 5`` (one less than ``x`` from the input table,\n    because we want to include the probability of ``x = 6`` in the sum)::\n\n        >>> hypergeom.sf(5, M, n, N)\n        0.08624708624708627\n\n    For ``alternative='less'``, the one-sided p-value is the probability\n    that a random table has ``x <= a``, (i.e. ``x <= 6`` in our example),\n    or ``0.0163 + 0.163 + 0.408 + 0.326 + 0.0816 ~= 0.9949``::\n\n        >>> res = fisher_exact(table, alternative='less')\n        >>> res.pvalue\n        0.9953379953379957\n\n    This is equivalent to computing the cumulative distribution function\n    of the distribution at ``x = 6``:\n\n        >>> hypergeom.cdf(6, M, n, N)\n        0.9953379953379957\n\n    *Odds ratio*\n\n    The calculated odds ratio is different from the value computed by the\n    R function ``fisher.test``.  This implementation returns the \"sample\"\n    or \"unconditional\" maximum likelihood estimate, while ``fisher.test``\n    in R uses the conditional maximum likelihood estimate.  To compute the\n    conditional maximum likelihood estimate of the odds ratio, use\n    `scipy.stats.contingency.odds_ratio`.\n\n    References\n    ----------\n    .. [1] Fisher, Sir Ronald A, \"The Design of Experiments:\n           Mathematics of a Lady Tasting Tea.\" ISBN 978-0-486-41151-4, 1935.\n    .. [2] \"Fisher's exact test\",\n           https://en.wikipedia.org/wiki/Fisher's_exact_test\n\n    Examples\n    --------\n\n    >>> from scipy.stats import fisher_exact\n    >>> res = fisher_exact([[8, 2], [1, 5]])\n    >>> res.statistic\n    20.0\n    >>> res.pvalue\n    0.034965034965034975\n\n    For tables with shape other than ``(2, 2)``, provide an instance of\n    `scipy.stats.MonteCarloMethod` or `scipy.stats.PermutationMethod` for the\n    `method` parameter:\n\n    >>> import numpy as np\n    >>> from scipy.stats import MonteCarloMethod\n    >>> rng = np.random.default_rng(4507195762371367)\n    >>> method = MonteCarloMethod(rng=rng)\n    >>> fisher_exact([[8, 2, 3], [1, 5, 4]], method=method)\n    SignificanceResult(statistic=np.float64(0.005782), pvalue=np.float64(0.0603))\n\n    For a more detailed example, see :ref:`hypothesis_fisher_exact`.\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 5980, "code": "class TtestResult(TtestResultBase):", "documentation": "    \"\"\"\n    Result of a t-test.\n\n    See the documentation of the particular t-test function for more\n    information about the definition of the statistic and meaning of\n    the confidence interval.\n\n    Attributes\n    ----------\n    statistic : float or array\n        The t-statistic of the sample.\n    pvalue : float or array\n        The p-value associated with the given alternative.\n    df : float or array\n        The number of degrees of freedom used in calculation of the\n        t-statistic; this is one less than the size of the sample\n        (``a.shape[axis]-1`` if there are no masked elements or omitted NaNs).\n\n    Methods\n    -------\n    confidence_interval\n        Computes a confidence interval around the population statistic\n        for the given confidence level.\n        The confidence interval is returned in a ``namedtuple`` with\n        fields `low` and `high`.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 6021, "code": "    def confidence_interval(self, confidence_level=0.95):\n        low, high = _t_confidence_interval(self.df, self._statistic_np,\n                                           confidence_level, self._alternative,\n                                           self._dtype, self._xp)\n        low = low * self._standard_error + self._estimate\n        high = high * self._standard_error + self._estimate\n        return ConfidenceInterval(low=low, high=high)", "documentation": "        \"\"\"\n        Parameters\n        ----------\n        confidence_level : float\n            The confidence level for the calculation of the population mean\n            confidence interval. Default is 0.95.\n\n        Returns\n        -------\n        ci : namedtuple\n            The confidence interval is returned in a ``namedtuple`` with\n            fields `low` and `high`.\n\n        \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 6067, "code": "def ttest_1samp(a, popmean, axis=0, nan_policy=\"propagate\", alternative=\"two-sided\"):\n    xp = array_namespace(a)\n    a, popmean = xp_promote(a, popmean, force_floating=True, xp=xp)\n    a, axis = _chk_asarray(a, axis, xp=xp)\n    n = _length_nonmasked(a, axis)\n    df = n - 1\n    if a.shape[axis] == 0:\n        NaN = _get_nan(a)\n        return TtestResult(NaN, NaN, df=NaN, alternative=NaN,\n                           standard_error=NaN, estimate=NaN)\n    mean = xp.mean(a, axis=axis)", "documentation": "    \"\"\"Calculate the T-test for the mean of ONE group of scores.\n\n    This is a test for the null hypothesis that the expected value\n    (mean) of a sample of independent observations `a` is equal to the given\n    population mean, `popmean`.\n\n    Parameters\n    ----------\n    a : array_like\n        Sample observations.\n    popmean : float or array_like\n        Expected value in null hypothesis. If array_like, then its length along\n        `axis` must equal 1, and it must otherwise be broadcastable with `a`.\n    axis : int or None, optional\n        Axis along which to compute test; default is 0. If None, compute over\n        the whole array `a`.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis.\n        The following options are available (default is 'two-sided'):\n\n        * 'two-sided': the mean of the underlying distribution of the sample\n          is different than the given population mean (`popmean`)\n        * 'less': the mean of the underlying distribution of the sample is\n          less than the given population mean (`popmean`)\n        * 'greater': the mean of the underlying distribution of the sample is\n          greater than the given population mean (`popmean`)\n\n    Returns\n    -------\n    result : `~scipy.stats._result_classes.TtestResult`\n        An object with the following attributes:\n\n        statistic : float or array\n            The t-statistic.\n        pvalue : float or array\n            The p-value associated with the given alternative.\n        df : float or array\n            The number of degrees of freedom used in calculation of the\n            t-statistic; this is one less than the size of the sample\n            (``a.shape[axis]``).\n\n            .. versionadded:: 1.10.0\n\n        The object also has the following method:\n\n        confidence_interval(confidence_level=0.95)\n            Computes a confidence interval around the population\n            mean for the given confidence level.\n            The confidence interval is returned in a ``namedtuple`` with\n            fields `low` and `high`.\n\n            .. versionadded:: 1.10.0\n\n    Notes\n    -----\n    The statistic is calculated as ``(np.mean(a) - popmean)/se``, where\n    ``se`` is the standard error. Therefore, the statistic will be positive\n    when the sample mean is greater than the population mean and negative when\n    the sample mean is less than the population mean.\n\n    Examples\n    --------\n    Suppose we wish to test the null hypothesis that the mean of a population\n    is equal to 0.5. We choose a confidence level of 99%; that is, we will\n    reject the null hypothesis in favor of the alternative if the p-value is\n    less than 0.01.\n\n    When testing random variates from the standard uniform distribution, which\n    has a mean of 0.5, we expect the data to be consistent with the null\n    hypothesis most of the time.\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> rvs = stats.uniform.rvs(size=50, random_state=rng)\n    >>> stats.ttest_1samp(rvs, popmean=0.5)\n    TtestResult(statistic=2.456308468440, pvalue=0.017628209047638, df=49)\n\n    As expected, the p-value of 0.017 is not below our threshold of 0.01, so\n    we cannot reject the null hypothesis.\n\n    When testing data from the standard *normal* distribution, which has a mean\n    of 0, we would expect the null hypothesis to be rejected.\n\n    >>> rvs = stats.norm.rvs(size=50, random_state=rng)\n    >>> stats.ttest_1samp(rvs, popmean=0.5)\n    TtestResult(statistic=-7.433605518875, pvalue=1.416760157221e-09, df=49)\n\n    Indeed, the p-value is lower than our threshold of 0.01, so we reject the\n    null hypothesis in favor of the default \"two-sided\" alternative: the mean\n    of the population is *not* equal to 0.5.\n\n    However, suppose we were to test the null hypothesis against the\n    one-sided alternative that the mean of the population is *greater* than\n    0.5. Since the mean of the standard normal is less than 0.5, we would not\n    expect the null hypothesis to be rejected.\n\n    >>> stats.ttest_1samp(rvs, popmean=0.5, alternative='greater')\n    TtestResult(statistic=-7.433605518875, pvalue=0.99999999929, df=49)\n\n    Unsurprisingly, with a p-value greater than our threshold, we would not\n    reject the null hypothesis.\n\n    Note that when working with a confidence level of 99%, a true null\n    hypothesis will be rejected approximately 1% of the time.\n\n    >>> rvs = stats.uniform.rvs(size=(100, 50), random_state=rng)\n    >>> res = stats.ttest_1samp(rvs, popmean=0.5, axis=1)\n    >>> np.sum(res.pvalue < 0.01)\n    1\n\n    Indeed, even though all 100 samples above were drawn from the standard\n    uniform distribution, which *does* have a population mean of 0.5, we would\n    mistakenly reject the null hypothesis for one of them.\n\n    `ttest_1samp` can also compute a confidence interval around the population\n    mean.\n\n    >>> rvs = stats.norm.rvs(size=50, random_state=rng)\n    >>> res = stats.ttest_1samp(rvs, popmean=0)\n    >>> ci = res.confidence_interval(confidence_level=0.95)\n    >>> ci\n    ConfidenceInterval(low=-0.3193887540880017, high=0.2898583388980972)\n\n    The bounds of the 95% confidence interval are the\n    minimum and maximum values of the parameter `popmean` for which the\n    p-value of the test would be 0.05.\n\n    >>> res = stats.ttest_1samp(rvs, popmean=ci.low)\n    >>> np.testing.assert_allclose(res.pvalue, 0.05)\n    >>> res = stats.ttest_1samp(rvs, popmean=ci.high)\n    >>> np.testing.assert_allclose(res.pvalue, 0.05)\n\n    Under certain assumptions about the population from which a sample\n    is drawn, the confidence interval with confidence level 95% is expected\n    to contain the true population mean in 95% of sample replications.\n\n    >>> rvs = stats.norm.rvs(size=(50, 1000), loc=1, random_state=rng)\n    >>> res = stats.ttest_1samp(rvs, popmean=0)\n    >>> ci = res.confidence_interval()\n    >>> contains_pop_mean = (ci.low < 1) & (ci.high > 1)\n    >>> contains_pop_mean.sum()\n    953\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 6821, "code": "def _ttest_trim_var_mean_len(a, trim, axis):\n    a = np.sort(a, axis=axis)\n    n = a.shape[axis]\n    g = int(n * trim)\n    v = _calculate_winsorized_variance(a, g, axis)\n    n -= 2 * g\n    m = trim_mean(a, trim, axis=axis)\n    return v, m, n", "documentation": "    \"\"\"Variance, mean, and length of winsorized input along specified axis\"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 6845, "code": "def _calculate_winsorized_variance(a, g, axis):\n    if g == 0:\n        return _var(a, ddof=1, axis=axis)\n    a_win = np.moveaxis(a, axis, -1)\n    nans_indices = np.any(np.isnan(a_win), axis=-1)\n    a_win[..., :g] = a_win[..., [g]]\n    a_win[..., -g:] = a_win[..., [-g - 1]]\n    var_win = np.asarray(_var(a_win, ddof=(2 * g + 1), axis=-1))\n    var_win[nans_indices] = np.nan\n    return var_win\n@xp_capabilities(cpu_only=True, exceptions=[\"cupy\", \"jax.numpy\"])", "documentation": "    \"\"\"Calculates g-times winsorized variance along specified axis\"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 6884, "code": "def ttest_rel(a, b, axis=0, nan_policy='propagate', alternative=\"two-sided\"):\n    return ttest_1samp(a - b, popmean=0., axis=axis, alternative=alternative,\n                       _no_deco=True)\n_power_div_lambda_names = {\n    \"pearson\": 1,\n    \"log-likelihood\": 0,\n    \"freeman-tukey\": -0.5,\n    \"mod-log-likelihood\": -1,\n    \"neyman\": -2,\n    \"cressie-read\": 2/3,\n}", "documentation": "    \"\"\"Calculate the t-test on TWO RELATED samples of scores, a and b.\n\n    This is a test for the null hypothesis that two related or\n    repeated samples have identical average (expected) values.\n\n    Parameters\n    ----------\n    a, b : array_like\n        The arrays must have the same shape.\n    axis : int or None, optional\n        Axis along which to compute test. If None, compute over the whole\n        arrays, `a`, and `b`.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis.\n        The following options are available (default is 'two-sided'):\n\n        * 'two-sided': the means of the distributions underlying the samples\n          are unequal.\n        * 'less': the mean of the distribution underlying the first sample\n          is less than the mean of the distribution underlying the second\n          sample.\n        * 'greater': the mean of the distribution underlying the first\n          sample is greater than the mean of the distribution underlying\n          the second sample.\n\n        .. versionadded:: 1.6.0\n\n    Returns\n    -------\n    result : `~scipy.stats._result_classes.TtestResult`\n        An object with the following attributes:\n\n        statistic : float or array\n            The t-statistic.\n        pvalue : float or array\n            The p-value associated with the given alternative.\n        df : float or array\n            The number of degrees of freedom used in calculation of the\n            t-statistic; this is one less than the size of the sample\n            (``a.shape[axis]``).\n\n            .. versionadded:: 1.10.0\n\n        The object also has the following method:\n\n        confidence_interval(confidence_level=0.95)\n            Computes a confidence interval around the difference in\n            population means for the given confidence level.\n            The confidence interval is returned in a ``namedtuple`` with\n            fields `low` and `high`.\n\n            .. versionadded:: 1.10.0\n\n    Notes\n    -----\n    Examples for use are scores of the same set of student in\n    different exams, or repeated sampling from the same units. The\n    test measures whether the average score differs significantly\n    across samples (e.g. exams). If we observe a large p-value, for\n    example greater than 0.05 or 0.1 then we cannot reject the null\n    hypothesis of identical average scores. If the p-value is smaller\n    than the threshold, e.g. 1%, 5% or 10%, then we reject the null\n    hypothesis of equal averages. Small p-values are associated with\n    large t-statistics.\n\n    The t-statistic is calculated as ``np.mean(a - b)/se``, where ``se`` is the\n    standard error. Therefore, the t-statistic will be positive when the sample\n    mean of ``a - b`` is greater than zero and negative when the sample mean of\n    ``a - b`` is less than zero.\n\n    References\n    ----------\n    https://en.wikipedia.org/wiki/T-test#Dependent_t-test_for_paired_samples\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n\n    >>> rvs1 = stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n    >>> rvs2 = (stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n    ...         + stats.norm.rvs(scale=0.2, size=500, random_state=rng))\n    >>> stats.ttest_rel(rvs1, rvs2)\n    TtestResult(statistic=-0.4549717054410304, pvalue=0.6493274702088672, df=499)\n    >>> rvs3 = (stats.norm.rvs(loc=8, scale=10, size=500, random_state=rng)\n    ...         + stats.norm.rvs(scale=0.2, size=500, random_state=rng))\n    >>> stats.ttest_rel(rvs1, rvs3)\n    TtestResult(statistic=-5.879467544540889, pvalue=7.540777129099917e-09, df=499)\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7010, "code": "def power_divergence(f_obs, f_exp=None, ddof=0, axis=0, lambda_=None):\n    return _power_divergence(f_obs, f_exp=f_exp, ddof=ddof, axis=axis, lambda_=lambda_)", "documentation": "    \"\"\"Cressie-Read power divergence statistic and goodness of fit test.\n\n    This function tests the null hypothesis that the categorical data\n    has the given frequencies, using the Cressie-Read power divergence\n    statistic.\n\n    Parameters\n    ----------\n    f_obs : array_like\n        Observed frequencies in each category.\n    f_exp : array_like, optional\n        Expected frequencies in each category.  By default the categories are\n        assumed to be equally likely.\n    ddof : int, optional\n        \"Delta degrees of freedom\": adjustment to the degrees of freedom\n        for the p-value.  The p-value is computed using a chi-squared\n        distribution with ``k - 1 - ddof`` degrees of freedom, where `k`\n        is the number of observed frequencies.  The default value of `ddof`\n        is 0.\n    axis : int or None, optional\n        The axis of the broadcast result of `f_obs` and `f_exp` along which to\n        apply the test.  If axis is None, all values in `f_obs` are treated\n        as a single data set.  Default is 0.\n    lambda_ : float or str, optional\n        The power in the Cressie-Read power divergence statistic.  The default\n        is 1.  For convenience, `lambda_` may be assigned one of the following\n        strings, in which case the corresponding numerical value is used:\n\n        * ``\"pearson\"`` (value 1)\n            Pearson's chi-squared statistic. In this case, the function is\n            equivalent to `chisquare`.\n        * ``\"log-likelihood\"`` (value 0)\n            Log-likelihood ratio. Also known as the G-test [3]_.\n        * ``\"freeman-tukey\"`` (value -1/2)\n            Freeman-Tukey statistic.\n        * ``\"mod-log-likelihood\"`` (value -1)\n            Modified log-likelihood ratio.\n        * ``\"neyman\"`` (value -2)\n            Neyman's statistic.\n        * ``\"cressie-read\"`` (value 2/3)\n            The power recommended in [5]_.\n\n    Returns\n    -------\n    res: Power_divergenceResult\n        An object containing attributes:\n\n        statistic : float or ndarray\n            The Cressie-Read power divergence test statistic.  The value is\n            a float if `axis` is None or if` `f_obs` and `f_exp` are 1-D.\n        pvalue : float or ndarray\n            The p-value of the test.  The value is a float if `ddof` and the\n            return value `stat` are scalars.\n\n    See Also\n    --------\n    chisquare\n\n    Notes\n    -----\n    This test is invalid when the observed or expected frequencies in each\n    category are too small.  A typical rule is that all of the observed\n    and expected frequencies should be at least 5.\n\n    Also, the sum of the observed and expected frequencies must be the same\n    for the test to be valid; `power_divergence` raises an error if the sums\n    do not agree within a relative tolerance of ``eps**0.5``, where ``eps``\n    is the precision of the input dtype.\n\n    When `lambda_` is less than zero, the formula for the statistic involves\n    dividing by `f_obs`, so a warning or error may be generated if any value\n    in `f_obs` is 0.\n\n    Similarly, a warning or error may be generated if any value in `f_exp` is\n    zero when `lambda_` >= 0.\n\n    The default degrees of freedom, k-1, are for the case when no parameters\n    of the distribution are estimated. If p parameters are estimated by\n    efficient maximum likelihood then the correct degrees of freedom are\n    k-1-p. If the parameters are estimated in a different way, then the\n    dof can be between k-1-p and k-1. However, it is also possible that\n    the asymptotic distribution is not a chisquare, in which case this\n    test is not appropriate.\n\n    References\n    ----------\n    .. [1] Lowry, Richard.  \"Concepts and Applications of Inferential\n           Statistics\". Chapter 8.\n           https://web.archive.org/web/20171015035606/http://faculty.vassar.edu/lowry/ch8pt1.html\n    .. [2] \"Chi-squared test\", https://en.wikipedia.org/wiki/Chi-squared_test\n    .. [3] \"G-test\", https://en.wikipedia.org/wiki/G-test\n    .. [4] Sokal, R. R. and Rohlf, F. J. \"Biometry: the principles and\n           practice of statistics in biological research\", New York: Freeman\n           (1981)\n    .. [5] Cressie, N. and Read, T. R. C., \"Multinomial Goodness-of-Fit\n           Tests\", J. Royal Stat. Soc. Series B, Vol. 46, No. 3 (1984),\n           pp. 440-464.\n\n    Examples\n    --------\n    (See `chisquare` for more examples.)\n\n    When just `f_obs` is given, it is assumed that the expected frequencies\n    are uniform and given by the mean of the observed frequencies.  Here we\n    perform a G-test (i.e. use the log-likelihood ratio statistic):\n\n    >>> import numpy as np\n    >>> from scipy.stats import power_divergence\n    >>> power_divergence([16, 18, 16, 14, 12, 12], lambda_='log-likelihood')\n    (2.006573162632538, 0.84823476779463769)\n\n    The expected frequencies can be given with the `f_exp` argument:\n\n    >>> power_divergence([16, 18, 16, 14, 12, 12],\n    ...                  f_exp=[16, 16, 16, 16, 16, 8],\n    ...                  lambda_='log-likelihood')\n    (3.3281031458963746, 0.6495419288047497)\n\n    When `f_obs` is 2-D, by default the test is applied to each column.\n\n    >>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T\n    >>> obs.shape\n    (6, 2)\n    >>> power_divergence(obs, lambda_=\"log-likelihood\")\n    (array([ 2.00657316,  6.77634498]), array([ 0.84823477,  0.23781225]))\n\n    By setting ``axis=None``, the test is applied to all data in the array,\n    which is equivalent to applying the test to the flattened array.\n\n    >>> power_divergence(obs, axis=None)\n    (23.31034482758621, 0.015975692534127565)\n    >>> power_divergence(obs.ravel())\n    (23.31034482758621, 0.015975692534127565)\n\n    `ddof` is the change to make to the default degrees of freedom.\n\n    >>> power_divergence([16, 18, 16, 14, 12, 12], ddof=1)\n    (2.0, 0.73575888234288467)\n\n    The calculation of the p-values is done by broadcasting the\n    test statistic with `ddof`.\n\n    >>> power_divergence([16, 18, 16, 14, 12, 12], ddof=[0,1,2])\n    (2.0, array([ 0.84914504,  0.73575888,  0.5724067 ]))\n\n    `f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has\n    shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting\n    `f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared\n    statistics, we must use ``axis=1``:\n\n    >>> power_divergence([16, 18, 16, 14, 12, 12],\n    ...                  f_exp=[[16, 16, 16, 16, 16, 8],\n    ...                         [8, 20, 20, 16, 12, 12]],\n    ...                  axis=1)\n    (array([ 3.5 ,  9.25]), array([ 0.62338763,  0.09949846]))\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7257, "code": "def chisquare(f_obs, f_exp=None, ddof=0, axis=0, *, sum_check=True):\n    return _power_divergence(f_obs, f_exp=f_exp, ddof=ddof, axis=axis,\n                             lambda_=\"pearson\", sum_check=sum_check)\nKstestResult = _make_tuple_bunch('KstestResult', ['statistic', 'pvalue'],\n                                 ['statistic_location', 'statistic_sign'])", "documentation": "    \"\"\"Perform Pearson's chi-squared test.\n\n    Pearson's chi-squared test [1]_ is a goodness-of-fit test for a multinomial\n    distribution with given probabilities; that is, it assesses the null hypothesis\n    that the observed frequencies (counts) are obtained by independent\n    sampling of *N* observations from a categorical distribution with given\n    expected frequencies.\n\n    Parameters\n    ----------\n    f_obs : array_like\n        Observed frequencies in each category.\n    f_exp : array_like, optional\n        Expected frequencies in each category. By default, the categories are\n        assumed to be equally likely.\n    ddof : int, optional\n        \"Delta degrees of freedom\": adjustment to the degrees of freedom\n        for the p-value.  The p-value is computed using a chi-squared\n        distribution with ``k - 1 - ddof`` degrees of freedom, where ``k``\n        is the number of categories.  The default value of `ddof` is 0.\n    axis : int or None, optional\n        The axis of the broadcast result of `f_obs` and `f_exp` along which to\n        apply the test.  If axis is None, all values in `f_obs` are treated\n        as a single data set.  Default is 0.\n    sum_check : bool, optional\n        Whether to perform a check that ``sum(f_obs) - sum(f_exp) == 0``. If True,\n        (default) raise an error (or, for lazy backends, return NaN) when the relative\n        difference exceeds the square root of the precision of the data type.\n        See Notes for rationale and possible exceptions.\n\n    Returns\n    -------\n    res: Power_divergenceResult\n        An object containing attributes:\n\n        statistic : float or ndarray\n            The chi-squared test statistic.  The value is a float if `axis` is\n            None or `f_obs` and `f_exp` are 1-D.\n        pvalue : float or ndarray\n            The p-value of the test.  The value is a float if `ddof` and the\n            result attribute `statistic` are scalars.\n\n    See Also\n    --------\n    scipy.stats.power_divergence\n    scipy.stats.fisher_exact : Fisher exact test on a 2x2 contingency table.\n    scipy.stats.barnard_exact : An unconditional exact test. An alternative\n        to chi-squared test for small sample sizes.\n    :ref:`hypothesis_chisquare` : Extended example\n\n    Notes\n    -----\n    This test is invalid when the observed or expected frequencies in each\n    category are too small.  A typical rule is that all of the observed\n    and expected frequencies should be at least 5. According to [2]_, the\n    total number of observations is recommended to be greater than 13,\n    otherwise exact tests (such as Barnard's Exact test) should be used\n    because they do not overreject.\n\n    The default degrees of freedom, k-1, are for the case when no parameters\n    of the distribution are estimated. If p parameters are estimated by\n    efficient maximum likelihood then the correct degrees of freedom are\n    k-1-p. If the parameters are estimated in a different way, then the\n    dof can be between k-1-p and k-1. However, it is also possible that\n    the asymptotic distribution is not chi-square, in which case this test\n    is not appropriate.\n\n    For Pearson's chi-squared test, the total observed and expected counts must match\n    for the p-value to accurately reflect the probability of observing such an extreme\n    value of the statistic under the null hypothesis.\n    This function may be used to perform other statistical tests that do not require\n    the total counts to be equal. For instance, to test the null hypothesis that\n    ``f_obs[i]`` is Poisson-distributed with expectation ``f_exp[i]``, set ``ddof=-1``\n    and ``sum_check=False``. This test follows from the fact that a Poisson random\n    variable with mean and variance ``f_exp[i]`` is approximately normal with the\n    same mean and variance; the chi-squared statistic standardizes, squares, and sums\n    the observations; and the sum of ``n`` squared standard normal variables follows\n    the chi-squared distribution with ``n`` degrees of freedom.\n\n    References\n    ----------\n    .. [1] \"Pearson's chi-squared test\".\n           *Wikipedia*. https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test\n    .. [2] Pearson, Karl. \"On the criterion that a given system of deviations from the probable\n           in the case of a correlated system of variables is such that it can be reasonably\n           supposed to have arisen from random sampling\", Philosophical Magazine. Series 5. 50\n           (1900), pp. 157-175.\n\n    Examples\n    --------\n    When only the mandatory `f_obs` argument is given, it is assumed that the\n    expected frequencies are uniform and given by the mean of the observed\n    frequencies:\n\n    >>> import numpy as np\n    >>> from scipy.stats import chisquare\n    >>> chisquare([16, 18, 16, 14, 12, 12])\n    Power_divergenceResult(statistic=2.0, pvalue=0.84914503608460956)\n\n    The optional `f_exp` argument gives the expected frequencies.\n\n    >>> chisquare([16, 18, 16, 14, 12, 12], f_exp=[16, 16, 16, 16, 16, 8])\n    Power_divergenceResult(statistic=3.5, pvalue=0.62338762774958223)\n\n    When `f_obs` is 2-D, by default the test is applied to each column.\n\n    >>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T\n    >>> obs.shape\n    (6, 2)\n    >>> chisquare(obs)\n    Power_divergenceResult(statistic=array([2.        , 6.66666667]), pvalue=array([0.84914504, 0.24663415]))\n\n    By setting ``axis=None``, the test is applied to all data in the array,\n    which is equivalent to applying the test to the flattened array.\n\n    >>> chisquare(obs, axis=None)\n    Power_divergenceResult(statistic=23.31034482758621, pvalue=0.015975692534127565)\n    >>> chisquare(obs.ravel())\n    Power_divergenceResult(statistic=23.310344827586206, pvalue=0.01597569253412758)\n\n    `ddof` is the change to make to the default degrees of freedom.\n\n    >>> chisquare([16, 18, 16, 14, 12, 12], ddof=1)\n    Power_divergenceResult(statistic=2.0, pvalue=0.7357588823428847)\n\n    The calculation of the p-values is done by broadcasting the\n    chi-squared statistic with `ddof`.\n\n    >>> chisquare([16, 18, 16, 14, 12, 12], ddof=[0, 1, 2])\n    Power_divergenceResult(statistic=2.0, pvalue=array([0.84914504, 0.73575888, 0.5724067 ]))\n\n    `f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has\n    shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting\n    `f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared\n    statistics, we use ``axis=1``:\n\n    >>> chisquare([16, 18, 16, 14, 12, 12],\n    ...           f_exp=[[16, 16, 16, 16, 16, 8], [8, 20, 20, 16, 12, 12]],\n    ...           axis=1)\n    Power_divergenceResult(statistic=array([3.5 , 9.25]), pvalue=array([0.62338763, 0.09949846]))\n\n    For a more detailed example, see :ref:`hypothesis_chisquare`.\n    \"\"\"  # noqa: E501"}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7409, "code": "def _compute_d(cdfvals, x, sign, xp=None):\n    xp = array_namespace(cdfvals, x) if xp is None else xp\n    n = cdfvals.shape[-1]\n    D = (xp.arange(1.0, n + 1, dtype=x.dtype) / n - cdfvals if sign == +1\n         else (cdfvals - xp.arange(0.0, n, dtype=x.dtype)/n))\n    amax = xp.argmax(D, axis=-1, keepdims=True)\n    loc_max = xp.squeeze(xp.take_along_axis(x, amax, axis=-1), axis=-1)\n    D = xp.squeeze(xp.take_along_axis(D, amax, axis=-1), axis=-1)\n    return D[()] if D.ndim == 0 else D, loc_max[()] if loc_max.ndim == 0 else loc_max", "documentation": "    \"\"\"Computes D+/D- as used in the Kolmogorov-Smirnov test.\n\n    Vectorized along the last axis.\n\n    Parameters\n    ----------\n    cdfvals : array_like\n        Sorted array of CDF values between 0 and 1\n    x: array_like\n        Sorted array of the stochastic variable itself\n    sign: int\n        Indicates whether to compute D+ (+1) or D- (-1).\n\n    Returns\n    -------\n    D : float or array\n        The maximum distance of the CDF values below/above (D+/D-) Uniform(0, 1).\n    loc_max : float or array\n        The location at which the maximum is reached.\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7457, "code": "def ks_1samp(x, cdf, args=(), alternative='two-sided', method='auto', *, axis=0):\n    xp = array_namespace(x)\n    mode = method\n    alternative = {'t': 'two-sided', 'g': 'greater', 'l': 'less'}.get(\n        alternative.lower()[0], alternative)\n    if alternative not in ['two-sided', 'greater', 'less']:\n        raise ValueError(f\"Unexpected value {alternative=}\")\n    N = x.shape[-1]\n    x = xp.sort(x, axis=-1)\n    x = xp_promote(x, force_floating=True, xp=xp)\n    cdfvals = cdf(x, *args)", "documentation": "    \"\"\"\n    Performs the one-sample Kolmogorov-Smirnov test for goodness of fit.\n\n    This test compares the underlying distribution F(x) of a sample\n    against a given continuous distribution G(x). See Notes for a description\n    of the available null and alternative hypotheses.\n\n    Parameters\n    ----------\n    x : array_like\n        a 1-D array of observations of iid random variables.\n    cdf : callable\n        callable used to calculate the cdf.\n    args : tuple, sequence, optional\n        Distribution parameters, used with `cdf`.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the null and alternative hypotheses. Default is 'two-sided'.\n        Please see explanations in the Notes below.\n    method : {'auto', 'exact', 'approx', 'asymp'}, optional\n        Defines the distribution used for calculating the p-value.\n        The following options are available (default is 'auto'):\n\n        * 'auto' : selects one of the other options.\n        * 'exact' : uses the exact distribution of test statistic.\n        * 'approx' : approximates the two-sided probability with twice\n          the one-sided probability\n        * 'asymp': uses asymptotic distribution of test statistic\n\n    axis : int or tuple of ints, default: 0\n        If an int or tuple of ints, the axis or axes of the input along which\n        to compute the statistic. The statistic of each axis-slice (e.g. row)\n        of the input will appear in a corresponding element of the output.\n        If ``None``, the input will be raveled before computing the statistic.\n\n    Returns\n    -------\n    res: KstestResult\n        An object containing attributes:\n\n        statistic : float\n            KS test statistic, either D+, D-, or D (the maximum of the two)\n        pvalue : float\n            One-tailed or two-tailed p-value.\n        statistic_location : float\n            Value of `x` corresponding with the KS statistic; i.e., the\n            distance between the empirical distribution function and the\n            hypothesized cumulative distribution function is measured at this\n            observation.\n        statistic_sign : int\n            +1 if the KS statistic is the maximum positive difference between\n            the empirical distribution function and the hypothesized cumulative\n            distribution function (D+); -1 if the KS statistic is the maximum\n            negative difference (D-).\n\n\n    See Also\n    --------\n    ks_2samp, kstest\n\n    Notes\n    -----\n    There are three options for the null and corresponding alternative\n    hypothesis that can be selected using the `alternative` parameter.\n\n    - `two-sided`: The null hypothesis is that the two distributions are\n      identical, F(x)=G(x) for all x; the alternative is that they are not\n      identical.\n\n    - `less`: The null hypothesis is that F(x) >= G(x) for all x; the\n      alternative is that F(x) < G(x) for at least one x.\n\n    - `greater`: The null hypothesis is that F(x) <= G(x) for all x; the\n      alternative is that F(x) > G(x) for at least one x.\n\n    Note that the alternative hypotheses describe the *CDFs* of the\n    underlying distributions, not the observed values. For example,\n    suppose x1 ~ F and x2 ~ G. If F(x) > G(x) for all x, the values in\n    x1 tend to be less than those in x2.\n\n    Examples\n    --------\n    Suppose we wish to test the null hypothesis that a sample is distributed\n    according to the standard normal.\n    We choose a confidence level of 95%; that is, we will reject the null\n    hypothesis in favor of the alternative if the p-value is less than 0.05.\n\n    When testing uniformly distributed data, we would expect the\n    null hypothesis to be rejected.\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> stats.ks_1samp(stats.uniform.rvs(size=100, random_state=rng),\n    ...                stats.norm.cdf)\n    KstestResult(statistic=0.5001899973268688,\n                 pvalue=1.1616392184763533e-23,\n                 statistic_location=0.00047625268963724654,\n                 statistic_sign=-1)\n\n    Indeed, the p-value is lower than our threshold of 0.05, so we reject the\n    null hypothesis in favor of the default \"two-sided\" alternative: the data\n    are *not* distributed according to the standard normal.\n\n    When testing random variates from the standard normal distribution, we\n    expect the data to be consistent with the null hypothesis most of the time.\n\n    >>> x = stats.norm.rvs(size=100, random_state=rng)\n    >>> stats.ks_1samp(x, stats.norm.cdf)\n    KstestResult(statistic=0.05345882212970396,\n                 pvalue=0.9227159037744717,\n                 statistic_location=-1.2451343873745018,\n                 statistic_sign=1)\n\n    As expected, the p-value of 0.92 is not below our threshold of 0.05, so\n    we cannot reject the null hypothesis.\n\n    Suppose, however, that the random variates are distributed according to\n    a normal distribution that is shifted toward greater values. In this case,\n    the cumulative density function (CDF) of the underlying distribution tends\n    to be *less* than the CDF of the standard normal. Therefore, we would\n    expect the null hypothesis to be rejected with ``alternative='less'``:\n\n    >>> x = stats.norm.rvs(size=100, loc=0.5, random_state=rng)\n    >>> stats.ks_1samp(x, stats.norm.cdf, alternative='less')\n    KstestResult(statistic=0.17482387821055168,\n                 pvalue=0.001913921057766743,\n                 statistic_location=0.3713830565352756,\n                 statistic_sign=-1)\n\n    and indeed, with p-value smaller than our threshold, we reject the null\n    hypothesis in favor of the alternative.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7651, "code": "def _compute_prob_outside_square(n, h):\n    P = 0.0\n    k = int(np.floor(n / h))\n    while k >= 0:\n        p1 = 1.0\n        for j in range(h):\n            p1 = (n - k * h - j) * p1 / (n + k * h + j + 1)\n        P = p1 * (1.0 - P)\n        k -= 1\n    return 2 * P", "documentation": "    \"\"\"\n    Compute the proportion of paths that pass outside the two diagonal lines.\n\n    Parameters\n    ----------\n    n : int\n        n > 0\n    h : int\n        0 <= h <= n\n\n    Returns\n    -------\n    p : float\n        The proportion of paths that pass outside the lines x-y = +/-h.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7689, "code": "def _count_paths_outside_method(m, n, g, h):\n    if m < n:\n        m, n = n, m\n    mg = m // g\n    ng = n // g\n    lxj = n + (mg-h)//mg\n    xj = [(h + mg * j + ng-1)//ng for j in range(lxj)]\n    if lxj == 0:\n        return special.binom(m + n, n)\n    B = np.zeros(lxj)\n    B[0] = 1", "documentation": "    \"\"\"Count the number of paths that pass outside the specified diagonal.\n\n    Parameters\n    ----------\n    m : int\n        m > 0\n    n : int\n        n > 0\n    g : int\n        g is greatest common divisor of m and n\n    h : int\n        0 <= h <= lcm(m,n)\n\n    Returns\n    -------\n    p : float\n        The number of paths that go low.\n        The calculation may overflow - check for a finite answer.\n\n    Notes\n    -----\n    Count the integer lattice paths from (0, 0) to (m, n), which at some\n    point (x, y) along the path, satisfy:\n      m*y <= n*x - h*g\n    The paths make steps of size +1 in either positive x or positive y\n    directions.\n\n    We generally follow Hodges' treatment of Drion/Gnedenko/Korolyuk.\n    Hodges, J.L. Jr.,\n    \"The Significance Probability of the Smirnov Two-Sample Test,\"\n    Arkiv fiur Matematik, 3, No. 43 (1958), 469-86.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7763, "code": "def _attempt_exact_2kssamp(n1, n2, g, d, alternative):\n    lcm = (n1 // g) * n2\n    h = int(np.round(d * lcm))\n    d = h * 1.0 / lcm\n    if h == 0:\n        return True, d, 1.0\n    saw_fp_error, prob = False, np.nan\n    try:\n        with np.errstate(invalid=\"raise\", over=\"raise\"):\n            if alternative == 'two-sided':\n                if n1 == n2:", "documentation": "    \"\"\"Attempts to compute the exact 2sample probability.\n\n    n1, n2 are the sample sizes\n    g is the gcd(n1, n2)\n    d is the computed max difference in ECDFs\n\n    Returns (success, d, probability)\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7817, "code": "def ks_2samp(data1, data2, alternative='two-sided', method='auto', *, axis=0):\n    mode = method\n    if mode not in ['auto', 'exact', 'asymp']:\n        raise ValueError(f'Invalid value for mode: {mode}')\n    alternative = {'t': 'two-sided', 'g': 'greater', 'l': 'less'}.get(\n        alternative.lower()[0], alternative)\n    if alternative not in ['two-sided', 'less', 'greater']:\n        raise ValueError(f'Invalid value for alternative: {alternative}')\n    MAX_AUTO_N = 10000  # 'auto' will attempt to be exact if n1,n2 <= MAX_AUTO_N\n    xp = array_namespace(data1, data2)\n    data1 = xp.sort(data1, axis=-1)", "documentation": "    \"\"\"\n    Performs the two-sample Kolmogorov-Smirnov test for goodness of fit.\n\n    This test compares the underlying continuous distributions F(x) and G(x)\n    of two independent samples.  See Notes for a description of the available\n    null and alternative hypotheses.\n\n    Parameters\n    ----------\n    data1, data2 : array_like, 1-Dimensional\n        Two arrays of sample observations assumed to be drawn from a continuous\n        distribution, sample sizes can be different.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the null and alternative hypotheses. Default is 'two-sided'.\n        Please see explanations in the Notes below.\n    method : {'auto', 'exact', 'asymp'}, optional\n        Defines the method used for calculating the p-value.\n        The following options are available (default is 'auto'):\n\n        * 'auto' : use 'exact' for small size arrays, 'asymp' for large\n        * 'exact' : use exact distribution of test statistic\n        * 'asymp' : use asymptotic distribution of test statistic\n\n    axis : int or tuple of ints, default: 0\n        If an int or tuple of ints, the axis or axes of the input along which\n        to compute the statistic. The statistic of each axis-slice (e.g. row)\n        of the input will appear in a corresponding element of the output.\n        If ``None``, the input will be raveled before computing the statistic.\n\n    Returns\n    -------\n    res: KstestResult\n        An object containing attributes:\n\n        statistic : float\n            KS test statistic.\n        pvalue : float\n            One-tailed or two-tailed p-value.\n        statistic_location : float\n            Value from `data1` or `data2` corresponding with the KS statistic;\n            i.e., the distance between the empirical distribution functions is\n            measured at this observation.\n        statistic_sign : int\n            +1 if the empirical distribution function of `data1` exceeds\n            the empirical distribution function of `data2` at\n            `statistic_location`, otherwise -1.\n\n    See Also\n    --------\n    kstest, ks_1samp, epps_singleton_2samp, anderson_ksamp\n\n    Notes\n    -----\n    There are three options for the null and corresponding alternative\n    hypothesis that can be selected using the `alternative` parameter.\n\n    - `less`: The null hypothesis is that F(x) >= G(x) for all x; the\n      alternative is that F(x) < G(x) for at least one x. The statistic\n      is the magnitude of the minimum (most negative) difference between the\n      empirical distribution functions of the samples.\n\n    - `greater`: The null hypothesis is that F(x) <= G(x) for all x; the\n      alternative is that F(x) > G(x) for at least one x. The statistic\n      is the maximum (most positive) difference between the empirical\n      distribution functions of the samples.\n\n    - `two-sided`: The null hypothesis is that the two distributions are\n      identical, F(x)=G(x) for all x; the alternative is that they are not\n      identical. The statistic is the maximum absolute difference between the\n      empirical distribution functions of the samples.\n\n    Note that the alternative hypotheses describe the *CDFs* of the\n    underlying distributions, not the observed values of the data. For example,\n    suppose x1 ~ F and x2 ~ G. If F(x) > G(x) for all x, the values in\n    x1 tend to be less than those in x2.\n\n    If the KS statistic is large, then the p-value will be small, and this may\n    be taken as evidence against the null hypothesis in favor of the\n    alternative.\n\n    If ``method='exact'``, `ks_2samp` attempts to compute an exact p-value,\n    that is, the probability under the null hypothesis of obtaining a test\n    statistic value as extreme as the value computed from the data.\n    If ``method='asymp'``, the asymptotic Kolmogorov-Smirnov distribution is\n    used to compute an approximate p-value.\n    If ``method='auto'``, an exact p-value computation is attempted if both\n    sample sizes are less than 10000; otherwise, the asymptotic method is used.\n    In any case, if an exact p-value calculation is attempted and fails, a\n    warning will be emitted, and the asymptotic p-value will be returned.\n\n    The 'two-sided' 'exact' computation computes the complementary probability\n    and then subtracts from 1.  As such, the minimum probability it can return\n    is about 1e-16.  While the algorithm itself is exact, numerical\n    errors may accumulate for large sample sizes.   It is most suited to\n    situations in which one of the sample sizes is only a few thousand.\n\n    We generally follow Hodges' treatment of Drion/Gnedenko/Korolyuk [1]_.\n\n    References\n    ----------\n    .. [1] Hodges, J.L. Jr.,  \"The Significance Probability of the Smirnov\n           Two-Sample Test,\" Arkiv fiur Matematik, 3, No. 43 (1958), 469-486.\n\n    Examples\n    --------\n    Suppose we wish to test the null hypothesis that two samples were drawn\n    from the same distribution.\n    We choose a confidence level of 95%; that is, we will reject the null\n    hypothesis in favor of the alternative if the p-value is less than 0.05.\n\n    If the first sample were drawn from a uniform distribution and the second\n    were drawn from the standard normal, we would expect the null hypothesis\n    to be rejected.\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> sample1 = stats.uniform.rvs(size=100, random_state=rng)\n    >>> sample2 = stats.norm.rvs(size=110, random_state=rng)\n    >>> stats.ks_2samp(sample1, sample2)\n    KstestResult(statistic=0.5454545454545454,\n                 pvalue=7.37417839555191e-15,\n                 statistic_location=-0.014071496412861274,\n                 statistic_sign=-1)\n\n\n    Indeed, the p-value is lower than our threshold of 0.05, so we reject the\n    null hypothesis in favor of the default \"two-sided\" alternative: the data\n    were *not* drawn from the same distribution.\n\n    When both samples are drawn from the same distribution, we expect the data\n    to be consistent with the null hypothesis most of the time.\n\n    >>> sample1 = stats.norm.rvs(size=105, random_state=rng)\n    >>> sample2 = stats.norm.rvs(size=95, random_state=rng)\n    >>> stats.ks_2samp(sample1, sample2)\n    KstestResult(statistic=0.10927318295739348,\n                 pvalue=0.5438289009927495,\n                 statistic_location=-0.1670157701848795,\n                 statistic_sign=-1)\n\n    As expected, the p-value of 0.54 is not below our threshold of 0.05, so\n    we cannot reject the null hypothesis.\n\n    Suppose, however, that the first sample were drawn from\n    a normal distribution shifted toward greater values. In this case,\n    the cumulative density function (CDF) of the underlying distribution tends\n    to be *less* than the CDF underlying the second sample. Therefore, we would\n    expect the null hypothesis to be rejected with ``alternative='less'``:\n\n    >>> sample1 = stats.norm.rvs(size=105, loc=0.5, random_state=rng)\n    >>> stats.ks_2samp(sample1, sample2, alternative='less')\n    KstestResult(statistic=0.4055137844611529,\n                 pvalue=3.5474563068855554e-08,\n                 statistic_location=-0.13249370614972575,\n                 statistic_sign=-1)\n\n    and indeed, with p-value smaller than our threshold, we reject the null\n    hypothesis in favor of the alternative.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 8148, "code": "def kstest(rvs, cdf, args=(), N=20, alternative='two-sided', method='auto'):\n    if alternative == 'two_sided':\n        alternative = 'two-sided'\n    if alternative not in ['two-sided', 'greater', 'less']:\n        raise ValueError(f\"Unexpected alternative: {alternative}\")\n    xvals, yvals, cdf = _parse_kstest_args(rvs, cdf, args, N)\n    if cdf:\n        return ks_1samp(xvals, cdf, args=args, alternative=alternative,\n                        method=method, _no_deco=True)\n    return ks_2samp(xvals, yvals, alternative=alternative, method=method,\n                    _no_deco=True)", "documentation": "    \"\"\"\n    Performs the (one-sample or two-sample) Kolmogorov-Smirnov test for\n    goodness of fit.\n\n    The one-sample test compares the underlying distribution F(x) of a sample\n    against a given distribution G(x). The two-sample test compares the\n    underlying distributions of two independent samples. Both tests are valid\n    only for continuous distributions.\n\n    Parameters\n    ----------\n    rvs : str, array_like, or callable\n        If an array, it should be a 1-D array of observations of random\n        variables.\n        If a callable, it should be a function to generate random variables;\n        it is required to have a keyword argument `size`.\n        If a string, it should be the name of a distribution in `scipy.stats`,\n        which will be used to generate random variables.\n    cdf : str, array_like or callable\n        If array_like, it should be a 1-D array of observations of random\n        variables, and the two-sample test is performed\n        (and rvs must be array_like).\n        If a callable, that callable is used to calculate the cdf.\n        If a string, it should be the name of a distribution in `scipy.stats`,\n        which will be used as the cdf function.\n    args : tuple, sequence, optional\n        Distribution parameters, used if `rvs` or `cdf` are strings or\n        callables.\n    N : int, optional\n        Sample size if `rvs` is string or callable.  Default is 20.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the null and alternative hypotheses. Default is 'two-sided'.\n        Please see explanations in the Notes below.\n    method : {'auto', 'exact', 'approx', 'asymp'}, optional\n        Defines the distribution used for calculating the p-value.\n        The following options are available (default is 'auto'):\n\n        * 'auto' : selects one of the other options.\n        * 'exact' : uses the exact distribution of test statistic.\n        * 'approx' : approximates the two-sided probability with twice the\n          one-sided probability\n        * 'asymp': uses asymptotic distribution of test statistic\n\n    Returns\n    -------\n    res: KstestResult\n        An object containing attributes:\n\n        statistic : float\n            KS test statistic, either D+, D-, or D (the maximum of the two)\n        pvalue : float\n            One-tailed or two-tailed p-value.\n        statistic_location : float\n            In a one-sample test, this is the value of `rvs`\n            corresponding with the KS statistic; i.e., the distance between\n            the empirical distribution function and the hypothesized cumulative\n            distribution function is measured at this observation.\n\n            In a two-sample test, this is the value from `rvs` or `cdf`\n            corresponding with the KS statistic; i.e., the distance between\n            the empirical distribution functions is measured at this\n            observation.\n        statistic_sign : int\n            In a one-sample test, this is +1 if the KS statistic is the\n            maximum positive difference between the empirical distribution\n            function and the hypothesized cumulative distribution function\n            (D+); it is -1 if the KS statistic is the maximum negative\n            difference (D-).\n\n            In a two-sample test, this is +1 if the empirical distribution\n            function of `rvs` exceeds the empirical distribution\n            function of `cdf` at `statistic_location`, otherwise -1.\n\n    See Also\n    --------\n    ks_1samp, ks_2samp\n\n    Notes\n    -----\n    There are three options for the null and corresponding alternative\n    hypothesis that can be selected using the `alternative` parameter.\n\n    - `two-sided`: The null hypothesis is that the two distributions are\n      identical, F(x)=G(x) for all x; the alternative is that they are not\n      identical.\n\n    - `less`: The null hypothesis is that F(x) >= G(x) for all x; the\n      alternative is that F(x) < G(x) for at least one x.\n\n    - `greater`: The null hypothesis is that F(x) <= G(x) for all x; the\n      alternative is that F(x) > G(x) for at least one x.\n\n    Note that the alternative hypotheses describe the *CDFs* of the\n    underlying distributions, not the observed values. For example,\n    suppose x1 ~ F and x2 ~ G. If F(x) > G(x) for all x, the values in\n    x1 tend to be less than those in x2.\n\n\n    Examples\n    --------\n    Suppose we wish to test the null hypothesis that a sample is distributed\n    according to the standard normal.\n    We choose a confidence level of 95%; that is, we will reject the null\n    hypothesis in favor of the alternative if the p-value is less than 0.05.\n\n    When testing uniformly distributed data, we would expect the\n    null hypothesis to be rejected.\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> stats.kstest(stats.uniform.rvs(size=100, random_state=rng),\n    ...              stats.norm.cdf)\n    KstestResult(statistic=0.5001899973268688,\n                 pvalue=1.1616392184763533e-23,\n                 statistic_location=0.00047625268963724654,\n                 statistic_sign=-1)\n\n    Indeed, the p-value is lower than our threshold of 0.05, so we reject the\n    null hypothesis in favor of the default \"two-sided\" alternative: the data\n    are *not* distributed according to the standard normal.\n\n    When testing random variates from the standard normal distribution, we\n    expect the data to be consistent with the null hypothesis most of the time.\n\n    >>> x = stats.norm.rvs(size=100, random_state=rng)\n    >>> stats.kstest(x, stats.norm.cdf)\n    KstestResult(statistic=0.05345882212970396,\n                 pvalue=0.9227159037744717,\n                 statistic_location=-1.2451343873745018,\n                 statistic_sign=1)\n\n\n    As expected, the p-value of 0.92 is not below our threshold of 0.05, so\n    we cannot reject the null hypothesis.\n\n    Suppose, however, that the random variates are distributed according to\n    a normal distribution that is shifted toward greater values. In this case,\n    the cumulative density function (CDF) of the underlying distribution tends\n    to be *less* than the CDF of the standard normal. Therefore, we would\n    expect the null hypothesis to be rejected with ``alternative='less'``:\n\n    >>> x = stats.norm.rvs(size=100, loc=0.5, random_state=rng)\n    >>> stats.kstest(x, stats.norm.cdf, alternative='less')\n    KstestResult(statistic=0.17482387821055168,\n                 pvalue=0.001913921057766743,\n                 statistic_location=0.3713830565352756,\n                 statistic_sign=-1)\n\n    and indeed, with p-value smaller than our threshold, we reject the null\n    hypothesis in favor of the alternative.\n\n    For convenience, the previous test can be performed using the name of the\n    distribution as the second argument.\n\n    >>> stats.kstest(x, \"norm\", alternative='less')\n    KstestResult(statistic=0.17482387821055168,\n                 pvalue=0.001913921057766743,\n                 statistic_location=0.3713830565352756,\n                 statistic_sign=-1)\n\n    The examples above have all been one-sample tests identical to those\n    performed by `ks_1samp`. Note that `kstest` can also perform two-sample\n    tests identical to those performed by `ks_2samp`. For example, when two\n    samples are drawn from the same distribution, we expect the data to be\n    consistent with the null hypothesis most of the time.\n\n    >>> sample1 = stats.laplace.rvs(size=105, random_state=rng)\n    >>> sample2 = stats.laplace.rvs(size=95, random_state=rng)\n    >>> stats.kstest(sample1, sample2)\n    KstestResult(statistic=0.11779448621553884,\n                 pvalue=0.4494256912629795,\n                 statistic_location=0.6138814275424155,\n                 statistic_sign=1)\n\n    As expected, the p-value of 0.45 is not below our threshold of 0.05, so\n    we cannot reject the null hypothesis.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 8342, "code": "def tiecorrect(rankvals):\n    arr = np.sort(rankvals)\n    idx = np.nonzero(np.r_[True, arr[1:] != arr[:-1], True])[0]\n    cnt = np.diff(idx).astype(np.float64)\n    size = np.float64(arr.size)\n    return 1.0 if size < 2 else 1.0 - (cnt**3 - cnt).sum() / (size**3 - size)\nRanksumsResult = namedtuple('RanksumsResult', ('statistic', 'pvalue'))\n@xp_capabilities(np_only=True)\n@_axis_nan_policy_factory(RanksumsResult, n_samples=2)", "documentation": "    \"\"\"Tie correction factor for Mann-Whitney U and Kruskal-Wallis H tests.\n\n    Parameters\n    ----------\n    rankvals : array_like\n        A 1-D sequence of ranks.  Typically this will be the array\n        returned by `~scipy.stats.rankdata`.\n\n    Returns\n    -------\n    factor : float\n        Correction factor for U or H.\n\n    See Also\n    --------\n    rankdata : Assign ranks to the data\n    mannwhitneyu : Mann-Whitney rank test\n    kruskal : Kruskal-Wallis H test\n\n    References\n    ----------\n    .. [1] Siegel, S. (1956) Nonparametric Statistics for the Behavioral\n           Sciences.  New York: McGraw-Hill.\n\n    Examples\n    --------\n    >>> from scipy.stats import tiecorrect, rankdata\n    >>> tiecorrect([1, 2.5, 2.5, 4])\n    0.9\n    >>> ranks = rankdata([1, 3, 2, 4, 5, 7, 2, 8, 4])\n    >>> ranks\n    array([ 1. ,  4. ,  2.5,  5.5,  7. ,  8. ,  2.5,  9. ,  5.5])\n    >>> tiecorrect(ranks)\n    0.9833333333333333\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 8392, "code": "def ranksums(x, y, alternative='two-sided'):\n    x, y = map(np.asarray, (x, y))\n    n1 = len(x)\n    n2 = len(y)\n    alldata = np.concatenate((x, y))\n    ranked = rankdata(alldata)\n    x = ranked[:n1]\n    s = np.sum(x, axis=0)\n    expected = n1 * (n1+n2+1) / 2.0\n    z = (s - expected) / np.sqrt(n1*n2*(n1+n2+1)/12.0)\n    pvalue = _get_pvalue(z, _SimpleNormal(), alternative, xp=np)", "documentation": "    \"\"\"Compute the Wilcoxon rank-sum statistic for two samples.\n\n    The Wilcoxon rank-sum test tests the null hypothesis that two sets\n    of measurements are drawn from the same distribution.  The alternative\n    hypothesis is that values in one sample are more likely to be\n    larger than the values in the other sample.\n\n    This test should be used to compare two samples from continuous\n    distributions.  It does not handle ties between measurements\n    in x and y.  For tie-handling and an optional continuity correction\n    see `scipy.stats.mannwhitneyu`.\n\n    Parameters\n    ----------\n    x, y : array_like\n        The data from the two samples.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n        The following options are available:\n\n        * 'two-sided': one of the distributions (underlying `x` or `y`) is\n          stochastically greater than the other.\n        * 'less': the distribution underlying `x` is stochastically less\n          than the distribution underlying `y`.\n        * 'greater': the distribution underlying `x` is stochastically greater\n          than the distribution underlying `y`.\n\n        .. versionadded:: 1.7.0\n\n    Returns\n    -------\n    statistic : float\n        The test statistic under the large-sample approximation that the\n        rank sum statistic is normally distributed.\n    pvalue : float\n        The p-value of the test.\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/Wilcoxon_rank-sum_test\n\n    Examples\n    --------\n    We can test the hypothesis that two independent unequal-sized samples are\n    drawn from the same distribution with computing the Wilcoxon rank-sum\n    statistic.\n\n    >>> import numpy as np\n    >>> from scipy.stats import ranksums\n    >>> rng = np.random.default_rng()\n    >>> sample1 = rng.uniform(-1, 1, 200)\n    >>> sample2 = rng.uniform(-0.5, 1.5, 300) # a shifted distribution\n    >>> ranksums(sample1, sample2)\n    RanksumsResult(statistic=-7.887059,\n                   pvalue=3.09390448e-15) # may vary\n    >>> ranksums(sample1, sample2, alternative='less')\n    RanksumsResult(statistic=-7.750585297581713,\n                   pvalue=4.573497606342543e-15) # may vary\n    >>> ranksums(sample1, sample2, alternative='greater')\n    RanksumsResult(statistic=-7.750585297581713,\n                   pvalue=0.9999999999999954) # may vary\n\n    The p-value of less than ``0.05`` indicates that this test rejects the\n    hypothesis at the 5% significance level.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 8478, "code": "def kruskal(*samples, nan_policy='propagate', axis=0):\n    xp = array_namespace(*samples)\n    samples = xp_promote(*samples, force_floating=True, xp=xp)\n    num_groups = len(samples)\n    if num_groups < 2:\n        raise ValueError(\"Need at least two groups in stats.kruskal()\")\n    n = [sample.shape[-1] for sample in samples]\n    totaln = sum(n)\n    if any(n) < 1:  # Only needed for `test_axis_nan_policy`\n        raise ValueError(\"Inputs must not be empty.\")\n    alldata = xp.concat(samples, axis=-1)", "documentation": "    \"\"\"Compute the Kruskal-Wallis H-test for independent samples.\n\n    The Kruskal-Wallis H-test tests the null hypothesis that the population\n    median of all of the groups are equal.  It is a non-parametric version of\n    ANOVA.  The test works on 2 or more independent samples, which may have\n    different sizes.  Note that rejecting the null hypothesis does not\n    indicate which of the groups differs.  Post hoc comparisons between\n    groups are required to determine which groups are different.\n\n    Parameters\n    ----------\n    sample1, sample2, ... : array_like\n       Two or more arrays with the sample measurements can be given as\n       arguments. Samples must be one-dimensional.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    axis : int or tuple of ints, default: 0\n        If an int or tuple of ints, the axis or axes of the input along which\n        to compute the statistic. The statistic of each axis-slice (e.g. row)\n        of the input will appear in a corresponding element of the output.\n        If ``None``, the input will be raveled before computing the statistic.\n\n    Returns\n    -------\n    statistic : float\n       The Kruskal-Wallis H statistic, corrected for ties.\n    pvalue : float\n       The p-value for the test using the assumption that H has a chi\n       square distribution. The p-value returned is the survival function of\n       the chi square distribution evaluated at H.\n\n    See Also\n    --------\n    f_oneway : 1-way ANOVA.\n    mannwhitneyu : Mann-Whitney rank test on two samples.\n    friedmanchisquare : Friedman test for repeated measurements.\n\n    Notes\n    -----\n    Due to the assumption that H has a chi square distribution, the number\n    of samples in each group must not be too small.  A typical rule is\n    that each sample must have at least 5 measurements.\n\n    References\n    ----------\n    .. [1] W. H. Kruskal & W. W. Wallis, \"Use of Ranks in\n       One-Criterion Variance Analysis\", Journal of the American Statistical\n       Association, Vol. 47, Issue 260, pp. 583-621, 1952.\n    .. [2] https://en.wikipedia.org/wiki/Kruskal-Wallis_one-way_analysis_of_variance\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> x = [1, 3, 5, 7, 9]\n    >>> y = [2, 4, 6, 8, 10]\n    >>> stats.kruskal(x, y)\n    KruskalResult(statistic=0.2727272727272734, pvalue=0.6015081344405895)\n\n    >>> x = [1, 1, 1]\n    >>> y = [2, 2, 2]\n    >>> z = [2, 2]\n    >>> stats.kruskal(x, y, z)\n    KruskalResult(statistic=7.0, pvalue=0.0301973834223185)\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 8586, "code": "def friedmanchisquare(*samples, axis=0):\n    k = len(samples)\n    if k < 3:\n        raise ValueError('At least 3 samples must be given '\n                         f'for Friedman test, got {k}.')\n    xp = array_namespace(*samples)\n    samples = xp_promote(*samples, force_floating=True, xp=xp)\n    dtype = samples[0].dtype\n    n = samples[0].shape[-1]\n    if n == 0:  # only for `test_axis_nan_policy`; user doesn't see this\n        raise ValueError(\"One or more sample arguments is too small.\")", "documentation": "    \"\"\"Compute the Friedman test for repeated samples.\n\n    The Friedman test tests the null hypothesis that repeated samples of\n    the same individuals have the same distribution.  It is often used\n    to test for consistency among samples obtained in different ways.\n    For example, if two sampling techniques are used on the same set of\n    individuals, the Friedman test can be used to determine if the two\n    sampling techniques are consistent.\n\n    Parameters\n    ----------\n    sample1, sample2, sample3... : array_like\n        Arrays of observations.  All of the arrays must have the same number\n        of elements.  At least three samples must be given.\n    axis : int or tuple of ints, default: 0\n        If an int or tuple of ints, the axis or axes of the input along which\n        to compute the statistic. The statistic of each axis-slice (e.g. row)\n        of the input will appear in a corresponding element of the output.\n        If ``None``, the input will be raveled before computing the statistic.\n\n    Returns\n    -------\n    statistic : float\n        The test statistic, correcting for ties.\n    pvalue : float\n        The associated p-value assuming that the test statistic has a chi\n        squared distribution.\n\n    See Also\n    --------\n    :ref:`hypothesis_friedmanchisquare` : Extended example\n\n    Notes\n    -----\n    Due to the assumption that the test statistic has a chi squared\n    distribution, the p-value is only reliable for n > 10 and more than\n    6 repeated samples.\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/Friedman_test\n    .. [2] Demsar, J. (2006). Statistical comparisons of classifiers over\n           multiple data sets. Journal of Machine Learning Research, 7, 1-30.\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> rng = np.random.default_rng(seed=18)\n    >>> x = rng.random((6, 10))\n    >>> from scipy.stats import friedmanchisquare\n    >>> res = friedmanchisquare(x[0], x[1], x[2], x[3], x[4], x[5])\n    >>> res.statistic, res.pvalue\n    (11.428571428571416, 0.043514520866727614)\n\n    The p-value is less than 0.05; however, as noted above, the results may not\n    be reliable since we have a small number of repeated samples.\n\n    For a more detailed example, see :ref:`hypothesis_friedmanchisquare`.\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 8826, "code": "def combine_pvalues(pvalues, method='fisher', weights=None, *, axis=0):\n    xp = array_namespace(pvalues, weights)\n    pvalues, weights = xp_promote(pvalues, weights, broadcast=True,\n                                  force_floating=True, xp=xp)\n    if xp_size(pvalues) == 0:\n        NaN = _get_nan(pvalues)\n        return SignificanceResult(NaN, NaN)\n    n = _length_nonmasked(pvalues, axis)\n    n = xp.asarray(n, dtype=pvalues.dtype, device=xp_device(pvalues))\n    if method == 'fisher':\n        statistic = -2 * xp.sum(xp.log(pvalues), axis=axis)", "documentation": "    \"\"\"\n    Combine p-values from independent tests that bear upon the same hypothesis.\n\n    These methods are intended only for combining p-values from hypothesis\n    tests based upon continuous distributions.\n\n    Each method assumes that under the null hypothesis, the p-values are\n    sampled independently and uniformly from the interval [0, 1]. A test\n    statistic (different for each method) is computed and a combined\n    p-value is calculated based upon the distribution of this test statistic\n    under the null hypothesis.\n\n    Parameters\n    ----------\n    pvalues : array_like\n        Array of p-values assumed to come from independent tests based on\n        continuous distributions.\n    method : {'fisher', 'pearson', 'tippett', 'stouffer', 'mudholkar_george'}\n\n        Name of method to use to combine p-values.\n\n        The available methods are (see Notes for details):\n\n        * 'fisher': Fisher's method (Fisher's combined probability test)\n        * 'pearson': Pearson's method\n        * 'mudholkar_george': Mudholkar's and George's method\n        * 'tippett': Tippett's method\n        * 'stouffer': Stouffer's Z-score method\n\n    weights : array_like, optional\n        Optional array of weights used only for Stouffer's Z-score method.\n        Ignored by other methods.\n\n    Returns\n    -------\n    res : SignificanceResult\n        An object containing attributes:\n\n        statistic : float\n            The statistic calculated by the specified method.\n        pvalue : float\n            The combined p-value.\n\n    Examples\n    --------\n    Suppose we wish to combine p-values from four independent tests\n    of the same null hypothesis using Fisher's method (default).\n\n    >>> from scipy.stats import combine_pvalues\n    >>> pvalues = [0.1, 0.05, 0.02, 0.3]\n    >>> combine_pvalues(pvalues)\n    SignificanceResult(statistic=20.828626352604235, pvalue=0.007616871850449092)\n\n    When the individual p-values carry different weights, consider Stouffer's\n    method.\n\n    >>> weights = [1, 2, 3, 4]\n    >>> res = combine_pvalues(pvalues, method='stouffer', weights=weights)\n    >>> res.pvalue\n    0.009578891494533616\n\n    Notes\n    -----\n    If this function is applied to tests with a discrete statistics such as\n    any rank test or contingency-table test, it will yield systematically\n    wrong results, e.g. Fisher's method will systematically overestimate the\n    p-value [1]_. This problem becomes less severe for large sample sizes\n    when the discrete distributions become approximately continuous.\n\n    The differences between the methods can be best illustrated by their\n    statistics and what aspects of a combination of p-values they emphasise\n    when considering significance [2]_. For example, methods emphasising large\n    p-values are more sensitive to strong false and true negatives; conversely\n    methods focussing on small p-values are sensitive to positives.\n\n    * The statistics of Fisher's method (also known as Fisher's combined\n      probability test) [3]_ is :math:`-2\\\\sum_i \\\\log(p_i)`, which is\n      equivalent (as a test statistics) to the product of individual p-values:\n      :math:`\\\\prod_i p_i`. Under the null hypothesis, this statistics follows\n      a :math:`\\\\chi^2` distribution. This method emphasises small p-values.\n    * Pearson's method uses :math:`-2\\\\sum_i\\\\log(1-p_i)`, which is equivalent\n      to :math:`\\\\prod_i \\\\frac{1}{1-p_i}` [2]_.\n      It thus emphasises large p-values.\n    * Mudholkar and George compromise between Fisher's and Pearson's method by\n      averaging their statistics [4]_. Their method emphasises extreme\n      p-values, both close to 1 and 0.\n    * Stouffer's method [5]_ uses Z-scores and the statistic:\n      :math:`\\\\sum_i \\\\Phi^{-1} (p_i)`, where :math:`\\\\Phi` is the CDF of the\n      standard normal distribution. The advantage of this method is that it is\n      straightforward to introduce weights, which can make Stouffer's method\n      more powerful than Fisher's method when the p-values are from studies\n      of different size [6]_ [7]_.\n    * Tippett's method uses the smallest p-value as a statistic.\n      (Mind that this minimum is not the combined p-value.)\n\n    Fisher's method may be extended to combine p-values from dependent tests\n    [8]_. Extensions such as Brown's method and Kost's method are not currently\n    implemented.\n\n    .. versionadded:: 0.15.0\n\n    References\n    ----------\n    .. [1] Kincaid, W. M., \"The Combination of Tests Based on Discrete\n           Distributions.\" Journal of the American Statistical Association 57,\n           no. 297 (1962), 10-19.\n    .. [2] Heard, N. and Rubin-Delanchey, P. \"Choosing between methods of\n           combining p-values.\"  Biometrika 105.1 (2018): 239-246.\n    .. [3] https://en.wikipedia.org/wiki/Fisher%27s_method\n    .. [4] George, E. O., and G. S. Mudholkar. \"On the convolution of logistic\n           random variables.\" Metrika 30.1 (1983): 1-13.\n    .. [5] https://en.wikipedia.org/wiki/Fisher%27s_method#Relation_to_Stouffer.27s_Z-score_method\n    .. [6] Whitlock, M. C. \"Combining probability from independent tests: the\n           weighted Z-method is superior to Fisher's approach.\" Journal of\n           Evolutionary Biology 18, no. 5 (2005): 1368-1373.\n    .. [7] Zaykin, Dmitri V. \"Optimally weighted Z-test is a powerful method\n           for combining probabilities in meta-analysis.\" Journal of\n           Evolutionary Biology 24, no. 8 (2011): 1836-1841.\n    .. [8] https://en.wikipedia.org/wiki/Extensions_of_Fisher%27s_method\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 9039, "code": "    def confidence_interval(self, confidence_level=0.95):\n        alternative = self._alternative\n        p = self._p\n        x = np.sort(self._x)\n        n = len(x)\n        bd = stats.binom(n, p)\n        if confidence_level <= 0 or confidence_level >= 1:\n            message = \"`confidence_level` must be a number between 0 and 1.\"\n            raise ValueError(message)\n        low_index = np.nan\n        high_index = np.nan", "documentation": "        \"\"\"\n        Compute the confidence interval of the quantile.\n\n        Parameters\n        ----------\n        confidence_level : float, default: 0.95\n            Confidence level for the computed confidence interval\n            of the quantile. Default is 0.95.\n\n        Returns\n        -------\n        ci : ``ConfidenceInterval`` object\n            The object has attributes ``low`` and ``high`` that hold the\n            lower and upper bounds of the confidence interval.\n\n        Examples\n        --------\n        >>> import numpy as np\n        >>> import scipy.stats as stats\n        >>> p = 0.75  # quantile of interest\n        >>> q = 0  # hypothesized value of the quantile\n        >>> x = np.exp(np.arange(0, 1.01, 0.01))\n        >>> res = stats.quantile_test(x, q=q, p=p, alternative='less')\n        >>> lb, ub = res.confidence_interval()\n        >>> lb, ub\n        (-inf, 2.293318740264183)\n        >>> res = stats.quantile_test(x, q=q, p=p, alternative='two-sided')\n        >>> lb, ub = res.confidence_interval(0.9)\n        >>> lb, ub\n        (1.9542373206359396, 2.293318740264183)\n        \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 9941, "code": "def _validate_distribution(values, weights):\n    values = np.asarray(values, dtype=float)\n    if len(values) == 0:\n        raise ValueError(\"Distribution can't be empty.\")\n    if weights is not None:\n        weights = np.asarray(weights, dtype=float)\n        if len(weights) != len(values):\n            raise ValueError('Value and weight array-likes for the same '\n                             'empirical distribution must be of the same size.')\n        if np.any(weights < 0):\n            raise ValueError('All weights must be non-negative.')", "documentation": "    \"\"\"\n    Validate the values and weights from a distribution input of `cdf_distance`\n    and return them as ndarray objects.\n\n    Parameters\n    ----------\n    values : array_like\n        Values observed in the (empirical) distribution.\n    weights : array_like\n        Weight for each value.\n\n    Returns\n    -------\n    values : ndarray\n        Values as ndarray.\n    weights : ndarray\n        Weights as ndarray.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 9986, "code": "def rankdata(a, method='average', *, axis=None, nan_policy='propagate'):\n    methods = ('average', 'min', 'max', 'dense', 'ordinal')\n    if method not in methods:\n        raise ValueError(f'unknown method \"{method}\"')\n    xp = array_namespace(a)\n    x = xp.asarray(a)\n    if axis is None:\n        x = xp_ravel(x)\n        axis = -1\n    if xp_size(x) == 0:\n        dtype = xp_result_type(x, force_floating=True, xp=xp)", "documentation": "    \"\"\"Assign ranks to data, dealing with ties appropriately.\n\n    By default (``axis=None``), the data array is first flattened, and a flat\n    array of ranks is returned. Separately reshape the rank array to the\n    shape of the data array if desired (see Examples).\n\n    Ranks begin at 1.  The `method` argument controls how ranks are assigned\n    to equal values.  See [1]_ for further discussion of ranking methods.\n\n    Parameters\n    ----------\n    a : array_like\n        The array of values to be ranked.\n    method : {'average', 'min', 'max', 'dense', 'ordinal'}, optional\n        The method used to assign ranks to tied elements.\n        The following methods are available (default is 'average'):\n\n        * 'average': The average of the ranks that would have been assigned to\n          all the tied values is assigned to each value.\n        * 'min': The minimum of the ranks that would have been assigned to all\n          the tied values is assigned to each value.  (This is also\n          referred to as \"competition\" ranking.)\n        * 'max': The maximum of the ranks that would have been assigned to all\n          the tied values is assigned to each value.\n        * 'dense': Like 'min', but the rank of the next highest element is\n          assigned the rank immediately after those assigned to the tied\n          elements.\n        * 'ordinal': All values are given a distinct rank, corresponding to\n          the order that the values occur in `a`.\n\n    axis : {None, int}, optional\n        Axis along which to perform the ranking. If ``None``, the data array\n        is first flattened.\n    nan_policy : {'propagate', 'omit', 'raise'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': propagates nans through the rank calculation\n        * 'omit': performs the calculations ignoring nan values\n        * 'raise': raises an error\n\n        .. note::\n\n            When `nan_policy` is 'propagate', the output is an array of *all*\n            nans because ranks relative to nans in the input are undefined.\n            When `nan_policy` is 'omit', nans in `a` are ignored when ranking\n            the other values, and the corresponding locations of the output\n            are nan.\n\n        .. versionadded:: 1.10\n\n    Returns\n    -------\n    ranks : ndarray\n         An array of size equal to the size of `a`, containing rank\n         scores. The dtype is the result dtype of `a` and a Python float.\n\n    References\n    ----------\n    .. [1] \"Ranking\", https://en.wikipedia.org/wiki/Ranking\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import rankdata\n    >>> rankdata([0, 2, 3, 2])\n    array([1. , 2.5, 4. , 2.5])\n    >>> rankdata([0, 2, 3, 2], method='min')\n    array([1., 2., 4., 2.])\n    >>> rankdata([0, 2, 3, 2], method='max')\n    array([1., 3., 4., 3.])\n    >>> rankdata([0, 2, 3, 2], method='dense')\n    array([1., 2., 3., 2.])\n    >>> rankdata([0, 2, 3, 2], method='ordinal')\n    array([1., 2., 4., 3.])\n    >>> rankdata([[0, 2], [3, 2]]).reshape(2, 2)\n    array([[1. , 2.5],\n           [4. , 2.5]])\n    >>> rankdata([[0, 2, 2], [3, 2, 5]], axis=1)\n    array([[1. , 2.5, 2.5],\n           [2. , 1. , 3. ]])\n    >>> rankdata([0, 2, 3, np.nan, -2, np.nan], nan_policy=\"propagate\")\n    array([nan, nan, nan, nan, nan, nan])\n    >>> rankdata([0, 2, 3, np.nan, -2, np.nan], nan_policy=\"omit\")\n    array([ 2.,  3.,  4., nan,  1., nan])\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 10533, "code": "def linregress(x, y, alternative='two-sided', *, axis=0):\n    xp = array_namespace(x, y)\n    x, y = xp_promote(x, y, force_floating=True, xp=xp)\n    TINY = 1.0e-20\n    n = x.shape[-1]\n    xmean = xp.mean(x, axis=-1, keepdims=True)\n    ymean = xp.mean(y, axis=-1, keepdims=True)\n    x_ = _demean(x, xmean, axis=-1, xp=xp)\n    y_ = _demean(y, ymean, axis=-1, xp=xp, precision_warning=False)\n    xmean = xp.squeeze(xmean, axis=-1)\n    ymean = xp.squeeze(ymean, axis=-1)", "documentation": "    \"\"\"\n    Calculate a linear least-squares regression for two sets of measurements.\n\n    Parameters\n    ----------\n    x, y : array_like\n        Two sets of measurements.  Both arrays should have the same length N.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n        The following options are available:\n\n        * 'two-sided': the slope of the regression line is nonzero\n        * 'less': the slope of the regression line is less than zero\n        * 'greater':  the slope of the regression line is greater than zero\n\n        .. versionadded:: 1.7.0\n    axis : int or None, default: 0\n        If an int, the axis of the input along which to compute the statistic.\n        The statistic of each axis-slice (e.g. row) of the input will appear in a\n        corresponding element of the output.\n        If ``None``, the input will be raveled before computing the statistic.\n\n    Returns\n    -------\n    result : ``LinregressResult`` instance\n        The return value is an object with the following attributes:\n\n        slope : float\n            Slope of the regression line.\n        intercept : float\n            Intercept of the regression line.\n        rvalue : float\n            The Pearson correlation coefficient. The square of ``rvalue``\n            is equal to the coefficient of determination.\n        pvalue : float\n            The p-value for a hypothesis test whose null hypothesis is\n            that the slope is zero, using Wald Test with t-distribution of\n            the test statistic. See `alternative` above for alternative\n            hypotheses.\n        stderr : float\n            Standard error of the estimated slope (gradient), under the\n            assumption of residual normality.\n        intercept_stderr : float\n            Standard error of the estimated intercept, under the assumption\n            of residual normality.\n\n    See Also\n    --------\n    scipy.optimize.curve_fit :\n        Use non-linear least squares to fit a function to data.\n    scipy.optimize.leastsq :\n        Minimize the sum of squares of a set of equations.\n\n    Notes\n    -----\n    For compatibility with older versions of SciPy, the return value acts\n    like a ``namedtuple`` of length 5, with fields ``slope``, ``intercept``,\n    ``rvalue``, ``pvalue`` and ``stderr``, so one can continue to write::\n\n        slope, intercept, r, p, se = linregress(x, y)\n\n    With that style, however, the standard error of the intercept is not\n    available.  To have access to all the computed values, including the\n    standard error of the intercept, use the return value as an object\n    with attributes, e.g.::\n\n        result = linregress(x, y)\n        print(result.intercept, result.intercept_stderr)\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n\n    Generate some data:\n\n    >>> x = rng.random(10)\n    >>> y = 1.6*x + rng.random(10)\n\n    Perform the linear regression:\n\n    >>> res = stats.linregress(x, y)\n\n    Coefficient of determination (R-squared):\n\n    >>> print(f\"R-squared: {res.rvalue**2:.6f}\")\n    R-squared: 0.717533\n\n    Plot the data along with the fitted line:\n\n    >>> plt.plot(x, y, 'o', label='original data')\n    >>> plt.plot(x, res.intercept + res.slope*x, 'r', label='fitted line')\n    >>> plt.legend()\n    >>> plt.show()\n\n    Calculate 95% confidence interval on slope and intercept:\n\n    >>> # Two-sided inverse Students t-distribution\n    >>> # p - probability, df - degrees of freedom\n    >>> from scipy.stats import t\n    >>> tinv = lambda p, df: abs(t.ppf(p/2, df))\n\n    >>> ts = tinv(0.05, len(x)-2)\n    >>> print(f\"slope (95%): {res.slope:.6f} +/- {ts*res.stderr:.6f}\")\n    slope (95%): 1.453392 +/- 0.743465\n    >>> print(f\"intercept (95%): {res.intercept:.6f}\"\n    ...       f\" +/- {ts*res.intercept_stderr:.6f}\")\n    intercept (95%): 0.616950 +/- 0.544475\n\n    \"\"\""}]}
{"repository": "scipy/scipy", "commit_sha": "98307e9d953289d30edb1fad6799a86c46476558", "commit_message": "MAINT: stats.rankdata: consistently return floating point dtype (#24420)\n\n* MAINT: stats.rankdata: always output floating point dtype\n\n* MAINT: stats: avoid unnecessary dtype conversions\n\n* DOC: stats.rankdata: update documentation to reflect new output dtype\n\n* MAINT: stats.ks_2samp: update after change in rankdata dtype\n\n* TST: stats.mood: adjust tolerance to address failing test", "commit_date": "2026-02-03T07:12:12+00:00", "author": "Matt Haberland", "file": "scipy/stats/tests/test_morestats.py", "patch": "@@ -1513,7 +1513,7 @@ def test_mood_with_axis_none(self, xp):\n                                                   ('integers', (8,))])\n     def test_mood_2d(self, rng_method, args, xp):\n         # Test if the results of mood test in 2-D vectorized call are consistent\n-        # result when looping over the slices.\n+        # with result when looping over the slices.\n         ny = 5\n         rng = np.random.default_rng()\n         rng_method = getattr(rng, rng_method)\n@@ -1536,7 +1536,7 @@ def test_mood_2d(self, rng_method, args, xp):\n         for i in range(ny):\n             # check axis handling is self consistent\n             ref = stats.mood(x1[i, :], x2[i, :])\n-            xp_assert_close(res.statistic[i], xp.asarray(ref.statistic))\n+            xp_assert_close(res.statistic[i], xp.asarray(ref.statistic), atol=1e-14)\n             xp_assert_close(res.pvalue[i], xp.asarray(ref.pvalue))\n \n     @pytest.mark.parametrize('rng_method, args', [('standard_normal', tuple()),", "before_segments": [{"filename": "scipy/stats/tests/test_morestats.py", "start_line": 996, "code": "class TestBinomTest:\n    @pytest.mark.parametrize(\"k, n, p, ref, rtol\",\n                             [(10079999, 21000000, 0.48, 1.0, 1e-10),\n                              (10079990, 21000000, 0.48, 0.9966892187965, 1e-10),\n                              (10080009, 21000000, 0.48, 0.9970377203856, 1e-10),\n                              (10080017, 21000000, 0.48, 0.9940754817328, 1e-9)])", "documentation": "    \"\"\"Tests for stats.binomtest.\"\"\""}, {"filename": "scipy/stats/tests/test_morestats.py", "start_line": 1420, "code": "    def test_against_SAS(self, x, y, alternative, stat_expect, p_expect, xp):\n        x, y = xp.asarray(x.tolist()), xp.asarray(y.tolist())\n        statistic, pvalue = stats.mood(x, y, alternative=alternative)\n        xp_assert_close(statistic, xp.asarray(stat_expect), atol=1e-16)\n        xp_assert_close(pvalue, xp.asarray(p_expect), atol=1e-16)\n    @pytest.mark.parametrize(\"dtype\", [None, 'float32', 'float64'])\n    @pytest.mark.parametrize(\"alternative, expected\",\n                             [('two-sided', (1.019938533549930,\n                                             .3077576129778760)),\n                              ('less', (1.019938533549930,\n                                        1 - .1538788064889380)),", "documentation": "        \"\"\"\n        Example code used to generate SAS output:\n        DATA myData;\n        INPUT X Y;\n        CARDS;\n        1 0\n        1 1\n        1 2\n        1 3\n        1 4\n        2 0\n        2 1\n        2 4\n        2 9\n        2 16\n        ods graphics on;\n        proc npar1way mood data=myData ;\n           class X;\n            ods output  MoodTest=mt;\n        proc contents data=mt;\n        proc print data=mt;\n          format     Prob1 17.16 Prob2 17.16 Statistic 17.16 Z 17.16 ;\n            title \"Mood Two-Sample Test\";\n        proc print data=myData;\n            title \"Data for above results\";\n          run;\n        \"\"\""}, {"filename": "scipy/stats/tests/test_morestats.py", "start_line": 1642, "code": "        class custom_dist:", "documentation": "            \"\"\"Some class that looks just enough like a distribution.\"\"\""}, {"filename": "scipy/stats/tests/test_morestats.py", "start_line": 2547, "code": "    def test_negative_x_value_raises_error(self, bad_x):\n        message = \"only positive, finite, real numbers\"\n        with pytest.raises(ValueError, match=message):\n            stats.boxcox_normmax(bad_x)\n    @pytest.mark.parametrize('x', [\n        np.array([2003.0, 1950.0, 1997.0, 2000.0, 2009.0,\n                  2009.0, 1980.0, 1999.0, 2007.0, 1991.0]),\n        np.array([2003.0, 1950.0, 1997.0, 2000.0, 2009.0]),\n        np.array([2003.0e200, 1950.0e200, 1997.0e200, 2000.0e200, 2009.0e200])\n    ])", "documentation": "        \"\"\"Test boxcox_normmax raises ValueError if x contains non-positive values.\"\"\""}], "after_segments": [{"filename": "scipy/stats/tests/test_morestats.py", "start_line": 996, "code": "class TestBinomTest:\n    @pytest.mark.parametrize(\"k, n, p, ref, rtol\",\n                             [(10079999, 21000000, 0.48, 1.0, 1e-10),\n                              (10079990, 21000000, 0.48, 0.9966892187965, 1e-10),\n                              (10080009, 21000000, 0.48, 0.9970377203856, 1e-10),\n                              (10080017, 21000000, 0.48, 0.9940754817328, 1e-9)])", "documentation": "    \"\"\"Tests for stats.binomtest.\"\"\""}, {"filename": "scipy/stats/tests/test_morestats.py", "start_line": 1420, "code": "    def test_against_SAS(self, x, y, alternative, stat_expect, p_expect, xp):\n        x, y = xp.asarray(x.tolist()), xp.asarray(y.tolist())\n        statistic, pvalue = stats.mood(x, y, alternative=alternative)\n        xp_assert_close(statistic, xp.asarray(stat_expect), atol=1e-16)\n        xp_assert_close(pvalue, xp.asarray(p_expect), atol=1e-16)\n    @pytest.mark.parametrize(\"dtype\", [None, 'float32', 'float64'])\n    @pytest.mark.parametrize(\"alternative, expected\",\n                             [('two-sided', (1.019938533549930,\n                                             .3077576129778760)),\n                              ('less', (1.019938533549930,\n                                        1 - .1538788064889380)),", "documentation": "        \"\"\"\n        Example code used to generate SAS output:\n        DATA myData;\n        INPUT X Y;\n        CARDS;\n        1 0\n        1 1\n        1 2\n        1 3\n        1 4\n        2 0\n        2 1\n        2 4\n        2 9\n        2 16\n        ods graphics on;\n        proc npar1way mood data=myData ;\n           class X;\n            ods output  MoodTest=mt;\n        proc contents data=mt;\n        proc print data=mt;\n          format     Prob1 17.16 Prob2 17.16 Statistic 17.16 Z 17.16 ;\n            title \"Mood Two-Sample Test\";\n        proc print data=myData;\n            title \"Data for above results\";\n          run;\n        \"\"\""}, {"filename": "scipy/stats/tests/test_morestats.py", "start_line": 1642, "code": "        class custom_dist:", "documentation": "            \"\"\"Some class that looks just enough like a distribution.\"\"\""}, {"filename": "scipy/stats/tests/test_morestats.py", "start_line": 2547, "code": "    def test_negative_x_value_raises_error(self, bad_x):\n        message = \"only positive, finite, real numbers\"\n        with pytest.raises(ValueError, match=message):\n            stats.boxcox_normmax(bad_x)\n    @pytest.mark.parametrize('x', [\n        np.array([2003.0, 1950.0, 1997.0, 2000.0, 2009.0,\n                  2009.0, 1980.0, 1999.0, 2007.0, 1991.0]),\n        np.array([2003.0, 1950.0, 1997.0, 2000.0, 2009.0]),\n        np.array([2003.0e200, 1950.0e200, 1997.0e200, 2000.0e200, 2009.0e200])\n    ])", "documentation": "        \"\"\"Test boxcox_normmax raises ValueError if x contains non-positive values.\"\"\""}]}
{"repository": "scipy/scipy", "commit_sha": "98307e9d953289d30edb1fad6799a86c46476558", "commit_message": "MAINT: stats.rankdata: consistently return floating point dtype (#24420)\n\n* MAINT: stats.rankdata: always output floating point dtype\n\n* MAINT: stats: avoid unnecessary dtype conversions\n\n* DOC: stats.rankdata: update documentation to reflect new output dtype\n\n* MAINT: stats.ks_2samp: update after change in rankdata dtype\n\n* TST: stats.mood: adjust tolerance to address failing test", "commit_date": "2026-02-03T07:12:12+00:00", "author": "Matt Haberland", "file": "scipy/stats/tests/test_rank.py", "patch": "@@ -5,7 +5,8 @@\n from scipy import stats\n from scipy.conftest import skip_xp_invalid_arg\n from scipy.stats import rankdata, tiecorrect\n-from scipy._lib._array_api import xp_assert_equal, make_xp_test_case\n+from scipy._lib._array_api import (xp_assert_equal, make_xp_test_case, xp_result_type,\n+                                   xp_default_dtype)\n \n skip_xp_backends = pytest.mark.skip_xp_backends\n \n@@ -78,16 +79,14 @@ def test_overflow(self):\n @make_xp_test_case(stats.rankdata)\n class TestRankData:\n \n-    def desired_dtype(self, method='average', has_nans=False, *, xp):\n-        if has_nans:\n-            return xp.asarray(1.).dtype\n-        return xp.asarray(1.).dtype if method=='average' else xp.asarray(1).dtype\n+    def desired_dtype(self, x, *, xp):\n+        return xp_result_type(x, force_floating=True, xp=xp)\n \n     def test_empty(self, xp):\n         \"\"\"stats.rankdata of empty array should return an empty array.\"\"\"\n         a = xp.asarray([], dtype=xp.int64)\n         r = rankdata(a)\n-        xp_assert_equal(r, xp.asarray([], dtype=self.desired_dtype(xp=xp)))\n+        xp_assert_equal(r, xp.asarray([], dtype=self.desired_dtype(a, xp=xp)))\n \n     def test_list(self):\n         # test that NumPy still accepts lists\n@@ -103,18 +102,18 @@ def test_empty_multidim(self, shape, axis, xp):\n         a = xp.empty(shape, dtype=xp.int64)\n         r = rankdata(a, axis=axis)\n         expected_shape = (0,) if axis is None else shape\n-        xp_assert_equal(r, xp.empty(expected_shape, dtype=self.desired_dtype(xp=xp)))\n+        xp_assert_equal(r, xp.empty(expected_shape, dtype=self.desired_dtype(a, xp=xp)))\n \n     def test_one(self, xp):\n         \"\"\"Check stats.rankdata with an array of length 1.\"\"\"\n         data = [100]\n         a = xp.asarray(data, dtype=xp.int64)\n         r = rankdata(a)\n-        xp_assert_equal(r, xp.asarray([1.0], dtype=self.desired_dtype(xp=xp)))\n+        xp_assert_equal(r, xp.asarray([1.0], dtype=self.desired_dtype(a, xp=xp)))\n \n     def test_basic(self, xp):\n         \"\"\"Basic tests of stats.rankdata.\"\"\"\n-        desired_dtype = self.desired_dtype(xp=xp)\n+        desired_dtype = xp_default_dtype(xp)\n \n         data = [100, 10, 50]\n         expected = xp.asarray([3.0, 1.0, 2.0], dtype=desired_dtype)\n@@ -177,23 +176,25 @@ def check_ranks(a):\n     def test_large_uint(self, xp):\n         data = xp.asarray([2**60, 2**60+1], dtype=xp.uint64)\n         r = rankdata(data)\n-        xp_assert_equal(r, xp.asarray([1.0, 2.0], dtype=self.desired_dtype(xp=xp)))\n+        desired_dtype = self.desired_dtype(data, xp=xp)\n+        xp_assert_equal(r, xp.asarray([1.0, 2.0], dtype=desired_dtype))\n \n     def test_large_int(self, xp):\n         data = xp.asarray([2**60, 2**60+1], dtype=xp.int64)\n         r = rankdata(data)\n-        xp_assert_equal(r, xp.asarray([1.0, 2.0], dtype=self.desired_dtype(xp=xp)))\n+        desired_dtype = self.desired_dtype(data, xp=xp)\n+        xp_assert_equal(r, xp.asarray([1.0, 2.0], dtype=desired_dtype))\n \n         data = xp.asarray([2**60, -2**60+1], dtype=xp.int64)\n         r = rankdata(data)\n-        xp_assert_equal(r, xp.asarray([2.0, 1.0], dtype=self.desired_dtype(xp=xp)))\n+        xp_assert_equal(r, xp.asarray([2.0, 1.0], dtype=desired_dtype))\n \n     @pytest.mark.parametrize('n', [10000, 100000, 1000000])\n     def test_big_tie(self, n, xp):\n         data = xp.ones(n)\n         r = rankdata(data)\n         expected_rank = 0.5 * (n + 1)\n-        ref = xp.asarray(expected_rank * data, dtype=self.desired_dtype(xp=xp))\n+        ref = xp.asarray(expected_rank * data, dtype=self.desired_dtype(data, xp=xp))\n         xp_assert_equal(r, ref)\n \n     def test_axis(self, xp):\n@@ -213,8 +214,8 @@ def test_axis(self, xp):\n     @pytest.mark.parametrize(\"method\", methods)\n     def test_size_0_axis(self, axis, method, xp):\n         shape = (3, 0)\n-        desired_dtype = self.desired_dtype(method, xp=xp)\n         data = xp.zeros(shape)\n+        desired_dtype = self.desired_dtype(data, xp=xp)\n         r = rankdata(data, method=method, axis=axis)\n         assert_equal(r.shape, shape)\n         assert_equal(r.dtype, desired_dtype)\n@@ -343,6 +344,7 @@ def test_nan_policy_propagate(self):\n     @pytest.mark.parametrize('case', _rankdata_cases)\n     def test_cases(self, case, xp):\n         values, method, expected = case\n+        values = xp.asarray(values)\n         r = rankdata(xp.asarray(values), method=method)\n-        ref = xp.asarray(expected, dtype=self.desired_dtype(method, xp=xp))\n+        ref = xp.asarray(expected, dtype=self.desired_dtype(values, xp=xp))\n         xp_assert_equal(r, ref)", "before_segments": [{"filename": "scipy/stats/tests/test_rank.py", "start_line": 13, "code": "    def test_empty(self):\n        ranks = np.array([], dtype=np.float64)\n        c = tiecorrect(ranks)\n        assert_equal(c, 1.0)", "documentation": "        \"\"\"An empty array requires no correction, should return 1.0.\"\"\""}, {"filename": "scipy/stats/tests/test_rank.py", "start_line": 19, "code": "    def test_one(self):\n        ranks = np.array([1.0], dtype=np.float64)\n        c = tiecorrect(ranks)\n        assert_equal(c, 1.0)", "documentation": "        \"\"\"A single element requires no correction, should return 1.0.\"\"\""}, {"filename": "scipy/stats/tests/test_rank.py", "start_line": 25, "code": "    def test_no_correction(self):\n        ranks = np.arange(2.0)\n        c = tiecorrect(ranks)\n        assert_equal(c, 1.0)\n        ranks = np.arange(3.0)\n        c = tiecorrect(ranks)\n        assert_equal(c, 1.0)", "documentation": "        \"\"\"Arrays with no ties require no correction.\"\"\""}, {"filename": "scipy/stats/tests/test_rank.py", "start_line": 34, "code": "    def test_basic(self):\n        ranks = np.array([1.0, 2.5, 2.5])\n        c = tiecorrect(ranks)\n        T = 2.0\n        N = ranks.size\n        expected = 1.0 - (T**3 - T) / (N**3 - N)\n        assert_equal(c, expected)\n        ranks = np.array([1.5, 1.5, 3.0])\n        c = tiecorrect(ranks)\n        T = 2.0\n        N = ranks.size", "documentation": "        \"\"\"Check a few basic examples of the tie correction factor.\"\"\""}, {"filename": "scipy/stats/tests/test_rank.py", "start_line": 85, "code": "    def test_empty(self, xp):\n        a = xp.asarray([], dtype=xp.int64)\n        r = rankdata(a)\n        xp_assert_equal(r, xp.asarray([], dtype=self.desired_dtype(xp=xp)))", "documentation": "        \"\"\"stats.rankdata of empty array should return an empty array.\"\"\""}, {"filename": "scipy/stats/tests/test_rank.py", "start_line": 107, "code": "    def test_one(self, xp):\n        data = [100]\n        a = xp.asarray(data, dtype=xp.int64)\n        r = rankdata(a)\n        xp_assert_equal(r, xp.asarray([1.0], dtype=self.desired_dtype(xp=xp)))", "documentation": "        \"\"\"Check stats.rankdata with an array of length 1.\"\"\""}, {"filename": "scipy/stats/tests/test_rank.py", "start_line": 114, "code": "    def test_basic(self, xp):\n        desired_dtype = self.desired_dtype(xp=xp)\n        data = [100, 10, 50]\n        expected = xp.asarray([3.0, 1.0, 2.0], dtype=desired_dtype)\n        a = xp.asarray(data, dtype=xp.int64)\n        r = rankdata(a)\n        xp_assert_equal(r, expected)\n        data = [40, 10, 30, 10, 50]\n        expected = xp.asarray([4.0, 1.5, 3.0, 1.5, 5.0], dtype=desired_dtype)\n        a = xp.asarray(data, dtype=xp.int64)\n        r = rankdata(a)", "documentation": "        \"\"\"Basic tests of stats.rankdata.\"\"\""}], "after_segments": [{"filename": "scipy/stats/tests/test_rank.py", "start_line": 14, "code": "    def test_empty(self):\n        ranks = np.array([], dtype=np.float64)\n        c = tiecorrect(ranks)\n        assert_equal(c, 1.0)", "documentation": "        \"\"\"An empty array requires no correction, should return 1.0.\"\"\""}, {"filename": "scipy/stats/tests/test_rank.py", "start_line": 20, "code": "    def test_one(self):\n        ranks = np.array([1.0], dtype=np.float64)\n        c = tiecorrect(ranks)\n        assert_equal(c, 1.0)", "documentation": "        \"\"\"A single element requires no correction, should return 1.0.\"\"\""}, {"filename": "scipy/stats/tests/test_rank.py", "start_line": 26, "code": "    def test_no_correction(self):\n        ranks = np.arange(2.0)\n        c = tiecorrect(ranks)\n        assert_equal(c, 1.0)\n        ranks = np.arange(3.0)\n        c = tiecorrect(ranks)\n        assert_equal(c, 1.0)", "documentation": "        \"\"\"Arrays with no ties require no correction.\"\"\""}, {"filename": "scipy/stats/tests/test_rank.py", "start_line": 35, "code": "    def test_basic(self):\n        ranks = np.array([1.0, 2.5, 2.5])\n        c = tiecorrect(ranks)\n        T = 2.0\n        N = ranks.size\n        expected = 1.0 - (T**3 - T) / (N**3 - N)\n        assert_equal(c, expected)\n        ranks = np.array([1.5, 1.5, 3.0])\n        c = tiecorrect(ranks)\n        T = 2.0\n        N = ranks.size", "documentation": "        \"\"\"Check a few basic examples of the tie correction factor.\"\"\""}, {"filename": "scipy/stats/tests/test_rank.py", "start_line": 84, "code": "    def test_empty(self, xp):\n        a = xp.asarray([], dtype=xp.int64)\n        r = rankdata(a)\n        xp_assert_equal(r, xp.asarray([], dtype=self.desired_dtype(a, xp=xp)))", "documentation": "        \"\"\"stats.rankdata of empty array should return an empty array.\"\"\""}, {"filename": "scipy/stats/tests/test_rank.py", "start_line": 106, "code": "    def test_one(self, xp):\n        data = [100]\n        a = xp.asarray(data, dtype=xp.int64)\n        r = rankdata(a)\n        xp_assert_equal(r, xp.asarray([1.0], dtype=self.desired_dtype(a, xp=xp)))", "documentation": "        \"\"\"Check stats.rankdata with an array of length 1.\"\"\""}, {"filename": "scipy/stats/tests/test_rank.py", "start_line": 113, "code": "    def test_basic(self, xp):\n        desired_dtype = xp_default_dtype(xp)\n        data = [100, 10, 50]\n        expected = xp.asarray([3.0, 1.0, 2.0], dtype=desired_dtype)\n        a = xp.asarray(data, dtype=xp.int64)\n        r = rankdata(a)\n        xp_assert_equal(r, expected)\n        data = [40, 10, 30, 10, 50]\n        expected = xp.asarray([4.0, 1.5, 3.0, 1.5, 5.0], dtype=desired_dtype)\n        a = xp.asarray(data, dtype=xp.int64)\n        r = rankdata(a)", "documentation": "        \"\"\"Basic tests of stats.rankdata.\"\"\""}]}
{"repository": "scipy/scipy", "commit_sha": "3093d3ead614a538612285cfff265db430a8e07a", "commit_message": "ENH: stats.ContinuousDistribution.lmoment: add population L-moments (#23766)\n\n* ENH: stats.ContinuousDistribution: add l-moments\n\n* ENH: stats.ContinuousDistribution: add l-moments from integral over icdf\n\n* MAINT: stats.ContinuousDistribution: simplify l-moment ratio implementation\n\n* ENH: stats.ContinuousDistribution: draft lmoment for custom and shifted/scaled distributions\n\n* DOC: stats.ContinuousDistribution.lmoment: draft documentation\n\n* TST: stats.ContinuousDistribution.lmoment: add tests\n\n* ENH: stats.ContinuousDistribution.lmoment: override for built-in distributions\n\n* MAINT: stats.ContinuousDistribution.lmoment: fixups\n\n* DOC: stats.lmoment: update docs per new default standardize=True\"", "commit_date": "2026-01-31T09:21:12+00:00", "author": "Matt Haberland", "file": "scipy/stats/_distribution_infrastructure.py", "patch": "@@ -11,6 +11,7 @@\n from scipy._lib._array_api import xp_capabilities, xp_promote\n from scipy._lib._util import _rng_spawn, _RichResult\n from scipy._lib._docscrape import ClassDoc, NumpyDocString\n+from scipy._lib import array_api_extra as xpx\n from scipy import special, stats\n from scipy.special._ufuncs import _log1mexp\n from scipy.integrate import tanhsinh as _tanhsinh, nsum\n@@ -1547,6 +1548,7 @@ class UnivariateDistribution(_ProbabilityDistribution):\n     sample\n \n     moment\n+    lmoment\n \n     mean\n     median\n@@ -1714,6 +1716,7 @@ def reset_cache(self):\n         self._moment_raw_cache = {}\n         self._moment_central_cache = {}\n         self._moment_standardized_cache = {}\n+        self._lmoment_cache = {}\n         self._support_cache = None\n         self._method_cache = {}\n         self._constant_cache = None\n@@ -2012,7 +2015,7 @@ def __abs__(self):\n \n     ## Input validation\n \n-    def _validate_order_kind(self, order, kind, kinds):\n+    def _validate_order(self, order, fname='moment', min_order=0):\n         # Yet another integer validating function. Unlike others in SciPy, it\n         # Is quite flexible about what is allowed as an integer, and it\n         # raises a distribution-specific error message to facilitate\n@@ -2021,24 +2024,27 @@ def _validate_order_kind(self, order, kind, kinds):\n             return order\n \n         order = np.asarray(order, dtype=self._dtype)[()]\n-        message = (f\"Argument `order` of `{self.__class__.__name__}.moment` \"\n-                   \"must be a finite, positive integer.\")\n+        message = (f\"Argument `order` of `{self.__class__.__name__}.{fname}` \"\n+                   f\"must be a finite integer greater than or equal to {min_order}.\")\n         try:\n             order_int = round(order.item())\n             # If this fails for any reason (e.g. it's an array, it's infinite)\n             # it's not a valid `order`.\n         except Exception as e:\n             raise ValueError(message) from e\n \n-        if order_int <0 or order_int != order:\n+        if order_int < min_order or order_int != order:\n             raise ValueError(message)\n \n+        return order\n+\n+    def _validate_kind(self, kind, kinds):\n         message = (f\"Argument `kind` of `{self.__class__.__name__}.moment` \"\n                    f\"must be one of {set(kinds)}.\")\n-        if kind.lower() not in kinds:\n+        kind = kind.lower()\n+        if kind not in kinds:\n             raise ValueError(message)\n-\n-        return order\n+        return kind\n \n     def _preserve_type(self, x):\n         x = np.asarray(x)\n@@ -3122,6 +3128,10 @@ def _moment_methods(self):\n         return {'cache', 'formula', 'transform',\n                 'normalize', 'general', 'quadrature'}\n \n+    @cached_property\n+    def _lmoment_methods(self):\n+        return {'cache', 'formula', 'general', 'order_statistics', 'quadrature_icdf'}\n+\n     @property\n     def _zero(self):\n         return self._constants()[0]\n@@ -3146,7 +3156,8 @@ def moment(self, order=1, kind='raw', *, method=None):\n         kinds = {'raw': self._moment_raw,\n                  'central': self._moment_central,\n                  'standardized': self._moment_standardized}\n-        order = self._validate_order_kind(order, kind, kinds)\n+        order = self._validate_order(order)\n+        kind = self._validate_kind(kind, kinds)\n         moment_kind = kinds[kind]\n         return moment_kind(order, method=method)\n \n@@ -3381,6 +3392,75 @@ def logintegrand(x, order, logcenter, **params):\n         return self._quadrature(logintegrand, args=(order, logcenter),\n                                 params=params, log=True)\n \n+    ### L-Moments\n+\n+    @_set_invalid_nan_property\n+    def lmoment(self, order=1, *, standardize=True, method=None):\n+        order = self._validate_order(order, fname='lmoment', min_order=1)\n+        return self._lmoment(order, standardize=standardize, method=method)\n+\n+    def _lmoment(self, order, *, standardize, method):\n+        methods = self._lmoment_methods if method is None else {method}\n+\n+        lmoment = self._lmoment_dispatch(order, methods=methods, **self._parameters)\n+        if lmoment is None:\n+            return None\n+\n+        if standardize and order >= 3 and lmoment is not None:\n+            lscale = self._lmoment_dispatch(2, methods=methods, **self._parameters)\n+            if lscale is None:\n+                return None\n+            lmoment = lmoment / lscale\n+\n+        return lmoment\n+\n+    def _lmoment_dispatch(self, order, *, methods, **params):\n+        lmoment = None\n+\n+        if 'cache' in methods:\n+            lmoment = self._lmoment_cache.get(order, None)\n+\n+        if lmoment is None and 'formula' in methods:\n+            lmoment = self._lmoment_formula(order, **params)\n+\n+        if lmoment is None and 'general' in methods:\n+            lmoment = self._lmoment_general(order, **params)\n+\n+        if lmoment is None and 'quadrature_icdf' in methods and (\n+                self._overrides('_icdf_formula') or self._overrides('_iccdf_formula')):\n+            lmoment = self._lmoment_integrate_icdf(order, **params)\n+\n+        if lmoment is None and 'order_statistics' in methods:\n+            lmoment = self._lmoment_from_order_statistics(order, **params)\n+\n+        if lmoment is None and 'quadrature_icdf' in methods:\n+            lmoment = self._lmoment_integrate_icdf(order, **params)\n+\n+        if lmoment is not None and self.cache_policy != _NO_CACHE:\n+            self._lmoment_cache[order] = lmoment\n+\n+        return lmoment\n+\n+    def _lmoment_formula(self, order, **params):\n+        return None\n+\n+    def _lmoment_general(self, order, **params):\n+        return self.mean() if order == 1 else None\n+\n+    def _lmoment_from_order_statistics(self, order, **params):\n+        k = np.arange(order)\n+        k = xpx.atleast_nd(k, ndim=self._ndim + 1).T\n+        E = order_statistic(self, r=order-k, n=order).mean()\n+        bc = special.binom(order-1, k)\n+        return np.sum((-1)**k * bc * E, axis=0) / order\n+\n+    def _lmoment_integrate_icdf(self, order, **params):\n+        def integrand(p, **params):\n+            x = self._icdf_dispatch(p, **params)\n+            P = special.eval_sh_legendre(order - 1, p)\n+            return x * P\n+        return self._quadrature(integrand, limits=(0., 1.), params=params)\n+\n     ### Convenience\n \n     def plot(self, x='x', y=None, *, t=None, ax=None):\n@@ -3686,6 +3766,11 @@ def _logccdf2(self, x, y, *, method):\n             \"Two argument cdf functions are currently only supported for \"\n             \"continuous distributions.\")\n \n+    def _lmoment(self, order, *, standardize, method):\n+        raise NotImplementedError(\n+            \"L-moments are currently available only \"\n+            \"for continuous distributions.\")\n+\n     def _solve_bounded_discrete(self, func, p, params, comp):\n         res = self._solve_bounded(func, p, params=params, xatol=0.9)\n         x = np.asarray(np.floor(res.xr))\n@@ -3944,15 +4029,18 @@ def make_distribution(dist):\n         ``logentropy``, ``entropy``, ``median``, ``mode``, ``logpdf``,\n         ``logcdf``, ``cdf``, ``logccdf``, ``ccdf``,\n         ``ilogcdf``, ``icdf``, ``ilogccdf``, ``iccdf``,\n-        ``moment``, and ``sample``.\n+        ``moment``, ``lmoment``, and ``sample``.\n         If defined, these methods must accept the parameters of the distribution as\n         keyword arguments and also accept any positional-only arguments accepted by\n         the corresponding method of `ContinuousDistribution`.\n         When multiple parameterizations are defined, these methods must accept\n         all parameters from all parameterizations. The ``moment`` method\n         must accept the ``order`` and ``kind`` arguments by position or keyword, but\n         may return ``None`` if a formula is not available for the arguments; in this\n-        case, the infrastructure will fall back to a default implementation. The\n+        case, the infrastructure will fall back to a default implementation.\n+        The ``lmoment`` method must accept the ``order`` argument by position or\n+        keyword and return the specified L-moment (not the L-moment ratio), but may\n+        return ``None`` if a formula is not available for the arguments. The\n         ``sample`` method must accept ``shape`` by position or keyword, but contrary\n         to the public method of the same name, the argument it receives will be the\n         *full* shape of the output array - that is, the shape passed to the public\n@@ -4286,7 +4374,8 @@ def __str__(self):\n                'median', 'mode', 'logpdf', 'pdf',\n                'logcdf2', 'logcdf', 'cdf2', 'cdf',\n                'logccdf2', 'logccdf', 'ccdf2', 'ccdf',\n-               'ilogcdf', 'icdf', 'ilogccdf', 'iccdf'}\n+               'ilogcdf', 'icdf', 'ilogccdf', 'iccdf',\n+               'lmoment'}\n \n     for method in methods:\n         if hasattr(dist, method):\n@@ -4795,6 +4884,13 @@ def _moment_raw_dispatch(self, order, *, loc, scale, sign, methods,\n         return self._moment_transform_center(\n             order, raw_moments, loc, self._zero)\n \n+    def _lmoment_dispatch(self, order, *, loc, scale, sign, methods, **params):\n+        res = self._dist._lmoment_dispatch(order, methods=methods, **params)\n+        if res is None:  # if a specific method is requested but not available\n+            return None\n+        res = res * np.abs(scale) * np.sign(scale)**order\n+        return res + loc if order == 1 else res\n+\n     def _sample_dispatch(self, full_shape, *,\n                          rng, loc, scale, sign, method, **params):\n         rvs = self._dist._sample_dispatch(full_shape, method=method, rng=rng, **params)\n@@ -5276,7 +5372,8 @@ def moment(self, order=1, kind='raw', *, method=None):\n         kinds = {'raw': self._moment_raw,\n                  'central': self._moment_central,\n                  'standardized': self._moment_standardized}\n-        order = ContinuousDistribution._validate_order_kind(self, order, kind, kinds)\n+        order = ContinuousDistribution._validate_order(self, order)\n+        kind = ContinuousDistribution._validate_kind(self, kind, kinds)\n         moment_kind = kinds[kind]\n         return moment_kind(order)\n \n@@ -5297,6 +5394,10 @@ def _moment_central(self, order):\n             out += moment * weight\n         return out[()]\n \n+    def lmoment(self, order=1, *, standardize=False, method=None):\n+        message = \"L-moments are not currently available for mixture distributions.\"\n+        raise NotImplementedError(message)\n+\n     def _moment_standardized(self, order):\n         return self._moment_central(order) / self.standard_deviation()**order\n ", "before_segments": [{"filename": "scipy/stats/_distribution_infrastructure.py", "start_line": 1264, "code": "def _logexpxmexpy(x, y):\n    i = np.isneginf(np.real(y))\n    if np.any(i):\n        y = np.asarray(y.copy())\n        y[i] = np.finfo(y.dtype).min\n    x, y = np.broadcast_arrays(x, y)\n    res = np.asarray(special.logsumexp([x, y+np.pi*1j], axis=0))\n    i = (x == y)\n    res[i] = -np.inf\n    return res", "documentation": "    \"\"\" Compute the log of the difference of the exponentials of two arguments.\n\n    Avoids over/underflow, but does not prevent loss of precision otherwise.\n    \"\"\""}, {"filename": "scipy/stats/_distribution_infrastructure.py", "start_line": 1302, "code": "def _log_real_standardize(x):\n    shape = x.shape\n    x = np.atleast_1d(x)\n    real = np.real(x).astype(x.dtype)\n    complex = np.imag(x)\n    y = real\n    negative = np.exp(complex*1j) < 0.5\n    y[negative] = y[negative] + np.pi * 1j\n    return y.reshape(shape)[()]", "documentation": "    \"\"\"Standardizes the (complex) logarithm of a real number.\n\n    The logarithm of a real number may be represented by a complex number with\n    imaginary part that is a multiple of pi*1j. Even multiples correspond with\n    a positive real and odd multiples correspond with a negative real.\n\n    Given a logarithm of a real number `x`, this function returns an equivalent\n    representation in a standard form: the log of a positive real has imaginary\n    part `0` and the log of a negative real has imaginary part `pi`.\n\n    \"\"\""}, {"filename": "scipy/stats/_distribution_infrastructure.py", "start_line": 3152, "code": "    def _moment_raw(self, order=1, *, method=None):\n        methods = self._moment_methods if method is None else {method}\n        return self._moment_raw_dispatch(order, methods=methods, **self._parameters)", "documentation": "        \"\"\"Raw distribution moment about the origin.\"\"\""}, {"filename": "scipy/stats/_distribution_infrastructure.py", "start_line": 3213, "code": "    def _moment_central(self, order=1, *, method=None):\n        methods = self._moment_methods if method is None else {method}\n        return self._moment_central_dispatch(order, methods=methods, **self._parameters)", "documentation": "        \"\"\"Distribution moment about the mean.\"\"\""}, {"filename": "scipy/stats/_distribution_infrastructure.py", "start_line": 3283, "code": "    def _moment_standardized(self, order=1, *, method=None):\n        methods = self._moment_methods if method is None else {method}\n        return self._moment_standardized_dispatch(order, methods=methods,\n                                                  **self._parameters)", "documentation": "        \"\"\"Standardized distribution moment.\"\"\""}, {"filename": "scipy/stats/_distribution_infrastructure.py", "start_line": 3868, "code": "def make_distribution(dist):\n    if dist in {stats.levy_stable, stats.vonmises, stats.hypergeom,\n                stats.nchypergeom_fisher, stats.nchypergeom_wallenius,\n                stats.poisson_binom}:\n        raise NotImplementedError(f\"`{dist.name}` is not supported.\")\n    if isinstance(dist, stats.rv_continuous | stats.rv_discrete):\n        return _make_distribution_rv_generic(dist)\n    elif getattr(dist, \"__make_distribution_version__\", \"0.0.0\") >= \"1.16.0\":\n        return _make_distribution_custom(dist)\n    else:\n        message = (\"The argument must be an instance of `rv_continuous`, \"", "documentation": "    \"\"\"Generate a `UnivariateDistribution` class from a compatible object\n\n    The argument may be an instance of `rv_continuous` or an instance of\n    another class that satisfies the interface described below.\n\n    The returned value is a `ContinuousDistribution` subclass if the input is an\n    instance of `rv_continuous` or a `DiscreteDistribution` subclass if the input\n    is an instance of `rv_discrete`. Like any subclass of `UnivariateDistribution`,\n    it must be instantiated (i.e. by passing all shape parameters as keyword\n    arguments) before use. Once instantiated, the resulting object will have the\n    same interface as any other instance of `UnivariateDistribution`; e.g.,\n    `scipy.stats.Normal`, `scipy.stats.Binomial`.\n\n    .. note::\n\n        `make_distribution` does not work perfectly with all instances of\n        `rv_continuous`. Known failures include `levy_stable`, `vonmises`,\n        `hypergeom`, 'nchypergeom_fisher', 'nchypergeom_wallenius', and\n        `poisson_binom`. Some methods of some distributions will not support\n        array shape parameters.\n\n    Parameters\n    ----------\n    dist : `rv_continuous`\n        Instance of `rv_continuous`, `rv_discrete`, or an instance of any class with\n        the following attributes:\n\n        __make_distribution_version__ : str\n            A string containing the version number of SciPy in which this interface\n            is defined. The preferred interface may change in future SciPy versions,\n            in which case support for an old interface version may be deprecated\n            and eventually removed.\n        parameters : dict or tuple\n            If a dictionary, each key is the name of a parameter,\n            and the corresponding value is either a dictionary or tuple.\n            If the value is a dictionary, it may have the following items, with default\n            values used for entries which aren't present.\n\n            endpoints : tuple, default: (-inf, inf)\n                A tuple defining the lower and upper endpoints of the domain of the\n                parameter; allowable values are floats, the name (string) of another\n                parameter, or a callable taking parameters as keyword only\n                arguments and returning the numerical value of an endpoint for\n                given parameter values.\n\n            inclusive : tuple of bool, default: (False, False)\n                A tuple specifying whether the endpoints are included within the domain\n                of the parameter.\n\n            typical : tuple, default: ``endpoints``\n                Defining endpoints of a typical range of values of a parameter. Can be\n                used for sampling parameter values for testing. Behaves like the\n                ``endpoints`` tuple above, and should define a subinterval of the\n                domain given by ``endpoints``.\n\n            A tuple value ``(a, b)`` associated to a key in the ``parameters``\n            dictionary is equivalent to ``{endpoints: (a, b)}``.\n\n            Custom distributions with multiple parameterizations can be defined by\n            having the ``parameters`` attribute be a tuple of dictionaries with\n            the structure described above. In this case, ``dist``\\'s class must also\n            define a method ``process_parameters`` to map between the different\n            parameterizations. It must take all parameters from all parameterizations\n            as optional keyword arguments and return a dictionary mapping parameters to\n            values, filling in values from other parameterizations using values from\n            the supplied parameterization. See example.\n\n        support : dict or tuple\n            A dictionary describing the support of the distribution or a tuple\n            describing the endpoints of the support. This behaves identically to\n            the values of the parameters dict described above, except that the key\n            ``typical`` is ignored.\n\n        The class **must** also define a ``pdf`` method and **may** define methods\n        ``logentropy``, ``entropy``, ``median``, ``mode``, ``logpdf``,\n        ``logcdf``, ``cdf``, ``logccdf``, ``ccdf``,\n        ``ilogcdf``, ``icdf``, ``ilogccdf``, ``iccdf``,\n        ``moment``, and ``sample``.\n        If defined, these methods must accept the parameters of the distribution as\n        keyword arguments and also accept any positional-only arguments accepted by\n        the corresponding method of `ContinuousDistribution`.\n        When multiple parameterizations are defined, these methods must accept\n        all parameters from all parameterizations. The ``moment`` method\n        must accept the ``order`` and ``kind`` arguments by position or keyword, but\n        may return ``None`` if a formula is not available for the arguments; in this\n        case, the infrastructure will fall back to a default implementation. The\n        ``sample`` method must accept ``shape`` by position or keyword, but contrary\n        to the public method of the same name, the argument it receives will be the\n        *full* shape of the output array - that is, the shape passed to the public\n        method prepended to the broadcasted shape of random variable parameters.\n\n    Returns\n    -------\n    CustomDistribution : `UnivariateDistribution`\n        A subclass of `UnivariateDistribution` corresponding with `dist`. The\n        initializer requires all shape parameters to be passed as keyword arguments\n        (using the same names as the instance of `rv_continuous`/`rv_discrete`).\n\n    Notes\n    -----\n    The documentation of `UnivariateDistribution` is not rendered. See below for\n    an example of how to instantiate the class (i.e. pass all shape parameters of\n    `dist` to the initializer as keyword arguments). Documentation of all methods\n    is identical to that of `scipy.stats.Normal`. Use ``help`` on the returned\n    class or its methods for more information.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy import stats\n    >>> from scipy import special\n\n    Create a `ContinuousDistribution` from `scipy.stats.loguniform`.\n\n    >>> LogUniform = stats.make_distribution(stats.loguniform)\n    >>> X = LogUniform(a=1.0, b=3.0)\n    >>> np.isclose((X + 0.25).median(), stats.loguniform.ppf(0.5, 1, 3, loc=0.25))\n    np.True_\n    >>> X.plot()\n    >>> sample = X.sample(10000, rng=np.random.default_rng())\n    >>> plt.hist(sample, density=True, bins=30)\n    >>> plt.legend(('pdf', 'histogram'))\n    >>> plt.show()\n\n    Create a custom distribution.\n\n    >>> class MyLogUniform:\n    ...     @property\n    ...     def __make_distribution_version__(self):\n    ...         return \"1.16.0\"\n    ...\n    ...     @property\n    ...     def parameters(self):\n    ...         return {'a': {'endpoints': (0, np.inf),\n    ...                       'inclusive': (False, False)},\n    ...                 'b': {'endpoints': ('a', np.inf),\n    ...                       'inclusive': (False, False)}}\n    ...\n    ...     @property\n    ...     def support(self):\n    ...         return {'endpoints': ('a', 'b'), 'inclusive': (True, True)}\n    ...\n    ...     def pdf(self, x, a, b):\n    ...         return 1 / (x * (np.log(b)- np.log(a)))\n    >>>\n    >>> MyLogUniform = stats.make_distribution(MyLogUniform())\n    >>> Y = MyLogUniform(a=1.0, b=3.0)\n    >>> np.isclose(Y.cdf(2.), X.cdf(2.))\n    np.True_\n\n    Create a custom distribution with variable support.\n\n    >>> class MyUniformCube:\n    ...     @property\n    ...     def __make_distribution_version__(self):\n    ...         return \"1.16.0\"\n    ...\n    ...     @property\n    ...     def parameters(self):\n    ...         return {\"a\": (-np.inf, np.inf),\n    ...                 \"b\": {'endpoints':('a', np.inf), 'inclusive':(True, False)}}\n    ...\n    ...     @property\n    ...     def support(self):\n    ...         def left(*, a, b):\n    ...             return a**3\n    ...\n    ...         def right(*, a, b):\n    ...             return b**3\n    ...         return (left, right)\n    ...\n    ...     def pdf(self, x, *, a, b):\n    ...         return 1 / (3*(b - a)*np.cbrt(x)**2)\n    ...\n    ...     def cdf(self, x, *, a, b):\n    ...         return (np.cbrt(x) - a) / (b - a)\n    >>>\n    >>> MyUniformCube = stats.make_distribution(MyUniformCube())\n    >>> X = MyUniformCube(a=-2, b=2)\n    >>> Y = stats.Uniform(a=-2, b=2)**3\n    >>> X.support()\n    (-8.0, 8.0)\n    >>> np.isclose(X.cdf(2.1), Y.cdf(2.1))\n    np.True_\n\n    Create a custom distribution with multiple parameterizations. Here we create a\n    custom version of the beta distribution that has an alternative parameterization\n    in terms of the mean ``mu`` and a dispersion parameter ``nu``.\n\n    >>> class MyBeta:\n    ...     @property\n    ...     def __make_distribution_version__(self):\n    ...         return \"1.16.0\"\n    ...\n    ...     @property\n    ...     def parameters(self):\n    ...         return ({\"a\": (0, np.inf), \"b\": (0, np.inf)},\n    ...                 {\"mu\": (0, 1), \"nu\": (0, np.inf)})\n    ...\n    ...     def process_parameters(self, a=None, b=None, mu=None, nu=None):\n    ...         if a is not None and b is not None:\n    ...             nu = a + b\n    ...             mu = a / nu\n    ...         else:\n    ...             a = mu * nu\n    ...             b = nu - a\n    ...         return dict(a=a, b=b, mu=mu, nu=nu)\n    ...\n    ...     @property\n    ...     def support(self):\n    ...         return {'endpoints': (0, 1)}\n    ...\n    ...     def pdf(self, x, a, b, mu, nu):\n    ...         return special._ufuncs._beta_pdf(x, a, b)\n    ...\n    ...     def cdf(self, x, a, b, mu, nu):\n    ...         return special.betainc(a, b, x)\n    >>>\n    >>> MyBeta = stats.make_distribution(MyBeta())\n    >>> X = MyBeta(a=2.0, b=2.0)\n    >>> Y = MyBeta(mu=0.5, nu=4.0)\n    >>> np.isclose(X.pdf(0.3), Y.pdf(0.3))\n    np.True_\n\n    \"\"\""}, {"filename": "scipy/stats/_distribution_infrastructure.py", "start_line": 4450, "code": "class TruncatedDistribution(TransformedDistribution):\n    _lb_domain = _RealInterval(endpoints=(-inf, 'ub'), inclusive=(True, False))\n    _lb_param = _RealParameter('lb', symbol=r'b_l',\n                                domain=_lb_domain, typical=(0.1, 0.2))\n    _ub_domain = _RealInterval(endpoints=('lb', inf), inclusive=(False, True))\n    _ub_param = _RealParameter('ub', symbol=r'b_u',\n                                  domain=_ub_domain, typical=(0.8, 0.9))\n    _parameterizations = [_Parameterization(_lb_param, _ub_param),\n                          _Parameterization(_lb_param),\n                          _Parameterization(_ub_param)]", "documentation": "    \"\"\"Truncated distribution.\"\"\""}, {"filename": "scipy/stats/_distribution_infrastructure.py", "start_line": 4537, "code": "def truncate(X, lb=-np.inf, ub=np.inf):\n    return TruncatedDistribution(X, lb=lb, ub=ub)", "documentation": "    \"\"\"Truncate the support of a random variable.\n\n    Given a random variable `X`, `truncate` returns a random variable with\n    support truncated to the interval between `lb` and `ub`. The underlying\n    probability density function is normalized accordingly.\n\n    Parameters\n    ----------\n    X : `ContinuousDistribution`\n        The random variable to be truncated.\n    lb, ub : float array-like\n        The lower and upper truncation points, respectively. Must be\n        broadcastable with one another and the shape of `X`.\n\n    Returns\n    -------\n    X : `ContinuousDistribution`\n        The truncated random variable.\n\n    References\n    ----------\n    .. [1] \"Truncated Distribution\". *Wikipedia*.\n           https://en.wikipedia.org/wiki/Truncated_distribution\n\n    Examples\n    --------\n    Compare against `scipy.stats.truncnorm`, which truncates a standard normal,\n    *then* shifts and scales it.\n\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy import stats\n    >>> loc, scale, lb, ub = 1, 2, -2, 2\n    >>> X = stats.truncnorm(lb, ub, loc, scale)\n    >>> Y = scale * stats.truncate(stats.Normal(), lb, ub) + loc\n    >>> x = np.linspace(-3, 5, 300)\n    >>> plt.plot(x, X.pdf(x), '-', label='X')\n    >>> plt.plot(x, Y.pdf(x), '--', label='Y')\n    >>> plt.xlabel('x')\n    >>> plt.ylabel('PDF')\n    >>> plt.title('Truncated, then Shifted/Scaled Normal')\n    >>> plt.legend()\n    >>> plt.show()\n\n    However, suppose we wish to shift and scale a normal random variable,\n    then truncate its support to given values. This is straightforward with\n    `truncate`.\n\n    >>> Z = stats.truncate(scale * stats.Normal() + loc, lb, ub)\n    >>> Z.plot()\n    >>> plt.show()\n\n    Furthermore, `truncate` can be applied to any random variable:\n\n    >>> Rayleigh = stats.make_distribution(stats.rayleigh)\n    >>> W = stats.truncate(Rayleigh(), lb=0.5, ub=3)\n    >>> W.plot()\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_distribution_infrastructure.py", "start_line": 4601, "code": "class ShiftedScaledDistribution(TransformedDistribution):\n    _loc_domain = _RealInterval(endpoints=(-inf, inf), inclusive=(True, True))\n    _loc_param = _RealParameter('loc', symbol=r'\\mu',\n                                domain=_loc_domain, typical=(1, 2))\n    _scale_domain = _RealInterval(endpoints=(-inf, inf), inclusive=(True, True))\n    _scale_param = _RealParameter('scale', symbol=r'\\sigma',\n                                  domain=_scale_domain, typical=(0.1, 10))\n    _parameterizations = [_Parameterization(_loc_param, _scale_param),\n                          _Parameterization(_loc_param),\n                          _Parameterization(_scale_param)]", "documentation": "    \"\"\"Distribution with a standard shift/scale transformation.\"\"\""}], "after_segments": [{"filename": "scipy/stats/_distribution_infrastructure.py", "start_line": 1265, "code": "def _logexpxmexpy(x, y):\n    i = np.isneginf(np.real(y))\n    if np.any(i):\n        y = np.asarray(y.copy())\n        y[i] = np.finfo(y.dtype).min\n    x, y = np.broadcast_arrays(x, y)\n    res = np.asarray(special.logsumexp([x, y+np.pi*1j], axis=0))\n    i = (x == y)\n    res[i] = -np.inf\n    return res", "documentation": "    \"\"\" Compute the log of the difference of the exponentials of two arguments.\n\n    Avoids over/underflow, but does not prevent loss of precision otherwise.\n    \"\"\""}, {"filename": "scipy/stats/_distribution_infrastructure.py", "start_line": 1303, "code": "def _log_real_standardize(x):\n    shape = x.shape\n    x = np.atleast_1d(x)\n    real = np.real(x).astype(x.dtype)\n    complex = np.imag(x)\n    y = real\n    negative = np.exp(complex*1j) < 0.5\n    y[negative] = y[negative] + np.pi * 1j\n    return y.reshape(shape)[()]", "documentation": "    \"\"\"Standardizes the (complex) logarithm of a real number.\n\n    The logarithm of a real number may be represented by a complex number with\n    imaginary part that is a multiple of pi*1j. Even multiples correspond with\n    a positive real and odd multiples correspond with a negative real.\n\n    Given a logarithm of a real number `x`, this function returns an equivalent\n    representation in a standard form: the log of a positive real has imaginary\n    part `0` and the log of a negative real has imaginary part `pi`.\n\n    \"\"\""}, {"filename": "scipy/stats/_distribution_infrastructure.py", "start_line": 3163, "code": "    def _moment_raw(self, order=1, *, method=None):\n        methods = self._moment_methods if method is None else {method}\n        return self._moment_raw_dispatch(order, methods=methods, **self._parameters)", "documentation": "        \"\"\"Raw distribution moment about the origin.\"\"\""}, {"filename": "scipy/stats/_distribution_infrastructure.py", "start_line": 3224, "code": "    def _moment_central(self, order=1, *, method=None):\n        methods = self._moment_methods if method is None else {method}\n        return self._moment_central_dispatch(order, methods=methods, **self._parameters)", "documentation": "        \"\"\"Distribution moment about the mean.\"\"\""}, {"filename": "scipy/stats/_distribution_infrastructure.py", "start_line": 3294, "code": "    def _moment_standardized(self, order=1, *, method=None):\n        methods = self._moment_methods if method is None else {method}\n        return self._moment_standardized_dispatch(order, methods=methods,\n                                                  **self._parameters)", "documentation": "        \"\"\"Standardized distribution moment.\"\"\""}, {"filename": "scipy/stats/_distribution_infrastructure.py", "start_line": 3953, "code": "def make_distribution(dist):\n    if dist in {stats.levy_stable, stats.vonmises, stats.hypergeom,\n                stats.nchypergeom_fisher, stats.nchypergeom_wallenius,\n                stats.poisson_binom}:\n        raise NotImplementedError(f\"`{dist.name}` is not supported.\")\n    if isinstance(dist, stats.rv_continuous | stats.rv_discrete):\n        return _make_distribution_rv_generic(dist)\n    elif getattr(dist, \"__make_distribution_version__\", \"0.0.0\") >= \"1.16.0\":\n        return _make_distribution_custom(dist)\n    else:\n        message = (\"The argument must be an instance of `rv_continuous`, \"", "documentation": "    \"\"\"Generate a `UnivariateDistribution` class from a compatible object\n\n    The argument may be an instance of `rv_continuous` or an instance of\n    another class that satisfies the interface described below.\n\n    The returned value is a `ContinuousDistribution` subclass if the input is an\n    instance of `rv_continuous` or a `DiscreteDistribution` subclass if the input\n    is an instance of `rv_discrete`. Like any subclass of `UnivariateDistribution`,\n    it must be instantiated (i.e. by passing all shape parameters as keyword\n    arguments) before use. Once instantiated, the resulting object will have the\n    same interface as any other instance of `UnivariateDistribution`; e.g.,\n    `scipy.stats.Normal`, `scipy.stats.Binomial`.\n\n    .. note::\n\n        `make_distribution` does not work perfectly with all instances of\n        `rv_continuous`. Known failures include `levy_stable`, `vonmises`,\n        `hypergeom`, 'nchypergeom_fisher', 'nchypergeom_wallenius', and\n        `poisson_binom`. Some methods of some distributions will not support\n        array shape parameters.\n\n    Parameters\n    ----------\n    dist : `rv_continuous`\n        Instance of `rv_continuous`, `rv_discrete`, or an instance of any class with\n        the following attributes:\n\n        __make_distribution_version__ : str\n            A string containing the version number of SciPy in which this interface\n            is defined. The preferred interface may change in future SciPy versions,\n            in which case support for an old interface version may be deprecated\n            and eventually removed.\n        parameters : dict or tuple\n            If a dictionary, each key is the name of a parameter,\n            and the corresponding value is either a dictionary or tuple.\n            If the value is a dictionary, it may have the following items, with default\n            values used for entries which aren't present.\n\n            endpoints : tuple, default: (-inf, inf)\n                A tuple defining the lower and upper endpoints of the domain of the\n                parameter; allowable values are floats, the name (string) of another\n                parameter, or a callable taking parameters as keyword only\n                arguments and returning the numerical value of an endpoint for\n                given parameter values.\n\n            inclusive : tuple of bool, default: (False, False)\n                A tuple specifying whether the endpoints are included within the domain\n                of the parameter.\n\n            typical : tuple, default: ``endpoints``\n                Defining endpoints of a typical range of values of a parameter. Can be\n                used for sampling parameter values for testing. Behaves like the\n                ``endpoints`` tuple above, and should define a subinterval of the\n                domain given by ``endpoints``.\n\n            A tuple value ``(a, b)`` associated to a key in the ``parameters``\n            dictionary is equivalent to ``{endpoints: (a, b)}``.\n\n            Custom distributions with multiple parameterizations can be defined by\n            having the ``parameters`` attribute be a tuple of dictionaries with\n            the structure described above. In this case, ``dist``\\'s class must also\n            define a method ``process_parameters`` to map between the different\n            parameterizations. It must take all parameters from all parameterizations\n            as optional keyword arguments and return a dictionary mapping parameters to\n            values, filling in values from other parameterizations using values from\n            the supplied parameterization. See example.\n\n        support : dict or tuple\n            A dictionary describing the support of the distribution or a tuple\n            describing the endpoints of the support. This behaves identically to\n            the values of the parameters dict described above, except that the key\n            ``typical`` is ignored.\n\n        The class **must** also define a ``pdf`` method and **may** define methods\n        ``logentropy``, ``entropy``, ``median``, ``mode``, ``logpdf``,\n        ``logcdf``, ``cdf``, ``logccdf``, ``ccdf``,\n        ``ilogcdf``, ``icdf``, ``ilogccdf``, ``iccdf``,\n        ``moment``, ``lmoment``, and ``sample``.\n        If defined, these methods must accept the parameters of the distribution as\n        keyword arguments and also accept any positional-only arguments accepted by\n        the corresponding method of `ContinuousDistribution`.\n        When multiple parameterizations are defined, these methods must accept\n        all parameters from all parameterizations. The ``moment`` method\n        must accept the ``order`` and ``kind`` arguments by position or keyword, but\n        may return ``None`` if a formula is not available for the arguments; in this\n        case, the infrastructure will fall back to a default implementation.\n        The ``lmoment`` method must accept the ``order`` argument by position or\n        keyword and return the specified L-moment (not the L-moment ratio), but may\n        return ``None`` if a formula is not available for the arguments. The\n        ``sample`` method must accept ``shape`` by position or keyword, but contrary\n        to the public method of the same name, the argument it receives will be the\n        *full* shape of the output array - that is, the shape passed to the public\n        method prepended to the broadcasted shape of random variable parameters.\n\n    Returns\n    -------\n    CustomDistribution : `UnivariateDistribution`\n        A subclass of `UnivariateDistribution` corresponding with `dist`. The\n        initializer requires all shape parameters to be passed as keyword arguments\n        (using the same names as the instance of `rv_continuous`/`rv_discrete`).\n\n    Notes\n    -----\n    The documentation of `UnivariateDistribution` is not rendered. See below for\n    an example of how to instantiate the class (i.e. pass all shape parameters of\n    `dist` to the initializer as keyword arguments). Documentation of all methods\n    is identical to that of `scipy.stats.Normal`. Use ``help`` on the returned\n    class or its methods for more information.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy import stats\n    >>> from scipy import special\n\n    Create a `ContinuousDistribution` from `scipy.stats.loguniform`.\n\n    >>> LogUniform = stats.make_distribution(stats.loguniform)\n    >>> X = LogUniform(a=1.0, b=3.0)\n    >>> np.isclose((X + 0.25).median(), stats.loguniform.ppf(0.5, 1, 3, loc=0.25))\n    np.True_\n    >>> X.plot()\n    >>> sample = X.sample(10000, rng=np.random.default_rng())\n    >>> plt.hist(sample, density=True, bins=30)\n    >>> plt.legend(('pdf', 'histogram'))\n    >>> plt.show()\n\n    Create a custom distribution.\n\n    >>> class MyLogUniform:\n    ...     @property\n    ...     def __make_distribution_version__(self):\n    ...         return \"1.16.0\"\n    ...\n    ...     @property\n    ...     def parameters(self):\n    ...         return {'a': {'endpoints': (0, np.inf),\n    ...                       'inclusive': (False, False)},\n    ...                 'b': {'endpoints': ('a', np.inf),\n    ...                       'inclusive': (False, False)}}\n    ...\n    ...     @property\n    ...     def support(self):\n    ...         return {'endpoints': ('a', 'b'), 'inclusive': (True, True)}\n    ...\n    ...     def pdf(self, x, a, b):\n    ...         return 1 / (x * (np.log(b)- np.log(a)))\n    >>>\n    >>> MyLogUniform = stats.make_distribution(MyLogUniform())\n    >>> Y = MyLogUniform(a=1.0, b=3.0)\n    >>> np.isclose(Y.cdf(2.), X.cdf(2.))\n    np.True_\n\n    Create a custom distribution with variable support.\n\n    >>> class MyUniformCube:\n    ...     @property\n    ...     def __make_distribution_version__(self):\n    ...         return \"1.16.0\"\n    ...\n    ...     @property\n    ...     def parameters(self):\n    ...         return {\"a\": (-np.inf, np.inf),\n    ...                 \"b\": {'endpoints':('a', np.inf), 'inclusive':(True, False)}}\n    ...\n    ...     @property\n    ...     def support(self):\n    ...         def left(*, a, b):\n    ...             return a**3\n    ...\n    ...         def right(*, a, b):\n    ...             return b**3\n    ...         return (left, right)\n    ...\n    ...     def pdf(self, x, *, a, b):\n    ...         return 1 / (3*(b - a)*np.cbrt(x)**2)\n    ...\n    ...     def cdf(self, x, *, a, b):\n    ...         return (np.cbrt(x) - a) / (b - a)\n    >>>\n    >>> MyUniformCube = stats.make_distribution(MyUniformCube())\n    >>> X = MyUniformCube(a=-2, b=2)\n    >>> Y = stats.Uniform(a=-2, b=2)**3\n    >>> X.support()\n    (-8.0, 8.0)\n    >>> np.isclose(X.cdf(2.1), Y.cdf(2.1))\n    np.True_\n\n    Create a custom distribution with multiple parameterizations. Here we create a\n    custom version of the beta distribution that has an alternative parameterization\n    in terms of the mean ``mu`` and a dispersion parameter ``nu``.\n\n    >>> class MyBeta:\n    ...     @property\n    ...     def __make_distribution_version__(self):\n    ...         return \"1.16.0\"\n    ...\n    ...     @property\n    ...     def parameters(self):\n    ...         return ({\"a\": (0, np.inf), \"b\": (0, np.inf)},\n    ...                 {\"mu\": (0, 1), \"nu\": (0, np.inf)})\n    ...\n    ...     def process_parameters(self, a=None, b=None, mu=None, nu=None):\n    ...         if a is not None and b is not None:\n    ...             nu = a + b\n    ...             mu = a / nu\n    ...         else:\n    ...             a = mu * nu\n    ...             b = nu - a\n    ...         return dict(a=a, b=b, mu=mu, nu=nu)\n    ...\n    ...     @property\n    ...     def support(self):\n    ...         return {'endpoints': (0, 1)}\n    ...\n    ...     def pdf(self, x, a, b, mu, nu):\n    ...         return special._ufuncs._beta_pdf(x, a, b)\n    ...\n    ...     def cdf(self, x, a, b, mu, nu):\n    ...         return special.betainc(a, b, x)\n    >>>\n    >>> MyBeta = stats.make_distribution(MyBeta())\n    >>> X = MyBeta(a=2.0, b=2.0)\n    >>> Y = MyBeta(mu=0.5, nu=4.0)\n    >>> np.isclose(X.pdf(0.3), Y.pdf(0.3))\n    np.True_\n\n    \"\"\""}, {"filename": "scipy/stats/_distribution_infrastructure.py", "start_line": 4539, "code": "class TruncatedDistribution(TransformedDistribution):\n    _lb_domain = _RealInterval(endpoints=(-inf, 'ub'), inclusive=(True, False))\n    _lb_param = _RealParameter('lb', symbol=r'b_l',\n                                domain=_lb_domain, typical=(0.1, 0.2))\n    _ub_domain = _RealInterval(endpoints=('lb', inf), inclusive=(False, True))\n    _ub_param = _RealParameter('ub', symbol=r'b_u',\n                                  domain=_ub_domain, typical=(0.8, 0.9))\n    _parameterizations = [_Parameterization(_lb_param, _ub_param),\n                          _Parameterization(_lb_param),\n                          _Parameterization(_ub_param)]", "documentation": "    \"\"\"Truncated distribution.\"\"\""}, {"filename": "scipy/stats/_distribution_infrastructure.py", "start_line": 4626, "code": "def truncate(X, lb=-np.inf, ub=np.inf):\n    return TruncatedDistribution(X, lb=lb, ub=ub)", "documentation": "    \"\"\"Truncate the support of a random variable.\n\n    Given a random variable `X`, `truncate` returns a random variable with\n    support truncated to the interval between `lb` and `ub`. The underlying\n    probability density function is normalized accordingly.\n\n    Parameters\n    ----------\n    X : `ContinuousDistribution`\n        The random variable to be truncated.\n    lb, ub : float array-like\n        The lower and upper truncation points, respectively. Must be\n        broadcastable with one another and the shape of `X`.\n\n    Returns\n    -------\n    X : `ContinuousDistribution`\n        The truncated random variable.\n\n    References\n    ----------\n    .. [1] \"Truncated Distribution\". *Wikipedia*.\n           https://en.wikipedia.org/wiki/Truncated_distribution\n\n    Examples\n    --------\n    Compare against `scipy.stats.truncnorm`, which truncates a standard normal,\n    *then* shifts and scales it.\n\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy import stats\n    >>> loc, scale, lb, ub = 1, 2, -2, 2\n    >>> X = stats.truncnorm(lb, ub, loc, scale)\n    >>> Y = scale * stats.truncate(stats.Normal(), lb, ub) + loc\n    >>> x = np.linspace(-3, 5, 300)\n    >>> plt.plot(x, X.pdf(x), '-', label='X')\n    >>> plt.plot(x, Y.pdf(x), '--', label='Y')\n    >>> plt.xlabel('x')\n    >>> plt.ylabel('PDF')\n    >>> plt.title('Truncated, then Shifted/Scaled Normal')\n    >>> plt.legend()\n    >>> plt.show()\n\n    However, suppose we wish to shift and scale a normal random variable,\n    then truncate its support to given values. This is straightforward with\n    `truncate`.\n\n    >>> Z = stats.truncate(scale * stats.Normal() + loc, lb, ub)\n    >>> Z.plot()\n    >>> plt.show()\n\n    Furthermore, `truncate` can be applied to any random variable:\n\n    >>> Rayleigh = stats.make_distribution(stats.rayleigh)\n    >>> W = stats.truncate(Rayleigh(), lb=0.5, ub=3)\n    >>> W.plot()\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_distribution_infrastructure.py", "start_line": 4690, "code": "class ShiftedScaledDistribution(TransformedDistribution):\n    _loc_domain = _RealInterval(endpoints=(-inf, inf), inclusive=(True, True))\n    _loc_param = _RealParameter('loc', symbol=r'\\mu',\n                                domain=_loc_domain, typical=(1, 2))\n    _scale_domain = _RealInterval(endpoints=(-inf, inf), inclusive=(True, True))\n    _scale_param = _RealParameter('scale', symbol=r'\\sigma',\n                                  domain=_scale_domain, typical=(0.1, 10))\n    _parameterizations = [_Parameterization(_loc_param, _scale_param),\n                          _Parameterization(_loc_param),\n                          _Parameterization(_scale_param)]", "documentation": "    \"\"\"Distribution with a standard shift/scale transformation.\"\"\""}]}
{"repository": "scipy/scipy", "commit_sha": "3093d3ead614a538612285cfff265db430a8e07a", "commit_message": "ENH: stats.ContinuousDistribution.lmoment: add population L-moments (#23766)\n\n* ENH: stats.ContinuousDistribution: add l-moments\n\n* ENH: stats.ContinuousDistribution: add l-moments from integral over icdf\n\n* MAINT: stats.ContinuousDistribution: simplify l-moment ratio implementation\n\n* ENH: stats.ContinuousDistribution: draft lmoment for custom and shifted/scaled distributions\n\n* DOC: stats.ContinuousDistribution.lmoment: draft documentation\n\n* TST: stats.ContinuousDistribution.lmoment: add tests\n\n* ENH: stats.ContinuousDistribution.lmoment: override for built-in distributions\n\n* MAINT: stats.ContinuousDistribution.lmoment: fixups\n\n* DOC: stats.lmoment: update docs per new default standardize=True\"", "commit_date": "2026-01-31T09:21:12+00:00", "author": "Matt Haberland", "file": "scipy/stats/_new_distributions.py", "patch": "@@ -116,6 +116,12 @@ def _moment_central_formula(self, order, *, mu, sigma, **kwargs):\n             # exact is faster (and obviously more accurate) for reasonable orders\n             return sigma**order * special.factorial2(int(order) - 1, exact=True)\n \n+    def _lmoment_formula(self, order, *, mu, sigma, **kwargs):\n+        lscale = sigma / np.sqrt(np.pi)\n+        lkurtosis = 30*np.arctan(np.sqrt(2))/np.pi - 9\n+        lmoments = {1: mu, 2: lscale, 3: 0, 4: lkurtosis * lscale}\n+        return lmoments.get(order, None)\n+\n     def _sample_formula(self, full_shape, rng, *, mu, sigma, **kwargs):\n         return rng.normal(loc=mu, scale=sigma, size=full_shape)[()]\n \n@@ -198,6 +204,9 @@ def _moment_central_formula(self, order, **kwargs):\n     def _moment_standardized_formula(self, order, **kwargs):\n         return self._moment_raw_formula(order, **kwargs)\n \n+    def _lmoment_formula(self, order, **kwargs):\n+        return super()._lmoment_formula(order, mu=0., sigma=1., **kwargs)\n+\n     def _sample_formula(self, full_shape, rng, **kwargs):\n         return rng.normal(size=full_shape)[()]\n \n@@ -268,6 +277,10 @@ def _moment_central_formula(self, order, **kwargs):\n     def _moment_standardized_formula(self, order, **kwargs):\n         return self._moment_raw_formula(order, **kwargs) / self._scale**order\n \n+    def _lmoment_formula(self, order, **kwargs):\n+        lmoments = {1: 0, 2: 1, 3: 0, 4: 1/6}\n+        return lmoments.get(order, None)\n+\n     def _sample_formula(self, full_shape, rng, **kwargs):\n         return rng.logistic(size=full_shape)[()]\n \n@@ -417,6 +430,10 @@ def _moment_central_formula(self, order, ab, **kwargs):\n \n     _moment_central_formula.orders = [2]  # type: ignore[attr-defined]\n \n+    def _lmoment_formula(self, order, *, a, b, ab, **kwargs):\n+        lmoments = {1: 0.5*(a + b), 2: ab / 6}\n+        return lmoments.get(order, np.zeros_like(ab))\n+\n     def _sample_formula(self, full_shape, rng, a, b, ab, **kwargs):\n         try:\n             return rng.uniform(a, b, size=full_shape)[()]", "before_segments": [], "after_segments": []}
{"repository": "scipy/scipy", "commit_sha": "3093d3ead614a538612285cfff265db430a8e07a", "commit_message": "ENH: stats.ContinuousDistribution.lmoment: add population L-moments (#23766)\n\n* ENH: stats.ContinuousDistribution: add l-moments\n\n* ENH: stats.ContinuousDistribution: add l-moments from integral over icdf\n\n* MAINT: stats.ContinuousDistribution: simplify l-moment ratio implementation\n\n* ENH: stats.ContinuousDistribution: draft lmoment for custom and shifted/scaled distributions\n\n* DOC: stats.ContinuousDistribution.lmoment: draft documentation\n\n* TST: stats.ContinuousDistribution.lmoment: add tests\n\n* ENH: stats.ContinuousDistribution.lmoment: override for built-in distributions\n\n* MAINT: stats.ContinuousDistribution.lmoment: fixups\n\n* DOC: stats.lmoment: update docs per new default standardize=True\"", "commit_date": "2026-01-31T09:21:12+00:00", "author": "Matt Haberland", "file": "scipy/stats/_probability_distribution.py", "patch": "@@ -248,6 +248,7 @@ def moment(self, order, kind, *, method):\n         standard_deviation\n         skewness\n         kurtosis\n+        lmoment\n \n         Notes\n         -----\n@@ -336,6 +337,124 @@ def moment(self, order, kind, *, method):\n         \"\"\"  # noqa:E501\n         raise NotImplementedError()\n \n+    @abstractmethod\n+    def lmoment(self, order, kind, *, method):\n+        r\"\"\"L-moment or L-moment ratio of positive integer order.\n+\n+        The L-moment of order :math:`n` of a continuous random variable :math:`X` is:\n+\n+        .. math::\n+\n+            \\lambda_n(X) = \\frac{1}{n} \\sum_{k=0}^{n-1} (-1)^{k} {{n-1}\\choose k} E[X_{(n-k)}]\n+\n+        where :math:`X_{(1)}, \\dots, X_{(r)}, \\dots, X_{(n)}` are the order statistics\n+        of an independent sample of size :math:`n`.\n+\n+        The L-moment can also be expressed in terms of the random variable's inverse\n+        cumulative distribution function :math:`F^{-1}` and the shifted Legendre\n+        polynomial :math:`\\widetilde{P}_{n-1}`:\n+\n+        .. math::\n+\n+            \\lambda_n(X) = \\int_0^1 F^{-1}(p) \\widetilde{P}_{n-1}(p) dp\n+\n+        For order :math:`n \\geq 3`, the \"standardized\" L-moment, known as the L-moment\n+        ratio, is the L-moment normalized by the L-moment of order 2, resulting in a\n+        scale invariant quantity:\n+\n+        .. math::\n+\n+            \\tau_n(X) = \\frac{\\lambda_n(X)}\n+                             {\\lambda_2(X)}\n+\n+        Parameters\n+        ----------\n+        order : int\n+            The positive integer order of the L-moment; i.e. :math:`n` in the formulae\n+            above.\n+        standardize : bool, default: True\n+            Whether to return L-moment ratios for orders 3 and higher. L-moment ratios\n+            are analogous to standardized conventional moments: they are the\n+            non-standardized L-moments divided by the L-moment of order 2.\n+        method : {None, 'formula', 'general', 'order_statistics', 'quadrature_icdf', 'cache'}\n+            The strategy used to evaluate the L-moment. By default (``None``),\n+            the infrastructure chooses between the following options,\n+            listed in order of precedence.\n+\n+            - ``'cache'``: use the value of the L-moment most recently calculated\n+              via another method\n+            - ``'formula'``: use a formula specific to the distribution.\n+            - ``'general'``: use a general result that is true for all distributions\n+              with finite L-moments; for instance, the first L-moment is identically\n+              equal to the mean.\n+            - ``'quadrature_icdf'``: numerically integrate according to the definition\n+              in terms of the inverse cumulative distribution function.\n+            - ``'order_statistics'``: compute according to the definition in terms of\n+              order statistics.\n+\n+            Not all `method` options are available for all orders and distributions.\n+            If the selected `method` is not available, a ``NotImplementedError`` will\n+            be raised.\n+\n+        Returns\n+        -------\n+        out : array\n+            The L-moment of the random variable of the specified order.\n+\n+        See Also\n+        --------\n+        moment\n+        order_statistic\n+\n+        Notes\n+        -----\n+        L-moments are only defined for distributions with finite mean. If a formula for\n+        the L-moment is not specifically implemented for the chosen distribution, SciPy\n+        will attempt to compute the moment via a generic method, which may yield a\n+        finite result where none exists. This is not a critical bug, but an opportunity\n+        for an enhancement.\n+\n+        SciPy offers only basic capabilities for working with L-moments. For more advanced\n+        features, consider the ``lmo`` package [2]_.\n+\n+        References\n+        ----------\n+        .. [1] L-moment, *Wikipedia*,\n+               https://en.wikipedia.org/wiki/L-moment\n+        .. [2] @jorenham, *Lmo*, https://github.com/jorenham/Lmo/\n+\n+        Examples\n+        --------\n+        Instantiate a distribution with the desired parameters:\n+\n+        >>> import numpy as np\n+        >>> from scipy import stats\n+        >>> X = stats.Normal(mu=1., sigma=2.)\n+\n+        Evaluate the first L-moment:\n+\n+        >>> X.lmoment(order=1)\n+        1.0\n+        >>> X.lmoment(order=1) == X.mean() == X.mu\n+        True\n+\n+        Evaluate the second L-moment:\n+\n+        >>> X.lmoment(order=2)\n+        np.float64(1.1283791670955123)\n+        >>> np.allclose(X.lmoment(order=2), X.sigma / np.sqrt(np.pi))\n+        True\n+\n+        Evaluate the fourth L-moment ratio, that is, the L-kurtosis:\n+\n+        >>> X.lmoment(order=4)\n+        np.float64(0.12260171954089069)\n+        >>> X.lmoment(order=4) == X.lmoment(order=4, standardize=False) / X.lmoment(order=2)\n+        True\n+\n+        \"\"\"  # noqa:E501\n+        raise NotImplementedError()\n+\n     @abstractmethod\n     def mean(self, *, method):\n         r\"\"\"Mean (raw first moment about the origin)", "before_segments": [], "after_segments": []}
{"repository": "scipy/scipy", "commit_sha": "3093d3ead614a538612285cfff265db430a8e07a", "commit_message": "ENH: stats.ContinuousDistribution.lmoment: add population L-moments (#23766)\n\n* ENH: stats.ContinuousDistribution: add l-moments\n\n* ENH: stats.ContinuousDistribution: add l-moments from integral over icdf\n\n* MAINT: stats.ContinuousDistribution: simplify l-moment ratio implementation\n\n* ENH: stats.ContinuousDistribution: draft lmoment for custom and shifted/scaled distributions\n\n* DOC: stats.ContinuousDistribution.lmoment: draft documentation\n\n* TST: stats.ContinuousDistribution.lmoment: add tests\n\n* ENH: stats.ContinuousDistribution.lmoment: override for built-in distributions\n\n* MAINT: stats.ContinuousDistribution.lmoment: fixups\n\n* DOC: stats.lmoment: update docs per new default standardize=True\"", "commit_date": "2026-01-31T09:21:12+00:00", "author": "Matt Haberland", "file": "scipy/stats/_stats_py.py", "patch": "@@ -10461,6 +10461,11 @@ def lmoment(sample, order=None, *, axis=0, sorted=False, standardize=True):\n     --------\n     moment\n \n+    Notes\n+    -----\n+    SciPy offers only basic capabilities for working with L-moments. For more advanced\n+    features, consider the ``lmo`` package [4]_.\n+\n     References\n     ----------\n     .. [1] D. Bilkova. \"L-Moments and TL-Moments as an Alternative Tool of\n@@ -10470,6 +10475,7 @@ def lmoment(sample, order=None, *, axis=0, sorted=False, standardize=True):\n            Using Linear Combinations of Order Statistics\". Journal of the Royal\n            Statistical Society. 1990. :doi:`10.1111/j.2517-6161.1990.tb01775.x`\n     .. [3] \"L-moment\". *Wikipedia*. https://en.wikipedia.org/wiki/L-moment.\n+    .. [4] @jorenham, *Lmo*, https://github.com/jorenham/Lmo/\n \n     Examples\n     --------", "before_segments": [{"filename": "scipy/stats/_stats_py.py", "start_line": 603, "code": "def _put_val_to_limits(a, limits, inclusive, val=np.nan, xp=None):\n    xp = array_namespace(a) if xp is None else xp\n    mask = xp.zeros_like(a, dtype=xp.bool)\n    if limits is None:\n        return a, mask\n    lower_limit, upper_limit = limits\n    lower_include, upper_include = inclusive\n    if lower_limit is not None:\n        mask = mask | ((a < lower_limit) if lower_include else a <= lower_limit)\n    if upper_limit is not None:\n        mask = mask | ((a > upper_limit) if upper_include else a >= upper_limit)", "documentation": "    \"\"\"Replace elements outside limits with a value.\n\n    This is primarily a utility function.\n\n    Parameters\n    ----------\n    a : array\n    limits : (float or None, float or None)\n        A tuple consisting of the (lower limit, upper limit).  Elements in the\n        input array less than the lower limit or greater than the upper limit\n        will be replaced with `val`. None implies no limit.\n    inclusive : (bool, bool)\n        A tuple consisting of the (lower flag, upper flag).  These flags\n        determine whether values exactly equal to lower or upper are allowed.\n    val : float, default: NaN\n        The value with which extreme elements of the array are replaced.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 645, "code": "def tmean(a, limits=None, inclusive=(True, True), axis=None):\n    xp = array_namespace(a)\n    a, mask = _put_val_to_limits(a, limits, inclusive, val=0., xp=xp)\n    sum = xp.sum(a, axis=axis, dtype=a.dtype)\n    n = xp.sum(xp.asarray(~mask, dtype=a.dtype, device=xp_device(a)), axis=axis,\n               dtype=a.dtype)\n    mean = xpx.apply_where(n != 0, (sum, n), operator.truediv, fill_value=xp.nan)\n    return mean[()] if mean.ndim == 0 else mean\n@xp_capabilities()\n@_axis_nan_policy_factory(\n    lambda x: x, n_outputs=1, result_to_tuple=lambda x, _: (x,)", "documentation": "    \"\"\"Compute the trimmed mean.\n\n    This function finds the arithmetic mean of given values, ignoring values\n    outside the given `limits`.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    limits : None or (lower limit, upper limit), optional\n        Values in the input array less than the lower limit or greater than the\n        upper limit will be ignored.  When limits is None (default), then all\n        values are used.  Either of the limit values in the tuple can also be\n        None representing a half-open interval.\n    inclusive : (bool, bool), optional\n        A tuple consisting of the (lower flag, upper flag).  These flags\n        determine whether values exactly equal to the lower or upper limits\n        are included.  The default value is (True, True).\n    axis : int or None, optional\n        Axis along which to compute test. Default is None.\n\n    Returns\n    -------\n    tmean : ndarray\n        Trimmed mean.\n\n    See Also\n    --------\n    trim_mean : Returns mean after trimming a proportion from both tails.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tmean(x)\n    9.5\n    >>> stats.tmean(x, (3,17))\n    10.0\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 701, "code": "def tvar(a, limits=None, inclusive=(True, True), axis=0, ddof=1):\n    xp = array_namespace(a)\n    a, _ = _put_val_to_limits(a, limits, inclusive, xp=xp)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", SmallSampleWarning)\n        return _xp_var(a, correction=ddof, axis=axis, nan_policy='omit', xp=xp)\n@xp_capabilities()\n@_axis_nan_policy_factory(\n    lambda x: x, n_outputs=1, result_to_tuple=lambda x, _: (x,)\n)", "documentation": "    \"\"\"Compute the trimmed variance.\n\n    This function computes the sample variance of an array of values,\n    while ignoring values which are outside of given `limits`.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    limits : None or (lower limit, upper limit), optional\n        Values in the input array less than the lower limit or greater than the\n        upper limit will be ignored. When limits is None, then all values are\n        used. Either of the limit values in the tuple can also be None\n        representing a half-open interval.  The default value is None.\n    inclusive : (bool, bool), optional\n        A tuple consisting of the (lower flag, upper flag).  These flags\n        determine whether values exactly equal to the lower or upper limits\n        are included.  The default value is (True, True).\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over the\n        whole array `a`.\n    ddof : int, optional\n        Delta degrees of freedom.  Default is 1.\n\n    Returns\n    -------\n    tvar : float\n        Trimmed variance.\n\n    Notes\n    -----\n    `tvar` computes the unbiased sample variance, i.e. it uses a correction\n    factor ``n / (n - 1)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tvar(x)\n    35.0\n    >>> stats.tvar(x, (3,17))\n    20.0\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 761, "code": "def tmin(a, lowerlimit=None, axis=0, inclusive=True, nan_policy='propagate'):\n    xp = array_namespace(a)\n    max_ = xp.iinfo(a.dtype).max if xp.isdtype(a.dtype, 'integral') else xp.inf\n    a, mask = _put_val_to_limits(a, (lowerlimit, None), (inclusive, None),\n                                 val=max_, xp=xp)\n    res = xp.min(a, axis=axis)\n    invalid = xp.all(mask, axis=axis)  # All elements are below lowerlimit\n    if is_lazy_array(invalid) or xp.any(invalid):\n        res = xp_promote(res, force_floating=True, xp=xp)\n        res = xp.where(invalid, xp.nan, res)\n    return res[()] if res.ndim == 0 else res", "documentation": "    \"\"\"Compute the trimmed minimum.\n\n    This function finds the minimum value of an array `a` along the\n    specified axis, but only considering values greater than a specified\n    lower limit.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    lowerlimit : None or float, optional\n        Values in the input array less than the given limit will be ignored.\n        When lowerlimit is None, then all values are used. The default value\n        is None.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over the\n        whole array `a`.\n    inclusive : {True, False}, optional\n        This flag determines whether values exactly equal to the lower limit\n        are included.  The default value is True.\n\n    Returns\n    -------\n    tmin : float, int or ndarray\n        Trimmed minimum.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tmin(x)\n    0\n\n    >>> stats.tmin(x, 13)\n    13\n\n    >>> stats.tmin(x, 13, inclusive=False)\n    14\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 825, "code": "def tmax(a, upperlimit=None, axis=0, inclusive=True, nan_policy='propagate'):\n    xp = array_namespace(a)\n    min_ = xp.iinfo(a.dtype).min if xp.isdtype(a.dtype, 'integral') else -xp.inf\n    a, mask = _put_val_to_limits(a, (None, upperlimit), (None, inclusive),\n                                 val=min_, xp=xp)\n    res = xp.max(a, axis=axis)\n    invalid = xp.all(mask, axis=axis)  # All elements are above upperlimit\n    if is_lazy_array(invalid) or xp.any(invalid):\n        res = xp_promote(res, force_floating=True, xp=xp)\n        res = xp.where(invalid, xp.nan, res)\n    return res[()] if res.ndim == 0 else res", "documentation": "    \"\"\"Compute the trimmed maximum.\n\n    This function computes the maximum value of an array along a given axis,\n    while ignoring values larger than a specified upper limit.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    upperlimit : None or float, optional\n        Values in the input array greater than the given limit will be ignored.\n        When upperlimit is None, then all values are used. The default value\n        is None.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over the\n        whole array `a`.\n    inclusive : {True, False}, optional\n        This flag determines whether values exactly equal to the upper limit\n        are included.  The default value is True.\n\n    Returns\n    -------\n    tmax : float, int or ndarray\n        Trimmed maximum.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tmax(x)\n    19\n\n    >>> stats.tmax(x, 13)\n    13\n\n    >>> stats.tmax(x, 13, inclusive=False)\n    12\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 888, "code": "def tstd(a, limits=None, inclusive=(True, True), axis=0, ddof=1):\n    return tvar(a, limits, inclusive, axis, ddof, _no_deco=True)**0.5\n@xp_capabilities()\n@_axis_nan_policy_factory(\n    lambda x: x, n_outputs=1, result_to_tuple=lambda x, _: (x,)\n)", "documentation": "    \"\"\"Compute the trimmed sample standard deviation.\n\n    This function finds the sample standard deviation of given values,\n    ignoring values outside the given `limits`.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    limits : None or (lower limit, upper limit), optional\n        Values in the input array less than the lower limit or greater than the\n        upper limit will be ignored. When limits is None, then all values are\n        used. Either of the limit values in the tuple can also be None\n        representing a half-open interval.  The default value is None.\n    inclusive : (bool, bool), optional\n        A tuple consisting of the (lower flag, upper flag).  These flags\n        determine whether values exactly equal to the lower or upper limits\n        are included.  The default value is (True, True).\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over the\n        whole array `a`.\n    ddof : int, optional\n        Delta degrees of freedom.  Default is 1.\n\n    Returns\n    -------\n    tstd : float\n        Trimmed sample standard deviation.\n\n    Notes\n    -----\n    `tstd` computes the unbiased sample standard deviation, i.e. it uses a\n    correction factor ``n / (n - 1)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tstd(x)\n    5.9160797830996161\n    >>> stats.tstd(x, (3,17))\n    4.4721359549995796\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 941, "code": "def tsem(a, limits=None, inclusive=(True, True), axis=0, ddof=1):\n    xp = array_namespace(a)\n    a, _ = _put_val_to_limits(a, limits, inclusive, xp=xp)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", SmallSampleWarning)\n        sd = _xp_var(a, correction=ddof, axis=axis, nan_policy='omit', xp=xp)**0.5\n    not_nan = xp.astype(~xp.isnan(a), a.dtype)\n    n_obs = xp.sum(not_nan, axis=axis, dtype=sd.dtype)\n    return sd / n_obs**0.5", "documentation": "    \"\"\"Compute the trimmed standard error of the mean.\n\n    This function finds the standard error of the mean for given\n    values, ignoring values outside the given `limits`.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    limits : None or (lower limit, upper limit), optional\n        Values in the input array less than the lower limit or greater than the\n        upper limit will be ignored. When limits is None, then all values are\n        used. Either of the limit values in the tuple can also be None\n        representing a half-open interval.  The default value is None.\n    inclusive : (bool, bool), optional\n        A tuple consisting of the (lower flag, upper flag).  These flags\n        determine whether values exactly equal to the lower or upper limits\n        are included.  The default value is (True, True).\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over the\n        whole array `a`.\n    ddof : int, optional\n        Delta degrees of freedom.  Default is 1.\n\n    Returns\n    -------\n    tsem : float\n        Trimmed standard error of the mean.\n\n    Notes\n    -----\n    `tsem` uses unbiased sample standard deviation, i.e. it uses a\n    correction factor ``n / (n - 1)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tsem(x)\n    1.3228756555322954\n    >>> stats.tsem(x, (3,17))\n    1.1547005383792515\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 1192, "code": "def _moment(a, order, axis, *, mean=None, xp=None):\n    xp = array_namespace(a) if xp is None else xp\n    a = xp_promote(a, force_floating=True, xp=xp)\n    dtype = a.dtype\n    if xp_size(a) == 0:\n        return xp.mean(a, axis=axis)\n    if order == 0 or (order == 1 and mean is None):\n        shape = list(a.shape)\n        del shape[axis]\n        temp = (xp.ones(shape, dtype=dtype, device=xp_device(a)) if order == 0\n                else xp.zeros(shape, dtype=dtype, device=xp_device(a)))", "documentation": "    \"\"\"Vectorized calculation of raw moment about specified center\n\n    When `mean` is None, the mean is computed and used as the center;\n    otherwise, the provided value is used as the center.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 1365, "code": "def kurtosis(a, axis=0, fisher=True, bias=True, nan_policy='propagate'):\n    xp = array_namespace(a)\n    a, axis = _chk_asarray(a, axis, xp=xp)\n    n = _length_nonmasked(a, axis, xp=xp)\n    mean = xp.mean(a, axis=axis, keepdims=True)\n    mean_reduced = xp.squeeze(mean, axis=axis)  # needed later\n    m2 = _moment(a, 2, axis, mean=mean, xp=xp)\n    m4 = _moment(a, 4, axis, mean=mean, xp=xp)\n    with np.errstate(all='ignore'):\n        zero = m2 <= (xp.finfo(m2.dtype).eps * mean_reduced)**2\n        vals = xp.where(zero, xp.nan, m4 / m2**2.0)", "documentation": "    \"\"\"Compute the kurtosis (Fisher or Pearson) of a dataset.\n\n    Kurtosis is the fourth central moment divided by the square of the\n    variance. If Fisher's definition is used, then 3.0 is subtracted from\n    the result to give 0.0 for a normal distribution.\n\n    If bias is False then the kurtosis is calculated using k statistics to\n    eliminate bias coming from biased moment estimators\n\n    Use `kurtosistest` to see if result is close enough to normal.\n\n    Parameters\n    ----------\n    a : array\n        Data for which the kurtosis is calculated.\n    axis : int or None, optional\n        Axis along which the kurtosis is calculated. Default is 0.\n        If None, compute over the whole array `a`.\n    fisher : bool, optional\n        If True, Fisher's definition is used (normal ==> 0.0). If False,\n        Pearson's definition is used (normal ==> 3.0).\n    bias : bool, optional\n        If False, then the calculations are corrected for statistical bias.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan. 'propagate' returns nan,\n        'raise' throws an error, 'omit' performs the calculations ignoring nan\n        values. Default is 'propagate'.\n\n    Returns\n    -------\n    kurtosis : array\n        The kurtosis of values along an axis, returning NaN where all values\n        are equal.\n\n    References\n    ----------\n    .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n       Probability and Statistics Tables and Formulae. Chapman & Hall: New\n       York. 2000.\n\n    Examples\n    --------\n    In Fisher's definition, the kurtosis of the normal distribution is zero.\n    In the following example, the kurtosis is close to zero, because it was\n    calculated from the dataset, not from the continuous distribution.\n\n    >>> import numpy as np\n    >>> from scipy.stats import norm, kurtosis\n    >>> data = norm.rvs(size=1000, random_state=3)\n    >>> kurtosis(data)\n    -0.06928694200380558\n\n    The distribution with a higher kurtosis has a heavier tail.\n    The zero valued kurtosis of the normal distribution in Fisher's definition\n    can serve as a reference point.\n\n    >>> import matplotlib.pyplot as plt\n    >>> import scipy.stats as stats\n    >>> from scipy.stats import kurtosis\n\n    >>> x = np.linspace(-5, 5, 100)\n    >>> ax = plt.subplot()\n    >>> distnames = ['laplace', 'norm', 'uniform']\n\n    >>> for distname in distnames:\n    ...     if distname == 'uniform':\n    ...         dist = getattr(stats, distname)(loc=-2, scale=4)\n    ...     else:\n    ...         dist = getattr(stats, distname)\n    ...     data = dist.rvs(size=1000)\n    ...     kur = kurtosis(data, fisher=True)\n    ...     y = dist.pdf(x)\n    ...     ax.plot(x, y, label=\"{}, {}\".format(distname, round(kur, 3)))\n    ...     ax.legend()\n\n    The Laplace distribution has a heavier tail than the normal distribution.\n    The uniform distribution (which has negative kurtosis) has the thinnest\n    tail.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 1474, "code": "def describe(a, axis=0, ddof=1, bias=True, nan_policy='propagate'):\n    xp = array_namespace(a)\n    a, axis = _chk_asarray(a, axis, xp=xp)\n    contains_nan = _contains_nan(a, nan_policy)\n    if nan_policy == 'omit' and contains_nan:\n        a = ma.masked_invalid(a)\n        return mstats_basic.describe(a, axis, ddof, bias)\n    if xp_size(a) == 0:\n        raise ValueError(\"The input must not be empty.\")\n    n = xp.asarray(_length_nonmasked(a, axis, xp=xp), dtype=xp.int64,\n                   device=xp_device(a))", "documentation": "    \"\"\"Compute several descriptive statistics of the passed array.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n    axis : int or None, optional\n        Axis along which statistics are calculated. Default is 0.\n        If None, compute over the whole array `a`.\n    ddof : int, optional\n        Delta degrees of freedom (only for variance).  Default is 1.\n    bias : bool, optional\n        If False, then the skewness and kurtosis calculations are corrected\n        for statistical bias.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    nobs : int or ndarray of ints\n        Number of observations (length of data along `axis`).\n        When 'omit' is chosen as nan_policy, the length along each axis\n        slice is counted separately.\n    minmax: tuple of ndarrays or floats\n        Minimum and maximum value of `a` along the given axis.\n    mean : ndarray or float\n        Arithmetic mean of `a` along the given axis.\n    variance : ndarray or float\n        Unbiased variance of `a` along the given axis; denominator is number\n        of observations minus one.\n    skewness : ndarray or float\n        Skewness of `a` along the given axis, based on moment calculations\n        with denominator equal to the number of observations, i.e. no degrees\n        of freedom correction.\n    kurtosis : ndarray or float\n        Kurtosis (Fisher) of `a` along the given axis.  The kurtosis is\n        normalized so that it is zero for the normal distribution.  No\n        degrees of freedom are used.\n\n    Raises\n    ------\n    ValueError\n        If size of `a` is 0.\n\n    See Also\n    --------\n    skew, kurtosis\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> a = np.arange(10)\n    >>> stats.describe(a)\n    DescribeResult(nobs=10, minmax=(0, 9), mean=4.5,\n                   variance=9.166666666666666, skewness=0.0,\n                   kurtosis=-1.2242424242424244)\n    >>> b = [[1, 2], [3, 4]]\n    >>> stats.describe(b)\n    DescribeResult(nobs=2, minmax=(array([1, 2]), array([3, 4])),\n                   mean=array([2., 3.]), variance=array([2., 2.]),\n                   skewness=array([0., 0.]), kurtosis=array([-2., -2.]))\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 1576, "code": "def _get_pvalue(statistic, distribution, alternative, symmetric=True, xp=None):\n    xp = array_namespace(statistic) if xp is None else xp\n    if alternative == 'less':\n        pvalue = distribution.cdf(statistic)\n    elif alternative == 'greater':\n        pvalue = distribution.sf(statistic)\n    elif alternative == 'two-sided':\n        pvalue = 2 * (distribution.sf(xp.abs(statistic)) if symmetric\n                      else xp.minimum(distribution.cdf(statistic),\n                                      distribution.sf(statistic)))\n    else:", "documentation": "    \"\"\"Get p-value given the statistic, (continuous) distribution, and alternative\"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2103, "code": "def percentileofscore(a, score, kind='rank', nan_policy='propagate'):\n    a = np.asarray(a)\n    score = np.asarray(score)\n    if a.ndim != 1:\n        raise ValueError(\"`a` must be 1-dimensional.\")\n    n = len(a)\n    cna = _contains_nan(a, nan_policy)\n    cns = _contains_nan(score, nan_policy)\n    if cns:\n        score = ma.masked_where(np.isnan(score), score)\n    if cna:", "documentation": "    \"\"\"Compute the percentile rank of a score relative to a list of scores.\n\n    A `percentileofscore` of, for example, 80% means that 80% of the\n    scores in `a` are below the given score. In the case of gaps or\n    ties, the exact definition depends on the optional keyword, `kind`.\n\n    Parameters\n    ----------\n    a : array_like\n        A 1-D array to which `score` is compared.\n    score : float or array_like\n        A float score or array of scores for which to compute the percentile(s).\n    kind : {'rank', 'weak', 'strict', 'mean'}, optional\n        Specifies the interpretation of the resulting score.\n        The following options are available (default is 'rank'):\n\n        * 'rank': Average percentage ranking of score.  In case of multiple\n          matches, average the percentage rankings of all matching scores.\n        * 'weak': This kind corresponds to the definition of a cumulative\n          distribution function.  A percentileofscore of 80% means that 80%\n          of values are less than or equal to the provided score.\n        * 'strict': Similar to \"weak\", except that only values that are\n          strictly less than the given score are counted.\n        * 'mean': The average of the \"weak\" and \"strict\" scores, often used\n          in testing.  See https://en.wikipedia.org/wiki/Percentile_rank\n\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Specifies how to treat `nan` values in `a`.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan (for each value in `score`).\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    pcos : float or array-like\n        Percentile-position(s) of `score` (0-100) relative to `a`.\n\n    See Also\n    --------\n    numpy.percentile\n    scipy.stats.scoreatpercentile, scipy.stats.rankdata\n\n    Examples\n    --------\n    Three-quarters of the given values lie below a given score:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> stats.percentileofscore([1, 2, 3, 4], 3)\n    75.0\n\n    With multiple matches, note how the scores of the two matches, 0.6\n    and 0.8 respectively, are averaged:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3)\n    70.0\n\n    Only 2/5 values are strictly less than 3:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='strict')\n    40.0\n\n    But 4/5 values are less than or equal to 3:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='weak')\n    80.0\n\n    The average between the weak and the strict scores is:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='mean')\n    60.0\n\n    Score arrays (of any dimensionality) are supported:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], [2, 3])\n    array([40., 70.])\n\n    The inputs can be infinite:\n\n    >>> stats.percentileofscore([-np.inf, 0, 1, np.inf], [1, 2, np.inf])\n    array([75., 75., 100.])\n\n    If `a` is empty, then the resulting percentiles are all `nan`:\n\n    >>> stats.percentileofscore([], [1, 2])\n    array([nan, nan])\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2346, "code": "def cumfreq(a, numbins=10, defaultreallimits=None, weights=None):\n    h, l, b, e = _histogram(a, numbins, defaultreallimits, weights=weights)\n    cumhist = np.cumsum(h * 1, axis=0)\n    return CumfreqResult(cumhist, l, b, e)\nRelfreqResult = namedtuple('RelfreqResult',\n                           ('frequency', 'lowerlimit', 'binsize',\n                            'extrapoints'))\n@xp_capabilities(np_only=True)", "documentation": "    \"\"\"Return a cumulative frequency histogram, using the histogram function.\n\n    A cumulative histogram is a mapping that counts the cumulative number of\n    observations in all of the bins up to the specified bin.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    numbins : int, optional\n        The number of bins to use for the histogram. Default is 10.\n    defaultreallimits : tuple (lower, upper), optional\n        The lower and upper values for the range of the histogram.\n        If no value is given, a range slightly larger than the range of the\n        values in `a` is used. Specifically ``(a.min() - s, a.max() + s)``,\n        where ``s = (1/2)(a.max() - a.min()) / (numbins - 1)``.\n    weights : array_like, optional\n        The weights for each value in `a`. Default is None, which gives each\n        value a weight of 1.0\n\n    Returns\n    -------\n    cumcount : ndarray\n        Binned values of cumulative frequency.\n    lowerlimit : float\n        Lower real limit\n    binsize : float\n        Width of each bin.\n    extrapoints : int\n        Extra points.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> x = [1, 4, 2, 1, 3, 1]\n    >>> res = stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))\n    >>> res.cumcount\n    array([ 1.,  2.,  3.,  3.])\n    >>> res.extrapoints\n    3\n\n    Create a normal distribution with 1000 random values\n\n    >>> samples = stats.norm.rvs(size=1000, random_state=rng)\n\n    Calculate cumulative frequencies\n\n    >>> res = stats.cumfreq(samples, numbins=25)\n\n    Calculate space of values for x\n\n    >>> x = res.lowerlimit + np.linspace(0, res.binsize*res.cumcount.size,\n    ...                                  res.cumcount.size + 1)\n\n    Plot histogram and cumulative histogram\n\n    >>> fig = plt.figure(figsize=(10, 4))\n    >>> ax1 = fig.add_subplot(1, 2, 1)\n    >>> ax2 = fig.add_subplot(1, 2, 2)\n    >>> ax1.hist(samples, bins=25)\n    >>> ax1.set_title('Histogram')\n    >>> ax2.bar(x[:-1], res.cumcount, width=res.binsize, align='edge')\n    >>> ax2.set_title('Cumulative histogram')\n    >>> ax2.set_xlim([x.min(), x.max()])\n\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2429, "code": "def relfreq(a, numbins=10, defaultreallimits=None, weights=None):\n    a = np.asanyarray(a)\n    h, l, b, e = _histogram(a, numbins, defaultreallimits, weights=weights)\n    h = h / a.shape[0]\n    return RelfreqResult(h, l, b, e)\n@xp_capabilities(np_only=True)", "documentation": "    \"\"\"Return a relative frequency histogram, using the histogram function.\n\n    A relative frequency  histogram is a mapping of the number of\n    observations in each of the bins relative to the total of observations.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    numbins : int, optional\n        The number of bins to use for the histogram. Default is 10.\n    defaultreallimits : tuple (lower, upper), optional\n        The lower and upper values for the range of the histogram.\n        If no value is given, a range slightly larger than the range of the\n        values in a is used. Specifically ``(a.min() - s, a.max() + s)``,\n        where ``s = (1/2)(a.max() - a.min()) / (numbins - 1)``.\n    weights : array_like, optional\n        The weights for each value in `a`. Default is None, which gives each\n        value a weight of 1.0\n\n    Returns\n    -------\n    frequency : ndarray\n        Binned values of relative frequency.\n    lowerlimit : float\n        Lower real limit.\n    binsize : float\n        Width of each bin.\n    extrapoints : int\n        Extra points.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> a = np.array([2, 4, 1, 2, 3, 2])\n    >>> res = stats.relfreq(a, numbins=4)\n    >>> res.frequency\n    array([ 0.16666667, 0.5       , 0.16666667,  0.16666667])\n    >>> np.sum(res.frequency)  # relative frequencies should add up to 1\n    1.0\n\n    Create a normal distribution with 1000 random values\n\n    >>> samples = stats.norm.rvs(size=1000, random_state=rng)\n\n    Calculate relative frequencies\n\n    >>> res = stats.relfreq(samples, numbins=25)\n\n    Calculate space of values for x\n\n    >>> x = res.lowerlimit + np.linspace(0, res.binsize*res.frequency.size,\n    ...                                  res.frequency.size)\n\n    Plot relative frequency histogram\n\n    >>> fig = plt.figure(figsize=(5, 4))\n    >>> ax = fig.add_subplot(1, 1, 1)\n    >>> ax.bar(x, res.frequency, width=res.binsize)\n    >>> ax.set_title('Relative frequency histogram')\n    >>> ax.set_xlim([x.min(), x.max()])\n\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2510, "code": "def obrientransform(*samples):\n    TINY = np.sqrt(np.finfo(float).eps)\n    arrays = []\n    sLast = None\n    for sample in samples:\n        a = np.asarray(sample)\n        n = len(a)\n        mu = np.mean(a)\n        sq = (a - mu)**2\n        sumsq = sq.sum()\n        t = ((n - 1.5) * n * sq - 0.5 * sumsq) / ((n - 1) * (n - 2))", "documentation": "    \"\"\"Compute the O'Brien transform on input data (any number of arrays).\n\n    Used to test for homogeneity of variance prior to running one-way stats.\n    Each array in ``*samples`` is one level of a factor.\n    If `f_oneway` is run on the transformed data and found significant,\n    the variances are unequal.  From Maxwell and Delaney [1]_, p.112.\n\n    Parameters\n    ----------\n    sample1, sample2, ... : array_like\n        Any number of arrays.\n\n    Returns\n    -------\n    obrientransform : ndarray\n        Transformed data for use in an ANOVA.  The first dimension\n        of the result corresponds to the sequence of transformed\n        arrays.  If the arrays given are all 1-D of the same length,\n        the return value is a 2-D array; otherwise it is a 1-D array\n        of type object, with each element being an ndarray.\n\n    Raises\n    ------\n    ValueError\n        If the mean of the transformed data is not equal to the original\n        variance, indicating a lack of convergence in the O'Brien transform.\n\n    References\n    ----------\n    .. [1] S. E. Maxwell and H. D. Delaney, \"Designing Experiments and\n           Analyzing Data: A Model Comparison Perspective\", Wadsworth, 1990.\n\n    Examples\n    --------\n    We'll test the following data sets for differences in their variance.\n\n    >>> x = [10, 11, 13, 9, 7, 12, 12, 9, 10]\n    >>> y = [13, 21, 5, 10, 8, 14, 10, 12, 7, 15]\n\n    Apply the O'Brien transform to the data.\n\n    >>> from scipy.stats import obrientransform\n    >>> tx, ty = obrientransform(x, y)\n\n    Use `scipy.stats.f_oneway` to apply a one-way ANOVA test to the\n    transformed data.\n\n    >>> from scipy.stats import f_oneway\n    >>> F, p = f_oneway(tx, ty)\n    >>> p\n    0.1314139477040335\n\n    If we require that ``p < 0.05`` for significance, we cannot conclude\n    that the variances are different.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2603, "code": "def sem(a, axis=0, ddof=1, nan_policy='propagate'):\n    xp = array_namespace(a)\n    if axis is None:\n        a = xp.reshape(a, (-1,))\n        axis = 0\n    a = xpx.atleast_nd(xp.asarray(a), ndim=1, xp=xp)\n    n = _length_nonmasked(a, axis, xp=xp)\n    s = xp.std(a, axis=axis, correction=ddof) / n**0.5\n    return s", "documentation": "    \"\"\"Compute standard error of the mean.\n\n    Calculate the standard error of the mean (or standard error of\n    measurement) of the values in the input array.\n\n    Parameters\n    ----------\n    a : array_like\n        An array containing the values for which the standard error is\n        returned. Must contain at least two observations.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over\n        the whole array `a`.\n    ddof : int, optional\n        Delta degrees-of-freedom. How many degrees of freedom to adjust\n        for bias in limited samples relative to the population estimate\n        of variance. Defaults to 1.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    s : ndarray or float\n        The standard error of the mean in the sample(s), along the input axis.\n\n    Notes\n    -----\n    The default value for `ddof` is different to the default (0) used by other\n    ddof containing routines, such as np.std and np.nanstd.\n\n    Examples\n    --------\n    Find standard error along the first axis:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> a = np.arange(20).reshape(5,4)\n    >>> stats.sem(a)\n    array([ 2.8284,  2.8284,  2.8284,  2.8284])\n\n    Find standard error across the whole array, using n degrees of freedom:\n\n    >>> stats.sem(a, axis=None, ddof=0)\n    1.2893796958227628\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2665, "code": "def _isconst(x):\n    y = x[~np.isnan(x)]\n    if y.size == 0:\n        return np.array([True])\n    else:\n        return (y[0] == y).all(keepdims=True)\n@xp_capabilities()", "documentation": "    \"\"\"\n    Check if all values in x are the same.  nans are ignored.\n\n    x must be a 1d array.\n\n    The return value is a 1d array with length 1, so it can be used\n    in np.apply_along_axis.\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2682, "code": "def zscore(a, axis=0, ddof=0, nan_policy='propagate'):\n    return zmap(a, a, axis=axis, ddof=ddof, nan_policy=nan_policy)\n@xp_capabilities()", "documentation": "    \"\"\"\n    Compute the z score.\n\n    Compute the z score of each value in the sample, relative to the\n    sample mean and standard deviation.\n\n    Parameters\n    ----------\n    a : array_like\n        An array like object containing the sample data.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over\n        the whole array `a`.\n    ddof : int, optional\n        Degrees of freedom correction in the calculation of the\n        standard deviation. Default is 0.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan. 'propagate' returns nan,\n        'raise' throws an error, 'omit' performs the calculations ignoring nan\n        values. Default is 'propagate'.  Note that when the value is 'omit',\n        nans in the input also propagate to the output, but they do not affect\n        the z-scores computed for the non-nan values.\n\n    Returns\n    -------\n    zscore : array_like\n        The z-scores, standardized by mean and standard deviation of\n        input array `a`.\n\n    See Also\n    --------\n    numpy.mean : Arithmetic average\n    numpy.std : Arithmetic standard deviation\n    scipy.stats.gzscore : Geometric standard score\n\n    Notes\n    -----\n    This function preserves ndarray subclasses, and works also with\n    matrices and masked arrays (it uses `asanyarray` instead of\n    `asarray` for parameters).\n\n    References\n    ----------\n    .. [1] \"Standard score\", *Wikipedia*,\n           https://en.wikipedia.org/wiki/Standard_score.\n    .. [2] Huck, S. W., Cross, T. L., Clark, S. B, \"Overcoming misconceptions\n           about Z-scores\", Teaching Statistics, vol. 8, pp. 38-40, 1986\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> a = np.array([ 0.7972,  0.0767,  0.4383,  0.7866,  0.8091,\n    ...                0.1954,  0.6307,  0.6599,  0.1065,  0.0508])\n    >>> from scipy import stats\n    >>> stats.zscore(a)\n    array([ 1.1273, -1.247 , -0.0552,  1.0923,  1.1664, -0.8559,  0.5786,\n            0.6748, -1.1488, -1.3324])\n\n    Computing along a specified axis, using n-1 degrees of freedom\n    (``ddof=1``) to calculate the standard deviation:\n\n    >>> b = np.array([[ 0.3148,  0.0478,  0.6243,  0.4608],\n    ...               [ 0.7149,  0.0775,  0.6072,  0.9656],\n    ...               [ 0.6341,  0.1403,  0.9759,  0.4064],\n    ...               [ 0.5918,  0.6948,  0.904 ,  0.3721],\n    ...               [ 0.0921,  0.2481,  0.1188,  0.1366]])\n    >>> stats.zscore(b, axis=1, ddof=1)\n    array([[-0.19264823, -1.28415119,  1.07259584,  0.40420358],\n           [ 0.33048416, -1.37380874,  0.04251374,  1.00081084],\n           [ 0.26796377, -1.12598418,  1.23283094, -0.37481053],\n           [-0.22095197,  0.24468594,  1.19042819, -1.21416216],\n           [-0.82780366,  1.4457416 , -0.43867764, -0.1792603 ]])\n\n    An example with ``nan_policy='omit'``:\n\n    >>> x = np.array([[25.11, 30.10, np.nan, 32.02, 43.15],\n    ...               [14.95, 16.06, 121.25, 94.35, 29.81]])\n    >>> stats.zscore(x, axis=1, nan_policy='omit')\n    array([[-1.13490897, -0.37830299,         nan, -0.08718406,  1.60039602],\n           [-0.91611681, -0.89090508,  1.4983032 ,  0.88731639, -0.5785977 ]])\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2768, "code": "def gzscore(a, *, axis=0, ddof=0, nan_policy='propagate'):\n    xp = array_namespace(a)\n    a = xp_promote(a, force_floating=True, xp=xp)\n    log = ma.log if isinstance(a, ma.MaskedArray) else xp.log\n    return zscore(log(a), axis=axis, ddof=ddof, nan_policy=nan_policy)\n@xp_capabilities()", "documentation": "    \"\"\"\n    Compute the geometric standard score.\n\n    Compute the geometric z score of each strictly positive value in the\n    sample, relative to the geometric mean and standard deviation.\n    Mathematically the geometric z score can be evaluated as::\n\n        gzscore = log(a/gmu) / log(gsigma)\n\n    where ``gmu`` (resp. ``gsigma``) is the geometric mean (resp. standard\n    deviation).\n\n    Parameters\n    ----------\n    a : array_like\n        Sample data.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over\n        the whole array `a`.\n    ddof : int, optional\n        Degrees of freedom correction in the calculation of the\n        standard deviation. Default is 0.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan. 'propagate' returns nan,\n        'raise' throws an error, 'omit' performs the calculations ignoring nan\n        values. Default is 'propagate'.  Note that when the value is 'omit',\n        nans in the input also propagate to the output, but they do not affect\n        the geometric z scores computed for the non-nan values.\n\n    Returns\n    -------\n    gzscore : array_like\n        The geometric z scores, standardized by geometric mean and geometric\n        standard deviation of input array `a`.\n\n    See Also\n    --------\n    gmean : Geometric mean\n    gstd : Geometric standard deviation\n    zscore : Standard score\n\n    Notes\n    -----\n    This function preserves ndarray subclasses, and works also with\n    matrices and masked arrays (it uses ``asanyarray`` instead of\n    ``asarray`` for parameters).\n\n    .. versionadded:: 1.8\n\n    References\n    ----------\n    .. [1] \"Geometric standard score\", *Wikipedia*,\n           https://en.wikipedia.org/wiki/Geometric_standard_deviation#Geometric_standard_score.\n\n    Examples\n    --------\n    Draw samples from a log-normal distribution:\n\n    >>> import numpy as np\n    >>> from scipy.stats import zscore, gzscore\n    >>> import matplotlib.pyplot as plt\n\n    >>> rng = np.random.default_rng()\n    >>> mu, sigma = 3., 1.  # mean and standard deviation\n    >>> x = rng.lognormal(mu, sigma, size=500)\n\n    Display the histogram of the samples:\n\n    >>> fig, ax = plt.subplots()\n    >>> ax.hist(x, 50)\n    >>> plt.show()\n\n    Display the histogram of the samples standardized by the classical zscore.\n    Distribution is rescaled but its shape is unchanged.\n\n    >>> fig, ax = plt.subplots()\n    >>> ax.hist(zscore(x), 50)\n    >>> plt.show()\n\n    Demonstrate that the distribution of geometric zscores is rescaled and\n    quasinormal:\n\n    >>> fig, ax = plt.subplots()\n    >>> ax.hist(gzscore(x), 50)\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2863, "code": "def zmap(scores, compare, axis=0, ddof=0, nan_policy='propagate'):\n    like_zscore = (scores is compare)\n    xp = array_namespace(scores, compare)\n    scores, compare = xp_promote(scores, compare, force_floating=True, xp=xp)\n    with warnings.catch_warnings():\n        if like_zscore:  # zscore should not emit SmallSampleWarning\n            warnings.simplefilter('ignore', SmallSampleWarning)\n        mn = _xp_mean(compare, axis=axis, keepdims=True, nan_policy=nan_policy)\n        std = _xp_var(compare, axis=axis, correction=ddof,\n                      keepdims=True, nan_policy=nan_policy)**0.5\n    with np.errstate(invalid='ignore', divide='ignore'):", "documentation": "    \"\"\"\n    Calculate the relative z-scores.\n\n    Return an array of z-scores, i.e., scores that are standardized to\n    zero mean and unit variance, where mean and variance are calculated\n    from the comparison array.\n\n    Parameters\n    ----------\n    scores : array_like\n        The input for which z-scores are calculated.\n    compare : array_like\n        The input from which the mean and standard deviation of the\n        normalization are taken; assumed to have the same dimension as\n        `scores`.\n    axis : int or None, optional\n        Axis over which mean and variance of `compare` are calculated.\n        Default is 0. If None, compute over the whole array `scores`.\n    ddof : int, optional\n        Degrees of freedom correction in the calculation of the\n        standard deviation. Default is 0.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle the occurrence of nans in `compare`.\n        'propagate' returns nan, 'raise' raises an exception, 'omit'\n        performs the calculations ignoring nan values. Default is\n        'propagate'. Note that when the value is 'omit', nans in `scores`\n        also propagate to the output, but they do not affect the z-scores\n        computed for the non-nan values.\n\n    Returns\n    -------\n    zscore : array_like\n        Z-scores, in the same shape as `scores`.\n\n    Notes\n    -----\n    This function preserves ndarray subclasses, and works also with\n    matrices and masked arrays (it uses `asanyarray` instead of\n    `asarray` for parameters).\n\n    Examples\n    --------\n    >>> from scipy.stats import zmap\n    >>> a = [0.5, 2.0, 2.5, 3]\n    >>> b = [0, 1, 2, 3, 4]\n    >>> zmap(a, b)\n    array([-1.06066017,  0.        ,  0.35355339,  0.70710678])\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 3387, "code": "def sigmaclip(a, low=4., high=4., *, nan_policy='propagate'):\n    xp = array_namespace(a)\n    c = xp_ravel(xp.asarray(a))\n    contains_nan = _contains_nan(c, nan_policy, xp_omit_okay=True)\n    if contains_nan:\n        if nan_policy == 'propagate':\n            NaN = _get_nan(c, xp=xp)\n            clipped = xp.empty_like(c[0:0])\n            return SigmaclipResult(clipped, NaN, NaN)\n        elif nan_policy == 'omit':\n            c = c[~xp.isnan(c)]", "documentation": "    \"\"\"Perform iterative sigma-clipping of array elements.\n\n    Starting from the full sample, all elements outside the critical range are\n    removed, i.e. all elements of the input array `c` that satisfy either of\n    the following conditions::\n\n        c < mean(c) - std(c)*low\n        c > mean(c) + std(c)*high\n\n    The iteration continues with the updated sample until no\n    elements are outside the (updated) range.\n\n    Parameters\n    ----------\n    a : array_like\n        Data array, will be raveled if not 1-D.\n    low : float, optional\n        Lower bound factor of sigma clipping. Default is 4.\n    high : float, optional\n        Upper bound factor of sigma clipping. Default is 4.\n    nan_policy : {'propagate', 'omit', 'raise'}\n        Defines how to handle input NaNs.\n\n        - ``propagate``: if a NaN is present in the input, the clipped array will be\n          empty, and the upper and lower thresholds will be NaN.\n        - ``omit``: NaNs will be omitted when performing the calculation.\n        - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n\n    Returns\n    -------\n    clipped : ndarray\n        Input array with clipped elements removed.\n    lower : float\n        Lower threshold value use for clipping.\n    upper : float\n        Upper threshold value use for clipping.\n\n    Notes\n    -----\n    This function iteratively *removes* observations. Once observations are\n    removed, they are not re-added in subsequent iterations. Consequently,\n    although it is often the case that ``clipped`` is identical to\n    ``a[(a >= lower) & (a <= upper)]``, this property is not guaranteed to be\n    satisfied; ``clipped`` may have fewer elements.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import sigmaclip\n    >>> a = np.concatenate((np.linspace(9.5, 10.5, 31),\n    ...                     np.linspace(0, 20, 5)))\n    >>> fact = 1.5\n    >>> c, low, upp = sigmaclip(a, fact, fact)\n    >>> c\n    array([  9.96666667,  10.        ,  10.03333333,  10.        ])\n    >>> c.var(), c.std()\n    (0.00055555555555555165, 0.023570226039551501)\n    >>> low, c.mean() - fact*c.std(), c.min()\n    (9.9646446609406727, 9.9646446609406727, 9.9666666666666668)\n    >>> upp, c.mean() + fact*c.std(), c.max()\n    (10.035355339059327, 10.035355339059327, 10.033333333333333)\n\n    >>> a = np.concatenate((np.linspace(9.5, 10.5, 11),\n    ...                     np.linspace(-100, -50, 3)))\n    >>> c, low, upp = sigmaclip(a, 1.8, 1.8)\n    >>> (c == np.linspace(9.5, 10.5, 11)).all()\n    True\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 3482, "code": "def trimboth(a, proportiontocut, axis=0):\n    a = np.asarray(a)\n    if a.size == 0:\n        return a\n    if axis is None:\n        a = a.ravel()\n        axis = 0\n    nobs = a.shape[axis]\n    lowercut = int(proportiontocut * nobs)\n    uppercut = nobs - lowercut\n    if (lowercut >= uppercut):", "documentation": "    \"\"\"Slice off a proportion of items from both ends of an array.\n\n    Slice off the passed proportion of items from both ends of the passed\n    array (i.e., with `proportiontocut` = 0.1, slices leftmost 10% **and**\n    rightmost 10% of scores). The trimmed values are the lowest and\n    highest ones.\n    Slice off less if proportion results in a non-integer slice index (i.e.\n    conservatively slices off `proportiontocut`).\n\n    Parameters\n    ----------\n    a : array_like\n        Data to trim.\n    proportiontocut : float\n        Proportion (in range 0-1) of total data set to trim of each end.\n    axis : int or None, optional\n        Axis along which to trim data. Default is 0. If None, compute over\n        the whole array `a`.\n\n    Returns\n    -------\n    out : ndarray\n        Trimmed version of array `a`. The order of the trimmed content\n        is undefined.\n\n    See Also\n    --------\n    trim_mean\n\n    Examples\n    --------\n    Create an array of 10 values and trim 10% of those values from each end:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n    >>> stats.trimboth(a, 0.1)\n    array([1, 3, 2, 4, 5, 6, 7, 8])\n\n    Note that the elements of the input array are trimmed by value, but the\n    output array is not necessarily sorted.\n\n    The proportion to trim is rounded down to the nearest integer. For\n    instance, trimming 25% of the values from each end of an array of 10\n    values will return an array of 6 values:\n\n    >>> b = np.arange(10)\n    >>> stats.trimboth(b, 1/4).shape\n    (6,)\n\n    Multidimensional arrays can be trimmed along any axis or across the entire\n    array:\n\n    >>> c = [2, 4, 6, 8, 0, 1, 3, 5, 7, 9]\n    >>> d = np.array([a, b, c])\n    >>> stats.trimboth(d, 0.4, axis=0).shape\n    (1, 10)\n    >>> stats.trimboth(d, 0.4, axis=1).shape\n    (3, 2)\n    >>> stats.trimboth(d, 0.4, axis=None).shape\n    (6,)\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 3569, "code": "def trim1(a, proportiontocut, tail='right', axis=0):\n    a = np.asarray(a)\n    if axis is None:\n        a = a.ravel()\n        axis = 0\n    nobs = a.shape[axis]\n    if proportiontocut >= 1:\n        return []\n    if tail.lower() == 'right':\n        lowercut = 0\n        uppercut = nobs - int(proportiontocut * nobs)", "documentation": "    \"\"\"Slice off a proportion from ONE end of the passed array distribution.\n\n    If `proportiontocut` = 0.1, slices off 'leftmost' or 'rightmost'\n    10% of scores. The lowest or highest values are trimmed (depending on\n    the tail).\n    Slice off less if proportion results in a non-integer slice index\n    (i.e. conservatively slices off `proportiontocut` ).\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    proportiontocut : float\n        Fraction to cut off of 'left' or 'right' of distribution.\n    tail : {'left', 'right'}, optional\n        Defaults to 'right'.\n    axis : int or None, optional\n        Axis along which to trim data. Default is 0. If None, compute over\n        the whole array `a`.\n\n    Returns\n    -------\n    trim1 : ndarray\n        Trimmed version of array `a`. The order of the trimmed content is\n        undefined.\n\n    Examples\n    --------\n    Create an array of 10 values and trim 20% of its lowest values:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n    >>> stats.trim1(a, 0.2, 'left')\n    array([2, 4, 3, 5, 6, 7, 8, 9])\n\n    Note that the elements of the input array are trimmed by value, but the\n    output array is not necessarily sorted.\n\n    The proportion to trim is rounded down to the nearest integer. For\n    instance, trimming 25% of the values from an array of 10 values will\n    return an array of 8 values:\n\n    >>> b = np.arange(10)\n    >>> stats.trim1(b, 1/4).shape\n    (8,)\n\n    Multidimensional arrays can be trimmed along any axis or across the entire\n    array:\n\n    >>> c = [2, 4, 6, 8, 0, 1, 3, 5, 7, 9]\n    >>> d = np.array([a, b, c])\n    >>> stats.trim1(d, 0.8, axis=0).shape\n    (1, 10)\n    >>> stats.trim1(d, 0.8, axis=1).shape\n    (3, 2)\n    >>> stats.trim1(d, 0.8, axis=None).shape\n    (6,)\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 3658, "code": "def trim_mean(a, proportiontocut, axis=0):\n    xp = array_namespace(a)\n    a = xp.asarray(a)\n    if xp_size(a) == 0:\n        return _get_nan(a, xp=xp)\n    if axis is None:\n        a = xp_ravel(a)\n        axis = 0\n    nobs = a.shape[axis]\n    lowercut = int(proportiontocut * nobs)\n    uppercut = nobs - lowercut", "documentation": "    \"\"\"Return mean of array after trimming a specified fraction of extreme values\n\n    Removes the specified proportion of elements from *each* end of the\n    sorted array, then computes the mean of the remaining elements.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    proportiontocut : float\n        Fraction of the most positive and most negative elements to remove.\n        When the specified proportion does not result in an integer number of\n        elements, the number of elements to trim is rounded down.\n    axis : int or None, default: 0\n        Axis along which the trimmed means are computed.\n        If None, compute over the raveled array.\n\n    Returns\n    -------\n    trim_mean : ndarray\n        Mean of trimmed array.\n\n    See Also\n    --------\n    trimboth : Remove a proportion of elements from each end of an array.\n    tmean : Compute the mean after trimming values outside specified limits.\n\n    Notes\n    -----\n    For 1-D array `a`, `trim_mean` is approximately equivalent to the following\n    calculation::\n\n        import numpy as np\n        a = np.sort(a)\n        m = int(proportiontocut * len(a))\n        np.mean(a[m: len(a) - m])\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = [1, 2, 3, 5]\n    >>> stats.trim_mean(x, 0.25)\n    2.5\n\n    When the specified proportion does not result in an integer number of\n    elements, the number of elements to trim is rounded down.\n\n    >>> stats.trim_mean(x, 0.24999) == np.mean(x)\n    True\n\n    Use `axis` to specify the axis along which the calculation is performed.\n\n    >>> x2 = [[1, 2, 3, 5],\n    ...       [10, 20, 30, 50]]\n    >>> stats.trim_mean(x2, 0.25)\n    array([ 5.5, 11. , 16.5, 27.5])\n    >>> stats.trim_mean(x2, 0.25, axis=1)\n    array([ 2.5, 25. ])\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 3773, "code": "def f_oneway(*samples, axis=0, equal_var=True):\n    xp = array_namespace(*samples)\n    samples = xp_promote(*samples, force_floating=True, xp=xp)\n    if len(samples) < 2:\n        raise TypeError('at least two inputs are required;'\n                        f' got {len(samples)}.')\n    num_groups = len(samples)\n    alldata = xp.concat(samples, axis=-1)\n    bign = _length_nonmasked(alldata, axis=-1, xp=xp)\n    if _f_oneway_is_too_small(samples):\n        NaN = _get_nan(*samples, xp=xp)", "documentation": "    \"\"\"Perform one-way ANOVA.\n\n    The one-way ANOVA tests the null hypothesis that two or more groups have\n    the same population mean.  The test is applied to samples from two or\n    more groups, possibly with differing sizes.\n\n    Parameters\n    ----------\n    sample1, sample2, ... : array_like\n        The sample measurements for each group.  There must be at least\n        two arguments.  If the arrays are multidimensional, then all the\n        dimensions of the array must be the same except for `axis`.\n    axis : int, optional\n        Axis of the input arrays along which the test is applied.\n        Default is 0.\n    equal_var : bool, optional\n        If True (default), perform a standard one-way ANOVA test that\n        assumes equal population variances [2]_.\n        If False, perform Welch's ANOVA test, which does not assume\n        equal population variances [4]_.\n\n        .. versionadded:: 1.16.0\n\n    Returns\n    -------\n    statistic : float\n        The computed F statistic of the test.\n    pvalue : float\n        The associated p-value from the F distribution.\n\n    Warns\n    -----\n    `~scipy.stats.ConstantInputWarning`\n        Emitted if all values within each of the input arrays are identical.\n        In this case the F statistic is either infinite or isn't defined,\n        so ``np.inf`` or ``np.nan`` is returned.\n\n    RuntimeWarning\n        Emitted if the length of any input array is 0, or if all the input\n        arrays have length 1.  ``np.nan`` is returned for the F statistic\n        and the p-value in these cases.\n\n    Notes\n    -----\n    The ANOVA test has important assumptions that must be satisfied in order\n    for the associated p-value to be valid.\n\n    1. The samples are independent.\n    2. Each sample is from a normally distributed population.\n    3. The population standard deviations of the groups are all equal.  This\n       property is known as homoscedasticity.\n\n    If these assumptions are not true for a given set of data, it may still\n    be possible to use the Kruskal-Wallis H-test (`scipy.stats.kruskal`) or\n    the Alexander-Govern test (`scipy.stats.alexandergovern`) although with\n    some loss of power.\n\n    The length of each group must be at least one, and there must be at\n    least one group with length greater than one.  If these conditions\n    are not satisfied, a warning is generated and (``np.nan``, ``np.nan``)\n    is returned.\n\n    If all values in each group are identical, and there exist at least two\n    groups with different values, the function generates a warning and\n    returns (``np.inf``, 0).\n\n    If all values in all groups are the same, function generates a warning\n    and returns (``np.nan``, ``np.nan``).\n\n    The algorithm is from Heiman [2]_, pp.394-7.\n\n    References\n    ----------\n    .. [1] R. Lowry, \"Concepts and Applications of Inferential Statistics\",\n           Chapter 14, 2014, http://vassarstats.net/textbook/\n\n    .. [2] G.W. Heiman, \"Understanding research methods and statistics: An\n           integrated introduction for psychology\", Houghton, Mifflin and\n           Company, 2001.\n\n    .. [3] J.H. McDonald, \"Handbook of Biological Statistics\",\n           One-way ANOVA, 2014.\n           http://www.biostathandbook.com/onewayanova.html\n\n    .. [4] B. L. Welch, \"On the Comparison of Several Mean Values:\n           An Alternative Approach\", Biometrika, vol. 38, no. 3/4,\n           pp. 330-336, 1951. :doi:`10.2307/2332579`.\n\n    .. [5] J.H. McDonald, R. Seed and R.K. Koehn, \"Allozymes and\n           morphometric characters of three species of Mytilus in\n           the Northern and Southern Hemispheres\",\n           Marine Biology, vol. 111, pp. 323-333, 1991.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import f_oneway\n\n    Here are some data [3]_ on a shell measurement (the length of the anterior\n    adductor muscle scar, standardized by dividing by length) in the mussel\n    Mytilus trossulus from five locations: Tillamook, Oregon; Newport, Oregon;\n    Petersburg, Alaska; Magadan, Russia; and Tvarminne, Finland, taken from a\n    much larger data set used in [5]_.\n\n    >>> tillamook = [0.0571, 0.0813, 0.0831, 0.0976, 0.0817, 0.0859, 0.0735,\n    ...              0.0659, 0.0923, 0.0836]\n    >>> newport = [0.0873, 0.0662, 0.0672, 0.0819, 0.0749, 0.0649, 0.0835,\n    ...            0.0725]\n    >>> petersburg = [0.0974, 0.1352, 0.0817, 0.1016, 0.0968, 0.1064, 0.105]\n    >>> magadan = [0.1033, 0.0915, 0.0781, 0.0685, 0.0677, 0.0697, 0.0764,\n    ...            0.0689]\n    >>> tvarminne = [0.0703, 0.1026, 0.0956, 0.0973, 0.1039, 0.1045]\n    >>> f_oneway(tillamook, newport, petersburg, magadan, tvarminne)\n    F_onewayResult(statistic=7.121019471642447, pvalue=0.0002812242314534544)\n\n    `f_oneway` accepts multidimensional input arrays.  When the inputs\n    are multidimensional and `axis` is not given, the test is performed\n    along the first axis of the input arrays.  For the following data, the\n    test is performed three times, once for each column.\n\n    >>> a = np.array([[9.87, 9.03, 6.81],\n    ...               [7.18, 8.35, 7.00],\n    ...               [8.39, 7.58, 7.68],\n    ...               [7.45, 6.33, 9.35],\n    ...               [6.41, 7.10, 9.33],\n    ...               [8.00, 8.24, 8.44]])\n    >>> b = np.array([[6.35, 7.30, 7.16],\n    ...               [6.65, 6.68, 7.63],\n    ...               [5.72, 7.73, 6.72],\n    ...               [7.01, 9.19, 7.41],\n    ...               [7.75, 7.87, 8.30],\n    ...               [6.90, 7.97, 6.97]])\n    >>> c = np.array([[3.31, 8.77, 1.01],\n    ...               [8.25, 3.24, 3.62],\n    ...               [6.32, 8.81, 5.19],\n    ...               [7.48, 8.83, 8.91],\n    ...               [8.59, 6.01, 6.07],\n    ...               [3.07, 9.72, 7.48]])\n    >>> F = f_oneway(a, b, c)\n    >>> F.statistic\n    array([1.75676344, 0.03701228, 3.76439349])\n    >>> F.pvalue\n    array([0.20630784, 0.96375203, 0.04733157])\n\n    Welch ANOVA will be performed if `equal_var` is False.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 4070, "code": "def alexandergovern(*samples, nan_policy='propagate', axis=0):\n    xp = array_namespace(*samples)\n    samples = _alexandergovern_input_validation(samples, nan_policy, axis, xp=xp)\n    lengths = [sample.shape[-1] for sample in samples]\n    means = xp.stack([_xp_mean(sample, axis=-1) for sample in samples])\n    se2 = [(_xp_var(sample, correction=1, axis=-1) / length)\n           for sample, length in zip(samples, lengths)]\n    standard_errors_squared = xp.stack(se2)\n    standard_errors = standard_errors_squared**0.5\n    eps = xp.finfo(standard_errors.dtype).eps\n    zero = standard_errors <= xp.abs(eps * means)", "documentation": "    \"\"\"Performs the Alexander Govern test.\n\n    The Alexander-Govern approximation tests the equality of k independent\n    means in the face of heterogeneity of variance. The test is applied to\n    samples from two or more groups, possibly with differing sizes.\n\n    Parameters\n    ----------\n    sample1, sample2, ... : array_like\n        The sample measurements for each group.  There must be at least\n        two samples, and each sample must contain at least two observations.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    res : AlexanderGovernResult\n        An object with attributes:\n\n        statistic : float\n            The computed A statistic of the test.\n        pvalue : float\n            The associated p-value from the chi-squared distribution.\n\n    Warns\n    -----\n    `~scipy.stats.ConstantInputWarning`\n        Raised if an input is a constant array.  The statistic is not defined\n        in this case, so ``np.nan`` is returned.\n\n    See Also\n    --------\n    f_oneway : one-way ANOVA\n\n    Notes\n    -----\n    The use of this test relies on several assumptions.\n\n    1. The samples are independent.\n    2. Each sample is from a normally distributed population.\n    3. Unlike `f_oneway`, this test does not assume on homoscedasticity,\n       instead relaxing the assumption of equal variances.\n\n    Input samples must be finite, one dimensional, and with size greater than\n    one.\n\n    References\n    ----------\n    .. [1] Alexander, Ralph A., and Diane M. Govern. \"A New and Simpler\n           Approximation for ANOVA under Variance Heterogeneity.\" Journal\n           of Educational Statistics, vol. 19, no. 2, 1994, pp. 91-101.\n           https://www.jstor.org/stable/1165140\n\n    Examples\n    --------\n    >>> from scipy.stats import alexandergovern\n\n    Here are some data on annual percentage rate of interest charged on\n    new car loans at nine of the largest banks in four American cities\n    taken from the National Institute of Standards and Technology's\n    ANOVA dataset.\n\n    We use `alexandergovern` to test the null hypothesis that all cities\n    have the same mean APR against the alternative that the cities do not\n    all have the same mean APR. We decide that a significance level of 5%\n    is required to reject the null hypothesis in favor of the alternative.\n\n    >>> atlanta = [13.75, 13.75, 13.5, 13.5, 13.0, 13.0, 13.0, 12.75, 12.5]\n    >>> chicago = [14.25, 13.0, 12.75, 12.5, 12.5, 12.4, 12.3, 11.9, 11.9]\n    >>> houston = [14.0, 14.0, 13.51, 13.5, 13.5, 13.25, 13.0, 12.5, 12.5]\n    >>> memphis = [15.0, 14.0, 13.75, 13.59, 13.25, 12.97, 12.5, 12.25,\n    ...           11.89]\n    >>> alexandergovern(atlanta, chicago, houston, memphis)\n    AlexanderGovernResult(statistic=4.65087071883494,\n                          pvalue=0.19922132490385214)\n\n    The p-value is 0.1992, indicating a nearly 20% chance of observing\n    such an extreme value of the test statistic under the null hypothesis.\n    This exceeds 5%, so we do not reject the null hypothesis in favor of\n    the alternative.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 4231, "code": "def _pearsonr_fisher_ci(r, n, confidence_level, alternative):\n    xp = array_namespace(r)\n    ones = xp.ones_like(r)\n    n = xp.asarray(n, dtype=r.dtype, device=xp_device(r))\n    confidence_level = xp.asarray(confidence_level, dtype=r.dtype, device=xp_device(r))\n    with np.errstate(divide='ignore', invalid='ignore'):\n        zr = xp.atanh(r)\n        se = xp.sqrt(1 / (n - 3))\n    if alternative == \"two-sided\":\n        h = special.ndtri(0.5 + confidence_level/2)\n        zlo = zr - h*se", "documentation": "    \"\"\"\n    Compute the confidence interval for Pearson's R.\n\n    Fisher's transformation is used to compute the confidence interval\n    (https://en.wikipedia.org/wiki/Fisher_transformation).\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 4279, "code": "def _pearsonr_bootstrap_ci(confidence_level, method, x, y, alternative, axis):", "documentation": "    \"\"\"\n    Compute the confidence interval for Pearson's R using the bootstrap.\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 4301, "code": "class PearsonRResult(PearsonRResultBase):", "documentation": "    \"\"\"\n    Result of `scipy.stats.pearsonr`\n\n    Attributes\n    ----------\n    statistic : float\n        Pearson product-moment correlation coefficient.\n    pvalue : float\n        The p-value associated with the chosen alternative.\n\n    Methods\n    -------\n    confidence_interval\n        Computes the confidence interval of the correlation\n        coefficient `statistic` for the given confidence level.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 4330, "code": "    def confidence_interval(self, confidence_level=0.95, method=None):\n        if isinstance(method, BootstrapMethod):\n            xp = array_namespace(self._x)\n            message = ('`method` must be `None` if `pearsonr` '\n                       'arguments were not NumPy arrays.')\n            if not is_numpy(xp):\n                raise ValueError(message)\n            ci = _pearsonr_bootstrap_ci(confidence_level, method, self._x, self._y,\n                                        self._alternative, self._axis)\n        elif method is None:\n            ci = _pearsonr_fisher_ci(self.statistic, self._n, confidence_level,", "documentation": "        \"\"\"\n        The confidence interval for the correlation coefficient.\n\n        Compute the confidence interval for the correlation coefficient\n        ``statistic`` with the given confidence level.\n\n        If `method` is not provided,\n        The confidence interval is computed using the Fisher transformation\n        F(r) = arctanh(r) [1]_.  When the sample pairs are drawn from a\n        bivariate normal distribution, F(r) approximately follows a normal\n        distribution with standard error ``1/sqrt(n - 3)``, where ``n`` is the\n        length of the original samples along the calculation axis. When\n        ``n <= 3``, this approximation does not yield a finite, real standard\n        error, so we define the confidence interval to be -1 to 1.\n\n        If `method` is an instance of `BootstrapMethod`, the confidence\n        interval is computed using `scipy.stats.bootstrap` with the provided\n        configuration options and other appropriate settings. In some cases,\n        confidence limits may be NaN due to a degenerate resample, and this is\n        typical for very small samples (~6 observations).\n\n        Parameters\n        ----------\n        confidence_level : float\n            The confidence level for the calculation of the correlation\n            coefficient confidence interval. Default is 0.95.\n\n        method : BootstrapMethod, optional\n            Defines the method used to compute the confidence interval. See\n            method description for details.\n\n            .. versionadded:: 1.11.0\n\n        Returns\n        -------\n        ci : namedtuple\n            The confidence interval is returned in a ``namedtuple`` with\n            fields `low` and `high`.\n\n        References\n        ----------\n        .. [1] \"Pearson correlation coefficient\", Wikipedia,\n               https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\n        \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 4815, "code": "def fisher_exact(table, alternative=None, *, method=None):\n    hypergeom = distributions.hypergeom\n    c = np.asarray(table, dtype=np.int64)\n    if not c.ndim == 2:\n        raise ValueError(\"The input `table` must have two dimensions.\")\n    if np.any(c < 0):\n        raise ValueError(\"All values in `table` must be nonnegative.\")\n    if not c.shape == (2, 2) or method is not None:\n        return _fisher_exact_rxc(c, alternative, method)\n    alternative = 'two-sided' if alternative is None else alternative\n    if 0 in c.sum(axis=0) or 0 in c.sum(axis=1):", "documentation": "    \"\"\"Perform a Fisher exact test on a contingency table.\n\n    For a 2x2 table,\n    the null hypothesis is that the true odds ratio of the populations\n    underlying the observations is one, and the observations were sampled\n    from these populations under a condition: the marginals of the\n    resulting table must equal those of the observed table.\n    The statistic is the unconditional maximum likelihood estimate of the odds\n    ratio, and the p-value is the probability under the null hypothesis of\n    obtaining a table at least as extreme as the one that was actually\n    observed.\n\n    For other table sizes, or if `method` is provided, the null hypothesis\n    is that the rows and columns of the tables have fixed sums and are\n    independent; i.e., the table was sampled from a `scipy.stats.random_table`\n    distribution with the observed marginals. The statistic is the\n    probability mass of this distribution evaluated at `table`, and the\n    p-value is the percentage of the population of tables with statistic at\n    least as extreme (small) as that of `table`. There is only one alternative\n    hypothesis available: the rows and columns are not independent.\n\n    There are other possible choices of statistic and two-sided\n    p-value definition associated with Fisher's exact test; please see the\n    Notes for more information.\n\n    Parameters\n    ----------\n    table : array_like of ints\n        A contingency table.  Elements must be non-negative integers.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis for 2x2 tables; unused for other\n        table sizes.\n        The following options are available (default is 'two-sided'):\n\n        * 'two-sided': the odds ratio of the underlying population is not one\n        * 'less': the odds ratio of the underlying population is less than one\n        * 'greater': the odds ratio of the underlying population is greater\n          than one\n\n        See the Notes for more details.\n\n    method : ResamplingMethod, optional\n        Defines the method used to compute the p-value.\n        If `method` is an instance of `PermutationMethod`/`MonteCarloMethod`,\n        the p-value is computed using\n        `scipy.stats.permutation_test`/`scipy.stats.monte_carlo_test` with the\n        provided configuration options and other appropriate settings.\n        Note that if `method` is an instance of `MonteCarloMethod`, the ``rvs``\n        attribute must be left unspecified; Monte Carlo samples are always drawn\n        using the ``rvs`` method of `scipy.stats.random_table`.\n        Otherwise, the p-value is computed as documented in the notes.\n\n        .. versionadded:: 1.15.0\n\n    Returns\n    -------\n    res : SignificanceResult\n        An object containing attributes:\n\n        statistic : float\n            For a 2x2 table with default `method`, this is the odds ratio - the\n            prior odds ratio not a posterior estimate. In all other cases, this\n            is the probability density of obtaining the observed table under the\n            null hypothesis of independence with marginals fixed.\n        pvalue : float\n            The probability under the null hypothesis of obtaining a\n            table at least as extreme as the one that was actually observed.\n\n    Raises\n    ------\n    ValueError\n        If `table` is not two-dimensional or has negative entries.\n\n    See Also\n    --------\n    chi2_contingency : Chi-square test of independence of variables in a\n        contingency table.  This can be used as an alternative to\n        `fisher_exact` when the numbers in the table are large.\n    contingency.odds_ratio : Compute the odds ratio (sample or conditional\n        MLE) for a 2x2 contingency table.\n    barnard_exact : Barnard's exact test, which is a more powerful alternative\n        than Fisher's exact test for 2x2 contingency tables.\n    boschloo_exact : Boschloo's exact test, which is a more powerful\n        alternative than Fisher's exact test for 2x2 contingency tables.\n    :ref:`hypothesis_fisher_exact` : Extended example\n\n    Notes\n    -----\n    *Null hypothesis and p-values*\n\n    The null hypothesis is that the true odds ratio of the populations\n    underlying the observations is one, and the observations were sampled at\n    random from these populations under a condition: the marginals of the\n    resulting table must equal those of the observed table. Equivalently,\n    the null hypothesis is that the input table is from the hypergeometric\n    distribution with parameters (as used in `hypergeom`)\n    ``M = a + b + c + d``, ``n = a + b`` and ``N = a + c``, where the\n    input table is ``[[a, b], [c, d]]``.  This distribution has support\n    ``max(0, N + n - M) <= x <= min(N, n)``, or, in terms of the values\n    in the input table, ``min(0, a - d) <= x <= a + min(b, c)``.  ``x``\n    can be interpreted as the upper-left element of a 2x2 table, so the\n    tables in the distribution have form::\n\n        [  x           n - x     ]\n        [N - x    M - (n + N) + x]\n\n    For example, if::\n\n        table = [6  2]\n                [1  4]\n\n    then the support is ``2 <= x <= 7``, and the tables in the distribution\n    are::\n\n        [2 6]   [3 5]   [4 4]   [5 3]   [6 2]  [7 1]\n        [5 0]   [4 1]   [3 2]   [2 3]   [1 4]  [0 5]\n\n    The probability of each table is given by the hypergeometric distribution\n    ``hypergeom.pmf(x, M, n, N)``.  For this example, these are (rounded to\n    three significant digits)::\n\n        x       2      3      4      5       6        7\n        p  0.0163  0.163  0.408  0.326  0.0816  0.00466\n\n    These can be computed with::\n\n        >>> import numpy as np\n        >>> from scipy.stats import hypergeom\n        >>> table = np.array([[6, 2], [1, 4]])\n        >>> M = table.sum()\n        >>> n = table[0].sum()\n        >>> N = table[:, 0].sum()\n        >>> start, end = hypergeom.support(M, n, N)\n        >>> hypergeom.pmf(np.arange(start, end+1), M, n, N)\n        array([0.01631702, 0.16317016, 0.40792541, 0.32634033, 0.08158508,\n               0.004662  ])\n\n    The two-sided p-value is the probability that, under the null hypothesis,\n    a random table would have a probability equal to or less than the\n    probability of the input table.  For our example, the probability of\n    the input table (where ``x = 6``) is 0.0816.  The x values where the\n    probability does not exceed this are 2, 6 and 7, so the two-sided p-value\n    is ``0.0163 + 0.0816 + 0.00466 ~= 0.10256``::\n\n        >>> from scipy.stats import fisher_exact\n        >>> res = fisher_exact(table, alternative='two-sided')\n        >>> res.pvalue\n        0.10256410256410257\n\n    The one-sided p-value for ``alternative='greater'`` is the probability\n    that a random table has ``x >= a``, which in our example is ``x >= 6``,\n    or ``0.0816 + 0.00466 ~= 0.08626``::\n\n        >>> res = fisher_exact(table, alternative='greater')\n        >>> res.pvalue\n        0.08624708624708627\n\n    This is equivalent to computing the survival function of the\n    distribution at ``x = 5`` (one less than ``x`` from the input table,\n    because we want to include the probability of ``x = 6`` in the sum)::\n\n        >>> hypergeom.sf(5, M, n, N)\n        0.08624708624708627\n\n    For ``alternative='less'``, the one-sided p-value is the probability\n    that a random table has ``x <= a``, (i.e. ``x <= 6`` in our example),\n    or ``0.0163 + 0.163 + 0.408 + 0.326 + 0.0816 ~= 0.9949``::\n\n        >>> res = fisher_exact(table, alternative='less')\n        >>> res.pvalue\n        0.9953379953379957\n\n    This is equivalent to computing the cumulative distribution function\n    of the distribution at ``x = 6``:\n\n        >>> hypergeom.cdf(6, M, n, N)\n        0.9953379953379957\n\n    *Odds ratio*\n\n    The calculated odds ratio is different from the value computed by the\n    R function ``fisher.test``.  This implementation returns the \"sample\"\n    or \"unconditional\" maximum likelihood estimate, while ``fisher.test``\n    in R uses the conditional maximum likelihood estimate.  To compute the\n    conditional maximum likelihood estimate of the odds ratio, use\n    `scipy.stats.contingency.odds_ratio`.\n\n    References\n    ----------\n    .. [1] Fisher, Sir Ronald A, \"The Design of Experiments:\n           Mathematics of a Lady Tasting Tea.\" ISBN 978-0-486-41151-4, 1935.\n    .. [2] \"Fisher's exact test\",\n           https://en.wikipedia.org/wiki/Fisher's_exact_test\n\n    Examples\n    --------\n\n    >>> from scipy.stats import fisher_exact\n    >>> res = fisher_exact([[8, 2], [1, 5]])\n    >>> res.statistic\n    20.0\n    >>> res.pvalue\n    0.034965034965034975\n\n    For tables with shape other than ``(2, 2)``, provide an instance of\n    `scipy.stats.MonteCarloMethod` or `scipy.stats.PermutationMethod` for the\n    `method` parameter:\n\n    >>> import numpy as np\n    >>> from scipy.stats import MonteCarloMethod\n    >>> rng = np.random.default_rng(4507195762371367)\n    >>> method = MonteCarloMethod(rng=rng)\n    >>> fisher_exact([[8, 2, 3], [1, 5, 4]], method=method)\n    SignificanceResult(statistic=np.float64(0.005782), pvalue=np.float64(0.0603))\n\n    For a more detailed example, see :ref:`hypothesis_fisher_exact`.\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 5981, "code": "class TtestResult(TtestResultBase):", "documentation": "    \"\"\"\n    Result of a t-test.\n\n    See the documentation of the particular t-test function for more\n    information about the definition of the statistic and meaning of\n    the confidence interval.\n\n    Attributes\n    ----------\n    statistic : float or array\n        The t-statistic of the sample.\n    pvalue : float or array\n        The p-value associated with the given alternative.\n    df : float or array\n        The number of degrees of freedom used in calculation of the\n        t-statistic; this is one less than the size of the sample\n        (``a.shape[axis]-1`` if there are no masked elements or omitted NaNs).\n\n    Methods\n    -------\n    confidence_interval\n        Computes a confidence interval around the population statistic\n        for the given confidence level.\n        The confidence interval is returned in a ``namedtuple`` with\n        fields `low` and `high`.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 6022, "code": "    def confidence_interval(self, confidence_level=0.95):\n        low, high = _t_confidence_interval(self.df, self._statistic_np,\n                                           confidence_level, self._alternative,\n                                           self._dtype, self._xp)\n        low = low * self._standard_error + self._estimate\n        high = high * self._standard_error + self._estimate\n        return ConfidenceInterval(low=low, high=high)", "documentation": "        \"\"\"\n        Parameters\n        ----------\n        confidence_level : float\n            The confidence level for the calculation of the population mean\n            confidence interval. Default is 0.95.\n\n        Returns\n        -------\n        ci : namedtuple\n            The confidence interval is returned in a ``namedtuple`` with\n            fields `low` and `high`.\n\n        \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 6068, "code": "def ttest_1samp(a, popmean, axis=0, nan_policy=\"propagate\", alternative=\"two-sided\"):\n    xp = array_namespace(a)\n    a, popmean = xp_promote(a, popmean, force_floating=True, xp=xp)\n    a, axis = _chk_asarray(a, axis, xp=xp)\n    n = _length_nonmasked(a, axis)\n    df = n - 1\n    if a.shape[axis] == 0:\n        NaN = _get_nan(a)\n        return TtestResult(NaN, NaN, df=NaN, alternative=NaN,\n                           standard_error=NaN, estimate=NaN)\n    mean = xp.mean(a, axis=axis)", "documentation": "    \"\"\"Calculate the T-test for the mean of ONE group of scores.\n\n    This is a test for the null hypothesis that the expected value\n    (mean) of a sample of independent observations `a` is equal to the given\n    population mean, `popmean`.\n\n    Parameters\n    ----------\n    a : array_like\n        Sample observations.\n    popmean : float or array_like\n        Expected value in null hypothesis. If array_like, then its length along\n        `axis` must equal 1, and it must otherwise be broadcastable with `a`.\n    axis : int or None, optional\n        Axis along which to compute test; default is 0. If None, compute over\n        the whole array `a`.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis.\n        The following options are available (default is 'two-sided'):\n\n        * 'two-sided': the mean of the underlying distribution of the sample\n          is different than the given population mean (`popmean`)\n        * 'less': the mean of the underlying distribution of the sample is\n          less than the given population mean (`popmean`)\n        * 'greater': the mean of the underlying distribution of the sample is\n          greater than the given population mean (`popmean`)\n\n    Returns\n    -------\n    result : `~scipy.stats._result_classes.TtestResult`\n        An object with the following attributes:\n\n        statistic : float or array\n            The t-statistic.\n        pvalue : float or array\n            The p-value associated with the given alternative.\n        df : float or array\n            The number of degrees of freedom used in calculation of the\n            t-statistic; this is one less than the size of the sample\n            (``a.shape[axis]``).\n\n            .. versionadded:: 1.10.0\n\n        The object also has the following method:\n\n        confidence_interval(confidence_level=0.95)\n            Computes a confidence interval around the population\n            mean for the given confidence level.\n            The confidence interval is returned in a ``namedtuple`` with\n            fields `low` and `high`.\n\n            .. versionadded:: 1.10.0\n\n    Notes\n    -----\n    The statistic is calculated as ``(np.mean(a) - popmean)/se``, where\n    ``se`` is the standard error. Therefore, the statistic will be positive\n    when the sample mean is greater than the population mean and negative when\n    the sample mean is less than the population mean.\n\n    Examples\n    --------\n    Suppose we wish to test the null hypothesis that the mean of a population\n    is equal to 0.5. We choose a confidence level of 99%; that is, we will\n    reject the null hypothesis in favor of the alternative if the p-value is\n    less than 0.01.\n\n    When testing random variates from the standard uniform distribution, which\n    has a mean of 0.5, we expect the data to be consistent with the null\n    hypothesis most of the time.\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> rvs = stats.uniform.rvs(size=50, random_state=rng)\n    >>> stats.ttest_1samp(rvs, popmean=0.5)\n    TtestResult(statistic=2.456308468440, pvalue=0.017628209047638, df=49)\n\n    As expected, the p-value of 0.017 is not below our threshold of 0.01, so\n    we cannot reject the null hypothesis.\n\n    When testing data from the standard *normal* distribution, which has a mean\n    of 0, we would expect the null hypothesis to be rejected.\n\n    >>> rvs = stats.norm.rvs(size=50, random_state=rng)\n    >>> stats.ttest_1samp(rvs, popmean=0.5)\n    TtestResult(statistic=-7.433605518875, pvalue=1.416760157221e-09, df=49)\n\n    Indeed, the p-value is lower than our threshold of 0.01, so we reject the\n    null hypothesis in favor of the default \"two-sided\" alternative: the mean\n    of the population is *not* equal to 0.5.\n\n    However, suppose we were to test the null hypothesis against the\n    one-sided alternative that the mean of the population is *greater* than\n    0.5. Since the mean of the standard normal is less than 0.5, we would not\n    expect the null hypothesis to be rejected.\n\n    >>> stats.ttest_1samp(rvs, popmean=0.5, alternative='greater')\n    TtestResult(statistic=-7.433605518875, pvalue=0.99999999929, df=49)\n\n    Unsurprisingly, with a p-value greater than our threshold, we would not\n    reject the null hypothesis.\n\n    Note that when working with a confidence level of 99%, a true null\n    hypothesis will be rejected approximately 1% of the time.\n\n    >>> rvs = stats.uniform.rvs(size=(100, 50), random_state=rng)\n    >>> res = stats.ttest_1samp(rvs, popmean=0.5, axis=1)\n    >>> np.sum(res.pvalue < 0.01)\n    1\n\n    Indeed, even though all 100 samples above were drawn from the standard\n    uniform distribution, which *does* have a population mean of 0.5, we would\n    mistakenly reject the null hypothesis for one of them.\n\n    `ttest_1samp` can also compute a confidence interval around the population\n    mean.\n\n    >>> rvs = stats.norm.rvs(size=50, random_state=rng)\n    >>> res = stats.ttest_1samp(rvs, popmean=0)\n    >>> ci = res.confidence_interval(confidence_level=0.95)\n    >>> ci\n    ConfidenceInterval(low=-0.3193887540880017, high=0.2898583388980972)\n\n    The bounds of the 95% confidence interval are the\n    minimum and maximum values of the parameter `popmean` for which the\n    p-value of the test would be 0.05.\n\n    >>> res = stats.ttest_1samp(rvs, popmean=ci.low)\n    >>> np.testing.assert_allclose(res.pvalue, 0.05)\n    >>> res = stats.ttest_1samp(rvs, popmean=ci.high)\n    >>> np.testing.assert_allclose(res.pvalue, 0.05)\n\n    Under certain assumptions about the population from which a sample\n    is drawn, the confidence interval with confidence level 95% is expected\n    to contain the true population mean in 95% of sample replications.\n\n    >>> rvs = stats.norm.rvs(size=(50, 1000), loc=1, random_state=rng)\n    >>> res = stats.ttest_1samp(rvs, popmean=0)\n    >>> ci = res.confidence_interval()\n    >>> contains_pop_mean = (ci.low < 1) & (ci.high > 1)\n    >>> contains_pop_mean.sum()\n    953\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 6822, "code": "def _ttest_trim_var_mean_len(a, trim, axis):\n    a = np.sort(a, axis=axis)\n    n = a.shape[axis]\n    g = int(n * trim)\n    v = _calculate_winsorized_variance(a, g, axis)\n    n -= 2 * g\n    m = trim_mean(a, trim, axis=axis)\n    return v, m, n", "documentation": "    \"\"\"Variance, mean, and length of winsorized input along specified axis\"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 6846, "code": "def _calculate_winsorized_variance(a, g, axis):\n    if g == 0:\n        return _var(a, ddof=1, axis=axis)\n    a_win = np.moveaxis(a, axis, -1)\n    nans_indices = np.any(np.isnan(a_win), axis=-1)\n    a_win[..., :g] = a_win[..., [g]]\n    a_win[..., -g:] = a_win[..., [-g - 1]]\n    var_win = np.asarray(_var(a_win, ddof=(2 * g + 1), axis=-1))\n    var_win[nans_indices] = np.nan\n    return var_win\n@xp_capabilities(cpu_only=True, exceptions=[\"cupy\", \"jax.numpy\"])", "documentation": "    \"\"\"Calculates g-times winsorized variance along specified axis\"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 6885, "code": "def ttest_rel(a, b, axis=0, nan_policy='propagate', alternative=\"two-sided\"):\n    return ttest_1samp(a - b, popmean=0., axis=axis, alternative=alternative,\n                       _no_deco=True)\n_power_div_lambda_names = {\n    \"pearson\": 1,\n    \"log-likelihood\": 0,\n    \"freeman-tukey\": -0.5,\n    \"mod-log-likelihood\": -1,\n    \"neyman\": -2,\n    \"cressie-read\": 2/3,\n}", "documentation": "    \"\"\"Calculate the t-test on TWO RELATED samples of scores, a and b.\n\n    This is a test for the null hypothesis that two related or\n    repeated samples have identical average (expected) values.\n\n    Parameters\n    ----------\n    a, b : array_like\n        The arrays must have the same shape.\n    axis : int or None, optional\n        Axis along which to compute test. If None, compute over the whole\n        arrays, `a`, and `b`.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis.\n        The following options are available (default is 'two-sided'):\n\n        * 'two-sided': the means of the distributions underlying the samples\n          are unequal.\n        * 'less': the mean of the distribution underlying the first sample\n          is less than the mean of the distribution underlying the second\n          sample.\n        * 'greater': the mean of the distribution underlying the first\n          sample is greater than the mean of the distribution underlying\n          the second sample.\n\n        .. versionadded:: 1.6.0\n\n    Returns\n    -------\n    result : `~scipy.stats._result_classes.TtestResult`\n        An object with the following attributes:\n\n        statistic : float or array\n            The t-statistic.\n        pvalue : float or array\n            The p-value associated with the given alternative.\n        df : float or array\n            The number of degrees of freedom used in calculation of the\n            t-statistic; this is one less than the size of the sample\n            (``a.shape[axis]``).\n\n            .. versionadded:: 1.10.0\n\n        The object also has the following method:\n\n        confidence_interval(confidence_level=0.95)\n            Computes a confidence interval around the difference in\n            population means for the given confidence level.\n            The confidence interval is returned in a ``namedtuple`` with\n            fields `low` and `high`.\n\n            .. versionadded:: 1.10.0\n\n    Notes\n    -----\n    Examples for use are scores of the same set of student in\n    different exams, or repeated sampling from the same units. The\n    test measures whether the average score differs significantly\n    across samples (e.g. exams). If we observe a large p-value, for\n    example greater than 0.05 or 0.1 then we cannot reject the null\n    hypothesis of identical average scores. If the p-value is smaller\n    than the threshold, e.g. 1%, 5% or 10%, then we reject the null\n    hypothesis of equal averages. Small p-values are associated with\n    large t-statistics.\n\n    The t-statistic is calculated as ``np.mean(a - b)/se``, where ``se`` is the\n    standard error. Therefore, the t-statistic will be positive when the sample\n    mean of ``a - b`` is greater than zero and negative when the sample mean of\n    ``a - b`` is less than zero.\n\n    References\n    ----------\n    https://en.wikipedia.org/wiki/T-test#Dependent_t-test_for_paired_samples\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n\n    >>> rvs1 = stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n    >>> rvs2 = (stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n    ...         + stats.norm.rvs(scale=0.2, size=500, random_state=rng))\n    >>> stats.ttest_rel(rvs1, rvs2)\n    TtestResult(statistic=-0.4549717054410304, pvalue=0.6493274702088672, df=499)\n    >>> rvs3 = (stats.norm.rvs(loc=8, scale=10, size=500, random_state=rng)\n    ...         + stats.norm.rvs(scale=0.2, size=500, random_state=rng))\n    >>> stats.ttest_rel(rvs1, rvs3)\n    TtestResult(statistic=-5.879467544540889, pvalue=7.540777129099917e-09, df=499)\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7011, "code": "def power_divergence(f_obs, f_exp=None, ddof=0, axis=0, lambda_=None):\n    return _power_divergence(f_obs, f_exp=f_exp, ddof=ddof, axis=axis, lambda_=lambda_)", "documentation": "    \"\"\"Cressie-Read power divergence statistic and goodness of fit test.\n\n    This function tests the null hypothesis that the categorical data\n    has the given frequencies, using the Cressie-Read power divergence\n    statistic.\n\n    Parameters\n    ----------\n    f_obs : array_like\n        Observed frequencies in each category.\n    f_exp : array_like, optional\n        Expected frequencies in each category.  By default the categories are\n        assumed to be equally likely.\n    ddof : int, optional\n        \"Delta degrees of freedom\": adjustment to the degrees of freedom\n        for the p-value.  The p-value is computed using a chi-squared\n        distribution with ``k - 1 - ddof`` degrees of freedom, where `k`\n        is the number of observed frequencies.  The default value of `ddof`\n        is 0.\n    axis : int or None, optional\n        The axis of the broadcast result of `f_obs` and `f_exp` along which to\n        apply the test.  If axis is None, all values in `f_obs` are treated\n        as a single data set.  Default is 0.\n    lambda_ : float or str, optional\n        The power in the Cressie-Read power divergence statistic.  The default\n        is 1.  For convenience, `lambda_` may be assigned one of the following\n        strings, in which case the corresponding numerical value is used:\n\n        * ``\"pearson\"`` (value 1)\n            Pearson's chi-squared statistic. In this case, the function is\n            equivalent to `chisquare`.\n        * ``\"log-likelihood\"`` (value 0)\n            Log-likelihood ratio. Also known as the G-test [3]_.\n        * ``\"freeman-tukey\"`` (value -1/2)\n            Freeman-Tukey statistic.\n        * ``\"mod-log-likelihood\"`` (value -1)\n            Modified log-likelihood ratio.\n        * ``\"neyman\"`` (value -2)\n            Neyman's statistic.\n        * ``\"cressie-read\"`` (value 2/3)\n            The power recommended in [5]_.\n\n    Returns\n    -------\n    res: Power_divergenceResult\n        An object containing attributes:\n\n        statistic : float or ndarray\n            The Cressie-Read power divergence test statistic.  The value is\n            a float if `axis` is None or if` `f_obs` and `f_exp` are 1-D.\n        pvalue : float or ndarray\n            The p-value of the test.  The value is a float if `ddof` and the\n            return value `stat` are scalars.\n\n    See Also\n    --------\n    chisquare\n\n    Notes\n    -----\n    This test is invalid when the observed or expected frequencies in each\n    category are too small.  A typical rule is that all of the observed\n    and expected frequencies should be at least 5.\n\n    Also, the sum of the observed and expected frequencies must be the same\n    for the test to be valid; `power_divergence` raises an error if the sums\n    do not agree within a relative tolerance of ``eps**0.5``, where ``eps``\n    is the precision of the input dtype.\n\n    When `lambda_` is less than zero, the formula for the statistic involves\n    dividing by `f_obs`, so a warning or error may be generated if any value\n    in `f_obs` is 0.\n\n    Similarly, a warning or error may be generated if any value in `f_exp` is\n    zero when `lambda_` >= 0.\n\n    The default degrees of freedom, k-1, are for the case when no parameters\n    of the distribution are estimated. If p parameters are estimated by\n    efficient maximum likelihood then the correct degrees of freedom are\n    k-1-p. If the parameters are estimated in a different way, then the\n    dof can be between k-1-p and k-1. However, it is also possible that\n    the asymptotic distribution is not a chisquare, in which case this\n    test is not appropriate.\n\n    References\n    ----------\n    .. [1] Lowry, Richard.  \"Concepts and Applications of Inferential\n           Statistics\". Chapter 8.\n           https://web.archive.org/web/20171015035606/http://faculty.vassar.edu/lowry/ch8pt1.html\n    .. [2] \"Chi-squared test\", https://en.wikipedia.org/wiki/Chi-squared_test\n    .. [3] \"G-test\", https://en.wikipedia.org/wiki/G-test\n    .. [4] Sokal, R. R. and Rohlf, F. J. \"Biometry: the principles and\n           practice of statistics in biological research\", New York: Freeman\n           (1981)\n    .. [5] Cressie, N. and Read, T. R. C., \"Multinomial Goodness-of-Fit\n           Tests\", J. Royal Stat. Soc. Series B, Vol. 46, No. 3 (1984),\n           pp. 440-464.\n\n    Examples\n    --------\n    (See `chisquare` for more examples.)\n\n    When just `f_obs` is given, it is assumed that the expected frequencies\n    are uniform and given by the mean of the observed frequencies.  Here we\n    perform a G-test (i.e. use the log-likelihood ratio statistic):\n\n    >>> import numpy as np\n    >>> from scipy.stats import power_divergence\n    >>> power_divergence([16, 18, 16, 14, 12, 12], lambda_='log-likelihood')\n    (2.006573162632538, 0.84823476779463769)\n\n    The expected frequencies can be given with the `f_exp` argument:\n\n    >>> power_divergence([16, 18, 16, 14, 12, 12],\n    ...                  f_exp=[16, 16, 16, 16, 16, 8],\n    ...                  lambda_='log-likelihood')\n    (3.3281031458963746, 0.6495419288047497)\n\n    When `f_obs` is 2-D, by default the test is applied to each column.\n\n    >>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T\n    >>> obs.shape\n    (6, 2)\n    >>> power_divergence(obs, lambda_=\"log-likelihood\")\n    (array([ 2.00657316,  6.77634498]), array([ 0.84823477,  0.23781225]))\n\n    By setting ``axis=None``, the test is applied to all data in the array,\n    which is equivalent to applying the test to the flattened array.\n\n    >>> power_divergence(obs, axis=None)\n    (23.31034482758621, 0.015975692534127565)\n    >>> power_divergence(obs.ravel())\n    (23.31034482758621, 0.015975692534127565)\n\n    `ddof` is the change to make to the default degrees of freedom.\n\n    >>> power_divergence([16, 18, 16, 14, 12, 12], ddof=1)\n    (2.0, 0.73575888234288467)\n\n    The calculation of the p-values is done by broadcasting the\n    test statistic with `ddof`.\n\n    >>> power_divergence([16, 18, 16, 14, 12, 12], ddof=[0,1,2])\n    (2.0, array([ 0.84914504,  0.73575888,  0.5724067 ]))\n\n    `f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has\n    shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting\n    `f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared\n    statistics, we must use ``axis=1``:\n\n    >>> power_divergence([16, 18, 16, 14, 12, 12],\n    ...                  f_exp=[[16, 16, 16, 16, 16, 8],\n    ...                         [8, 20, 20, 16, 12, 12]],\n    ...                  axis=1)\n    (array([ 3.5 ,  9.25]), array([ 0.62338763,  0.09949846]))\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7258, "code": "def chisquare(f_obs, f_exp=None, ddof=0, axis=0, *, sum_check=True):\n    return _power_divergence(f_obs, f_exp=f_exp, ddof=ddof, axis=axis,\n                             lambda_=\"pearson\", sum_check=sum_check)\nKstestResult = _make_tuple_bunch('KstestResult', ['statistic', 'pvalue'],\n                                 ['statistic_location', 'statistic_sign'])", "documentation": "    \"\"\"Perform Pearson's chi-squared test.\n\n    Pearson's chi-squared test [1]_ is a goodness-of-fit test for a multinomial\n    distribution with given probabilities; that is, it assesses the null hypothesis\n    that the observed frequencies (counts) are obtained by independent\n    sampling of *N* observations from a categorical distribution with given\n    expected frequencies.\n\n    Parameters\n    ----------\n    f_obs : array_like\n        Observed frequencies in each category.\n    f_exp : array_like, optional\n        Expected frequencies in each category. By default, the categories are\n        assumed to be equally likely.\n    ddof : int, optional\n        \"Delta degrees of freedom\": adjustment to the degrees of freedom\n        for the p-value.  The p-value is computed using a chi-squared\n        distribution with ``k - 1 - ddof`` degrees of freedom, where ``k``\n        is the number of categories.  The default value of `ddof` is 0.\n    axis : int or None, optional\n        The axis of the broadcast result of `f_obs` and `f_exp` along which to\n        apply the test.  If axis is None, all values in `f_obs` are treated\n        as a single data set.  Default is 0.\n    sum_check : bool, optional\n        Whether to perform a check that ``sum(f_obs) - sum(f_exp) == 0``. If True,\n        (default) raise an error (or, for lazy backends, return NaN) when the relative\n        difference exceeds the square root of the precision of the data type.\n        See Notes for rationale and possible exceptions.\n\n    Returns\n    -------\n    res: Power_divergenceResult\n        An object containing attributes:\n\n        statistic : float or ndarray\n            The chi-squared test statistic.  The value is a float if `axis` is\n            None or `f_obs` and `f_exp` are 1-D.\n        pvalue : float or ndarray\n            The p-value of the test.  The value is a float if `ddof` and the\n            result attribute `statistic` are scalars.\n\n    See Also\n    --------\n    scipy.stats.power_divergence\n    scipy.stats.fisher_exact : Fisher exact test on a 2x2 contingency table.\n    scipy.stats.barnard_exact : An unconditional exact test. An alternative\n        to chi-squared test for small sample sizes.\n    :ref:`hypothesis_chisquare` : Extended example\n\n    Notes\n    -----\n    This test is invalid when the observed or expected frequencies in each\n    category are too small.  A typical rule is that all of the observed\n    and expected frequencies should be at least 5. According to [2]_, the\n    total number of observations is recommended to be greater than 13,\n    otherwise exact tests (such as Barnard's Exact test) should be used\n    because they do not overreject.\n\n    The default degrees of freedom, k-1, are for the case when no parameters\n    of the distribution are estimated. If p parameters are estimated by\n    efficient maximum likelihood then the correct degrees of freedom are\n    k-1-p. If the parameters are estimated in a different way, then the\n    dof can be between k-1-p and k-1. However, it is also possible that\n    the asymptotic distribution is not chi-square, in which case this test\n    is not appropriate.\n\n    For Pearson's chi-squared test, the total observed and expected counts must match\n    for the p-value to accurately reflect the probability of observing such an extreme\n    value of the statistic under the null hypothesis.\n    This function may be used to perform other statistical tests that do not require\n    the total counts to be equal. For instance, to test the null hypothesis that\n    ``f_obs[i]`` is Poisson-distributed with expectation ``f_exp[i]``, set ``ddof=-1``\n    and ``sum_check=False``. This test follows from the fact that a Poisson random\n    variable with mean and variance ``f_exp[i]`` is approximately normal with the\n    same mean and variance; the chi-squared statistic standardizes, squares, and sums\n    the observations; and the sum of ``n`` squared standard normal variables follows\n    the chi-squared distribution with ``n`` degrees of freedom.\n\n    References\n    ----------\n    .. [1] \"Pearson's chi-squared test\".\n           *Wikipedia*. https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test\n    .. [2] Pearson, Karl. \"On the criterion that a given system of deviations from the probable\n           in the case of a correlated system of variables is such that it can be reasonably\n           supposed to have arisen from random sampling\", Philosophical Magazine. Series 5. 50\n           (1900), pp. 157-175.\n\n    Examples\n    --------\n    When only the mandatory `f_obs` argument is given, it is assumed that the\n    expected frequencies are uniform and given by the mean of the observed\n    frequencies:\n\n    >>> import numpy as np\n    >>> from scipy.stats import chisquare\n    >>> chisquare([16, 18, 16, 14, 12, 12])\n    Power_divergenceResult(statistic=2.0, pvalue=0.84914503608460956)\n\n    The optional `f_exp` argument gives the expected frequencies.\n\n    >>> chisquare([16, 18, 16, 14, 12, 12], f_exp=[16, 16, 16, 16, 16, 8])\n    Power_divergenceResult(statistic=3.5, pvalue=0.62338762774958223)\n\n    When `f_obs` is 2-D, by default the test is applied to each column.\n\n    >>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T\n    >>> obs.shape\n    (6, 2)\n    >>> chisquare(obs)\n    Power_divergenceResult(statistic=array([2.        , 6.66666667]), pvalue=array([0.84914504, 0.24663415]))\n\n    By setting ``axis=None``, the test is applied to all data in the array,\n    which is equivalent to applying the test to the flattened array.\n\n    >>> chisquare(obs, axis=None)\n    Power_divergenceResult(statistic=23.31034482758621, pvalue=0.015975692534127565)\n    >>> chisquare(obs.ravel())\n    Power_divergenceResult(statistic=23.310344827586206, pvalue=0.01597569253412758)\n\n    `ddof` is the change to make to the default degrees of freedom.\n\n    >>> chisquare([16, 18, 16, 14, 12, 12], ddof=1)\n    Power_divergenceResult(statistic=2.0, pvalue=0.7357588823428847)\n\n    The calculation of the p-values is done by broadcasting the\n    chi-squared statistic with `ddof`.\n\n    >>> chisquare([16, 18, 16, 14, 12, 12], ddof=[0, 1, 2])\n    Power_divergenceResult(statistic=2.0, pvalue=array([0.84914504, 0.73575888, 0.5724067 ]))\n\n    `f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has\n    shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting\n    `f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared\n    statistics, we use ``axis=1``:\n\n    >>> chisquare([16, 18, 16, 14, 12, 12],\n    ...           f_exp=[[16, 16, 16, 16, 16, 8], [8, 20, 20, 16, 12, 12]],\n    ...           axis=1)\n    Power_divergenceResult(statistic=array([3.5 , 9.25]), pvalue=array([0.62338763, 0.09949846]))\n\n    For a more detailed example, see :ref:`hypothesis_chisquare`.\n    \"\"\"  # noqa: E501"}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7410, "code": "def _compute_d(cdfvals, x, sign, xp=None):\n    xp = array_namespace(cdfvals, x) if xp is None else xp\n    n = cdfvals.shape[-1]\n    D = (xp.arange(1.0, n + 1, dtype=x.dtype) / n - cdfvals if sign == +1\n         else (cdfvals - xp.arange(0.0, n, dtype=x.dtype)/n))\n    amax = xp.argmax(D, axis=-1, keepdims=True)\n    loc_max = xp.squeeze(xp.take_along_axis(x, amax, axis=-1), axis=-1)\n    D = xp.squeeze(xp.take_along_axis(D, amax, axis=-1), axis=-1)\n    return D[()] if D.ndim == 0 else D, loc_max[()] if loc_max.ndim == 0 else loc_max", "documentation": "    \"\"\"Computes D+/D- as used in the Kolmogorov-Smirnov test.\n\n    Vectorized along the last axis.\n\n    Parameters\n    ----------\n    cdfvals : array_like\n        Sorted array of CDF values between 0 and 1\n    x: array_like\n        Sorted array of the stochastic variable itself\n    sign: int\n        Indicates whether to compute D+ (+1) or D- (-1).\n\n    Returns\n    -------\n    D : float or array\n        The maximum distance of the CDF values below/above (D+/D-) Uniform(0, 1).\n    loc_max : float or array\n        The location at which the maximum is reached.\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7458, "code": "def ks_1samp(x, cdf, args=(), alternative='two-sided', method='auto', *, axis=0):\n    xp = array_namespace(x)\n    mode = method\n    alternative = {'t': 'two-sided', 'g': 'greater', 'l': 'less'}.get(\n        alternative.lower()[0], alternative)\n    if alternative not in ['two-sided', 'greater', 'less']:\n        raise ValueError(f\"Unexpected value {alternative=}\")\n    N = x.shape[-1]\n    x = xp.sort(x, axis=-1)\n    x = xp_promote(x, force_floating=True, xp=xp)\n    cdfvals = cdf(x, *args)", "documentation": "    \"\"\"\n    Performs the one-sample Kolmogorov-Smirnov test for goodness of fit.\n\n    This test compares the underlying distribution F(x) of a sample\n    against a given continuous distribution G(x). See Notes for a description\n    of the available null and alternative hypotheses.\n\n    Parameters\n    ----------\n    x : array_like\n        a 1-D array of observations of iid random variables.\n    cdf : callable\n        callable used to calculate the cdf.\n    args : tuple, sequence, optional\n        Distribution parameters, used with `cdf`.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the null and alternative hypotheses. Default is 'two-sided'.\n        Please see explanations in the Notes below.\n    method : {'auto', 'exact', 'approx', 'asymp'}, optional\n        Defines the distribution used for calculating the p-value.\n        The following options are available (default is 'auto'):\n\n        * 'auto' : selects one of the other options.\n        * 'exact' : uses the exact distribution of test statistic.\n        * 'approx' : approximates the two-sided probability with twice\n          the one-sided probability\n        * 'asymp': uses asymptotic distribution of test statistic\n\n    axis : int or tuple of ints, default: 0\n        If an int or tuple of ints, the axis or axes of the input along which\n        to compute the statistic. The statistic of each axis-slice (e.g. row)\n        of the input will appear in a corresponding element of the output.\n        If ``None``, the input will be raveled before computing the statistic.\n\n    Returns\n    -------\n    res: KstestResult\n        An object containing attributes:\n\n        statistic : float\n            KS test statistic, either D+, D-, or D (the maximum of the two)\n        pvalue : float\n            One-tailed or two-tailed p-value.\n        statistic_location : float\n            Value of `x` corresponding with the KS statistic; i.e., the\n            distance between the empirical distribution function and the\n            hypothesized cumulative distribution function is measured at this\n            observation.\n        statistic_sign : int\n            +1 if the KS statistic is the maximum positive difference between\n            the empirical distribution function and the hypothesized cumulative\n            distribution function (D+); -1 if the KS statistic is the maximum\n            negative difference (D-).\n\n\n    See Also\n    --------\n    ks_2samp, kstest\n\n    Notes\n    -----\n    There are three options for the null and corresponding alternative\n    hypothesis that can be selected using the `alternative` parameter.\n\n    - `two-sided`: The null hypothesis is that the two distributions are\n      identical, F(x)=G(x) for all x; the alternative is that they are not\n      identical.\n\n    - `less`: The null hypothesis is that F(x) >= G(x) for all x; the\n      alternative is that F(x) < G(x) for at least one x.\n\n    - `greater`: The null hypothesis is that F(x) <= G(x) for all x; the\n      alternative is that F(x) > G(x) for at least one x.\n\n    Note that the alternative hypotheses describe the *CDFs* of the\n    underlying distributions, not the observed values. For example,\n    suppose x1 ~ F and x2 ~ G. If F(x) > G(x) for all x, the values in\n    x1 tend to be less than those in x2.\n\n    Examples\n    --------\n    Suppose we wish to test the null hypothesis that a sample is distributed\n    according to the standard normal.\n    We choose a confidence level of 95%; that is, we will reject the null\n    hypothesis in favor of the alternative if the p-value is less than 0.05.\n\n    When testing uniformly distributed data, we would expect the\n    null hypothesis to be rejected.\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> stats.ks_1samp(stats.uniform.rvs(size=100, random_state=rng),\n    ...                stats.norm.cdf)\n    KstestResult(statistic=0.5001899973268688,\n                 pvalue=1.1616392184763533e-23,\n                 statistic_location=0.00047625268963724654,\n                 statistic_sign=-1)\n\n    Indeed, the p-value is lower than our threshold of 0.05, so we reject the\n    null hypothesis in favor of the default \"two-sided\" alternative: the data\n    are *not* distributed according to the standard normal.\n\n    When testing random variates from the standard normal distribution, we\n    expect the data to be consistent with the null hypothesis most of the time.\n\n    >>> x = stats.norm.rvs(size=100, random_state=rng)\n    >>> stats.ks_1samp(x, stats.norm.cdf)\n    KstestResult(statistic=0.05345882212970396,\n                 pvalue=0.9227159037744717,\n                 statistic_location=-1.2451343873745018,\n                 statistic_sign=1)\n\n    As expected, the p-value of 0.92 is not below our threshold of 0.05, so\n    we cannot reject the null hypothesis.\n\n    Suppose, however, that the random variates are distributed according to\n    a normal distribution that is shifted toward greater values. In this case,\n    the cumulative density function (CDF) of the underlying distribution tends\n    to be *less* than the CDF of the standard normal. Therefore, we would\n    expect the null hypothesis to be rejected with ``alternative='less'``:\n\n    >>> x = stats.norm.rvs(size=100, loc=0.5, random_state=rng)\n    >>> stats.ks_1samp(x, stats.norm.cdf, alternative='less')\n    KstestResult(statistic=0.17482387821055168,\n                 pvalue=0.001913921057766743,\n                 statistic_location=0.3713830565352756,\n                 statistic_sign=-1)\n\n    and indeed, with p-value smaller than our threshold, we reject the null\n    hypothesis in favor of the alternative.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7652, "code": "def _compute_prob_outside_square(n, h):\n    P = 0.0\n    k = int(np.floor(n / h))\n    while k >= 0:\n        p1 = 1.0\n        for j in range(h):\n            p1 = (n - k * h - j) * p1 / (n + k * h + j + 1)\n        P = p1 * (1.0 - P)\n        k -= 1\n    return 2 * P", "documentation": "    \"\"\"\n    Compute the proportion of paths that pass outside the two diagonal lines.\n\n    Parameters\n    ----------\n    n : integer\n        n > 0\n    h : integer\n        0 <= h <= n\n\n    Returns\n    -------\n    p : float\n        The proportion of paths that pass outside the lines x-y = +/-h.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7690, "code": "def _count_paths_outside_method(m, n, g, h):\n    if m < n:\n        m, n = n, m\n    mg = m // g\n    ng = n // g\n    lxj = n + (mg-h)//mg\n    xj = [(h + mg * j + ng-1)//ng for j in range(lxj)]\n    if lxj == 0:\n        return special.binom(m + n, n)\n    B = np.zeros(lxj)\n    B[0] = 1", "documentation": "    \"\"\"Count the number of paths that pass outside the specified diagonal.\n\n    Parameters\n    ----------\n    m : integer\n        m > 0\n    n : integer\n        n > 0\n    g : integer\n        g is greatest common divisor of m and n\n    h : integer\n        0 <= h <= lcm(m,n)\n\n    Returns\n    -------\n    p : float\n        The number of paths that go low.\n        The calculation may overflow - check for a finite answer.\n\n    Notes\n    -----\n    Count the integer lattice paths from (0, 0) to (m, n), which at some\n    point (x, y) along the path, satisfy:\n      m*y <= n*x - h*g\n    The paths make steps of size +1 in either positive x or positive y\n    directions.\n\n    We generally follow Hodges' treatment of Drion/Gnedenko/Korolyuk.\n    Hodges, J.L. Jr.,\n    \"The Significance Probability of the Smirnov Two-Sample Test,\"\n    Arkiv fiur Matematik, 3, No. 43 (1958), 469-86.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7764, "code": "def _attempt_exact_2kssamp(n1, n2, g, d, alternative):\n    lcm = (n1 // g) * n2\n    h = int(np.round(d * lcm))\n    d = h * 1.0 / lcm\n    if h == 0:\n        return True, d, 1.0\n    saw_fp_error, prob = False, np.nan\n    try:\n        with np.errstate(invalid=\"raise\", over=\"raise\"):\n            if alternative == 'two-sided':\n                if n1 == n2:", "documentation": "    \"\"\"Attempts to compute the exact 2sample probability.\n\n    n1, n2 are the sample sizes\n    g is the gcd(n1, n2)\n    d is the computed max difference in ECDFs\n\n    Returns (success, d, probability)\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7818, "code": "def ks_2samp(data1, data2, alternative='two-sided', method='auto', *, axis=0):\n    mode = method\n    if mode not in ['auto', 'exact', 'asymp']:\n        raise ValueError(f'Invalid value for mode: {mode}')\n    alternative = {'t': 'two-sided', 'g': 'greater', 'l': 'less'}.get(\n        alternative.lower()[0], alternative)\n    if alternative not in ['two-sided', 'less', 'greater']:\n        raise ValueError(f'Invalid value for alternative: {alternative}')\n    MAX_AUTO_N = 10000  # 'auto' will attempt to be exact if n1,n2 <= MAX_AUTO_N\n    xp = array_namespace(data1, data2)\n    data1 = xp.sort(data1, axis=-1)", "documentation": "    \"\"\"\n    Performs the two-sample Kolmogorov-Smirnov test for goodness of fit.\n\n    This test compares the underlying continuous distributions F(x) and G(x)\n    of two independent samples.  See Notes for a description of the available\n    null and alternative hypotheses.\n\n    Parameters\n    ----------\n    data1, data2 : array_like, 1-Dimensional\n        Two arrays of sample observations assumed to be drawn from a continuous\n        distribution, sample sizes can be different.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the null and alternative hypotheses. Default is 'two-sided'.\n        Please see explanations in the Notes below.\n    method : {'auto', 'exact', 'asymp'}, optional\n        Defines the method used for calculating the p-value.\n        The following options are available (default is 'auto'):\n\n        * 'auto' : use 'exact' for small size arrays, 'asymp' for large\n        * 'exact' : use exact distribution of test statistic\n        * 'asymp' : use asymptotic distribution of test statistic\n\n    axis : int or tuple of ints, default: 0\n        If an int or tuple of ints, the axis or axes of the input along which\n        to compute the statistic. The statistic of each axis-slice (e.g. row)\n        of the input will appear in a corresponding element of the output.\n        If ``None``, the input will be raveled before computing the statistic.\n\n    Returns\n    -------\n    res: KstestResult\n        An object containing attributes:\n\n        statistic : float\n            KS test statistic.\n        pvalue : float\n            One-tailed or two-tailed p-value.\n        statistic_location : float\n            Value from `data1` or `data2` corresponding with the KS statistic;\n            i.e., the distance between the empirical distribution functions is\n            measured at this observation.\n        statistic_sign : int\n            +1 if the empirical distribution function of `data1` exceeds\n            the empirical distribution function of `data2` at\n            `statistic_location`, otherwise -1.\n\n    See Also\n    --------\n    kstest, ks_1samp, epps_singleton_2samp, anderson_ksamp\n\n    Notes\n    -----\n    There are three options for the null and corresponding alternative\n    hypothesis that can be selected using the `alternative` parameter.\n\n    - `less`: The null hypothesis is that F(x) >= G(x) for all x; the\n      alternative is that F(x) < G(x) for at least one x. The statistic\n      is the magnitude of the minimum (most negative) difference between the\n      empirical distribution functions of the samples.\n\n    - `greater`: The null hypothesis is that F(x) <= G(x) for all x; the\n      alternative is that F(x) > G(x) for at least one x. The statistic\n      is the maximum (most positive) difference between the empirical\n      distribution functions of the samples.\n\n    - `two-sided`: The null hypothesis is that the two distributions are\n      identical, F(x)=G(x) for all x; the alternative is that they are not\n      identical. The statistic is the maximum absolute difference between the\n      empirical distribution functions of the samples.\n\n    Note that the alternative hypotheses describe the *CDFs* of the\n    underlying distributions, not the observed values of the data. For example,\n    suppose x1 ~ F and x2 ~ G. If F(x) > G(x) for all x, the values in\n    x1 tend to be less than those in x2.\n\n    If the KS statistic is large, then the p-value will be small, and this may\n    be taken as evidence against the null hypothesis in favor of the\n    alternative.\n\n    If ``method='exact'``, `ks_2samp` attempts to compute an exact p-value,\n    that is, the probability under the null hypothesis of obtaining a test\n    statistic value as extreme as the value computed from the data.\n    If ``method='asymp'``, the asymptotic Kolmogorov-Smirnov distribution is\n    used to compute an approximate p-value.\n    If ``method='auto'``, an exact p-value computation is attempted if both\n    sample sizes are less than 10000; otherwise, the asymptotic method is used.\n    In any case, if an exact p-value calculation is attempted and fails, a\n    warning will be emitted, and the asymptotic p-value will be returned.\n\n    The 'two-sided' 'exact' computation computes the complementary probability\n    and then subtracts from 1.  As such, the minimum probability it can return\n    is about 1e-16.  While the algorithm itself is exact, numerical\n    errors may accumulate for large sample sizes.   It is most suited to\n    situations in which one of the sample sizes is only a few thousand.\n\n    We generally follow Hodges' treatment of Drion/Gnedenko/Korolyuk [1]_.\n\n    References\n    ----------\n    .. [1] Hodges, J.L. Jr.,  \"The Significance Probability of the Smirnov\n           Two-Sample Test,\" Arkiv fiur Matematik, 3, No. 43 (1958), 469-486.\n\n    Examples\n    --------\n    Suppose we wish to test the null hypothesis that two samples were drawn\n    from the same distribution.\n    We choose a confidence level of 95%; that is, we will reject the null\n    hypothesis in favor of the alternative if the p-value is less than 0.05.\n\n    If the first sample were drawn from a uniform distribution and the second\n    were drawn from the standard normal, we would expect the null hypothesis\n    to be rejected.\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> sample1 = stats.uniform.rvs(size=100, random_state=rng)\n    >>> sample2 = stats.norm.rvs(size=110, random_state=rng)\n    >>> stats.ks_2samp(sample1, sample2)\n    KstestResult(statistic=0.5454545454545454,\n                 pvalue=7.37417839555191e-15,\n                 statistic_location=-0.014071496412861274,\n                 statistic_sign=-1)\n\n\n    Indeed, the p-value is lower than our threshold of 0.05, so we reject the\n    null hypothesis in favor of the default \"two-sided\" alternative: the data\n    were *not* drawn from the same distribution.\n\n    When both samples are drawn from the same distribution, we expect the data\n    to be consistent with the null hypothesis most of the time.\n\n    >>> sample1 = stats.norm.rvs(size=105, random_state=rng)\n    >>> sample2 = stats.norm.rvs(size=95, random_state=rng)\n    >>> stats.ks_2samp(sample1, sample2)\n    KstestResult(statistic=0.10927318295739348,\n                 pvalue=0.5438289009927495,\n                 statistic_location=-0.1670157701848795,\n                 statistic_sign=-1)\n\n    As expected, the p-value of 0.54 is not below our threshold of 0.05, so\n    we cannot reject the null hypothesis.\n\n    Suppose, however, that the first sample were drawn from\n    a normal distribution shifted toward greater values. In this case,\n    the cumulative density function (CDF) of the underlying distribution tends\n    to be *less* than the CDF underlying the second sample. Therefore, we would\n    expect the null hypothesis to be rejected with ``alternative='less'``:\n\n    >>> sample1 = stats.norm.rvs(size=105, loc=0.5, random_state=rng)\n    >>> stats.ks_2samp(sample1, sample2, alternative='less')\n    KstestResult(statistic=0.4055137844611529,\n                 pvalue=3.5474563068855554e-08,\n                 statistic_location=-0.13249370614972575,\n                 statistic_sign=-1)\n\n    and indeed, with p-value smaller than our threshold, we reject the null\n    hypothesis in favor of the alternative.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 8148, "code": "def kstest(rvs, cdf, args=(), N=20, alternative='two-sided', method='auto'):\n    if alternative == 'two_sided':\n        alternative = 'two-sided'\n    if alternative not in ['two-sided', 'greater', 'less']:\n        raise ValueError(f\"Unexpected alternative: {alternative}\")\n    xvals, yvals, cdf = _parse_kstest_args(rvs, cdf, args, N)\n    if cdf:\n        return ks_1samp(xvals, cdf, args=args, alternative=alternative,\n                        method=method, _no_deco=True)\n    return ks_2samp(xvals, yvals, alternative=alternative, method=method,\n                    _no_deco=True)", "documentation": "    \"\"\"\n    Performs the (one-sample or two-sample) Kolmogorov-Smirnov test for\n    goodness of fit.\n\n    The one-sample test compares the underlying distribution F(x) of a sample\n    against a given distribution G(x). The two-sample test compares the\n    underlying distributions of two independent samples. Both tests are valid\n    only for continuous distributions.\n\n    Parameters\n    ----------\n    rvs : str, array_like, or callable\n        If an array, it should be a 1-D array of observations of random\n        variables.\n        If a callable, it should be a function to generate random variables;\n        it is required to have a keyword argument `size`.\n        If a string, it should be the name of a distribution in `scipy.stats`,\n        which will be used to generate random variables.\n    cdf : str, array_like or callable\n        If array_like, it should be a 1-D array of observations of random\n        variables, and the two-sample test is performed\n        (and rvs must be array_like).\n        If a callable, that callable is used to calculate the cdf.\n        If a string, it should be the name of a distribution in `scipy.stats`,\n        which will be used as the cdf function.\n    args : tuple, sequence, optional\n        Distribution parameters, used if `rvs` or `cdf` are strings or\n        callables.\n    N : int, optional\n        Sample size if `rvs` is string or callable.  Default is 20.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the null and alternative hypotheses. Default is 'two-sided'.\n        Please see explanations in the Notes below.\n    method : {'auto', 'exact', 'approx', 'asymp'}, optional\n        Defines the distribution used for calculating the p-value.\n        The following options are available (default is 'auto'):\n\n        * 'auto' : selects one of the other options.\n        * 'exact' : uses the exact distribution of test statistic.\n        * 'approx' : approximates the two-sided probability with twice the\n          one-sided probability\n        * 'asymp': uses asymptotic distribution of test statistic\n\n    Returns\n    -------\n    res: KstestResult\n        An object containing attributes:\n\n        statistic : float\n            KS test statistic, either D+, D-, or D (the maximum of the two)\n        pvalue : float\n            One-tailed or two-tailed p-value.\n        statistic_location : float\n            In a one-sample test, this is the value of `rvs`\n            corresponding with the KS statistic; i.e., the distance between\n            the empirical distribution function and the hypothesized cumulative\n            distribution function is measured at this observation.\n\n            In a two-sample test, this is the value from `rvs` or `cdf`\n            corresponding with the KS statistic; i.e., the distance between\n            the empirical distribution functions is measured at this\n            observation.\n        statistic_sign : int\n            In a one-sample test, this is +1 if the KS statistic is the\n            maximum positive difference between the empirical distribution\n            function and the hypothesized cumulative distribution function\n            (D+); it is -1 if the KS statistic is the maximum negative\n            difference (D-).\n\n            In a two-sample test, this is +1 if the empirical distribution\n            function of `rvs` exceeds the empirical distribution\n            function of `cdf` at `statistic_location`, otherwise -1.\n\n    See Also\n    --------\n    ks_1samp, ks_2samp\n\n    Notes\n    -----\n    There are three options for the null and corresponding alternative\n    hypothesis that can be selected using the `alternative` parameter.\n\n    - `two-sided`: The null hypothesis is that the two distributions are\n      identical, F(x)=G(x) for all x; the alternative is that they are not\n      identical.\n\n    - `less`: The null hypothesis is that F(x) >= G(x) for all x; the\n      alternative is that F(x) < G(x) for at least one x.\n\n    - `greater`: The null hypothesis is that F(x) <= G(x) for all x; the\n      alternative is that F(x) > G(x) for at least one x.\n\n    Note that the alternative hypotheses describe the *CDFs* of the\n    underlying distributions, not the observed values. For example,\n    suppose x1 ~ F and x2 ~ G. If F(x) > G(x) for all x, the values in\n    x1 tend to be less than those in x2.\n\n\n    Examples\n    --------\n    Suppose we wish to test the null hypothesis that a sample is distributed\n    according to the standard normal.\n    We choose a confidence level of 95%; that is, we will reject the null\n    hypothesis in favor of the alternative if the p-value is less than 0.05.\n\n    When testing uniformly distributed data, we would expect the\n    null hypothesis to be rejected.\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> stats.kstest(stats.uniform.rvs(size=100, random_state=rng),\n    ...              stats.norm.cdf)\n    KstestResult(statistic=0.5001899973268688,\n                 pvalue=1.1616392184763533e-23,\n                 statistic_location=0.00047625268963724654,\n                 statistic_sign=-1)\n\n    Indeed, the p-value is lower than our threshold of 0.05, so we reject the\n    null hypothesis in favor of the default \"two-sided\" alternative: the data\n    are *not* distributed according to the standard normal.\n\n    When testing random variates from the standard normal distribution, we\n    expect the data to be consistent with the null hypothesis most of the time.\n\n    >>> x = stats.norm.rvs(size=100, random_state=rng)\n    >>> stats.kstest(x, stats.norm.cdf)\n    KstestResult(statistic=0.05345882212970396,\n                 pvalue=0.9227159037744717,\n                 statistic_location=-1.2451343873745018,\n                 statistic_sign=1)\n\n\n    As expected, the p-value of 0.92 is not below our threshold of 0.05, so\n    we cannot reject the null hypothesis.\n\n    Suppose, however, that the random variates are distributed according to\n    a normal distribution that is shifted toward greater values. In this case,\n    the cumulative density function (CDF) of the underlying distribution tends\n    to be *less* than the CDF of the standard normal. Therefore, we would\n    expect the null hypothesis to be rejected with ``alternative='less'``:\n\n    >>> x = stats.norm.rvs(size=100, loc=0.5, random_state=rng)\n    >>> stats.kstest(x, stats.norm.cdf, alternative='less')\n    KstestResult(statistic=0.17482387821055168,\n                 pvalue=0.001913921057766743,\n                 statistic_location=0.3713830565352756,\n                 statistic_sign=-1)\n\n    and indeed, with p-value smaller than our threshold, we reject the null\n    hypothesis in favor of the alternative.\n\n    For convenience, the previous test can be performed using the name of the\n    distribution as the second argument.\n\n    >>> stats.kstest(x, \"norm\", alternative='less')\n    KstestResult(statistic=0.17482387821055168,\n                 pvalue=0.001913921057766743,\n                 statistic_location=0.3713830565352756,\n                 statistic_sign=-1)\n\n    The examples above have all been one-sample tests identical to those\n    performed by `ks_1samp`. Note that `kstest` can also perform two-sample\n    tests identical to those performed by `ks_2samp`. For example, when two\n    samples are drawn from the same distribution, we expect the data to be\n    consistent with the null hypothesis most of the time.\n\n    >>> sample1 = stats.laplace.rvs(size=105, random_state=rng)\n    >>> sample2 = stats.laplace.rvs(size=95, random_state=rng)\n    >>> stats.kstest(sample1, sample2)\n    KstestResult(statistic=0.11779448621553884,\n                 pvalue=0.4494256912629795,\n                 statistic_location=0.6138814275424155,\n                 statistic_sign=1)\n\n    As expected, the p-value of 0.45 is not below our threshold of 0.05, so\n    we cannot reject the null hypothesis.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 8342, "code": "def tiecorrect(rankvals):\n    arr = np.sort(rankvals)\n    idx = np.nonzero(np.r_[True, arr[1:] != arr[:-1], True])[0]\n    cnt = np.diff(idx).astype(np.float64)\n    size = np.float64(arr.size)\n    return 1.0 if size < 2 else 1.0 - (cnt**3 - cnt).sum() / (size**3 - size)\nRanksumsResult = namedtuple('RanksumsResult', ('statistic', 'pvalue'))\n@xp_capabilities(np_only=True)\n@_axis_nan_policy_factory(RanksumsResult, n_samples=2)", "documentation": "    \"\"\"Tie correction factor for Mann-Whitney U and Kruskal-Wallis H tests.\n\n    Parameters\n    ----------\n    rankvals : array_like\n        A 1-D sequence of ranks.  Typically this will be the array\n        returned by `~scipy.stats.rankdata`.\n\n    Returns\n    -------\n    factor : float\n        Correction factor for U or H.\n\n    See Also\n    --------\n    rankdata : Assign ranks to the data\n    mannwhitneyu : Mann-Whitney rank test\n    kruskal : Kruskal-Wallis H test\n\n    References\n    ----------\n    .. [1] Siegel, S. (1956) Nonparametric Statistics for the Behavioral\n           Sciences.  New York: McGraw-Hill.\n\n    Examples\n    --------\n    >>> from scipy.stats import tiecorrect, rankdata\n    >>> tiecorrect([1, 2.5, 2.5, 4])\n    0.9\n    >>> ranks = rankdata([1, 3, 2, 4, 5, 7, 2, 8, 4])\n    >>> ranks\n    array([ 1. ,  4. ,  2.5,  5.5,  7. ,  8. ,  2.5,  9. ,  5.5])\n    >>> tiecorrect(ranks)\n    0.9833333333333333\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 8392, "code": "def ranksums(x, y, alternative='two-sided'):\n    x, y = map(np.asarray, (x, y))\n    n1 = len(x)\n    n2 = len(y)\n    alldata = np.concatenate((x, y))\n    ranked = rankdata(alldata)\n    x = ranked[:n1]\n    s = np.sum(x, axis=0)\n    expected = n1 * (n1+n2+1) / 2.0\n    z = (s - expected) / np.sqrt(n1*n2*(n1+n2+1)/12.0)\n    pvalue = _get_pvalue(z, _SimpleNormal(), alternative, xp=np)", "documentation": "    \"\"\"Compute the Wilcoxon rank-sum statistic for two samples.\n\n    The Wilcoxon rank-sum test tests the null hypothesis that two sets\n    of measurements are drawn from the same distribution.  The alternative\n    hypothesis is that values in one sample are more likely to be\n    larger than the values in the other sample.\n\n    This test should be used to compare two samples from continuous\n    distributions.  It does not handle ties between measurements\n    in x and y.  For tie-handling and an optional continuity correction\n    see `scipy.stats.mannwhitneyu`.\n\n    Parameters\n    ----------\n    x, y : array_like\n        The data from the two samples.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n        The following options are available:\n\n        * 'two-sided': one of the distributions (underlying `x` or `y`) is\n          stochastically greater than the other.\n        * 'less': the distribution underlying `x` is stochastically less\n          than the distribution underlying `y`.\n        * 'greater': the distribution underlying `x` is stochastically greater\n          than the distribution underlying `y`.\n\n        .. versionadded:: 1.7.0\n\n    Returns\n    -------\n    statistic : float\n        The test statistic under the large-sample approximation that the\n        rank sum statistic is normally distributed.\n    pvalue : float\n        The p-value of the test.\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/Wilcoxon_rank-sum_test\n\n    Examples\n    --------\n    We can test the hypothesis that two independent unequal-sized samples are\n    drawn from the same distribution with computing the Wilcoxon rank-sum\n    statistic.\n\n    >>> import numpy as np\n    >>> from scipy.stats import ranksums\n    >>> rng = np.random.default_rng()\n    >>> sample1 = rng.uniform(-1, 1, 200)\n    >>> sample2 = rng.uniform(-0.5, 1.5, 300) # a shifted distribution\n    >>> ranksums(sample1, sample2)\n    RanksumsResult(statistic=-7.887059,\n                   pvalue=3.09390448e-15) # may vary\n    >>> ranksums(sample1, sample2, alternative='less')\n    RanksumsResult(statistic=-7.750585297581713,\n                   pvalue=4.573497606342543e-15) # may vary\n    >>> ranksums(sample1, sample2, alternative='greater')\n    RanksumsResult(statistic=-7.750585297581713,\n                   pvalue=0.9999999999999954) # may vary\n\n    The p-value of less than ``0.05`` indicates that this test rejects the\n    hypothesis at the 5% significance level.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 8478, "code": "def kruskal(*samples, nan_policy='propagate', axis=0):\n    xp = array_namespace(*samples)\n    samples = xp_promote(*samples, force_floating=True, xp=xp)\n    num_groups = len(samples)\n    if num_groups < 2:\n        raise ValueError(\"Need at least two groups in stats.kruskal()\")\n    n = [sample.shape[-1] for sample in samples]\n    totaln = sum(n)\n    if any(n) < 1:  # Only needed for `test_axis_nan_policy`\n        raise ValueError(\"Inputs must not be empty.\")\n    alldata = xp.concat(samples, axis=-1)", "documentation": "    \"\"\"Compute the Kruskal-Wallis H-test for independent samples.\n\n    The Kruskal-Wallis H-test tests the null hypothesis that the population\n    median of all of the groups are equal.  It is a non-parametric version of\n    ANOVA.  The test works on 2 or more independent samples, which may have\n    different sizes.  Note that rejecting the null hypothesis does not\n    indicate which of the groups differs.  Post hoc comparisons between\n    groups are required to determine which groups are different.\n\n    Parameters\n    ----------\n    sample1, sample2, ... : array_like\n       Two or more arrays with the sample measurements can be given as\n       arguments. Samples must be one-dimensional.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    axis : int or tuple of ints, default: 0\n        If an int or tuple of ints, the axis or axes of the input along which\n        to compute the statistic. The statistic of each axis-slice (e.g. row)\n        of the input will appear in a corresponding element of the output.\n        If ``None``, the input will be raveled before computing the statistic.\n\n    Returns\n    -------\n    statistic : float\n       The Kruskal-Wallis H statistic, corrected for ties.\n    pvalue : float\n       The p-value for the test using the assumption that H has a chi\n       square distribution. The p-value returned is the survival function of\n       the chi square distribution evaluated at H.\n\n    See Also\n    --------\n    f_oneway : 1-way ANOVA.\n    mannwhitneyu : Mann-Whitney rank test on two samples.\n    friedmanchisquare : Friedman test for repeated measurements.\n\n    Notes\n    -----\n    Due to the assumption that H has a chi square distribution, the number\n    of samples in each group must not be too small.  A typical rule is\n    that each sample must have at least 5 measurements.\n\n    References\n    ----------\n    .. [1] W. H. Kruskal & W. W. Wallis, \"Use of Ranks in\n       One-Criterion Variance Analysis\", Journal of the American Statistical\n       Association, Vol. 47, Issue 260, pp. 583-621, 1952.\n    .. [2] https://en.wikipedia.org/wiki/Kruskal-Wallis_one-way_analysis_of_variance\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> x = [1, 3, 5, 7, 9]\n    >>> y = [2, 4, 6, 8, 10]\n    >>> stats.kruskal(x, y)\n    KruskalResult(statistic=0.2727272727272734, pvalue=0.6015081344405895)\n\n    >>> x = [1, 1, 1]\n    >>> y = [2, 2, 2]\n    >>> z = [2, 2]\n    >>> stats.kruskal(x, y, z)\n    KruskalResult(statistic=7.0, pvalue=0.0301973834223185)\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 8589, "code": "def friedmanchisquare(*samples, axis=0):\n    k = len(samples)\n    if k < 3:\n        raise ValueError('At least 3 samples must be given '\n                         f'for Friedman test, got {k}.')\n    xp = array_namespace(*samples)\n    samples = xp_promote(*samples, force_floating=True, xp=xp)\n    dtype = samples[0].dtype\n    n = samples[0].shape[-1]\n    if n == 0:  # only for `test_axis_nan_policy`; user doesn't see this\n        raise ValueError(\"One or more sample arguments is too small.\")", "documentation": "    \"\"\"Compute the Friedman test for repeated samples.\n\n    The Friedman test tests the null hypothesis that repeated samples of\n    the same individuals have the same distribution.  It is often used\n    to test for consistency among samples obtained in different ways.\n    For example, if two sampling techniques are used on the same set of\n    individuals, the Friedman test can be used to determine if the two\n    sampling techniques are consistent.\n\n    Parameters\n    ----------\n    sample1, sample2, sample3... : array_like\n        Arrays of observations.  All of the arrays must have the same number\n        of elements.  At least three samples must be given.\n    axis : int or tuple of ints, default: 0\n        If an int or tuple of ints, the axis or axes of the input along which\n        to compute the statistic. The statistic of each axis-slice (e.g. row)\n        of the input will appear in a corresponding element of the output.\n        If ``None``, the input will be raveled before computing the statistic.\n\n    Returns\n    -------\n    statistic : float\n        The test statistic, correcting for ties.\n    pvalue : float\n        The associated p-value assuming that the test statistic has a chi\n        squared distribution.\n\n    See Also\n    --------\n    :ref:`hypothesis_friedmanchisquare` : Extended example\n\n    Notes\n    -----\n    Due to the assumption that the test statistic has a chi squared\n    distribution, the p-value is only reliable for n > 10 and more than\n    6 repeated samples.\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/Friedman_test\n    .. [2] Demsar, J. (2006). Statistical comparisons of classifiers over\n           multiple data sets. Journal of Machine Learning Research, 7, 1-30.\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> rng = np.random.default_rng(seed=18)\n    >>> x = rng.random((6, 10))\n    >>> from scipy.stats import friedmanchisquare\n    >>> res = friedmanchisquare(x[0], x[1], x[2], x[3], x[4], x[5])\n    >>> res.statistic, res.pvalue\n    (11.428571428571416, 0.043514520866727614)\n\n    The p-value is less than 0.05; however, as noted above, the results may not\n    be reliable since we have a small number of repeated samples.\n\n    For a more detailed example, see :ref:`hypothesis_friedmanchisquare`.\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 8830, "code": "def combine_pvalues(pvalues, method='fisher', weights=None, *, axis=0):\n    xp = array_namespace(pvalues, weights)\n    pvalues, weights = xp_promote(pvalues, weights, broadcast=True,\n                                  force_floating=True, xp=xp)\n    if xp_size(pvalues) == 0:\n        NaN = _get_nan(pvalues)\n        return SignificanceResult(NaN, NaN)\n    n = _length_nonmasked(pvalues, axis)\n    n = xp.asarray(n, dtype=pvalues.dtype, device=xp_device(pvalues))\n    if method == 'fisher':\n        statistic = -2 * xp.sum(xp.log(pvalues), axis=axis)", "documentation": "    \"\"\"\n    Combine p-values from independent tests that bear upon the same hypothesis.\n\n    These methods are intended only for combining p-values from hypothesis\n    tests based upon continuous distributions.\n\n    Each method assumes that under the null hypothesis, the p-values are\n    sampled independently and uniformly from the interval [0, 1]. A test\n    statistic (different for each method) is computed and a combined\n    p-value is calculated based upon the distribution of this test statistic\n    under the null hypothesis.\n\n    Parameters\n    ----------\n    pvalues : array_like\n        Array of p-values assumed to come from independent tests based on\n        continuous distributions.\n    method : {'fisher', 'pearson', 'tippett', 'stouffer', 'mudholkar_george'}\n\n        Name of method to use to combine p-values.\n\n        The available methods are (see Notes for details):\n\n        * 'fisher': Fisher's method (Fisher's combined probability test)\n        * 'pearson': Pearson's method\n        * 'mudholkar_george': Mudholkar's and George's method\n        * 'tippett': Tippett's method\n        * 'stouffer': Stouffer's Z-score method\n\n    weights : array_like, optional\n        Optional array of weights used only for Stouffer's Z-score method.\n        Ignored by other methods.\n\n    Returns\n    -------\n    res : SignificanceResult\n        An object containing attributes:\n\n        statistic : float\n            The statistic calculated by the specified method.\n        pvalue : float\n            The combined p-value.\n\n    Examples\n    --------\n    Suppose we wish to combine p-values from four independent tests\n    of the same null hypothesis using Fisher's method (default).\n\n    >>> from scipy.stats import combine_pvalues\n    >>> pvalues = [0.1, 0.05, 0.02, 0.3]\n    >>> combine_pvalues(pvalues)\n    SignificanceResult(statistic=20.828626352604235, pvalue=0.007616871850449092)\n\n    When the individual p-values carry different weights, consider Stouffer's\n    method.\n\n    >>> weights = [1, 2, 3, 4]\n    >>> res = combine_pvalues(pvalues, method='stouffer', weights=weights)\n    >>> res.pvalue\n    0.009578891494533616\n\n    Notes\n    -----\n    If this function is applied to tests with a discrete statistics such as\n    any rank test or contingency-table test, it will yield systematically\n    wrong results, e.g. Fisher's method will systematically overestimate the\n    p-value [1]_. This problem becomes less severe for large sample sizes\n    when the discrete distributions become approximately continuous.\n\n    The differences between the methods can be best illustrated by their\n    statistics and what aspects of a combination of p-values they emphasise\n    when considering significance [2]_. For example, methods emphasising large\n    p-values are more sensitive to strong false and true negatives; conversely\n    methods focussing on small p-values are sensitive to positives.\n\n    * The statistics of Fisher's method (also known as Fisher's combined\n      probability test) [3]_ is :math:`-2\\\\sum_i \\\\log(p_i)`, which is\n      equivalent (as a test statistics) to the product of individual p-values:\n      :math:`\\\\prod_i p_i`. Under the null hypothesis, this statistics follows\n      a :math:`\\\\chi^2` distribution. This method emphasises small p-values.\n    * Pearson's method uses :math:`-2\\\\sum_i\\\\log(1-p_i)`, which is equivalent\n      to :math:`\\\\prod_i \\\\frac{1}{1-p_i}` [2]_.\n      It thus emphasises large p-values.\n    * Mudholkar and George compromise between Fisher's and Pearson's method by\n      averaging their statistics [4]_. Their method emphasises extreme\n      p-values, both close to 1 and 0.\n    * Stouffer's method [5]_ uses Z-scores and the statistic:\n      :math:`\\\\sum_i \\\\Phi^{-1} (p_i)`, where :math:`\\\\Phi` is the CDF of the\n      standard normal distribution. The advantage of this method is that it is\n      straightforward to introduce weights, which can make Stouffer's method\n      more powerful than Fisher's method when the p-values are from studies\n      of different size [6]_ [7]_.\n    * Tippett's method uses the smallest p-value as a statistic.\n      (Mind that this minimum is not the combined p-value.)\n\n    Fisher's method may be extended to combine p-values from dependent tests\n    [8]_. Extensions such as Brown's method and Kost's method are not currently\n    implemented.\n\n    .. versionadded:: 0.15.0\n\n    References\n    ----------\n    .. [1] Kincaid, W. M., \"The Combination of Tests Based on Discrete\n           Distributions.\" Journal of the American Statistical Association 57,\n           no. 297 (1962), 10-19.\n    .. [2] Heard, N. and Rubin-Delanchey, P. \"Choosing between methods of\n           combining p-values.\"  Biometrika 105.1 (2018): 239-246.\n    .. [3] https://en.wikipedia.org/wiki/Fisher%27s_method\n    .. [4] George, E. O., and G. S. Mudholkar. \"On the convolution of logistic\n           random variables.\" Metrika 30.1 (1983): 1-13.\n    .. [5] https://en.wikipedia.org/wiki/Fisher%27s_method#Relation_to_Stouffer.27s_Z-score_method\n    .. [6] Whitlock, M. C. \"Combining probability from independent tests: the\n           weighted Z-method is superior to Fisher's approach.\" Journal of\n           Evolutionary Biology 18, no. 5 (2005): 1368-1373.\n    .. [7] Zaykin, Dmitri V. \"Optimally weighted Z-test is a powerful method\n           for combining probabilities in meta-analysis.\" Journal of\n           Evolutionary Biology 24, no. 8 (2011): 1836-1841.\n    .. [8] https://en.wikipedia.org/wiki/Extensions_of_Fisher%27s_method\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 9043, "code": "    def confidence_interval(self, confidence_level=0.95):\n        alternative = self._alternative\n        p = self._p\n        x = np.sort(self._x)\n        n = len(x)\n        bd = stats.binom(n, p)\n        if confidence_level <= 0 or confidence_level >= 1:\n            message = \"`confidence_level` must be a number between 0 and 1.\"\n            raise ValueError(message)\n        low_index = np.nan\n        high_index = np.nan", "documentation": "        \"\"\"\n        Compute the confidence interval of the quantile.\n\n        Parameters\n        ----------\n        confidence_level : float, default: 0.95\n            Confidence level for the computed confidence interval\n            of the quantile. Default is 0.95.\n\n        Returns\n        -------\n        ci : ``ConfidenceInterval`` object\n            The object has attributes ``low`` and ``high`` that hold the\n            lower and upper bounds of the confidence interval.\n\n        Examples\n        --------\n        >>> import numpy as np\n        >>> import scipy.stats as stats\n        >>> p = 0.75  # quantile of interest\n        >>> q = 0  # hypothesized value of the quantile\n        >>> x = np.exp(np.arange(0, 1.01, 0.01))\n        >>> res = stats.quantile_test(x, q=q, p=p, alternative='less')\n        >>> lb, ub = res.confidence_interval()\n        >>> lb, ub\n        (-inf, 2.293318740264183)\n        >>> res = stats.quantile_test(x, q=q, p=p, alternative='two-sided')\n        >>> lb, ub = res.confidence_interval(0.9)\n        >>> lb, ub\n        (1.9542373206359396, 2.293318740264183)\n        \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 9945, "code": "def _validate_distribution(values, weights):\n    values = np.asarray(values, dtype=float)\n    if len(values) == 0:\n        raise ValueError(\"Distribution can't be empty.\")\n    if weights is not None:\n        weights = np.asarray(weights, dtype=float)\n        if len(weights) != len(values):\n            raise ValueError('Value and weight array-likes for the same '\n                             'empirical distribution must be of the same size.')\n        if np.any(weights < 0):\n            raise ValueError('All weights must be non-negative.')", "documentation": "    \"\"\"\n    Validate the values and weights from a distribution input of `cdf_distance`\n    and return them as ndarray objects.\n\n    Parameters\n    ----------\n    values : array_like\n        Values observed in the (empirical) distribution.\n    weights : array_like\n        Weight for each value.\n\n    Returns\n    -------\n    values : ndarray\n        Values as ndarray.\n    weights : ndarray\n        Weights as ndarray.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 9990, "code": "def rankdata(a, method='average', *, axis=None, nan_policy='propagate'):\n    methods = ('average', 'min', 'max', 'dense', 'ordinal')\n    if method not in methods:\n        raise ValueError(f'unknown method \"{method}\"')\n    xp = array_namespace(a)\n    x = xp.asarray(a)\n    if axis is None:\n        x = xp_ravel(x)\n        axis = -1\n    if xp_size(x) == 0:\n        dtype = xp.asarray(1.).dtype if method == 'average' else xp.asarray(1).dtype", "documentation": "    \"\"\"Assign ranks to data, dealing with ties appropriately.\n\n    By default (``axis=None``), the data array is first flattened, and a flat\n    array of ranks is returned. Separately reshape the rank array to the\n    shape of the data array if desired (see Examples).\n\n    Ranks begin at 1.  The `method` argument controls how ranks are assigned\n    to equal values.  See [1]_ for further discussion of ranking methods.\n\n    Parameters\n    ----------\n    a : array_like\n        The array of values to be ranked.\n    method : {'average', 'min', 'max', 'dense', 'ordinal'}, optional\n        The method used to assign ranks to tied elements.\n        The following methods are available (default is 'average'):\n\n        * 'average': The average of the ranks that would have been assigned to\n          all the tied values is assigned to each value.\n        * 'min': The minimum of the ranks that would have been assigned to all\n          the tied values is assigned to each value.  (This is also\n          referred to as \"competition\" ranking.)\n        * 'max': The maximum of the ranks that would have been assigned to all\n          the tied values is assigned to each value.\n        * 'dense': Like 'min', but the rank of the next highest element is\n          assigned the rank immediately after those assigned to the tied\n          elements.\n        * 'ordinal': All values are given a distinct rank, corresponding to\n          the order that the values occur in `a`.\n\n    axis : {None, int}, optional\n        Axis along which to perform the ranking. If ``None``, the data array\n        is first flattened.\n    nan_policy : {'propagate', 'omit', 'raise'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': propagates nans through the rank calculation\n        * 'omit': performs the calculations ignoring nan values\n        * 'raise': raises an error\n\n        .. note::\n\n            When `nan_policy` is 'propagate', the output is an array of *all*\n            nans because ranks relative to nans in the input are undefined.\n            When `nan_policy` is 'omit', nans in `a` are ignored when ranking\n            the other values, and the corresponding locations of the output\n            are nan.\n\n        .. versionadded:: 1.10\n\n    Returns\n    -------\n    ranks : ndarray\n         An array of size equal to the size of `a`, containing rank\n         scores.\n\n    References\n    ----------\n    .. [1] \"Ranking\", https://en.wikipedia.org/wiki/Ranking\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import rankdata\n    >>> rankdata([0, 2, 3, 2])\n    array([ 1. ,  2.5,  4. ,  2.5])\n    >>> rankdata([0, 2, 3, 2], method='min')\n    array([ 1,  2,  4,  2])\n    >>> rankdata([0, 2, 3, 2], method='max')\n    array([ 1,  3,  4,  3])\n    >>> rankdata([0, 2, 3, 2], method='dense')\n    array([ 1,  2,  3,  2])\n    >>> rankdata([0, 2, 3, 2], method='ordinal')\n    array([ 1,  2,  4,  3])\n    >>> rankdata([[0, 2], [3, 2]]).reshape(2,2)\n    array([[1. , 2.5],\n          [4. , 2.5]])\n    >>> rankdata([[0, 2, 2], [3, 2, 5]], axis=1)\n    array([[1. , 2.5, 2.5],\n           [2. , 1. , 3. ]])\n    >>> rankdata([0, 2, 3, np.nan, -2, np.nan], nan_policy=\"propagate\")\n    array([nan, nan, nan, nan, nan, nan])\n    >>> rankdata([0, 2, 3, np.nan, -2, np.nan], nan_policy=\"omit\")\n    array([ 2.,  3.,  4., nan,  1., nan])\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 10530, "code": "def linregress(x, y, alternative='two-sided', *, axis=0):\n    xp = array_namespace(x, y)\n    x, y = xp_promote(x, y, force_floating=True, xp=xp)\n    TINY = 1.0e-20\n    n = x.shape[-1]\n    xmean = xp.mean(x, axis=-1, keepdims=True)\n    ymean = xp.mean(y, axis=-1, keepdims=True)\n    x_ = _demean(x, xmean, axis=-1, xp=xp)\n    y_ = _demean(y, ymean, axis=-1, xp=xp, precision_warning=False)\n    xmean = xp.squeeze(xmean, axis=-1)\n    ymean = xp.squeeze(ymean, axis=-1)", "documentation": "    \"\"\"\n    Calculate a linear least-squares regression for two sets of measurements.\n\n    Parameters\n    ----------\n    x, y : array_like\n        Two sets of measurements.  Both arrays should have the same length N.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n        The following options are available:\n\n        * 'two-sided': the slope of the regression line is nonzero\n        * 'less': the slope of the regression line is less than zero\n        * 'greater':  the slope of the regression line is greater than zero\n\n        .. versionadded:: 1.7.0\n    axis : int or None, default: 0\n        If an int, the axis of the input along which to compute the statistic.\n        The statistic of each axis-slice (e.g. row) of the input will appear in a\n        corresponding element of the output.\n        If ``None``, the input will be raveled before computing the statistic.\n\n    Returns\n    -------\n    result : ``LinregressResult`` instance\n        The return value is an object with the following attributes:\n\n        slope : float\n            Slope of the regression line.\n        intercept : float\n            Intercept of the regression line.\n        rvalue : float\n            The Pearson correlation coefficient. The square of ``rvalue``\n            is equal to the coefficient of determination.\n        pvalue : float\n            The p-value for a hypothesis test whose null hypothesis is\n            that the slope is zero, using Wald Test with t-distribution of\n            the test statistic. See `alternative` above for alternative\n            hypotheses.\n        stderr : float\n            Standard error of the estimated slope (gradient), under the\n            assumption of residual normality.\n        intercept_stderr : float\n            Standard error of the estimated intercept, under the assumption\n            of residual normality.\n\n    See Also\n    --------\n    scipy.optimize.curve_fit :\n        Use non-linear least squares to fit a function to data.\n    scipy.optimize.leastsq :\n        Minimize the sum of squares of a set of equations.\n\n    Notes\n    -----\n    For compatibility with older versions of SciPy, the return value acts\n    like a ``namedtuple`` of length 5, with fields ``slope``, ``intercept``,\n    ``rvalue``, ``pvalue`` and ``stderr``, so one can continue to write::\n\n        slope, intercept, r, p, se = linregress(x, y)\n\n    With that style, however, the standard error of the intercept is not\n    available.  To have access to all the computed values, including the\n    standard error of the intercept, use the return value as an object\n    with attributes, e.g.::\n\n        result = linregress(x, y)\n        print(result.intercept, result.intercept_stderr)\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n\n    Generate some data:\n\n    >>> x = rng.random(10)\n    >>> y = 1.6*x + rng.random(10)\n\n    Perform the linear regression:\n\n    >>> res = stats.linregress(x, y)\n\n    Coefficient of determination (R-squared):\n\n    >>> print(f\"R-squared: {res.rvalue**2:.6f}\")\n    R-squared: 0.717533\n\n    Plot the data along with the fitted line:\n\n    >>> plt.plot(x, y, 'o', label='original data')\n    >>> plt.plot(x, res.intercept + res.slope*x, 'r', label='fitted line')\n    >>> plt.legend()\n    >>> plt.show()\n\n    Calculate 95% confidence interval on slope and intercept:\n\n    >>> # Two-sided inverse Students t-distribution\n    >>> # p - probability, df - degrees of freedom\n    >>> from scipy.stats import t\n    >>> tinv = lambda p, df: abs(t.ppf(p/2, df))\n\n    >>> ts = tinv(0.05, len(x)-2)\n    >>> print(f\"slope (95%): {res.slope:.6f} +/- {ts*res.stderr:.6f}\")\n    slope (95%): 1.453392 +/- 0.743465\n    >>> print(f\"intercept (95%): {res.intercept:.6f}\"\n    ...       f\" +/- {ts*res.intercept_stderr:.6f}\")\n    intercept (95%): 0.616950 +/- 0.544475\n\n    \"\"\""}], "after_segments": [{"filename": "scipy/stats/_stats_py.py", "start_line": 603, "code": "def _put_val_to_limits(a, limits, inclusive, val=np.nan, xp=None):\n    xp = array_namespace(a) if xp is None else xp\n    mask = xp.zeros_like(a, dtype=xp.bool)\n    if limits is None:\n        return a, mask\n    lower_limit, upper_limit = limits\n    lower_include, upper_include = inclusive\n    if lower_limit is not None:\n        mask = mask | ((a < lower_limit) if lower_include else a <= lower_limit)\n    if upper_limit is not None:\n        mask = mask | ((a > upper_limit) if upper_include else a >= upper_limit)", "documentation": "    \"\"\"Replace elements outside limits with a value.\n\n    This is primarily a utility function.\n\n    Parameters\n    ----------\n    a : array\n    limits : (float or None, float or None)\n        A tuple consisting of the (lower limit, upper limit).  Elements in the\n        input array less than the lower limit or greater than the upper limit\n        will be replaced with `val`. None implies no limit.\n    inclusive : (bool, bool)\n        A tuple consisting of the (lower flag, upper flag).  These flags\n        determine whether values exactly equal to lower or upper are allowed.\n    val : float, default: NaN\n        The value with which extreme elements of the array are replaced.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 645, "code": "def tmean(a, limits=None, inclusive=(True, True), axis=None):\n    xp = array_namespace(a)\n    a, mask = _put_val_to_limits(a, limits, inclusive, val=0., xp=xp)\n    sum = xp.sum(a, axis=axis, dtype=a.dtype)\n    n = xp.sum(xp.asarray(~mask, dtype=a.dtype, device=xp_device(a)), axis=axis,\n               dtype=a.dtype)\n    mean = xpx.apply_where(n != 0, (sum, n), operator.truediv, fill_value=xp.nan)\n    return mean[()] if mean.ndim == 0 else mean\n@xp_capabilities()\n@_axis_nan_policy_factory(\n    lambda x: x, n_outputs=1, result_to_tuple=lambda x, _: (x,)", "documentation": "    \"\"\"Compute the trimmed mean.\n\n    This function finds the arithmetic mean of given values, ignoring values\n    outside the given `limits`.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    limits : None or (lower limit, upper limit), optional\n        Values in the input array less than the lower limit or greater than the\n        upper limit will be ignored.  When limits is None (default), then all\n        values are used.  Either of the limit values in the tuple can also be\n        None representing a half-open interval.\n    inclusive : (bool, bool), optional\n        A tuple consisting of the (lower flag, upper flag).  These flags\n        determine whether values exactly equal to the lower or upper limits\n        are included.  The default value is (True, True).\n    axis : int or None, optional\n        Axis along which to compute test. Default is None.\n\n    Returns\n    -------\n    tmean : ndarray\n        Trimmed mean.\n\n    See Also\n    --------\n    trim_mean : Returns mean after trimming a proportion from both tails.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tmean(x)\n    9.5\n    >>> stats.tmean(x, (3,17))\n    10.0\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 701, "code": "def tvar(a, limits=None, inclusive=(True, True), axis=0, ddof=1):\n    xp = array_namespace(a)\n    a, _ = _put_val_to_limits(a, limits, inclusive, xp=xp)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", SmallSampleWarning)\n        return _xp_var(a, correction=ddof, axis=axis, nan_policy='omit', xp=xp)\n@xp_capabilities()\n@_axis_nan_policy_factory(\n    lambda x: x, n_outputs=1, result_to_tuple=lambda x, _: (x,)\n)", "documentation": "    \"\"\"Compute the trimmed variance.\n\n    This function computes the sample variance of an array of values,\n    while ignoring values which are outside of given `limits`.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    limits : None or (lower limit, upper limit), optional\n        Values in the input array less than the lower limit or greater than the\n        upper limit will be ignored. When limits is None, then all values are\n        used. Either of the limit values in the tuple can also be None\n        representing a half-open interval.  The default value is None.\n    inclusive : (bool, bool), optional\n        A tuple consisting of the (lower flag, upper flag).  These flags\n        determine whether values exactly equal to the lower or upper limits\n        are included.  The default value is (True, True).\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over the\n        whole array `a`.\n    ddof : int, optional\n        Delta degrees of freedom.  Default is 1.\n\n    Returns\n    -------\n    tvar : float\n        Trimmed variance.\n\n    Notes\n    -----\n    `tvar` computes the unbiased sample variance, i.e. it uses a correction\n    factor ``n / (n - 1)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tvar(x)\n    35.0\n    >>> stats.tvar(x, (3,17))\n    20.0\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 761, "code": "def tmin(a, lowerlimit=None, axis=0, inclusive=True, nan_policy='propagate'):\n    xp = array_namespace(a)\n    max_ = xp.iinfo(a.dtype).max if xp.isdtype(a.dtype, 'integral') else xp.inf\n    a, mask = _put_val_to_limits(a, (lowerlimit, None), (inclusive, None),\n                                 val=max_, xp=xp)\n    res = xp.min(a, axis=axis)\n    invalid = xp.all(mask, axis=axis)  # All elements are below lowerlimit\n    if is_lazy_array(invalid) or xp.any(invalid):\n        res = xp_promote(res, force_floating=True, xp=xp)\n        res = xp.where(invalid, xp.nan, res)\n    return res[()] if res.ndim == 0 else res", "documentation": "    \"\"\"Compute the trimmed minimum.\n\n    This function finds the minimum value of an array `a` along the\n    specified axis, but only considering values greater than a specified\n    lower limit.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    lowerlimit : None or float, optional\n        Values in the input array less than the given limit will be ignored.\n        When lowerlimit is None, then all values are used. The default value\n        is None.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over the\n        whole array `a`.\n    inclusive : {True, False}, optional\n        This flag determines whether values exactly equal to the lower limit\n        are included.  The default value is True.\n\n    Returns\n    -------\n    tmin : float, int or ndarray\n        Trimmed minimum.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tmin(x)\n    0\n\n    >>> stats.tmin(x, 13)\n    13\n\n    >>> stats.tmin(x, 13, inclusive=False)\n    14\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 825, "code": "def tmax(a, upperlimit=None, axis=0, inclusive=True, nan_policy='propagate'):\n    xp = array_namespace(a)\n    min_ = xp.iinfo(a.dtype).min if xp.isdtype(a.dtype, 'integral') else -xp.inf\n    a, mask = _put_val_to_limits(a, (None, upperlimit), (None, inclusive),\n                                 val=min_, xp=xp)\n    res = xp.max(a, axis=axis)\n    invalid = xp.all(mask, axis=axis)  # All elements are above upperlimit\n    if is_lazy_array(invalid) or xp.any(invalid):\n        res = xp_promote(res, force_floating=True, xp=xp)\n        res = xp.where(invalid, xp.nan, res)\n    return res[()] if res.ndim == 0 else res", "documentation": "    \"\"\"Compute the trimmed maximum.\n\n    This function computes the maximum value of an array along a given axis,\n    while ignoring values larger than a specified upper limit.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    upperlimit : None or float, optional\n        Values in the input array greater than the given limit will be ignored.\n        When upperlimit is None, then all values are used. The default value\n        is None.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over the\n        whole array `a`.\n    inclusive : {True, False}, optional\n        This flag determines whether values exactly equal to the upper limit\n        are included.  The default value is True.\n\n    Returns\n    -------\n    tmax : float, int or ndarray\n        Trimmed maximum.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tmax(x)\n    19\n\n    >>> stats.tmax(x, 13)\n    13\n\n    >>> stats.tmax(x, 13, inclusive=False)\n    12\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 888, "code": "def tstd(a, limits=None, inclusive=(True, True), axis=0, ddof=1):\n    return tvar(a, limits, inclusive, axis, ddof, _no_deco=True)**0.5\n@xp_capabilities()\n@_axis_nan_policy_factory(\n    lambda x: x, n_outputs=1, result_to_tuple=lambda x, _: (x,)\n)", "documentation": "    \"\"\"Compute the trimmed sample standard deviation.\n\n    This function finds the sample standard deviation of given values,\n    ignoring values outside the given `limits`.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    limits : None or (lower limit, upper limit), optional\n        Values in the input array less than the lower limit or greater than the\n        upper limit will be ignored. When limits is None, then all values are\n        used. Either of the limit values in the tuple can also be None\n        representing a half-open interval.  The default value is None.\n    inclusive : (bool, bool), optional\n        A tuple consisting of the (lower flag, upper flag).  These flags\n        determine whether values exactly equal to the lower or upper limits\n        are included.  The default value is (True, True).\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over the\n        whole array `a`.\n    ddof : int, optional\n        Delta degrees of freedom.  Default is 1.\n\n    Returns\n    -------\n    tstd : float\n        Trimmed sample standard deviation.\n\n    Notes\n    -----\n    `tstd` computes the unbiased sample standard deviation, i.e. it uses a\n    correction factor ``n / (n - 1)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tstd(x)\n    5.9160797830996161\n    >>> stats.tstd(x, (3,17))\n    4.4721359549995796\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 941, "code": "def tsem(a, limits=None, inclusive=(True, True), axis=0, ddof=1):\n    xp = array_namespace(a)\n    a, _ = _put_val_to_limits(a, limits, inclusive, xp=xp)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", SmallSampleWarning)\n        sd = _xp_var(a, correction=ddof, axis=axis, nan_policy='omit', xp=xp)**0.5\n    not_nan = xp.astype(~xp.isnan(a), a.dtype)\n    n_obs = xp.sum(not_nan, axis=axis, dtype=sd.dtype)\n    return sd / n_obs**0.5", "documentation": "    \"\"\"Compute the trimmed standard error of the mean.\n\n    This function finds the standard error of the mean for given\n    values, ignoring values outside the given `limits`.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    limits : None or (lower limit, upper limit), optional\n        Values in the input array less than the lower limit or greater than the\n        upper limit will be ignored. When limits is None, then all values are\n        used. Either of the limit values in the tuple can also be None\n        representing a half-open interval.  The default value is None.\n    inclusive : (bool, bool), optional\n        A tuple consisting of the (lower flag, upper flag).  These flags\n        determine whether values exactly equal to the lower or upper limits\n        are included.  The default value is (True, True).\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over the\n        whole array `a`.\n    ddof : int, optional\n        Delta degrees of freedom.  Default is 1.\n\n    Returns\n    -------\n    tsem : float\n        Trimmed standard error of the mean.\n\n    Notes\n    -----\n    `tsem` uses unbiased sample standard deviation, i.e. it uses a\n    correction factor ``n / (n - 1)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tsem(x)\n    1.3228756555322954\n    >>> stats.tsem(x, (3,17))\n    1.1547005383792515\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 1192, "code": "def _moment(a, order, axis, *, mean=None, xp=None):\n    xp = array_namespace(a) if xp is None else xp\n    a = xp_promote(a, force_floating=True, xp=xp)\n    dtype = a.dtype\n    if xp_size(a) == 0:\n        return xp.mean(a, axis=axis)\n    if order == 0 or (order == 1 and mean is None):\n        shape = list(a.shape)\n        del shape[axis]\n        temp = (xp.ones(shape, dtype=dtype, device=xp_device(a)) if order == 0\n                else xp.zeros(shape, dtype=dtype, device=xp_device(a)))", "documentation": "    \"\"\"Vectorized calculation of raw moment about specified center\n\n    When `mean` is None, the mean is computed and used as the center;\n    otherwise, the provided value is used as the center.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 1365, "code": "def kurtosis(a, axis=0, fisher=True, bias=True, nan_policy='propagate'):\n    xp = array_namespace(a)\n    a, axis = _chk_asarray(a, axis, xp=xp)\n    n = _length_nonmasked(a, axis, xp=xp)\n    mean = xp.mean(a, axis=axis, keepdims=True)\n    mean_reduced = xp.squeeze(mean, axis=axis)  # needed later\n    m2 = _moment(a, 2, axis, mean=mean, xp=xp)\n    m4 = _moment(a, 4, axis, mean=mean, xp=xp)\n    with np.errstate(all='ignore'):\n        zero = m2 <= (xp.finfo(m2.dtype).eps * mean_reduced)**2\n        vals = xp.where(zero, xp.nan, m4 / m2**2.0)", "documentation": "    \"\"\"Compute the kurtosis (Fisher or Pearson) of a dataset.\n\n    Kurtosis is the fourth central moment divided by the square of the\n    variance. If Fisher's definition is used, then 3.0 is subtracted from\n    the result to give 0.0 for a normal distribution.\n\n    If bias is False then the kurtosis is calculated using k statistics to\n    eliminate bias coming from biased moment estimators\n\n    Use `kurtosistest` to see if result is close enough to normal.\n\n    Parameters\n    ----------\n    a : array\n        Data for which the kurtosis is calculated.\n    axis : int or None, optional\n        Axis along which the kurtosis is calculated. Default is 0.\n        If None, compute over the whole array `a`.\n    fisher : bool, optional\n        If True, Fisher's definition is used (normal ==> 0.0). If False,\n        Pearson's definition is used (normal ==> 3.0).\n    bias : bool, optional\n        If False, then the calculations are corrected for statistical bias.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan. 'propagate' returns nan,\n        'raise' throws an error, 'omit' performs the calculations ignoring nan\n        values. Default is 'propagate'.\n\n    Returns\n    -------\n    kurtosis : array\n        The kurtosis of values along an axis, returning NaN where all values\n        are equal.\n\n    References\n    ----------\n    .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n       Probability and Statistics Tables and Formulae. Chapman & Hall: New\n       York. 2000.\n\n    Examples\n    --------\n    In Fisher's definition, the kurtosis of the normal distribution is zero.\n    In the following example, the kurtosis is close to zero, because it was\n    calculated from the dataset, not from the continuous distribution.\n\n    >>> import numpy as np\n    >>> from scipy.stats import norm, kurtosis\n    >>> data = norm.rvs(size=1000, random_state=3)\n    >>> kurtosis(data)\n    -0.06928694200380558\n\n    The distribution with a higher kurtosis has a heavier tail.\n    The zero valued kurtosis of the normal distribution in Fisher's definition\n    can serve as a reference point.\n\n    >>> import matplotlib.pyplot as plt\n    >>> import scipy.stats as stats\n    >>> from scipy.stats import kurtosis\n\n    >>> x = np.linspace(-5, 5, 100)\n    >>> ax = plt.subplot()\n    >>> distnames = ['laplace', 'norm', 'uniform']\n\n    >>> for distname in distnames:\n    ...     if distname == 'uniform':\n    ...         dist = getattr(stats, distname)(loc=-2, scale=4)\n    ...     else:\n    ...         dist = getattr(stats, distname)\n    ...     data = dist.rvs(size=1000)\n    ...     kur = kurtosis(data, fisher=True)\n    ...     y = dist.pdf(x)\n    ...     ax.plot(x, y, label=\"{}, {}\".format(distname, round(kur, 3)))\n    ...     ax.legend()\n\n    The Laplace distribution has a heavier tail than the normal distribution.\n    The uniform distribution (which has negative kurtosis) has the thinnest\n    tail.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 1474, "code": "def describe(a, axis=0, ddof=1, bias=True, nan_policy='propagate'):\n    xp = array_namespace(a)\n    a, axis = _chk_asarray(a, axis, xp=xp)\n    contains_nan = _contains_nan(a, nan_policy)\n    if nan_policy == 'omit' and contains_nan:\n        a = ma.masked_invalid(a)\n        return mstats_basic.describe(a, axis, ddof, bias)\n    if xp_size(a) == 0:\n        raise ValueError(\"The input must not be empty.\")\n    n = xp.asarray(_length_nonmasked(a, axis, xp=xp), dtype=xp.int64,\n                   device=xp_device(a))", "documentation": "    \"\"\"Compute several descriptive statistics of the passed array.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n    axis : int or None, optional\n        Axis along which statistics are calculated. Default is 0.\n        If None, compute over the whole array `a`.\n    ddof : int, optional\n        Delta degrees of freedom (only for variance).  Default is 1.\n    bias : bool, optional\n        If False, then the skewness and kurtosis calculations are corrected\n        for statistical bias.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    nobs : int or ndarray of ints\n        Number of observations (length of data along `axis`).\n        When 'omit' is chosen as nan_policy, the length along each axis\n        slice is counted separately.\n    minmax: tuple of ndarrays or floats\n        Minimum and maximum value of `a` along the given axis.\n    mean : ndarray or float\n        Arithmetic mean of `a` along the given axis.\n    variance : ndarray or float\n        Unbiased variance of `a` along the given axis; denominator is number\n        of observations minus one.\n    skewness : ndarray or float\n        Skewness of `a` along the given axis, based on moment calculations\n        with denominator equal to the number of observations, i.e. no degrees\n        of freedom correction.\n    kurtosis : ndarray or float\n        Kurtosis (Fisher) of `a` along the given axis.  The kurtosis is\n        normalized so that it is zero for the normal distribution.  No\n        degrees of freedom are used.\n\n    Raises\n    ------\n    ValueError\n        If size of `a` is 0.\n\n    See Also\n    --------\n    skew, kurtosis\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> a = np.arange(10)\n    >>> stats.describe(a)\n    DescribeResult(nobs=10, minmax=(0, 9), mean=4.5,\n                   variance=9.166666666666666, skewness=0.0,\n                   kurtosis=-1.2242424242424244)\n    >>> b = [[1, 2], [3, 4]]\n    >>> stats.describe(b)\n    DescribeResult(nobs=2, minmax=(array([1, 2]), array([3, 4])),\n                   mean=array([2., 3.]), variance=array([2., 2.]),\n                   skewness=array([0., 0.]), kurtosis=array([-2., -2.]))\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 1576, "code": "def _get_pvalue(statistic, distribution, alternative, symmetric=True, xp=None):\n    xp = array_namespace(statistic) if xp is None else xp\n    if alternative == 'less':\n        pvalue = distribution.cdf(statistic)\n    elif alternative == 'greater':\n        pvalue = distribution.sf(statistic)\n    elif alternative == 'two-sided':\n        pvalue = 2 * (distribution.sf(xp.abs(statistic)) if symmetric\n                      else xp.minimum(distribution.cdf(statistic),\n                                      distribution.sf(statistic)))\n    else:", "documentation": "    \"\"\"Get p-value given the statistic, (continuous) distribution, and alternative\"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2103, "code": "def percentileofscore(a, score, kind='rank', nan_policy='propagate'):\n    a = np.asarray(a)\n    score = np.asarray(score)\n    if a.ndim != 1:\n        raise ValueError(\"`a` must be 1-dimensional.\")\n    n = len(a)\n    cna = _contains_nan(a, nan_policy)\n    cns = _contains_nan(score, nan_policy)\n    if cns:\n        score = ma.masked_where(np.isnan(score), score)\n    if cna:", "documentation": "    \"\"\"Compute the percentile rank of a score relative to a list of scores.\n\n    A `percentileofscore` of, for example, 80% means that 80% of the\n    scores in `a` are below the given score. In the case of gaps or\n    ties, the exact definition depends on the optional keyword, `kind`.\n\n    Parameters\n    ----------\n    a : array_like\n        A 1-D array to which `score` is compared.\n    score : float or array_like\n        A float score or array of scores for which to compute the percentile(s).\n    kind : {'rank', 'weak', 'strict', 'mean'}, optional\n        Specifies the interpretation of the resulting score.\n        The following options are available (default is 'rank'):\n\n        * 'rank': Average percentage ranking of score.  In case of multiple\n          matches, average the percentage rankings of all matching scores.\n        * 'weak': This kind corresponds to the definition of a cumulative\n          distribution function.  A percentileofscore of 80% means that 80%\n          of values are less than or equal to the provided score.\n        * 'strict': Similar to \"weak\", except that only values that are\n          strictly less than the given score are counted.\n        * 'mean': The average of the \"weak\" and \"strict\" scores, often used\n          in testing.  See https://en.wikipedia.org/wiki/Percentile_rank\n\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Specifies how to treat `nan` values in `a`.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan (for each value in `score`).\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    pcos : float or array-like\n        Percentile-position(s) of `score` (0-100) relative to `a`.\n\n    See Also\n    --------\n    numpy.percentile\n    scipy.stats.scoreatpercentile, scipy.stats.rankdata\n\n    Examples\n    --------\n    Three-quarters of the given values lie below a given score:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> stats.percentileofscore([1, 2, 3, 4], 3)\n    75.0\n\n    With multiple matches, note how the scores of the two matches, 0.6\n    and 0.8 respectively, are averaged:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3)\n    70.0\n\n    Only 2/5 values are strictly less than 3:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='strict')\n    40.0\n\n    But 4/5 values are less than or equal to 3:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='weak')\n    80.0\n\n    The average between the weak and the strict scores is:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='mean')\n    60.0\n\n    Score arrays (of any dimensionality) are supported:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], [2, 3])\n    array([40., 70.])\n\n    The inputs can be infinite:\n\n    >>> stats.percentileofscore([-np.inf, 0, 1, np.inf], [1, 2, np.inf])\n    array([75., 75., 100.])\n\n    If `a` is empty, then the resulting percentiles are all `nan`:\n\n    >>> stats.percentileofscore([], [1, 2])\n    array([nan, nan])\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2346, "code": "def cumfreq(a, numbins=10, defaultreallimits=None, weights=None):\n    h, l, b, e = _histogram(a, numbins, defaultreallimits, weights=weights)\n    cumhist = np.cumsum(h * 1, axis=0)\n    return CumfreqResult(cumhist, l, b, e)\nRelfreqResult = namedtuple('RelfreqResult',\n                           ('frequency', 'lowerlimit', 'binsize',\n                            'extrapoints'))\n@xp_capabilities(np_only=True)", "documentation": "    \"\"\"Return a cumulative frequency histogram, using the histogram function.\n\n    A cumulative histogram is a mapping that counts the cumulative number of\n    observations in all of the bins up to the specified bin.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    numbins : int, optional\n        The number of bins to use for the histogram. Default is 10.\n    defaultreallimits : tuple (lower, upper), optional\n        The lower and upper values for the range of the histogram.\n        If no value is given, a range slightly larger than the range of the\n        values in `a` is used. Specifically ``(a.min() - s, a.max() + s)``,\n        where ``s = (1/2)(a.max() - a.min()) / (numbins - 1)``.\n    weights : array_like, optional\n        The weights for each value in `a`. Default is None, which gives each\n        value a weight of 1.0\n\n    Returns\n    -------\n    cumcount : ndarray\n        Binned values of cumulative frequency.\n    lowerlimit : float\n        Lower real limit\n    binsize : float\n        Width of each bin.\n    extrapoints : int\n        Extra points.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> x = [1, 4, 2, 1, 3, 1]\n    >>> res = stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))\n    >>> res.cumcount\n    array([ 1.,  2.,  3.,  3.])\n    >>> res.extrapoints\n    3\n\n    Create a normal distribution with 1000 random values\n\n    >>> samples = stats.norm.rvs(size=1000, random_state=rng)\n\n    Calculate cumulative frequencies\n\n    >>> res = stats.cumfreq(samples, numbins=25)\n\n    Calculate space of values for x\n\n    >>> x = res.lowerlimit + np.linspace(0, res.binsize*res.cumcount.size,\n    ...                                  res.cumcount.size + 1)\n\n    Plot histogram and cumulative histogram\n\n    >>> fig = plt.figure(figsize=(10, 4))\n    >>> ax1 = fig.add_subplot(1, 2, 1)\n    >>> ax2 = fig.add_subplot(1, 2, 2)\n    >>> ax1.hist(samples, bins=25)\n    >>> ax1.set_title('Histogram')\n    >>> ax2.bar(x[:-1], res.cumcount, width=res.binsize, align='edge')\n    >>> ax2.set_title('Cumulative histogram')\n    >>> ax2.set_xlim([x.min(), x.max()])\n\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2429, "code": "def relfreq(a, numbins=10, defaultreallimits=None, weights=None):\n    a = np.asanyarray(a)\n    h, l, b, e = _histogram(a, numbins, defaultreallimits, weights=weights)\n    h = h / a.shape[0]\n    return RelfreqResult(h, l, b, e)\n@xp_capabilities(np_only=True)", "documentation": "    \"\"\"Return a relative frequency histogram, using the histogram function.\n\n    A relative frequency  histogram is a mapping of the number of\n    observations in each of the bins relative to the total of observations.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    numbins : int, optional\n        The number of bins to use for the histogram. Default is 10.\n    defaultreallimits : tuple (lower, upper), optional\n        The lower and upper values for the range of the histogram.\n        If no value is given, a range slightly larger than the range of the\n        values in a is used. Specifically ``(a.min() - s, a.max() + s)``,\n        where ``s = (1/2)(a.max() - a.min()) / (numbins - 1)``.\n    weights : array_like, optional\n        The weights for each value in `a`. Default is None, which gives each\n        value a weight of 1.0\n\n    Returns\n    -------\n    frequency : ndarray\n        Binned values of relative frequency.\n    lowerlimit : float\n        Lower real limit.\n    binsize : float\n        Width of each bin.\n    extrapoints : int\n        Extra points.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> a = np.array([2, 4, 1, 2, 3, 2])\n    >>> res = stats.relfreq(a, numbins=4)\n    >>> res.frequency\n    array([ 0.16666667, 0.5       , 0.16666667,  0.16666667])\n    >>> np.sum(res.frequency)  # relative frequencies should add up to 1\n    1.0\n\n    Create a normal distribution with 1000 random values\n\n    >>> samples = stats.norm.rvs(size=1000, random_state=rng)\n\n    Calculate relative frequencies\n\n    >>> res = stats.relfreq(samples, numbins=25)\n\n    Calculate space of values for x\n\n    >>> x = res.lowerlimit + np.linspace(0, res.binsize*res.frequency.size,\n    ...                                  res.frequency.size)\n\n    Plot relative frequency histogram\n\n    >>> fig = plt.figure(figsize=(5, 4))\n    >>> ax = fig.add_subplot(1, 1, 1)\n    >>> ax.bar(x, res.frequency, width=res.binsize)\n    >>> ax.set_title('Relative frequency histogram')\n    >>> ax.set_xlim([x.min(), x.max()])\n\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2510, "code": "def obrientransform(*samples):\n    TINY = np.sqrt(np.finfo(float).eps)\n    arrays = []\n    sLast = None\n    for sample in samples:\n        a = np.asarray(sample)\n        n = len(a)\n        mu = np.mean(a)\n        sq = (a - mu)**2\n        sumsq = sq.sum()\n        t = ((n - 1.5) * n * sq - 0.5 * sumsq) / ((n - 1) * (n - 2))", "documentation": "    \"\"\"Compute the O'Brien transform on input data (any number of arrays).\n\n    Used to test for homogeneity of variance prior to running one-way stats.\n    Each array in ``*samples`` is one level of a factor.\n    If `f_oneway` is run on the transformed data and found significant,\n    the variances are unequal.  From Maxwell and Delaney [1]_, p.112.\n\n    Parameters\n    ----------\n    sample1, sample2, ... : array_like\n        Any number of arrays.\n\n    Returns\n    -------\n    obrientransform : ndarray\n        Transformed data for use in an ANOVA.  The first dimension\n        of the result corresponds to the sequence of transformed\n        arrays.  If the arrays given are all 1-D of the same length,\n        the return value is a 2-D array; otherwise it is a 1-D array\n        of type object, with each element being an ndarray.\n\n    Raises\n    ------\n    ValueError\n        If the mean of the transformed data is not equal to the original\n        variance, indicating a lack of convergence in the O'Brien transform.\n\n    References\n    ----------\n    .. [1] S. E. Maxwell and H. D. Delaney, \"Designing Experiments and\n           Analyzing Data: A Model Comparison Perspective\", Wadsworth, 1990.\n\n    Examples\n    --------\n    We'll test the following data sets for differences in their variance.\n\n    >>> x = [10, 11, 13, 9, 7, 12, 12, 9, 10]\n    >>> y = [13, 21, 5, 10, 8, 14, 10, 12, 7, 15]\n\n    Apply the O'Brien transform to the data.\n\n    >>> from scipy.stats import obrientransform\n    >>> tx, ty = obrientransform(x, y)\n\n    Use `scipy.stats.f_oneway` to apply a one-way ANOVA test to the\n    transformed data.\n\n    >>> from scipy.stats import f_oneway\n    >>> F, p = f_oneway(tx, ty)\n    >>> p\n    0.1314139477040335\n\n    If we require that ``p < 0.05`` for significance, we cannot conclude\n    that the variances are different.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2603, "code": "def sem(a, axis=0, ddof=1, nan_policy='propagate'):\n    xp = array_namespace(a)\n    if axis is None:\n        a = xp.reshape(a, (-1,))\n        axis = 0\n    a = xpx.atleast_nd(xp.asarray(a), ndim=1, xp=xp)\n    n = _length_nonmasked(a, axis, xp=xp)\n    s = xp.std(a, axis=axis, correction=ddof) / n**0.5\n    return s", "documentation": "    \"\"\"Compute standard error of the mean.\n\n    Calculate the standard error of the mean (or standard error of\n    measurement) of the values in the input array.\n\n    Parameters\n    ----------\n    a : array_like\n        An array containing the values for which the standard error is\n        returned. Must contain at least two observations.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over\n        the whole array `a`.\n    ddof : int, optional\n        Delta degrees-of-freedom. How many degrees of freedom to adjust\n        for bias in limited samples relative to the population estimate\n        of variance. Defaults to 1.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    s : ndarray or float\n        The standard error of the mean in the sample(s), along the input axis.\n\n    Notes\n    -----\n    The default value for `ddof` is different to the default (0) used by other\n    ddof containing routines, such as np.std and np.nanstd.\n\n    Examples\n    --------\n    Find standard error along the first axis:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> a = np.arange(20).reshape(5,4)\n    >>> stats.sem(a)\n    array([ 2.8284,  2.8284,  2.8284,  2.8284])\n\n    Find standard error across the whole array, using n degrees of freedom:\n\n    >>> stats.sem(a, axis=None, ddof=0)\n    1.2893796958227628\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2665, "code": "def _isconst(x):\n    y = x[~np.isnan(x)]\n    if y.size == 0:\n        return np.array([True])\n    else:\n        return (y[0] == y).all(keepdims=True)\n@xp_capabilities()", "documentation": "    \"\"\"\n    Check if all values in x are the same.  nans are ignored.\n\n    x must be a 1d array.\n\n    The return value is a 1d array with length 1, so it can be used\n    in np.apply_along_axis.\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2682, "code": "def zscore(a, axis=0, ddof=0, nan_policy='propagate'):\n    return zmap(a, a, axis=axis, ddof=ddof, nan_policy=nan_policy)\n@xp_capabilities()", "documentation": "    \"\"\"\n    Compute the z score.\n\n    Compute the z score of each value in the sample, relative to the\n    sample mean and standard deviation.\n\n    Parameters\n    ----------\n    a : array_like\n        An array like object containing the sample data.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over\n        the whole array `a`.\n    ddof : int, optional\n        Degrees of freedom correction in the calculation of the\n        standard deviation. Default is 0.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan. 'propagate' returns nan,\n        'raise' throws an error, 'omit' performs the calculations ignoring nan\n        values. Default is 'propagate'.  Note that when the value is 'omit',\n        nans in the input also propagate to the output, but they do not affect\n        the z-scores computed for the non-nan values.\n\n    Returns\n    -------\n    zscore : array_like\n        The z-scores, standardized by mean and standard deviation of\n        input array `a`.\n\n    See Also\n    --------\n    numpy.mean : Arithmetic average\n    numpy.std : Arithmetic standard deviation\n    scipy.stats.gzscore : Geometric standard score\n\n    Notes\n    -----\n    This function preserves ndarray subclasses, and works also with\n    matrices and masked arrays (it uses `asanyarray` instead of\n    `asarray` for parameters).\n\n    References\n    ----------\n    .. [1] \"Standard score\", *Wikipedia*,\n           https://en.wikipedia.org/wiki/Standard_score.\n    .. [2] Huck, S. W., Cross, T. L., Clark, S. B, \"Overcoming misconceptions\n           about Z-scores\", Teaching Statistics, vol. 8, pp. 38-40, 1986\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> a = np.array([ 0.7972,  0.0767,  0.4383,  0.7866,  0.8091,\n    ...                0.1954,  0.6307,  0.6599,  0.1065,  0.0508])\n    >>> from scipy import stats\n    >>> stats.zscore(a)\n    array([ 1.1273, -1.247 , -0.0552,  1.0923,  1.1664, -0.8559,  0.5786,\n            0.6748, -1.1488, -1.3324])\n\n    Computing along a specified axis, using n-1 degrees of freedom\n    (``ddof=1``) to calculate the standard deviation:\n\n    >>> b = np.array([[ 0.3148,  0.0478,  0.6243,  0.4608],\n    ...               [ 0.7149,  0.0775,  0.6072,  0.9656],\n    ...               [ 0.6341,  0.1403,  0.9759,  0.4064],\n    ...               [ 0.5918,  0.6948,  0.904 ,  0.3721],\n    ...               [ 0.0921,  0.2481,  0.1188,  0.1366]])\n    >>> stats.zscore(b, axis=1, ddof=1)\n    array([[-0.19264823, -1.28415119,  1.07259584,  0.40420358],\n           [ 0.33048416, -1.37380874,  0.04251374,  1.00081084],\n           [ 0.26796377, -1.12598418,  1.23283094, -0.37481053],\n           [-0.22095197,  0.24468594,  1.19042819, -1.21416216],\n           [-0.82780366,  1.4457416 , -0.43867764, -0.1792603 ]])\n\n    An example with ``nan_policy='omit'``:\n\n    >>> x = np.array([[25.11, 30.10, np.nan, 32.02, 43.15],\n    ...               [14.95, 16.06, 121.25, 94.35, 29.81]])\n    >>> stats.zscore(x, axis=1, nan_policy='omit')\n    array([[-1.13490897, -0.37830299,         nan, -0.08718406,  1.60039602],\n           [-0.91611681, -0.89090508,  1.4983032 ,  0.88731639, -0.5785977 ]])\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2768, "code": "def gzscore(a, *, axis=0, ddof=0, nan_policy='propagate'):\n    xp = array_namespace(a)\n    a = xp_promote(a, force_floating=True, xp=xp)\n    log = ma.log if isinstance(a, ma.MaskedArray) else xp.log\n    return zscore(log(a), axis=axis, ddof=ddof, nan_policy=nan_policy)\n@xp_capabilities()", "documentation": "    \"\"\"\n    Compute the geometric standard score.\n\n    Compute the geometric z score of each strictly positive value in the\n    sample, relative to the geometric mean and standard deviation.\n    Mathematically the geometric z score can be evaluated as::\n\n        gzscore = log(a/gmu) / log(gsigma)\n\n    where ``gmu`` (resp. ``gsigma``) is the geometric mean (resp. standard\n    deviation).\n\n    Parameters\n    ----------\n    a : array_like\n        Sample data.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over\n        the whole array `a`.\n    ddof : int, optional\n        Degrees of freedom correction in the calculation of the\n        standard deviation. Default is 0.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan. 'propagate' returns nan,\n        'raise' throws an error, 'omit' performs the calculations ignoring nan\n        values. Default is 'propagate'.  Note that when the value is 'omit',\n        nans in the input also propagate to the output, but they do not affect\n        the geometric z scores computed for the non-nan values.\n\n    Returns\n    -------\n    gzscore : array_like\n        The geometric z scores, standardized by geometric mean and geometric\n        standard deviation of input array `a`.\n\n    See Also\n    --------\n    gmean : Geometric mean\n    gstd : Geometric standard deviation\n    zscore : Standard score\n\n    Notes\n    -----\n    This function preserves ndarray subclasses, and works also with\n    matrices and masked arrays (it uses ``asanyarray`` instead of\n    ``asarray`` for parameters).\n\n    .. versionadded:: 1.8\n\n    References\n    ----------\n    .. [1] \"Geometric standard score\", *Wikipedia*,\n           https://en.wikipedia.org/wiki/Geometric_standard_deviation#Geometric_standard_score.\n\n    Examples\n    --------\n    Draw samples from a log-normal distribution:\n\n    >>> import numpy as np\n    >>> from scipy.stats import zscore, gzscore\n    >>> import matplotlib.pyplot as plt\n\n    >>> rng = np.random.default_rng()\n    >>> mu, sigma = 3., 1.  # mean and standard deviation\n    >>> x = rng.lognormal(mu, sigma, size=500)\n\n    Display the histogram of the samples:\n\n    >>> fig, ax = plt.subplots()\n    >>> ax.hist(x, 50)\n    >>> plt.show()\n\n    Display the histogram of the samples standardized by the classical zscore.\n    Distribution is rescaled but its shape is unchanged.\n\n    >>> fig, ax = plt.subplots()\n    >>> ax.hist(zscore(x), 50)\n    >>> plt.show()\n\n    Demonstrate that the distribution of geometric zscores is rescaled and\n    quasinormal:\n\n    >>> fig, ax = plt.subplots()\n    >>> ax.hist(gzscore(x), 50)\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 2863, "code": "def zmap(scores, compare, axis=0, ddof=0, nan_policy='propagate'):\n    like_zscore = (scores is compare)\n    xp = array_namespace(scores, compare)\n    scores, compare = xp_promote(scores, compare, force_floating=True, xp=xp)\n    with warnings.catch_warnings():\n        if like_zscore:  # zscore should not emit SmallSampleWarning\n            warnings.simplefilter('ignore', SmallSampleWarning)\n        mn = _xp_mean(compare, axis=axis, keepdims=True, nan_policy=nan_policy)\n        std = _xp_var(compare, axis=axis, correction=ddof,\n                      keepdims=True, nan_policy=nan_policy)**0.5\n    with np.errstate(invalid='ignore', divide='ignore'):", "documentation": "    \"\"\"\n    Calculate the relative z-scores.\n\n    Return an array of z-scores, i.e., scores that are standardized to\n    zero mean and unit variance, where mean and variance are calculated\n    from the comparison array.\n\n    Parameters\n    ----------\n    scores : array_like\n        The input for which z-scores are calculated.\n    compare : array_like\n        The input from which the mean and standard deviation of the\n        normalization are taken; assumed to have the same dimension as\n        `scores`.\n    axis : int or None, optional\n        Axis over which mean and variance of `compare` are calculated.\n        Default is 0. If None, compute over the whole array `scores`.\n    ddof : int, optional\n        Degrees of freedom correction in the calculation of the\n        standard deviation. Default is 0.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle the occurrence of nans in `compare`.\n        'propagate' returns nan, 'raise' raises an exception, 'omit'\n        performs the calculations ignoring nan values. Default is\n        'propagate'. Note that when the value is 'omit', nans in `scores`\n        also propagate to the output, but they do not affect the z-scores\n        computed for the non-nan values.\n\n    Returns\n    -------\n    zscore : array_like\n        Z-scores, in the same shape as `scores`.\n\n    Notes\n    -----\n    This function preserves ndarray subclasses, and works also with\n    matrices and masked arrays (it uses `asanyarray` instead of\n    `asarray` for parameters).\n\n    Examples\n    --------\n    >>> from scipy.stats import zmap\n    >>> a = [0.5, 2.0, 2.5, 3]\n    >>> b = [0, 1, 2, 3, 4]\n    >>> zmap(a, b)\n    array([-1.06066017,  0.        ,  0.35355339,  0.70710678])\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 3387, "code": "def sigmaclip(a, low=4., high=4., *, nan_policy='propagate'):\n    xp = array_namespace(a)\n    c = xp_ravel(xp.asarray(a))\n    contains_nan = _contains_nan(c, nan_policy, xp_omit_okay=True)\n    if contains_nan:\n        if nan_policy == 'propagate':\n            NaN = _get_nan(c, xp=xp)\n            clipped = xp.empty_like(c[0:0])\n            return SigmaclipResult(clipped, NaN, NaN)\n        elif nan_policy == 'omit':\n            c = c[~xp.isnan(c)]", "documentation": "    \"\"\"Perform iterative sigma-clipping of array elements.\n\n    Starting from the full sample, all elements outside the critical range are\n    removed, i.e. all elements of the input array `c` that satisfy either of\n    the following conditions::\n\n        c < mean(c) - std(c)*low\n        c > mean(c) + std(c)*high\n\n    The iteration continues with the updated sample until no\n    elements are outside the (updated) range.\n\n    Parameters\n    ----------\n    a : array_like\n        Data array, will be raveled if not 1-D.\n    low : float, optional\n        Lower bound factor of sigma clipping. Default is 4.\n    high : float, optional\n        Upper bound factor of sigma clipping. Default is 4.\n    nan_policy : {'propagate', 'omit', 'raise'}\n        Defines how to handle input NaNs.\n\n        - ``propagate``: if a NaN is present in the input, the clipped array will be\n          empty, and the upper and lower thresholds will be NaN.\n        - ``omit``: NaNs will be omitted when performing the calculation.\n        - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n\n    Returns\n    -------\n    clipped : ndarray\n        Input array with clipped elements removed.\n    lower : float\n        Lower threshold value use for clipping.\n    upper : float\n        Upper threshold value use for clipping.\n\n    Notes\n    -----\n    This function iteratively *removes* observations. Once observations are\n    removed, they are not re-added in subsequent iterations. Consequently,\n    although it is often the case that ``clipped`` is identical to\n    ``a[(a >= lower) & (a <= upper)]``, this property is not guaranteed to be\n    satisfied; ``clipped`` may have fewer elements.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import sigmaclip\n    >>> a = np.concatenate((np.linspace(9.5, 10.5, 31),\n    ...                     np.linspace(0, 20, 5)))\n    >>> fact = 1.5\n    >>> c, low, upp = sigmaclip(a, fact, fact)\n    >>> c\n    array([  9.96666667,  10.        ,  10.03333333,  10.        ])\n    >>> c.var(), c.std()\n    (0.00055555555555555165, 0.023570226039551501)\n    >>> low, c.mean() - fact*c.std(), c.min()\n    (9.9646446609406727, 9.9646446609406727, 9.9666666666666668)\n    >>> upp, c.mean() + fact*c.std(), c.max()\n    (10.035355339059327, 10.035355339059327, 10.033333333333333)\n\n    >>> a = np.concatenate((np.linspace(9.5, 10.5, 11),\n    ...                     np.linspace(-100, -50, 3)))\n    >>> c, low, upp = sigmaclip(a, 1.8, 1.8)\n    >>> (c == np.linspace(9.5, 10.5, 11)).all()\n    True\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 3482, "code": "def trimboth(a, proportiontocut, axis=0):\n    a = np.asarray(a)\n    if a.size == 0:\n        return a\n    if axis is None:\n        a = a.ravel()\n        axis = 0\n    nobs = a.shape[axis]\n    lowercut = int(proportiontocut * nobs)\n    uppercut = nobs - lowercut\n    if (lowercut >= uppercut):", "documentation": "    \"\"\"Slice off a proportion of items from both ends of an array.\n\n    Slice off the passed proportion of items from both ends of the passed\n    array (i.e., with `proportiontocut` = 0.1, slices leftmost 10% **and**\n    rightmost 10% of scores). The trimmed values are the lowest and\n    highest ones.\n    Slice off less if proportion results in a non-integer slice index (i.e.\n    conservatively slices off `proportiontocut`).\n\n    Parameters\n    ----------\n    a : array_like\n        Data to trim.\n    proportiontocut : float\n        Proportion (in range 0-1) of total data set to trim of each end.\n    axis : int or None, optional\n        Axis along which to trim data. Default is 0. If None, compute over\n        the whole array `a`.\n\n    Returns\n    -------\n    out : ndarray\n        Trimmed version of array `a`. The order of the trimmed content\n        is undefined.\n\n    See Also\n    --------\n    trim_mean\n\n    Examples\n    --------\n    Create an array of 10 values and trim 10% of those values from each end:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n    >>> stats.trimboth(a, 0.1)\n    array([1, 3, 2, 4, 5, 6, 7, 8])\n\n    Note that the elements of the input array are trimmed by value, but the\n    output array is not necessarily sorted.\n\n    The proportion to trim is rounded down to the nearest integer. For\n    instance, trimming 25% of the values from each end of an array of 10\n    values will return an array of 6 values:\n\n    >>> b = np.arange(10)\n    >>> stats.trimboth(b, 1/4).shape\n    (6,)\n\n    Multidimensional arrays can be trimmed along any axis or across the entire\n    array:\n\n    >>> c = [2, 4, 6, 8, 0, 1, 3, 5, 7, 9]\n    >>> d = np.array([a, b, c])\n    >>> stats.trimboth(d, 0.4, axis=0).shape\n    (1, 10)\n    >>> stats.trimboth(d, 0.4, axis=1).shape\n    (3, 2)\n    >>> stats.trimboth(d, 0.4, axis=None).shape\n    (6,)\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 3569, "code": "def trim1(a, proportiontocut, tail='right', axis=0):\n    a = np.asarray(a)\n    if axis is None:\n        a = a.ravel()\n        axis = 0\n    nobs = a.shape[axis]\n    if proportiontocut >= 1:\n        return []\n    if tail.lower() == 'right':\n        lowercut = 0\n        uppercut = nobs - int(proportiontocut * nobs)", "documentation": "    \"\"\"Slice off a proportion from ONE end of the passed array distribution.\n\n    If `proportiontocut` = 0.1, slices off 'leftmost' or 'rightmost'\n    10% of scores. The lowest or highest values are trimmed (depending on\n    the tail).\n    Slice off less if proportion results in a non-integer slice index\n    (i.e. conservatively slices off `proportiontocut` ).\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    proportiontocut : float\n        Fraction to cut off of 'left' or 'right' of distribution.\n    tail : {'left', 'right'}, optional\n        Defaults to 'right'.\n    axis : int or None, optional\n        Axis along which to trim data. Default is 0. If None, compute over\n        the whole array `a`.\n\n    Returns\n    -------\n    trim1 : ndarray\n        Trimmed version of array `a`. The order of the trimmed content is\n        undefined.\n\n    Examples\n    --------\n    Create an array of 10 values and trim 20% of its lowest values:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n    >>> stats.trim1(a, 0.2, 'left')\n    array([2, 4, 3, 5, 6, 7, 8, 9])\n\n    Note that the elements of the input array are trimmed by value, but the\n    output array is not necessarily sorted.\n\n    The proportion to trim is rounded down to the nearest integer. For\n    instance, trimming 25% of the values from an array of 10 values will\n    return an array of 8 values:\n\n    >>> b = np.arange(10)\n    >>> stats.trim1(b, 1/4).shape\n    (8,)\n\n    Multidimensional arrays can be trimmed along any axis or across the entire\n    array:\n\n    >>> c = [2, 4, 6, 8, 0, 1, 3, 5, 7, 9]\n    >>> d = np.array([a, b, c])\n    >>> stats.trim1(d, 0.8, axis=0).shape\n    (1, 10)\n    >>> stats.trim1(d, 0.8, axis=1).shape\n    (3, 2)\n    >>> stats.trim1(d, 0.8, axis=None).shape\n    (6,)\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 3658, "code": "def trim_mean(a, proportiontocut, axis=0):\n    xp = array_namespace(a)\n    a = xp.asarray(a)\n    if xp_size(a) == 0:\n        return _get_nan(a, xp=xp)\n    if axis is None:\n        a = xp_ravel(a)\n        axis = 0\n    nobs = a.shape[axis]\n    lowercut = int(proportiontocut * nobs)\n    uppercut = nobs - lowercut", "documentation": "    \"\"\"Return mean of array after trimming a specified fraction of extreme values\n\n    Removes the specified proportion of elements from *each* end of the\n    sorted array, then computes the mean of the remaining elements.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    proportiontocut : float\n        Fraction of the most positive and most negative elements to remove.\n        When the specified proportion does not result in an integer number of\n        elements, the number of elements to trim is rounded down.\n    axis : int or None, default: 0\n        Axis along which the trimmed means are computed.\n        If None, compute over the raveled array.\n\n    Returns\n    -------\n    trim_mean : ndarray\n        Mean of trimmed array.\n\n    See Also\n    --------\n    trimboth : Remove a proportion of elements from each end of an array.\n    tmean : Compute the mean after trimming values outside specified limits.\n\n    Notes\n    -----\n    For 1-D array `a`, `trim_mean` is approximately equivalent to the following\n    calculation::\n\n        import numpy as np\n        a = np.sort(a)\n        m = int(proportiontocut * len(a))\n        np.mean(a[m: len(a) - m])\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = [1, 2, 3, 5]\n    >>> stats.trim_mean(x, 0.25)\n    2.5\n\n    When the specified proportion does not result in an integer number of\n    elements, the number of elements to trim is rounded down.\n\n    >>> stats.trim_mean(x, 0.24999) == np.mean(x)\n    True\n\n    Use `axis` to specify the axis along which the calculation is performed.\n\n    >>> x2 = [[1, 2, 3, 5],\n    ...       [10, 20, 30, 50]]\n    >>> stats.trim_mean(x2, 0.25)\n    array([ 5.5, 11. , 16.5, 27.5])\n    >>> stats.trim_mean(x2, 0.25, axis=1)\n    array([ 2.5, 25. ])\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 3773, "code": "def f_oneway(*samples, axis=0, equal_var=True):\n    xp = array_namespace(*samples)\n    samples = xp_promote(*samples, force_floating=True, xp=xp)\n    if len(samples) < 2:\n        raise TypeError('at least two inputs are required;'\n                        f' got {len(samples)}.')\n    num_groups = len(samples)\n    alldata = xp.concat(samples, axis=-1)\n    bign = _length_nonmasked(alldata, axis=-1, xp=xp)\n    if _f_oneway_is_too_small(samples):\n        NaN = _get_nan(*samples, xp=xp)", "documentation": "    \"\"\"Perform one-way ANOVA.\n\n    The one-way ANOVA tests the null hypothesis that two or more groups have\n    the same population mean.  The test is applied to samples from two or\n    more groups, possibly with differing sizes.\n\n    Parameters\n    ----------\n    sample1, sample2, ... : array_like\n        The sample measurements for each group.  There must be at least\n        two arguments.  If the arrays are multidimensional, then all the\n        dimensions of the array must be the same except for `axis`.\n    axis : int, optional\n        Axis of the input arrays along which the test is applied.\n        Default is 0.\n    equal_var : bool, optional\n        If True (default), perform a standard one-way ANOVA test that\n        assumes equal population variances [2]_.\n        If False, perform Welch's ANOVA test, which does not assume\n        equal population variances [4]_.\n\n        .. versionadded:: 1.16.0\n\n    Returns\n    -------\n    statistic : float\n        The computed F statistic of the test.\n    pvalue : float\n        The associated p-value from the F distribution.\n\n    Warns\n    -----\n    `~scipy.stats.ConstantInputWarning`\n        Emitted if all values within each of the input arrays are identical.\n        In this case the F statistic is either infinite or isn't defined,\n        so ``np.inf`` or ``np.nan`` is returned.\n\n    RuntimeWarning\n        Emitted if the length of any input array is 0, or if all the input\n        arrays have length 1.  ``np.nan`` is returned for the F statistic\n        and the p-value in these cases.\n\n    Notes\n    -----\n    The ANOVA test has important assumptions that must be satisfied in order\n    for the associated p-value to be valid.\n\n    1. The samples are independent.\n    2. Each sample is from a normally distributed population.\n    3. The population standard deviations of the groups are all equal.  This\n       property is known as homoscedasticity.\n\n    If these assumptions are not true for a given set of data, it may still\n    be possible to use the Kruskal-Wallis H-test (`scipy.stats.kruskal`) or\n    the Alexander-Govern test (`scipy.stats.alexandergovern`) although with\n    some loss of power.\n\n    The length of each group must be at least one, and there must be at\n    least one group with length greater than one.  If these conditions\n    are not satisfied, a warning is generated and (``np.nan``, ``np.nan``)\n    is returned.\n\n    If all values in each group are identical, and there exist at least two\n    groups with different values, the function generates a warning and\n    returns (``np.inf``, 0).\n\n    If all values in all groups are the same, function generates a warning\n    and returns (``np.nan``, ``np.nan``).\n\n    The algorithm is from Heiman [2]_, pp.394-7.\n\n    References\n    ----------\n    .. [1] R. Lowry, \"Concepts and Applications of Inferential Statistics\",\n           Chapter 14, 2014, http://vassarstats.net/textbook/\n\n    .. [2] G.W. Heiman, \"Understanding research methods and statistics: An\n           integrated introduction for psychology\", Houghton, Mifflin and\n           Company, 2001.\n\n    .. [3] J.H. McDonald, \"Handbook of Biological Statistics\",\n           One-way ANOVA, 2014.\n           http://www.biostathandbook.com/onewayanova.html\n\n    .. [4] B. L. Welch, \"On the Comparison of Several Mean Values:\n           An Alternative Approach\", Biometrika, vol. 38, no. 3/4,\n           pp. 330-336, 1951. :doi:`10.2307/2332579`.\n\n    .. [5] J.H. McDonald, R. Seed and R.K. Koehn, \"Allozymes and\n           morphometric characters of three species of Mytilus in\n           the Northern and Southern Hemispheres\",\n           Marine Biology, vol. 111, pp. 323-333, 1991.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import f_oneway\n\n    Here are some data [3]_ on a shell measurement (the length of the anterior\n    adductor muscle scar, standardized by dividing by length) in the mussel\n    Mytilus trossulus from five locations: Tillamook, Oregon; Newport, Oregon;\n    Petersburg, Alaska; Magadan, Russia; and Tvarminne, Finland, taken from a\n    much larger data set used in [5]_.\n\n    >>> tillamook = [0.0571, 0.0813, 0.0831, 0.0976, 0.0817, 0.0859, 0.0735,\n    ...              0.0659, 0.0923, 0.0836]\n    >>> newport = [0.0873, 0.0662, 0.0672, 0.0819, 0.0749, 0.0649, 0.0835,\n    ...            0.0725]\n    >>> petersburg = [0.0974, 0.1352, 0.0817, 0.1016, 0.0968, 0.1064, 0.105]\n    >>> magadan = [0.1033, 0.0915, 0.0781, 0.0685, 0.0677, 0.0697, 0.0764,\n    ...            0.0689]\n    >>> tvarminne = [0.0703, 0.1026, 0.0956, 0.0973, 0.1039, 0.1045]\n    >>> f_oneway(tillamook, newport, petersburg, magadan, tvarminne)\n    F_onewayResult(statistic=7.121019471642447, pvalue=0.0002812242314534544)\n\n    `f_oneway` accepts multidimensional input arrays.  When the inputs\n    are multidimensional and `axis` is not given, the test is performed\n    along the first axis of the input arrays.  For the following data, the\n    test is performed three times, once for each column.\n\n    >>> a = np.array([[9.87, 9.03, 6.81],\n    ...               [7.18, 8.35, 7.00],\n    ...               [8.39, 7.58, 7.68],\n    ...               [7.45, 6.33, 9.35],\n    ...               [6.41, 7.10, 9.33],\n    ...               [8.00, 8.24, 8.44]])\n    >>> b = np.array([[6.35, 7.30, 7.16],\n    ...               [6.65, 6.68, 7.63],\n    ...               [5.72, 7.73, 6.72],\n    ...               [7.01, 9.19, 7.41],\n    ...               [7.75, 7.87, 8.30],\n    ...               [6.90, 7.97, 6.97]])\n    >>> c = np.array([[3.31, 8.77, 1.01],\n    ...               [8.25, 3.24, 3.62],\n    ...               [6.32, 8.81, 5.19],\n    ...               [7.48, 8.83, 8.91],\n    ...               [8.59, 6.01, 6.07],\n    ...               [3.07, 9.72, 7.48]])\n    >>> F = f_oneway(a, b, c)\n    >>> F.statistic\n    array([1.75676344, 0.03701228, 3.76439349])\n    >>> F.pvalue\n    array([0.20630784, 0.96375203, 0.04733157])\n\n    Welch ANOVA will be performed if `equal_var` is False.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 4070, "code": "def alexandergovern(*samples, nan_policy='propagate', axis=0):\n    xp = array_namespace(*samples)\n    samples = _alexandergovern_input_validation(samples, nan_policy, axis, xp=xp)\n    lengths = [sample.shape[-1] for sample in samples]\n    means = xp.stack([_xp_mean(sample, axis=-1) for sample in samples])\n    se2 = [(_xp_var(sample, correction=1, axis=-1) / length)\n           for sample, length in zip(samples, lengths)]\n    standard_errors_squared = xp.stack(se2)\n    standard_errors = standard_errors_squared**0.5\n    eps = xp.finfo(standard_errors.dtype).eps\n    zero = standard_errors <= xp.abs(eps * means)", "documentation": "    \"\"\"Performs the Alexander Govern test.\n\n    The Alexander-Govern approximation tests the equality of k independent\n    means in the face of heterogeneity of variance. The test is applied to\n    samples from two or more groups, possibly with differing sizes.\n\n    Parameters\n    ----------\n    sample1, sample2, ... : array_like\n        The sample measurements for each group.  There must be at least\n        two samples, and each sample must contain at least two observations.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    res : AlexanderGovernResult\n        An object with attributes:\n\n        statistic : float\n            The computed A statistic of the test.\n        pvalue : float\n            The associated p-value from the chi-squared distribution.\n\n    Warns\n    -----\n    `~scipy.stats.ConstantInputWarning`\n        Raised if an input is a constant array.  The statistic is not defined\n        in this case, so ``np.nan`` is returned.\n\n    See Also\n    --------\n    f_oneway : one-way ANOVA\n\n    Notes\n    -----\n    The use of this test relies on several assumptions.\n\n    1. The samples are independent.\n    2. Each sample is from a normally distributed population.\n    3. Unlike `f_oneway`, this test does not assume on homoscedasticity,\n       instead relaxing the assumption of equal variances.\n\n    Input samples must be finite, one dimensional, and with size greater than\n    one.\n\n    References\n    ----------\n    .. [1] Alexander, Ralph A., and Diane M. Govern. \"A New and Simpler\n           Approximation for ANOVA under Variance Heterogeneity.\" Journal\n           of Educational Statistics, vol. 19, no. 2, 1994, pp. 91-101.\n           https://www.jstor.org/stable/1165140\n\n    Examples\n    --------\n    >>> from scipy.stats import alexandergovern\n\n    Here are some data on annual percentage rate of interest charged on\n    new car loans at nine of the largest banks in four American cities\n    taken from the National Institute of Standards and Technology's\n    ANOVA dataset.\n\n    We use `alexandergovern` to test the null hypothesis that all cities\n    have the same mean APR against the alternative that the cities do not\n    all have the same mean APR. We decide that a significance level of 5%\n    is required to reject the null hypothesis in favor of the alternative.\n\n    >>> atlanta = [13.75, 13.75, 13.5, 13.5, 13.0, 13.0, 13.0, 12.75, 12.5]\n    >>> chicago = [14.25, 13.0, 12.75, 12.5, 12.5, 12.4, 12.3, 11.9, 11.9]\n    >>> houston = [14.0, 14.0, 13.51, 13.5, 13.5, 13.25, 13.0, 12.5, 12.5]\n    >>> memphis = [15.0, 14.0, 13.75, 13.59, 13.25, 12.97, 12.5, 12.25,\n    ...           11.89]\n    >>> alexandergovern(atlanta, chicago, houston, memphis)\n    AlexanderGovernResult(statistic=4.65087071883494,\n                          pvalue=0.19922132490385214)\n\n    The p-value is 0.1992, indicating a nearly 20% chance of observing\n    such an extreme value of the test statistic under the null hypothesis.\n    This exceeds 5%, so we do not reject the null hypothesis in favor of\n    the alternative.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 4231, "code": "def _pearsonr_fisher_ci(r, n, confidence_level, alternative):\n    xp = array_namespace(r)\n    ones = xp.ones_like(r)\n    n = xp.asarray(n, dtype=r.dtype, device=xp_device(r))\n    confidence_level = xp.asarray(confidence_level, dtype=r.dtype, device=xp_device(r))\n    with np.errstate(divide='ignore', invalid='ignore'):\n        zr = xp.atanh(r)\n        se = xp.sqrt(1 / (n - 3))\n    if alternative == \"two-sided\":\n        h = special.ndtri(0.5 + confidence_level/2)\n        zlo = zr - h*se", "documentation": "    \"\"\"\n    Compute the confidence interval for Pearson's R.\n\n    Fisher's transformation is used to compute the confidence interval\n    (https://en.wikipedia.org/wiki/Fisher_transformation).\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 4279, "code": "def _pearsonr_bootstrap_ci(confidence_level, method, x, y, alternative, axis):", "documentation": "    \"\"\"\n    Compute the confidence interval for Pearson's R using the bootstrap.\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 4301, "code": "class PearsonRResult(PearsonRResultBase):", "documentation": "    \"\"\"\n    Result of `scipy.stats.pearsonr`\n\n    Attributes\n    ----------\n    statistic : float\n        Pearson product-moment correlation coefficient.\n    pvalue : float\n        The p-value associated with the chosen alternative.\n\n    Methods\n    -------\n    confidence_interval\n        Computes the confidence interval of the correlation\n        coefficient `statistic` for the given confidence level.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 4330, "code": "    def confidence_interval(self, confidence_level=0.95, method=None):\n        if isinstance(method, BootstrapMethod):\n            xp = array_namespace(self._x)\n            message = ('`method` must be `None` if `pearsonr` '\n                       'arguments were not NumPy arrays.')\n            if not is_numpy(xp):\n                raise ValueError(message)\n            ci = _pearsonr_bootstrap_ci(confidence_level, method, self._x, self._y,\n                                        self._alternative, self._axis)\n        elif method is None:\n            ci = _pearsonr_fisher_ci(self.statistic, self._n, confidence_level,", "documentation": "        \"\"\"\n        The confidence interval for the correlation coefficient.\n\n        Compute the confidence interval for the correlation coefficient\n        ``statistic`` with the given confidence level.\n\n        If `method` is not provided,\n        The confidence interval is computed using the Fisher transformation\n        F(r) = arctanh(r) [1]_.  When the sample pairs are drawn from a\n        bivariate normal distribution, F(r) approximately follows a normal\n        distribution with standard error ``1/sqrt(n - 3)``, where ``n`` is the\n        length of the original samples along the calculation axis. When\n        ``n <= 3``, this approximation does not yield a finite, real standard\n        error, so we define the confidence interval to be -1 to 1.\n\n        If `method` is an instance of `BootstrapMethod`, the confidence\n        interval is computed using `scipy.stats.bootstrap` with the provided\n        configuration options and other appropriate settings. In some cases,\n        confidence limits may be NaN due to a degenerate resample, and this is\n        typical for very small samples (~6 observations).\n\n        Parameters\n        ----------\n        confidence_level : float\n            The confidence level for the calculation of the correlation\n            coefficient confidence interval. Default is 0.95.\n\n        method : BootstrapMethod, optional\n            Defines the method used to compute the confidence interval. See\n            method description for details.\n\n            .. versionadded:: 1.11.0\n\n        Returns\n        -------\n        ci : namedtuple\n            The confidence interval is returned in a ``namedtuple`` with\n            fields `low` and `high`.\n\n        References\n        ----------\n        .. [1] \"Pearson correlation coefficient\", Wikipedia,\n               https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\n        \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 4815, "code": "def fisher_exact(table, alternative=None, *, method=None):\n    hypergeom = distributions.hypergeom\n    c = np.asarray(table, dtype=np.int64)\n    if not c.ndim == 2:\n        raise ValueError(\"The input `table` must have two dimensions.\")\n    if np.any(c < 0):\n        raise ValueError(\"All values in `table` must be nonnegative.\")\n    if not c.shape == (2, 2) or method is not None:\n        return _fisher_exact_rxc(c, alternative, method)\n    alternative = 'two-sided' if alternative is None else alternative\n    if 0 in c.sum(axis=0) or 0 in c.sum(axis=1):", "documentation": "    \"\"\"Perform a Fisher exact test on a contingency table.\n\n    For a 2x2 table,\n    the null hypothesis is that the true odds ratio of the populations\n    underlying the observations is one, and the observations were sampled\n    from these populations under a condition: the marginals of the\n    resulting table must equal those of the observed table.\n    The statistic is the unconditional maximum likelihood estimate of the odds\n    ratio, and the p-value is the probability under the null hypothesis of\n    obtaining a table at least as extreme as the one that was actually\n    observed.\n\n    For other table sizes, or if `method` is provided, the null hypothesis\n    is that the rows and columns of the tables have fixed sums and are\n    independent; i.e., the table was sampled from a `scipy.stats.random_table`\n    distribution with the observed marginals. The statistic is the\n    probability mass of this distribution evaluated at `table`, and the\n    p-value is the percentage of the population of tables with statistic at\n    least as extreme (small) as that of `table`. There is only one alternative\n    hypothesis available: the rows and columns are not independent.\n\n    There are other possible choices of statistic and two-sided\n    p-value definition associated with Fisher's exact test; please see the\n    Notes for more information.\n\n    Parameters\n    ----------\n    table : array_like of ints\n        A contingency table.  Elements must be non-negative integers.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis for 2x2 tables; unused for other\n        table sizes.\n        The following options are available (default is 'two-sided'):\n\n        * 'two-sided': the odds ratio of the underlying population is not one\n        * 'less': the odds ratio of the underlying population is less than one\n        * 'greater': the odds ratio of the underlying population is greater\n          than one\n\n        See the Notes for more details.\n\n    method : ResamplingMethod, optional\n        Defines the method used to compute the p-value.\n        If `method` is an instance of `PermutationMethod`/`MonteCarloMethod`,\n        the p-value is computed using\n        `scipy.stats.permutation_test`/`scipy.stats.monte_carlo_test` with the\n        provided configuration options and other appropriate settings.\n        Note that if `method` is an instance of `MonteCarloMethod`, the ``rvs``\n        attribute must be left unspecified; Monte Carlo samples are always drawn\n        using the ``rvs`` method of `scipy.stats.random_table`.\n        Otherwise, the p-value is computed as documented in the notes.\n\n        .. versionadded:: 1.15.0\n\n    Returns\n    -------\n    res : SignificanceResult\n        An object containing attributes:\n\n        statistic : float\n            For a 2x2 table with default `method`, this is the odds ratio - the\n            prior odds ratio not a posterior estimate. In all other cases, this\n            is the probability density of obtaining the observed table under the\n            null hypothesis of independence with marginals fixed.\n        pvalue : float\n            The probability under the null hypothesis of obtaining a\n            table at least as extreme as the one that was actually observed.\n\n    Raises\n    ------\n    ValueError\n        If `table` is not two-dimensional or has negative entries.\n\n    See Also\n    --------\n    chi2_contingency : Chi-square test of independence of variables in a\n        contingency table.  This can be used as an alternative to\n        `fisher_exact` when the numbers in the table are large.\n    contingency.odds_ratio : Compute the odds ratio (sample or conditional\n        MLE) for a 2x2 contingency table.\n    barnard_exact : Barnard's exact test, which is a more powerful alternative\n        than Fisher's exact test for 2x2 contingency tables.\n    boschloo_exact : Boschloo's exact test, which is a more powerful\n        alternative than Fisher's exact test for 2x2 contingency tables.\n    :ref:`hypothesis_fisher_exact` : Extended example\n\n    Notes\n    -----\n    *Null hypothesis and p-values*\n\n    The null hypothesis is that the true odds ratio of the populations\n    underlying the observations is one, and the observations were sampled at\n    random from these populations under a condition: the marginals of the\n    resulting table must equal those of the observed table. Equivalently,\n    the null hypothesis is that the input table is from the hypergeometric\n    distribution with parameters (as used in `hypergeom`)\n    ``M = a + b + c + d``, ``n = a + b`` and ``N = a + c``, where the\n    input table is ``[[a, b], [c, d]]``.  This distribution has support\n    ``max(0, N + n - M) <= x <= min(N, n)``, or, in terms of the values\n    in the input table, ``min(0, a - d) <= x <= a + min(b, c)``.  ``x``\n    can be interpreted as the upper-left element of a 2x2 table, so the\n    tables in the distribution have form::\n\n        [  x           n - x     ]\n        [N - x    M - (n + N) + x]\n\n    For example, if::\n\n        table = [6  2]\n                [1  4]\n\n    then the support is ``2 <= x <= 7``, and the tables in the distribution\n    are::\n\n        [2 6]   [3 5]   [4 4]   [5 3]   [6 2]  [7 1]\n        [5 0]   [4 1]   [3 2]   [2 3]   [1 4]  [0 5]\n\n    The probability of each table is given by the hypergeometric distribution\n    ``hypergeom.pmf(x, M, n, N)``.  For this example, these are (rounded to\n    three significant digits)::\n\n        x       2      3      4      5       6        7\n        p  0.0163  0.163  0.408  0.326  0.0816  0.00466\n\n    These can be computed with::\n\n        >>> import numpy as np\n        >>> from scipy.stats import hypergeom\n        >>> table = np.array([[6, 2], [1, 4]])\n        >>> M = table.sum()\n        >>> n = table[0].sum()\n        >>> N = table[:, 0].sum()\n        >>> start, end = hypergeom.support(M, n, N)\n        >>> hypergeom.pmf(np.arange(start, end+1), M, n, N)\n        array([0.01631702, 0.16317016, 0.40792541, 0.32634033, 0.08158508,\n               0.004662  ])\n\n    The two-sided p-value is the probability that, under the null hypothesis,\n    a random table would have a probability equal to or less than the\n    probability of the input table.  For our example, the probability of\n    the input table (where ``x = 6``) is 0.0816.  The x values where the\n    probability does not exceed this are 2, 6 and 7, so the two-sided p-value\n    is ``0.0163 + 0.0816 + 0.00466 ~= 0.10256``::\n\n        >>> from scipy.stats import fisher_exact\n        >>> res = fisher_exact(table, alternative='two-sided')\n        >>> res.pvalue\n        0.10256410256410257\n\n    The one-sided p-value for ``alternative='greater'`` is the probability\n    that a random table has ``x >= a``, which in our example is ``x >= 6``,\n    or ``0.0816 + 0.00466 ~= 0.08626``::\n\n        >>> res = fisher_exact(table, alternative='greater')\n        >>> res.pvalue\n        0.08624708624708627\n\n    This is equivalent to computing the survival function of the\n    distribution at ``x = 5`` (one less than ``x`` from the input table,\n    because we want to include the probability of ``x = 6`` in the sum)::\n\n        >>> hypergeom.sf(5, M, n, N)\n        0.08624708624708627\n\n    For ``alternative='less'``, the one-sided p-value is the probability\n    that a random table has ``x <= a``, (i.e. ``x <= 6`` in our example),\n    or ``0.0163 + 0.163 + 0.408 + 0.326 + 0.0816 ~= 0.9949``::\n\n        >>> res = fisher_exact(table, alternative='less')\n        >>> res.pvalue\n        0.9953379953379957\n\n    This is equivalent to computing the cumulative distribution function\n    of the distribution at ``x = 6``:\n\n        >>> hypergeom.cdf(6, M, n, N)\n        0.9953379953379957\n\n    *Odds ratio*\n\n    The calculated odds ratio is different from the value computed by the\n    R function ``fisher.test``.  This implementation returns the \"sample\"\n    or \"unconditional\" maximum likelihood estimate, while ``fisher.test``\n    in R uses the conditional maximum likelihood estimate.  To compute the\n    conditional maximum likelihood estimate of the odds ratio, use\n    `scipy.stats.contingency.odds_ratio`.\n\n    References\n    ----------\n    .. [1] Fisher, Sir Ronald A, \"The Design of Experiments:\n           Mathematics of a Lady Tasting Tea.\" ISBN 978-0-486-41151-4, 1935.\n    .. [2] \"Fisher's exact test\",\n           https://en.wikipedia.org/wiki/Fisher's_exact_test\n\n    Examples\n    --------\n\n    >>> from scipy.stats import fisher_exact\n    >>> res = fisher_exact([[8, 2], [1, 5]])\n    >>> res.statistic\n    20.0\n    >>> res.pvalue\n    0.034965034965034975\n\n    For tables with shape other than ``(2, 2)``, provide an instance of\n    `scipy.stats.MonteCarloMethod` or `scipy.stats.PermutationMethod` for the\n    `method` parameter:\n\n    >>> import numpy as np\n    >>> from scipy.stats import MonteCarloMethod\n    >>> rng = np.random.default_rng(4507195762371367)\n    >>> method = MonteCarloMethod(rng=rng)\n    >>> fisher_exact([[8, 2, 3], [1, 5, 4]], method=method)\n    SignificanceResult(statistic=np.float64(0.005782), pvalue=np.float64(0.0603))\n\n    For a more detailed example, see :ref:`hypothesis_fisher_exact`.\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 5981, "code": "class TtestResult(TtestResultBase):", "documentation": "    \"\"\"\n    Result of a t-test.\n\n    See the documentation of the particular t-test function for more\n    information about the definition of the statistic and meaning of\n    the confidence interval.\n\n    Attributes\n    ----------\n    statistic : float or array\n        The t-statistic of the sample.\n    pvalue : float or array\n        The p-value associated with the given alternative.\n    df : float or array\n        The number of degrees of freedom used in calculation of the\n        t-statistic; this is one less than the size of the sample\n        (``a.shape[axis]-1`` if there are no masked elements or omitted NaNs).\n\n    Methods\n    -------\n    confidence_interval\n        Computes a confidence interval around the population statistic\n        for the given confidence level.\n        The confidence interval is returned in a ``namedtuple`` with\n        fields `low` and `high`.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 6022, "code": "    def confidence_interval(self, confidence_level=0.95):\n        low, high = _t_confidence_interval(self.df, self._statistic_np,\n                                           confidence_level, self._alternative,\n                                           self._dtype, self._xp)\n        low = low * self._standard_error + self._estimate\n        high = high * self._standard_error + self._estimate\n        return ConfidenceInterval(low=low, high=high)", "documentation": "        \"\"\"\n        Parameters\n        ----------\n        confidence_level : float\n            The confidence level for the calculation of the population mean\n            confidence interval. Default is 0.95.\n\n        Returns\n        -------\n        ci : namedtuple\n            The confidence interval is returned in a ``namedtuple`` with\n            fields `low` and `high`.\n\n        \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 6068, "code": "def ttest_1samp(a, popmean, axis=0, nan_policy=\"propagate\", alternative=\"two-sided\"):\n    xp = array_namespace(a)\n    a, popmean = xp_promote(a, popmean, force_floating=True, xp=xp)\n    a, axis = _chk_asarray(a, axis, xp=xp)\n    n = _length_nonmasked(a, axis)\n    df = n - 1\n    if a.shape[axis] == 0:\n        NaN = _get_nan(a)\n        return TtestResult(NaN, NaN, df=NaN, alternative=NaN,\n                           standard_error=NaN, estimate=NaN)\n    mean = xp.mean(a, axis=axis)", "documentation": "    \"\"\"Calculate the T-test for the mean of ONE group of scores.\n\n    This is a test for the null hypothesis that the expected value\n    (mean) of a sample of independent observations `a` is equal to the given\n    population mean, `popmean`.\n\n    Parameters\n    ----------\n    a : array_like\n        Sample observations.\n    popmean : float or array_like\n        Expected value in null hypothesis. If array_like, then its length along\n        `axis` must equal 1, and it must otherwise be broadcastable with `a`.\n    axis : int or None, optional\n        Axis along which to compute test; default is 0. If None, compute over\n        the whole array `a`.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis.\n        The following options are available (default is 'two-sided'):\n\n        * 'two-sided': the mean of the underlying distribution of the sample\n          is different than the given population mean (`popmean`)\n        * 'less': the mean of the underlying distribution of the sample is\n          less than the given population mean (`popmean`)\n        * 'greater': the mean of the underlying distribution of the sample is\n          greater than the given population mean (`popmean`)\n\n    Returns\n    -------\n    result : `~scipy.stats._result_classes.TtestResult`\n        An object with the following attributes:\n\n        statistic : float or array\n            The t-statistic.\n        pvalue : float or array\n            The p-value associated with the given alternative.\n        df : float or array\n            The number of degrees of freedom used in calculation of the\n            t-statistic; this is one less than the size of the sample\n            (``a.shape[axis]``).\n\n            .. versionadded:: 1.10.0\n\n        The object also has the following method:\n\n        confidence_interval(confidence_level=0.95)\n            Computes a confidence interval around the population\n            mean for the given confidence level.\n            The confidence interval is returned in a ``namedtuple`` with\n            fields `low` and `high`.\n\n            .. versionadded:: 1.10.0\n\n    Notes\n    -----\n    The statistic is calculated as ``(np.mean(a) - popmean)/se``, where\n    ``se`` is the standard error. Therefore, the statistic will be positive\n    when the sample mean is greater than the population mean and negative when\n    the sample mean is less than the population mean.\n\n    Examples\n    --------\n    Suppose we wish to test the null hypothesis that the mean of a population\n    is equal to 0.5. We choose a confidence level of 99%; that is, we will\n    reject the null hypothesis in favor of the alternative if the p-value is\n    less than 0.01.\n\n    When testing random variates from the standard uniform distribution, which\n    has a mean of 0.5, we expect the data to be consistent with the null\n    hypothesis most of the time.\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> rvs = stats.uniform.rvs(size=50, random_state=rng)\n    >>> stats.ttest_1samp(rvs, popmean=0.5)\n    TtestResult(statistic=2.456308468440, pvalue=0.017628209047638, df=49)\n\n    As expected, the p-value of 0.017 is not below our threshold of 0.01, so\n    we cannot reject the null hypothesis.\n\n    When testing data from the standard *normal* distribution, which has a mean\n    of 0, we would expect the null hypothesis to be rejected.\n\n    >>> rvs = stats.norm.rvs(size=50, random_state=rng)\n    >>> stats.ttest_1samp(rvs, popmean=0.5)\n    TtestResult(statistic=-7.433605518875, pvalue=1.416760157221e-09, df=49)\n\n    Indeed, the p-value is lower than our threshold of 0.01, so we reject the\n    null hypothesis in favor of the default \"two-sided\" alternative: the mean\n    of the population is *not* equal to 0.5.\n\n    However, suppose we were to test the null hypothesis against the\n    one-sided alternative that the mean of the population is *greater* than\n    0.5. Since the mean of the standard normal is less than 0.5, we would not\n    expect the null hypothesis to be rejected.\n\n    >>> stats.ttest_1samp(rvs, popmean=0.5, alternative='greater')\n    TtestResult(statistic=-7.433605518875, pvalue=0.99999999929, df=49)\n\n    Unsurprisingly, with a p-value greater than our threshold, we would not\n    reject the null hypothesis.\n\n    Note that when working with a confidence level of 99%, a true null\n    hypothesis will be rejected approximately 1% of the time.\n\n    >>> rvs = stats.uniform.rvs(size=(100, 50), random_state=rng)\n    >>> res = stats.ttest_1samp(rvs, popmean=0.5, axis=1)\n    >>> np.sum(res.pvalue < 0.01)\n    1\n\n    Indeed, even though all 100 samples above were drawn from the standard\n    uniform distribution, which *does* have a population mean of 0.5, we would\n    mistakenly reject the null hypothesis for one of them.\n\n    `ttest_1samp` can also compute a confidence interval around the population\n    mean.\n\n    >>> rvs = stats.norm.rvs(size=50, random_state=rng)\n    >>> res = stats.ttest_1samp(rvs, popmean=0)\n    >>> ci = res.confidence_interval(confidence_level=0.95)\n    >>> ci\n    ConfidenceInterval(low=-0.3193887540880017, high=0.2898583388980972)\n\n    The bounds of the 95% confidence interval are the\n    minimum and maximum values of the parameter `popmean` for which the\n    p-value of the test would be 0.05.\n\n    >>> res = stats.ttest_1samp(rvs, popmean=ci.low)\n    >>> np.testing.assert_allclose(res.pvalue, 0.05)\n    >>> res = stats.ttest_1samp(rvs, popmean=ci.high)\n    >>> np.testing.assert_allclose(res.pvalue, 0.05)\n\n    Under certain assumptions about the population from which a sample\n    is drawn, the confidence interval with confidence level 95% is expected\n    to contain the true population mean in 95% of sample replications.\n\n    >>> rvs = stats.norm.rvs(size=(50, 1000), loc=1, random_state=rng)\n    >>> res = stats.ttest_1samp(rvs, popmean=0)\n    >>> ci = res.confidence_interval()\n    >>> contains_pop_mean = (ci.low < 1) & (ci.high > 1)\n    >>> contains_pop_mean.sum()\n    953\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 6822, "code": "def _ttest_trim_var_mean_len(a, trim, axis):\n    a = np.sort(a, axis=axis)\n    n = a.shape[axis]\n    g = int(n * trim)\n    v = _calculate_winsorized_variance(a, g, axis)\n    n -= 2 * g\n    m = trim_mean(a, trim, axis=axis)\n    return v, m, n", "documentation": "    \"\"\"Variance, mean, and length of winsorized input along specified axis\"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 6846, "code": "def _calculate_winsorized_variance(a, g, axis):\n    if g == 0:\n        return _var(a, ddof=1, axis=axis)\n    a_win = np.moveaxis(a, axis, -1)\n    nans_indices = np.any(np.isnan(a_win), axis=-1)\n    a_win[..., :g] = a_win[..., [g]]\n    a_win[..., -g:] = a_win[..., [-g - 1]]\n    var_win = np.asarray(_var(a_win, ddof=(2 * g + 1), axis=-1))\n    var_win[nans_indices] = np.nan\n    return var_win\n@xp_capabilities(cpu_only=True, exceptions=[\"cupy\", \"jax.numpy\"])", "documentation": "    \"\"\"Calculates g-times winsorized variance along specified axis\"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 6885, "code": "def ttest_rel(a, b, axis=0, nan_policy='propagate', alternative=\"two-sided\"):\n    return ttest_1samp(a - b, popmean=0., axis=axis, alternative=alternative,\n                       _no_deco=True)\n_power_div_lambda_names = {\n    \"pearson\": 1,\n    \"log-likelihood\": 0,\n    \"freeman-tukey\": -0.5,\n    \"mod-log-likelihood\": -1,\n    \"neyman\": -2,\n    \"cressie-read\": 2/3,\n}", "documentation": "    \"\"\"Calculate the t-test on TWO RELATED samples of scores, a and b.\n\n    This is a test for the null hypothesis that two related or\n    repeated samples have identical average (expected) values.\n\n    Parameters\n    ----------\n    a, b : array_like\n        The arrays must have the same shape.\n    axis : int or None, optional\n        Axis along which to compute test. If None, compute over the whole\n        arrays, `a`, and `b`.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis.\n        The following options are available (default is 'two-sided'):\n\n        * 'two-sided': the means of the distributions underlying the samples\n          are unequal.\n        * 'less': the mean of the distribution underlying the first sample\n          is less than the mean of the distribution underlying the second\n          sample.\n        * 'greater': the mean of the distribution underlying the first\n          sample is greater than the mean of the distribution underlying\n          the second sample.\n\n        .. versionadded:: 1.6.0\n\n    Returns\n    -------\n    result : `~scipy.stats._result_classes.TtestResult`\n        An object with the following attributes:\n\n        statistic : float or array\n            The t-statistic.\n        pvalue : float or array\n            The p-value associated with the given alternative.\n        df : float or array\n            The number of degrees of freedom used in calculation of the\n            t-statistic; this is one less than the size of the sample\n            (``a.shape[axis]``).\n\n            .. versionadded:: 1.10.0\n\n        The object also has the following method:\n\n        confidence_interval(confidence_level=0.95)\n            Computes a confidence interval around the difference in\n            population means for the given confidence level.\n            The confidence interval is returned in a ``namedtuple`` with\n            fields `low` and `high`.\n\n            .. versionadded:: 1.10.0\n\n    Notes\n    -----\n    Examples for use are scores of the same set of student in\n    different exams, or repeated sampling from the same units. The\n    test measures whether the average score differs significantly\n    across samples (e.g. exams). If we observe a large p-value, for\n    example greater than 0.05 or 0.1 then we cannot reject the null\n    hypothesis of identical average scores. If the p-value is smaller\n    than the threshold, e.g. 1%, 5% or 10%, then we reject the null\n    hypothesis of equal averages. Small p-values are associated with\n    large t-statistics.\n\n    The t-statistic is calculated as ``np.mean(a - b)/se``, where ``se`` is the\n    standard error. Therefore, the t-statistic will be positive when the sample\n    mean of ``a - b`` is greater than zero and negative when the sample mean of\n    ``a - b`` is less than zero.\n\n    References\n    ----------\n    https://en.wikipedia.org/wiki/T-test#Dependent_t-test_for_paired_samples\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n\n    >>> rvs1 = stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n    >>> rvs2 = (stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n    ...         + stats.norm.rvs(scale=0.2, size=500, random_state=rng))\n    >>> stats.ttest_rel(rvs1, rvs2)\n    TtestResult(statistic=-0.4549717054410304, pvalue=0.6493274702088672, df=499)\n    >>> rvs3 = (stats.norm.rvs(loc=8, scale=10, size=500, random_state=rng)\n    ...         + stats.norm.rvs(scale=0.2, size=500, random_state=rng))\n    >>> stats.ttest_rel(rvs1, rvs3)\n    TtestResult(statistic=-5.879467544540889, pvalue=7.540777129099917e-09, df=499)\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7011, "code": "def power_divergence(f_obs, f_exp=None, ddof=0, axis=0, lambda_=None):\n    return _power_divergence(f_obs, f_exp=f_exp, ddof=ddof, axis=axis, lambda_=lambda_)", "documentation": "    \"\"\"Cressie-Read power divergence statistic and goodness of fit test.\n\n    This function tests the null hypothesis that the categorical data\n    has the given frequencies, using the Cressie-Read power divergence\n    statistic.\n\n    Parameters\n    ----------\n    f_obs : array_like\n        Observed frequencies in each category.\n    f_exp : array_like, optional\n        Expected frequencies in each category.  By default the categories are\n        assumed to be equally likely.\n    ddof : int, optional\n        \"Delta degrees of freedom\": adjustment to the degrees of freedom\n        for the p-value.  The p-value is computed using a chi-squared\n        distribution with ``k - 1 - ddof`` degrees of freedom, where `k`\n        is the number of observed frequencies.  The default value of `ddof`\n        is 0.\n    axis : int or None, optional\n        The axis of the broadcast result of `f_obs` and `f_exp` along which to\n        apply the test.  If axis is None, all values in `f_obs` are treated\n        as a single data set.  Default is 0.\n    lambda_ : float or str, optional\n        The power in the Cressie-Read power divergence statistic.  The default\n        is 1.  For convenience, `lambda_` may be assigned one of the following\n        strings, in which case the corresponding numerical value is used:\n\n        * ``\"pearson\"`` (value 1)\n            Pearson's chi-squared statistic. In this case, the function is\n            equivalent to `chisquare`.\n        * ``\"log-likelihood\"`` (value 0)\n            Log-likelihood ratio. Also known as the G-test [3]_.\n        * ``\"freeman-tukey\"`` (value -1/2)\n            Freeman-Tukey statistic.\n        * ``\"mod-log-likelihood\"`` (value -1)\n            Modified log-likelihood ratio.\n        * ``\"neyman\"`` (value -2)\n            Neyman's statistic.\n        * ``\"cressie-read\"`` (value 2/3)\n            The power recommended in [5]_.\n\n    Returns\n    -------\n    res: Power_divergenceResult\n        An object containing attributes:\n\n        statistic : float or ndarray\n            The Cressie-Read power divergence test statistic.  The value is\n            a float if `axis` is None or if` `f_obs` and `f_exp` are 1-D.\n        pvalue : float or ndarray\n            The p-value of the test.  The value is a float if `ddof` and the\n            return value `stat` are scalars.\n\n    See Also\n    --------\n    chisquare\n\n    Notes\n    -----\n    This test is invalid when the observed or expected frequencies in each\n    category are too small.  A typical rule is that all of the observed\n    and expected frequencies should be at least 5.\n\n    Also, the sum of the observed and expected frequencies must be the same\n    for the test to be valid; `power_divergence` raises an error if the sums\n    do not agree within a relative tolerance of ``eps**0.5``, where ``eps``\n    is the precision of the input dtype.\n\n    When `lambda_` is less than zero, the formula for the statistic involves\n    dividing by `f_obs`, so a warning or error may be generated if any value\n    in `f_obs` is 0.\n\n    Similarly, a warning or error may be generated if any value in `f_exp` is\n    zero when `lambda_` >= 0.\n\n    The default degrees of freedom, k-1, are for the case when no parameters\n    of the distribution are estimated. If p parameters are estimated by\n    efficient maximum likelihood then the correct degrees of freedom are\n    k-1-p. If the parameters are estimated in a different way, then the\n    dof can be between k-1-p and k-1. However, it is also possible that\n    the asymptotic distribution is not a chisquare, in which case this\n    test is not appropriate.\n\n    References\n    ----------\n    .. [1] Lowry, Richard.  \"Concepts and Applications of Inferential\n           Statistics\". Chapter 8.\n           https://web.archive.org/web/20171015035606/http://faculty.vassar.edu/lowry/ch8pt1.html\n    .. [2] \"Chi-squared test\", https://en.wikipedia.org/wiki/Chi-squared_test\n    .. [3] \"G-test\", https://en.wikipedia.org/wiki/G-test\n    .. [4] Sokal, R. R. and Rohlf, F. J. \"Biometry: the principles and\n           practice of statistics in biological research\", New York: Freeman\n           (1981)\n    .. [5] Cressie, N. and Read, T. R. C., \"Multinomial Goodness-of-Fit\n           Tests\", J. Royal Stat. Soc. Series B, Vol. 46, No. 3 (1984),\n           pp. 440-464.\n\n    Examples\n    --------\n    (See `chisquare` for more examples.)\n\n    When just `f_obs` is given, it is assumed that the expected frequencies\n    are uniform and given by the mean of the observed frequencies.  Here we\n    perform a G-test (i.e. use the log-likelihood ratio statistic):\n\n    >>> import numpy as np\n    >>> from scipy.stats import power_divergence\n    >>> power_divergence([16, 18, 16, 14, 12, 12], lambda_='log-likelihood')\n    (2.006573162632538, 0.84823476779463769)\n\n    The expected frequencies can be given with the `f_exp` argument:\n\n    >>> power_divergence([16, 18, 16, 14, 12, 12],\n    ...                  f_exp=[16, 16, 16, 16, 16, 8],\n    ...                  lambda_='log-likelihood')\n    (3.3281031458963746, 0.6495419288047497)\n\n    When `f_obs` is 2-D, by default the test is applied to each column.\n\n    >>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T\n    >>> obs.shape\n    (6, 2)\n    >>> power_divergence(obs, lambda_=\"log-likelihood\")\n    (array([ 2.00657316,  6.77634498]), array([ 0.84823477,  0.23781225]))\n\n    By setting ``axis=None``, the test is applied to all data in the array,\n    which is equivalent to applying the test to the flattened array.\n\n    >>> power_divergence(obs, axis=None)\n    (23.31034482758621, 0.015975692534127565)\n    >>> power_divergence(obs.ravel())\n    (23.31034482758621, 0.015975692534127565)\n\n    `ddof` is the change to make to the default degrees of freedom.\n\n    >>> power_divergence([16, 18, 16, 14, 12, 12], ddof=1)\n    (2.0, 0.73575888234288467)\n\n    The calculation of the p-values is done by broadcasting the\n    test statistic with `ddof`.\n\n    >>> power_divergence([16, 18, 16, 14, 12, 12], ddof=[0,1,2])\n    (2.0, array([ 0.84914504,  0.73575888,  0.5724067 ]))\n\n    `f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has\n    shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting\n    `f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared\n    statistics, we must use ``axis=1``:\n\n    >>> power_divergence([16, 18, 16, 14, 12, 12],\n    ...                  f_exp=[[16, 16, 16, 16, 16, 8],\n    ...                         [8, 20, 20, 16, 12, 12]],\n    ...                  axis=1)\n    (array([ 3.5 ,  9.25]), array([ 0.62338763,  0.09949846]))\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7258, "code": "def chisquare(f_obs, f_exp=None, ddof=0, axis=0, *, sum_check=True):\n    return _power_divergence(f_obs, f_exp=f_exp, ddof=ddof, axis=axis,\n                             lambda_=\"pearson\", sum_check=sum_check)\nKstestResult = _make_tuple_bunch('KstestResult', ['statistic', 'pvalue'],\n                                 ['statistic_location', 'statistic_sign'])", "documentation": "    \"\"\"Perform Pearson's chi-squared test.\n\n    Pearson's chi-squared test [1]_ is a goodness-of-fit test for a multinomial\n    distribution with given probabilities; that is, it assesses the null hypothesis\n    that the observed frequencies (counts) are obtained by independent\n    sampling of *N* observations from a categorical distribution with given\n    expected frequencies.\n\n    Parameters\n    ----------\n    f_obs : array_like\n        Observed frequencies in each category.\n    f_exp : array_like, optional\n        Expected frequencies in each category. By default, the categories are\n        assumed to be equally likely.\n    ddof : int, optional\n        \"Delta degrees of freedom\": adjustment to the degrees of freedom\n        for the p-value.  The p-value is computed using a chi-squared\n        distribution with ``k - 1 - ddof`` degrees of freedom, where ``k``\n        is the number of categories.  The default value of `ddof` is 0.\n    axis : int or None, optional\n        The axis of the broadcast result of `f_obs` and `f_exp` along which to\n        apply the test.  If axis is None, all values in `f_obs` are treated\n        as a single data set.  Default is 0.\n    sum_check : bool, optional\n        Whether to perform a check that ``sum(f_obs) - sum(f_exp) == 0``. If True,\n        (default) raise an error (or, for lazy backends, return NaN) when the relative\n        difference exceeds the square root of the precision of the data type.\n        See Notes for rationale and possible exceptions.\n\n    Returns\n    -------\n    res: Power_divergenceResult\n        An object containing attributes:\n\n        statistic : float or ndarray\n            The chi-squared test statistic.  The value is a float if `axis` is\n            None or `f_obs` and `f_exp` are 1-D.\n        pvalue : float or ndarray\n            The p-value of the test.  The value is a float if `ddof` and the\n            result attribute `statistic` are scalars.\n\n    See Also\n    --------\n    scipy.stats.power_divergence\n    scipy.stats.fisher_exact : Fisher exact test on a 2x2 contingency table.\n    scipy.stats.barnard_exact : An unconditional exact test. An alternative\n        to chi-squared test for small sample sizes.\n    :ref:`hypothesis_chisquare` : Extended example\n\n    Notes\n    -----\n    This test is invalid when the observed or expected frequencies in each\n    category are too small.  A typical rule is that all of the observed\n    and expected frequencies should be at least 5. According to [2]_, the\n    total number of observations is recommended to be greater than 13,\n    otherwise exact tests (such as Barnard's Exact test) should be used\n    because they do not overreject.\n\n    The default degrees of freedom, k-1, are for the case when no parameters\n    of the distribution are estimated. If p parameters are estimated by\n    efficient maximum likelihood then the correct degrees of freedom are\n    k-1-p. If the parameters are estimated in a different way, then the\n    dof can be between k-1-p and k-1. However, it is also possible that\n    the asymptotic distribution is not chi-square, in which case this test\n    is not appropriate.\n\n    For Pearson's chi-squared test, the total observed and expected counts must match\n    for the p-value to accurately reflect the probability of observing such an extreme\n    value of the statistic under the null hypothesis.\n    This function may be used to perform other statistical tests that do not require\n    the total counts to be equal. For instance, to test the null hypothesis that\n    ``f_obs[i]`` is Poisson-distributed with expectation ``f_exp[i]``, set ``ddof=-1``\n    and ``sum_check=False``. This test follows from the fact that a Poisson random\n    variable with mean and variance ``f_exp[i]`` is approximately normal with the\n    same mean and variance; the chi-squared statistic standardizes, squares, and sums\n    the observations; and the sum of ``n`` squared standard normal variables follows\n    the chi-squared distribution with ``n`` degrees of freedom.\n\n    References\n    ----------\n    .. [1] \"Pearson's chi-squared test\".\n           *Wikipedia*. https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test\n    .. [2] Pearson, Karl. \"On the criterion that a given system of deviations from the probable\n           in the case of a correlated system of variables is such that it can be reasonably\n           supposed to have arisen from random sampling\", Philosophical Magazine. Series 5. 50\n           (1900), pp. 157-175.\n\n    Examples\n    --------\n    When only the mandatory `f_obs` argument is given, it is assumed that the\n    expected frequencies are uniform and given by the mean of the observed\n    frequencies:\n\n    >>> import numpy as np\n    >>> from scipy.stats import chisquare\n    >>> chisquare([16, 18, 16, 14, 12, 12])\n    Power_divergenceResult(statistic=2.0, pvalue=0.84914503608460956)\n\n    The optional `f_exp` argument gives the expected frequencies.\n\n    >>> chisquare([16, 18, 16, 14, 12, 12], f_exp=[16, 16, 16, 16, 16, 8])\n    Power_divergenceResult(statistic=3.5, pvalue=0.62338762774958223)\n\n    When `f_obs` is 2-D, by default the test is applied to each column.\n\n    >>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T\n    >>> obs.shape\n    (6, 2)\n    >>> chisquare(obs)\n    Power_divergenceResult(statistic=array([2.        , 6.66666667]), pvalue=array([0.84914504, 0.24663415]))\n\n    By setting ``axis=None``, the test is applied to all data in the array,\n    which is equivalent to applying the test to the flattened array.\n\n    >>> chisquare(obs, axis=None)\n    Power_divergenceResult(statistic=23.31034482758621, pvalue=0.015975692534127565)\n    >>> chisquare(obs.ravel())\n    Power_divergenceResult(statistic=23.310344827586206, pvalue=0.01597569253412758)\n\n    `ddof` is the change to make to the default degrees of freedom.\n\n    >>> chisquare([16, 18, 16, 14, 12, 12], ddof=1)\n    Power_divergenceResult(statistic=2.0, pvalue=0.7357588823428847)\n\n    The calculation of the p-values is done by broadcasting the\n    chi-squared statistic with `ddof`.\n\n    >>> chisquare([16, 18, 16, 14, 12, 12], ddof=[0, 1, 2])\n    Power_divergenceResult(statistic=2.0, pvalue=array([0.84914504, 0.73575888, 0.5724067 ]))\n\n    `f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has\n    shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting\n    `f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared\n    statistics, we use ``axis=1``:\n\n    >>> chisquare([16, 18, 16, 14, 12, 12],\n    ...           f_exp=[[16, 16, 16, 16, 16, 8], [8, 20, 20, 16, 12, 12]],\n    ...           axis=1)\n    Power_divergenceResult(statistic=array([3.5 , 9.25]), pvalue=array([0.62338763, 0.09949846]))\n\n    For a more detailed example, see :ref:`hypothesis_chisquare`.\n    \"\"\"  # noqa: E501"}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7410, "code": "def _compute_d(cdfvals, x, sign, xp=None):\n    xp = array_namespace(cdfvals, x) if xp is None else xp\n    n = cdfvals.shape[-1]\n    D = (xp.arange(1.0, n + 1, dtype=x.dtype) / n - cdfvals if sign == +1\n         else (cdfvals - xp.arange(0.0, n, dtype=x.dtype)/n))\n    amax = xp.argmax(D, axis=-1, keepdims=True)\n    loc_max = xp.squeeze(xp.take_along_axis(x, amax, axis=-1), axis=-1)\n    D = xp.squeeze(xp.take_along_axis(D, amax, axis=-1), axis=-1)\n    return D[()] if D.ndim == 0 else D, loc_max[()] if loc_max.ndim == 0 else loc_max", "documentation": "    \"\"\"Computes D+/D- as used in the Kolmogorov-Smirnov test.\n\n    Vectorized along the last axis.\n\n    Parameters\n    ----------\n    cdfvals : array_like\n        Sorted array of CDF values between 0 and 1\n    x: array_like\n        Sorted array of the stochastic variable itself\n    sign: int\n        Indicates whether to compute D+ (+1) or D- (-1).\n\n    Returns\n    -------\n    D : float or array\n        The maximum distance of the CDF values below/above (D+/D-) Uniform(0, 1).\n    loc_max : float or array\n        The location at which the maximum is reached.\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7458, "code": "def ks_1samp(x, cdf, args=(), alternative='two-sided', method='auto', *, axis=0):\n    xp = array_namespace(x)\n    mode = method\n    alternative = {'t': 'two-sided', 'g': 'greater', 'l': 'less'}.get(\n        alternative.lower()[0], alternative)\n    if alternative not in ['two-sided', 'greater', 'less']:\n        raise ValueError(f\"Unexpected value {alternative=}\")\n    N = x.shape[-1]\n    x = xp.sort(x, axis=-1)\n    x = xp_promote(x, force_floating=True, xp=xp)\n    cdfvals = cdf(x, *args)", "documentation": "    \"\"\"\n    Performs the one-sample Kolmogorov-Smirnov test for goodness of fit.\n\n    This test compares the underlying distribution F(x) of a sample\n    against a given continuous distribution G(x). See Notes for a description\n    of the available null and alternative hypotheses.\n\n    Parameters\n    ----------\n    x : array_like\n        a 1-D array of observations of iid random variables.\n    cdf : callable\n        callable used to calculate the cdf.\n    args : tuple, sequence, optional\n        Distribution parameters, used with `cdf`.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the null and alternative hypotheses. Default is 'two-sided'.\n        Please see explanations in the Notes below.\n    method : {'auto', 'exact', 'approx', 'asymp'}, optional\n        Defines the distribution used for calculating the p-value.\n        The following options are available (default is 'auto'):\n\n        * 'auto' : selects one of the other options.\n        * 'exact' : uses the exact distribution of test statistic.\n        * 'approx' : approximates the two-sided probability with twice\n          the one-sided probability\n        * 'asymp': uses asymptotic distribution of test statistic\n\n    axis : int or tuple of ints, default: 0\n        If an int or tuple of ints, the axis or axes of the input along which\n        to compute the statistic. The statistic of each axis-slice (e.g. row)\n        of the input will appear in a corresponding element of the output.\n        If ``None``, the input will be raveled before computing the statistic.\n\n    Returns\n    -------\n    res: KstestResult\n        An object containing attributes:\n\n        statistic : float\n            KS test statistic, either D+, D-, or D (the maximum of the two)\n        pvalue : float\n            One-tailed or two-tailed p-value.\n        statistic_location : float\n            Value of `x` corresponding with the KS statistic; i.e., the\n            distance between the empirical distribution function and the\n            hypothesized cumulative distribution function is measured at this\n            observation.\n        statistic_sign : int\n            +1 if the KS statistic is the maximum positive difference between\n            the empirical distribution function and the hypothesized cumulative\n            distribution function (D+); -1 if the KS statistic is the maximum\n            negative difference (D-).\n\n\n    See Also\n    --------\n    ks_2samp, kstest\n\n    Notes\n    -----\n    There are three options for the null and corresponding alternative\n    hypothesis that can be selected using the `alternative` parameter.\n\n    - `two-sided`: The null hypothesis is that the two distributions are\n      identical, F(x)=G(x) for all x; the alternative is that they are not\n      identical.\n\n    - `less`: The null hypothesis is that F(x) >= G(x) for all x; the\n      alternative is that F(x) < G(x) for at least one x.\n\n    - `greater`: The null hypothesis is that F(x) <= G(x) for all x; the\n      alternative is that F(x) > G(x) for at least one x.\n\n    Note that the alternative hypotheses describe the *CDFs* of the\n    underlying distributions, not the observed values. For example,\n    suppose x1 ~ F and x2 ~ G. If F(x) > G(x) for all x, the values in\n    x1 tend to be less than those in x2.\n\n    Examples\n    --------\n    Suppose we wish to test the null hypothesis that a sample is distributed\n    according to the standard normal.\n    We choose a confidence level of 95%; that is, we will reject the null\n    hypothesis in favor of the alternative if the p-value is less than 0.05.\n\n    When testing uniformly distributed data, we would expect the\n    null hypothesis to be rejected.\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> stats.ks_1samp(stats.uniform.rvs(size=100, random_state=rng),\n    ...                stats.norm.cdf)\n    KstestResult(statistic=0.5001899973268688,\n                 pvalue=1.1616392184763533e-23,\n                 statistic_location=0.00047625268963724654,\n                 statistic_sign=-1)\n\n    Indeed, the p-value is lower than our threshold of 0.05, so we reject the\n    null hypothesis in favor of the default \"two-sided\" alternative: the data\n    are *not* distributed according to the standard normal.\n\n    When testing random variates from the standard normal distribution, we\n    expect the data to be consistent with the null hypothesis most of the time.\n\n    >>> x = stats.norm.rvs(size=100, random_state=rng)\n    >>> stats.ks_1samp(x, stats.norm.cdf)\n    KstestResult(statistic=0.05345882212970396,\n                 pvalue=0.9227159037744717,\n                 statistic_location=-1.2451343873745018,\n                 statistic_sign=1)\n\n    As expected, the p-value of 0.92 is not below our threshold of 0.05, so\n    we cannot reject the null hypothesis.\n\n    Suppose, however, that the random variates are distributed according to\n    a normal distribution that is shifted toward greater values. In this case,\n    the cumulative density function (CDF) of the underlying distribution tends\n    to be *less* than the CDF of the standard normal. Therefore, we would\n    expect the null hypothesis to be rejected with ``alternative='less'``:\n\n    >>> x = stats.norm.rvs(size=100, loc=0.5, random_state=rng)\n    >>> stats.ks_1samp(x, stats.norm.cdf, alternative='less')\n    KstestResult(statistic=0.17482387821055168,\n                 pvalue=0.001913921057766743,\n                 statistic_location=0.3713830565352756,\n                 statistic_sign=-1)\n\n    and indeed, with p-value smaller than our threshold, we reject the null\n    hypothesis in favor of the alternative.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7652, "code": "def _compute_prob_outside_square(n, h):\n    P = 0.0\n    k = int(np.floor(n / h))\n    while k >= 0:\n        p1 = 1.0\n        for j in range(h):\n            p1 = (n - k * h - j) * p1 / (n + k * h + j + 1)\n        P = p1 * (1.0 - P)\n        k -= 1\n    return 2 * P", "documentation": "    \"\"\"\n    Compute the proportion of paths that pass outside the two diagonal lines.\n\n    Parameters\n    ----------\n    n : integer\n        n > 0\n    h : integer\n        0 <= h <= n\n\n    Returns\n    -------\n    p : float\n        The proportion of paths that pass outside the lines x-y = +/-h.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7690, "code": "def _count_paths_outside_method(m, n, g, h):\n    if m < n:\n        m, n = n, m\n    mg = m // g\n    ng = n // g\n    lxj = n + (mg-h)//mg\n    xj = [(h + mg * j + ng-1)//ng for j in range(lxj)]\n    if lxj == 0:\n        return special.binom(m + n, n)\n    B = np.zeros(lxj)\n    B[0] = 1", "documentation": "    \"\"\"Count the number of paths that pass outside the specified diagonal.\n\n    Parameters\n    ----------\n    m : integer\n        m > 0\n    n : integer\n        n > 0\n    g : integer\n        g is greatest common divisor of m and n\n    h : integer\n        0 <= h <= lcm(m,n)\n\n    Returns\n    -------\n    p : float\n        The number of paths that go low.\n        The calculation may overflow - check for a finite answer.\n\n    Notes\n    -----\n    Count the integer lattice paths from (0, 0) to (m, n), which at some\n    point (x, y) along the path, satisfy:\n      m*y <= n*x - h*g\n    The paths make steps of size +1 in either positive x or positive y\n    directions.\n\n    We generally follow Hodges' treatment of Drion/Gnedenko/Korolyuk.\n    Hodges, J.L. Jr.,\n    \"The Significance Probability of the Smirnov Two-Sample Test,\"\n    Arkiv fiur Matematik, 3, No. 43 (1958), 469-86.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7764, "code": "def _attempt_exact_2kssamp(n1, n2, g, d, alternative):\n    lcm = (n1 // g) * n2\n    h = int(np.round(d * lcm))\n    d = h * 1.0 / lcm\n    if h == 0:\n        return True, d, 1.0\n    saw_fp_error, prob = False, np.nan\n    try:\n        with np.errstate(invalid=\"raise\", over=\"raise\"):\n            if alternative == 'two-sided':\n                if n1 == n2:", "documentation": "    \"\"\"Attempts to compute the exact 2sample probability.\n\n    n1, n2 are the sample sizes\n    g is the gcd(n1, n2)\n    d is the computed max difference in ECDFs\n\n    Returns (success, d, probability)\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 7818, "code": "def ks_2samp(data1, data2, alternative='two-sided', method='auto', *, axis=0):\n    mode = method\n    if mode not in ['auto', 'exact', 'asymp']:\n        raise ValueError(f'Invalid value for mode: {mode}')\n    alternative = {'t': 'two-sided', 'g': 'greater', 'l': 'less'}.get(\n        alternative.lower()[0], alternative)\n    if alternative not in ['two-sided', 'less', 'greater']:\n        raise ValueError(f'Invalid value for alternative: {alternative}')\n    MAX_AUTO_N = 10000  # 'auto' will attempt to be exact if n1,n2 <= MAX_AUTO_N\n    xp = array_namespace(data1, data2)\n    data1 = xp.sort(data1, axis=-1)", "documentation": "    \"\"\"\n    Performs the two-sample Kolmogorov-Smirnov test for goodness of fit.\n\n    This test compares the underlying continuous distributions F(x) and G(x)\n    of two independent samples.  See Notes for a description of the available\n    null and alternative hypotheses.\n\n    Parameters\n    ----------\n    data1, data2 : array_like, 1-Dimensional\n        Two arrays of sample observations assumed to be drawn from a continuous\n        distribution, sample sizes can be different.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the null and alternative hypotheses. Default is 'two-sided'.\n        Please see explanations in the Notes below.\n    method : {'auto', 'exact', 'asymp'}, optional\n        Defines the method used for calculating the p-value.\n        The following options are available (default is 'auto'):\n\n        * 'auto' : use 'exact' for small size arrays, 'asymp' for large\n        * 'exact' : use exact distribution of test statistic\n        * 'asymp' : use asymptotic distribution of test statistic\n\n    axis : int or tuple of ints, default: 0\n        If an int or tuple of ints, the axis or axes of the input along which\n        to compute the statistic. The statistic of each axis-slice (e.g. row)\n        of the input will appear in a corresponding element of the output.\n        If ``None``, the input will be raveled before computing the statistic.\n\n    Returns\n    -------\n    res: KstestResult\n        An object containing attributes:\n\n        statistic : float\n            KS test statistic.\n        pvalue : float\n            One-tailed or two-tailed p-value.\n        statistic_location : float\n            Value from `data1` or `data2` corresponding with the KS statistic;\n            i.e., the distance between the empirical distribution functions is\n            measured at this observation.\n        statistic_sign : int\n            +1 if the empirical distribution function of `data1` exceeds\n            the empirical distribution function of `data2` at\n            `statistic_location`, otherwise -1.\n\n    See Also\n    --------\n    kstest, ks_1samp, epps_singleton_2samp, anderson_ksamp\n\n    Notes\n    -----\n    There are three options for the null and corresponding alternative\n    hypothesis that can be selected using the `alternative` parameter.\n\n    - `less`: The null hypothesis is that F(x) >= G(x) for all x; the\n      alternative is that F(x) < G(x) for at least one x. The statistic\n      is the magnitude of the minimum (most negative) difference between the\n      empirical distribution functions of the samples.\n\n    - `greater`: The null hypothesis is that F(x) <= G(x) for all x; the\n      alternative is that F(x) > G(x) for at least one x. The statistic\n      is the maximum (most positive) difference between the empirical\n      distribution functions of the samples.\n\n    - `two-sided`: The null hypothesis is that the two distributions are\n      identical, F(x)=G(x) for all x; the alternative is that they are not\n      identical. The statistic is the maximum absolute difference between the\n      empirical distribution functions of the samples.\n\n    Note that the alternative hypotheses describe the *CDFs* of the\n    underlying distributions, not the observed values of the data. For example,\n    suppose x1 ~ F and x2 ~ G. If F(x) > G(x) for all x, the values in\n    x1 tend to be less than those in x2.\n\n    If the KS statistic is large, then the p-value will be small, and this may\n    be taken as evidence against the null hypothesis in favor of the\n    alternative.\n\n    If ``method='exact'``, `ks_2samp` attempts to compute an exact p-value,\n    that is, the probability under the null hypothesis of obtaining a test\n    statistic value as extreme as the value computed from the data.\n    If ``method='asymp'``, the asymptotic Kolmogorov-Smirnov distribution is\n    used to compute an approximate p-value.\n    If ``method='auto'``, an exact p-value computation is attempted if both\n    sample sizes are less than 10000; otherwise, the asymptotic method is used.\n    In any case, if an exact p-value calculation is attempted and fails, a\n    warning will be emitted, and the asymptotic p-value will be returned.\n\n    The 'two-sided' 'exact' computation computes the complementary probability\n    and then subtracts from 1.  As such, the minimum probability it can return\n    is about 1e-16.  While the algorithm itself is exact, numerical\n    errors may accumulate for large sample sizes.   It is most suited to\n    situations in which one of the sample sizes is only a few thousand.\n\n    We generally follow Hodges' treatment of Drion/Gnedenko/Korolyuk [1]_.\n\n    References\n    ----------\n    .. [1] Hodges, J.L. Jr.,  \"The Significance Probability of the Smirnov\n           Two-Sample Test,\" Arkiv fiur Matematik, 3, No. 43 (1958), 469-486.\n\n    Examples\n    --------\n    Suppose we wish to test the null hypothesis that two samples were drawn\n    from the same distribution.\n    We choose a confidence level of 95%; that is, we will reject the null\n    hypothesis in favor of the alternative if the p-value is less than 0.05.\n\n    If the first sample were drawn from a uniform distribution and the second\n    were drawn from the standard normal, we would expect the null hypothesis\n    to be rejected.\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> sample1 = stats.uniform.rvs(size=100, random_state=rng)\n    >>> sample2 = stats.norm.rvs(size=110, random_state=rng)\n    >>> stats.ks_2samp(sample1, sample2)\n    KstestResult(statistic=0.5454545454545454,\n                 pvalue=7.37417839555191e-15,\n                 statistic_location=-0.014071496412861274,\n                 statistic_sign=-1)\n\n\n    Indeed, the p-value is lower than our threshold of 0.05, so we reject the\n    null hypothesis in favor of the default \"two-sided\" alternative: the data\n    were *not* drawn from the same distribution.\n\n    When both samples are drawn from the same distribution, we expect the data\n    to be consistent with the null hypothesis most of the time.\n\n    >>> sample1 = stats.norm.rvs(size=105, random_state=rng)\n    >>> sample2 = stats.norm.rvs(size=95, random_state=rng)\n    >>> stats.ks_2samp(sample1, sample2)\n    KstestResult(statistic=0.10927318295739348,\n                 pvalue=0.5438289009927495,\n                 statistic_location=-0.1670157701848795,\n                 statistic_sign=-1)\n\n    As expected, the p-value of 0.54 is not below our threshold of 0.05, so\n    we cannot reject the null hypothesis.\n\n    Suppose, however, that the first sample were drawn from\n    a normal distribution shifted toward greater values. In this case,\n    the cumulative density function (CDF) of the underlying distribution tends\n    to be *less* than the CDF underlying the second sample. Therefore, we would\n    expect the null hypothesis to be rejected with ``alternative='less'``:\n\n    >>> sample1 = stats.norm.rvs(size=105, loc=0.5, random_state=rng)\n    >>> stats.ks_2samp(sample1, sample2, alternative='less')\n    KstestResult(statistic=0.4055137844611529,\n                 pvalue=3.5474563068855554e-08,\n                 statistic_location=-0.13249370614972575,\n                 statistic_sign=-1)\n\n    and indeed, with p-value smaller than our threshold, we reject the null\n    hypothesis in favor of the alternative.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 8148, "code": "def kstest(rvs, cdf, args=(), N=20, alternative='two-sided', method='auto'):\n    if alternative == 'two_sided':\n        alternative = 'two-sided'\n    if alternative not in ['two-sided', 'greater', 'less']:\n        raise ValueError(f\"Unexpected alternative: {alternative}\")\n    xvals, yvals, cdf = _parse_kstest_args(rvs, cdf, args, N)\n    if cdf:\n        return ks_1samp(xvals, cdf, args=args, alternative=alternative,\n                        method=method, _no_deco=True)\n    return ks_2samp(xvals, yvals, alternative=alternative, method=method,\n                    _no_deco=True)", "documentation": "    \"\"\"\n    Performs the (one-sample or two-sample) Kolmogorov-Smirnov test for\n    goodness of fit.\n\n    The one-sample test compares the underlying distribution F(x) of a sample\n    against a given distribution G(x). The two-sample test compares the\n    underlying distributions of two independent samples. Both tests are valid\n    only for continuous distributions.\n\n    Parameters\n    ----------\n    rvs : str, array_like, or callable\n        If an array, it should be a 1-D array of observations of random\n        variables.\n        If a callable, it should be a function to generate random variables;\n        it is required to have a keyword argument `size`.\n        If a string, it should be the name of a distribution in `scipy.stats`,\n        which will be used to generate random variables.\n    cdf : str, array_like or callable\n        If array_like, it should be a 1-D array of observations of random\n        variables, and the two-sample test is performed\n        (and rvs must be array_like).\n        If a callable, that callable is used to calculate the cdf.\n        If a string, it should be the name of a distribution in `scipy.stats`,\n        which will be used as the cdf function.\n    args : tuple, sequence, optional\n        Distribution parameters, used if `rvs` or `cdf` are strings or\n        callables.\n    N : int, optional\n        Sample size if `rvs` is string or callable.  Default is 20.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the null and alternative hypotheses. Default is 'two-sided'.\n        Please see explanations in the Notes below.\n    method : {'auto', 'exact', 'approx', 'asymp'}, optional\n        Defines the distribution used for calculating the p-value.\n        The following options are available (default is 'auto'):\n\n        * 'auto' : selects one of the other options.\n        * 'exact' : uses the exact distribution of test statistic.\n        * 'approx' : approximates the two-sided probability with twice the\n          one-sided probability\n        * 'asymp': uses asymptotic distribution of test statistic\n\n    Returns\n    -------\n    res: KstestResult\n        An object containing attributes:\n\n        statistic : float\n            KS test statistic, either D+, D-, or D (the maximum of the two)\n        pvalue : float\n            One-tailed or two-tailed p-value.\n        statistic_location : float\n            In a one-sample test, this is the value of `rvs`\n            corresponding with the KS statistic; i.e., the distance between\n            the empirical distribution function and the hypothesized cumulative\n            distribution function is measured at this observation.\n\n            In a two-sample test, this is the value from `rvs` or `cdf`\n            corresponding with the KS statistic; i.e., the distance between\n            the empirical distribution functions is measured at this\n            observation.\n        statistic_sign : int\n            In a one-sample test, this is +1 if the KS statistic is the\n            maximum positive difference between the empirical distribution\n            function and the hypothesized cumulative distribution function\n            (D+); it is -1 if the KS statistic is the maximum negative\n            difference (D-).\n\n            In a two-sample test, this is +1 if the empirical distribution\n            function of `rvs` exceeds the empirical distribution\n            function of `cdf` at `statistic_location`, otherwise -1.\n\n    See Also\n    --------\n    ks_1samp, ks_2samp\n\n    Notes\n    -----\n    There are three options for the null and corresponding alternative\n    hypothesis that can be selected using the `alternative` parameter.\n\n    - `two-sided`: The null hypothesis is that the two distributions are\n      identical, F(x)=G(x) for all x; the alternative is that they are not\n      identical.\n\n    - `less`: The null hypothesis is that F(x) >= G(x) for all x; the\n      alternative is that F(x) < G(x) for at least one x.\n\n    - `greater`: The null hypothesis is that F(x) <= G(x) for all x; the\n      alternative is that F(x) > G(x) for at least one x.\n\n    Note that the alternative hypotheses describe the *CDFs* of the\n    underlying distributions, not the observed values. For example,\n    suppose x1 ~ F and x2 ~ G. If F(x) > G(x) for all x, the values in\n    x1 tend to be less than those in x2.\n\n\n    Examples\n    --------\n    Suppose we wish to test the null hypothesis that a sample is distributed\n    according to the standard normal.\n    We choose a confidence level of 95%; that is, we will reject the null\n    hypothesis in favor of the alternative if the p-value is less than 0.05.\n\n    When testing uniformly distributed data, we would expect the\n    null hypothesis to be rejected.\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> stats.kstest(stats.uniform.rvs(size=100, random_state=rng),\n    ...              stats.norm.cdf)\n    KstestResult(statistic=0.5001899973268688,\n                 pvalue=1.1616392184763533e-23,\n                 statistic_location=0.00047625268963724654,\n                 statistic_sign=-1)\n\n    Indeed, the p-value is lower than our threshold of 0.05, so we reject the\n    null hypothesis in favor of the default \"two-sided\" alternative: the data\n    are *not* distributed according to the standard normal.\n\n    When testing random variates from the standard normal distribution, we\n    expect the data to be consistent with the null hypothesis most of the time.\n\n    >>> x = stats.norm.rvs(size=100, random_state=rng)\n    >>> stats.kstest(x, stats.norm.cdf)\n    KstestResult(statistic=0.05345882212970396,\n                 pvalue=0.9227159037744717,\n                 statistic_location=-1.2451343873745018,\n                 statistic_sign=1)\n\n\n    As expected, the p-value of 0.92 is not below our threshold of 0.05, so\n    we cannot reject the null hypothesis.\n\n    Suppose, however, that the random variates are distributed according to\n    a normal distribution that is shifted toward greater values. In this case,\n    the cumulative density function (CDF) of the underlying distribution tends\n    to be *less* than the CDF of the standard normal. Therefore, we would\n    expect the null hypothesis to be rejected with ``alternative='less'``:\n\n    >>> x = stats.norm.rvs(size=100, loc=0.5, random_state=rng)\n    >>> stats.kstest(x, stats.norm.cdf, alternative='less')\n    KstestResult(statistic=0.17482387821055168,\n                 pvalue=0.001913921057766743,\n                 statistic_location=0.3713830565352756,\n                 statistic_sign=-1)\n\n    and indeed, with p-value smaller than our threshold, we reject the null\n    hypothesis in favor of the alternative.\n\n    For convenience, the previous test can be performed using the name of the\n    distribution as the second argument.\n\n    >>> stats.kstest(x, \"norm\", alternative='less')\n    KstestResult(statistic=0.17482387821055168,\n                 pvalue=0.001913921057766743,\n                 statistic_location=0.3713830565352756,\n                 statistic_sign=-1)\n\n    The examples above have all been one-sample tests identical to those\n    performed by `ks_1samp`. Note that `kstest` can also perform two-sample\n    tests identical to those performed by `ks_2samp`. For example, when two\n    samples are drawn from the same distribution, we expect the data to be\n    consistent with the null hypothesis most of the time.\n\n    >>> sample1 = stats.laplace.rvs(size=105, random_state=rng)\n    >>> sample2 = stats.laplace.rvs(size=95, random_state=rng)\n    >>> stats.kstest(sample1, sample2)\n    KstestResult(statistic=0.11779448621553884,\n                 pvalue=0.4494256912629795,\n                 statistic_location=0.6138814275424155,\n                 statistic_sign=1)\n\n    As expected, the p-value of 0.45 is not below our threshold of 0.05, so\n    we cannot reject the null hypothesis.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 8342, "code": "def tiecorrect(rankvals):\n    arr = np.sort(rankvals)\n    idx = np.nonzero(np.r_[True, arr[1:] != arr[:-1], True])[0]\n    cnt = np.diff(idx).astype(np.float64)\n    size = np.float64(arr.size)\n    return 1.0 if size < 2 else 1.0 - (cnt**3 - cnt).sum() / (size**3 - size)\nRanksumsResult = namedtuple('RanksumsResult', ('statistic', 'pvalue'))\n@xp_capabilities(np_only=True)\n@_axis_nan_policy_factory(RanksumsResult, n_samples=2)", "documentation": "    \"\"\"Tie correction factor for Mann-Whitney U and Kruskal-Wallis H tests.\n\n    Parameters\n    ----------\n    rankvals : array_like\n        A 1-D sequence of ranks.  Typically this will be the array\n        returned by `~scipy.stats.rankdata`.\n\n    Returns\n    -------\n    factor : float\n        Correction factor for U or H.\n\n    See Also\n    --------\n    rankdata : Assign ranks to the data\n    mannwhitneyu : Mann-Whitney rank test\n    kruskal : Kruskal-Wallis H test\n\n    References\n    ----------\n    .. [1] Siegel, S. (1956) Nonparametric Statistics for the Behavioral\n           Sciences.  New York: McGraw-Hill.\n\n    Examples\n    --------\n    >>> from scipy.stats import tiecorrect, rankdata\n    >>> tiecorrect([1, 2.5, 2.5, 4])\n    0.9\n    >>> ranks = rankdata([1, 3, 2, 4, 5, 7, 2, 8, 4])\n    >>> ranks\n    array([ 1. ,  4. ,  2.5,  5.5,  7. ,  8. ,  2.5,  9. ,  5.5])\n    >>> tiecorrect(ranks)\n    0.9833333333333333\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 8392, "code": "def ranksums(x, y, alternative='two-sided'):\n    x, y = map(np.asarray, (x, y))\n    n1 = len(x)\n    n2 = len(y)\n    alldata = np.concatenate((x, y))\n    ranked = rankdata(alldata)\n    x = ranked[:n1]\n    s = np.sum(x, axis=0)\n    expected = n1 * (n1+n2+1) / 2.0\n    z = (s - expected) / np.sqrt(n1*n2*(n1+n2+1)/12.0)\n    pvalue = _get_pvalue(z, _SimpleNormal(), alternative, xp=np)", "documentation": "    \"\"\"Compute the Wilcoxon rank-sum statistic for two samples.\n\n    The Wilcoxon rank-sum test tests the null hypothesis that two sets\n    of measurements are drawn from the same distribution.  The alternative\n    hypothesis is that values in one sample are more likely to be\n    larger than the values in the other sample.\n\n    This test should be used to compare two samples from continuous\n    distributions.  It does not handle ties between measurements\n    in x and y.  For tie-handling and an optional continuity correction\n    see `scipy.stats.mannwhitneyu`.\n\n    Parameters\n    ----------\n    x, y : array_like\n        The data from the two samples.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n        The following options are available:\n\n        * 'two-sided': one of the distributions (underlying `x` or `y`) is\n          stochastically greater than the other.\n        * 'less': the distribution underlying `x` is stochastically less\n          than the distribution underlying `y`.\n        * 'greater': the distribution underlying `x` is stochastically greater\n          than the distribution underlying `y`.\n\n        .. versionadded:: 1.7.0\n\n    Returns\n    -------\n    statistic : float\n        The test statistic under the large-sample approximation that the\n        rank sum statistic is normally distributed.\n    pvalue : float\n        The p-value of the test.\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/Wilcoxon_rank-sum_test\n\n    Examples\n    --------\n    We can test the hypothesis that two independent unequal-sized samples are\n    drawn from the same distribution with computing the Wilcoxon rank-sum\n    statistic.\n\n    >>> import numpy as np\n    >>> from scipy.stats import ranksums\n    >>> rng = np.random.default_rng()\n    >>> sample1 = rng.uniform(-1, 1, 200)\n    >>> sample2 = rng.uniform(-0.5, 1.5, 300) # a shifted distribution\n    >>> ranksums(sample1, sample2)\n    RanksumsResult(statistic=-7.887059,\n                   pvalue=3.09390448e-15) # may vary\n    >>> ranksums(sample1, sample2, alternative='less')\n    RanksumsResult(statistic=-7.750585297581713,\n                   pvalue=4.573497606342543e-15) # may vary\n    >>> ranksums(sample1, sample2, alternative='greater')\n    RanksumsResult(statistic=-7.750585297581713,\n                   pvalue=0.9999999999999954) # may vary\n\n    The p-value of less than ``0.05`` indicates that this test rejects the\n    hypothesis at the 5% significance level.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 8478, "code": "def kruskal(*samples, nan_policy='propagate', axis=0):\n    xp = array_namespace(*samples)\n    samples = xp_promote(*samples, force_floating=True, xp=xp)\n    num_groups = len(samples)\n    if num_groups < 2:\n        raise ValueError(\"Need at least two groups in stats.kruskal()\")\n    n = [sample.shape[-1] for sample in samples]\n    totaln = sum(n)\n    if any(n) < 1:  # Only needed for `test_axis_nan_policy`\n        raise ValueError(\"Inputs must not be empty.\")\n    alldata = xp.concat(samples, axis=-1)", "documentation": "    \"\"\"Compute the Kruskal-Wallis H-test for independent samples.\n\n    The Kruskal-Wallis H-test tests the null hypothesis that the population\n    median of all of the groups are equal.  It is a non-parametric version of\n    ANOVA.  The test works on 2 or more independent samples, which may have\n    different sizes.  Note that rejecting the null hypothesis does not\n    indicate which of the groups differs.  Post hoc comparisons between\n    groups are required to determine which groups are different.\n\n    Parameters\n    ----------\n    sample1, sample2, ... : array_like\n       Two or more arrays with the sample measurements can be given as\n       arguments. Samples must be one-dimensional.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    axis : int or tuple of ints, default: 0\n        If an int or tuple of ints, the axis or axes of the input along which\n        to compute the statistic. The statistic of each axis-slice (e.g. row)\n        of the input will appear in a corresponding element of the output.\n        If ``None``, the input will be raveled before computing the statistic.\n\n    Returns\n    -------\n    statistic : float\n       The Kruskal-Wallis H statistic, corrected for ties.\n    pvalue : float\n       The p-value for the test using the assumption that H has a chi\n       square distribution. The p-value returned is the survival function of\n       the chi square distribution evaluated at H.\n\n    See Also\n    --------\n    f_oneway : 1-way ANOVA.\n    mannwhitneyu : Mann-Whitney rank test on two samples.\n    friedmanchisquare : Friedman test for repeated measurements.\n\n    Notes\n    -----\n    Due to the assumption that H has a chi square distribution, the number\n    of samples in each group must not be too small.  A typical rule is\n    that each sample must have at least 5 measurements.\n\n    References\n    ----------\n    .. [1] W. H. Kruskal & W. W. Wallis, \"Use of Ranks in\n       One-Criterion Variance Analysis\", Journal of the American Statistical\n       Association, Vol. 47, Issue 260, pp. 583-621, 1952.\n    .. [2] https://en.wikipedia.org/wiki/Kruskal-Wallis_one-way_analysis_of_variance\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> x = [1, 3, 5, 7, 9]\n    >>> y = [2, 4, 6, 8, 10]\n    >>> stats.kruskal(x, y)\n    KruskalResult(statistic=0.2727272727272734, pvalue=0.6015081344405895)\n\n    >>> x = [1, 1, 1]\n    >>> y = [2, 2, 2]\n    >>> z = [2, 2]\n    >>> stats.kruskal(x, y, z)\n    KruskalResult(statistic=7.0, pvalue=0.0301973834223185)\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 8589, "code": "def friedmanchisquare(*samples, axis=0):\n    k = len(samples)\n    if k < 3:\n        raise ValueError('At least 3 samples must be given '\n                         f'for Friedman test, got {k}.')\n    xp = array_namespace(*samples)\n    samples = xp_promote(*samples, force_floating=True, xp=xp)\n    dtype = samples[0].dtype\n    n = samples[0].shape[-1]\n    if n == 0:  # only for `test_axis_nan_policy`; user doesn't see this\n        raise ValueError(\"One or more sample arguments is too small.\")", "documentation": "    \"\"\"Compute the Friedman test for repeated samples.\n\n    The Friedman test tests the null hypothesis that repeated samples of\n    the same individuals have the same distribution.  It is often used\n    to test for consistency among samples obtained in different ways.\n    For example, if two sampling techniques are used on the same set of\n    individuals, the Friedman test can be used to determine if the two\n    sampling techniques are consistent.\n\n    Parameters\n    ----------\n    sample1, sample2, sample3... : array_like\n        Arrays of observations.  All of the arrays must have the same number\n        of elements.  At least three samples must be given.\n    axis : int or tuple of ints, default: 0\n        If an int or tuple of ints, the axis or axes of the input along which\n        to compute the statistic. The statistic of each axis-slice (e.g. row)\n        of the input will appear in a corresponding element of the output.\n        If ``None``, the input will be raveled before computing the statistic.\n\n    Returns\n    -------\n    statistic : float\n        The test statistic, correcting for ties.\n    pvalue : float\n        The associated p-value assuming that the test statistic has a chi\n        squared distribution.\n\n    See Also\n    --------\n    :ref:`hypothesis_friedmanchisquare` : Extended example\n\n    Notes\n    -----\n    Due to the assumption that the test statistic has a chi squared\n    distribution, the p-value is only reliable for n > 10 and more than\n    6 repeated samples.\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/Friedman_test\n    .. [2] Demsar, J. (2006). Statistical comparisons of classifiers over\n           multiple data sets. Journal of Machine Learning Research, 7, 1-30.\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> rng = np.random.default_rng(seed=18)\n    >>> x = rng.random((6, 10))\n    >>> from scipy.stats import friedmanchisquare\n    >>> res = friedmanchisquare(x[0], x[1], x[2], x[3], x[4], x[5])\n    >>> res.statistic, res.pvalue\n    (11.428571428571416, 0.043514520866727614)\n\n    The p-value is less than 0.05; however, as noted above, the results may not\n    be reliable since we have a small number of repeated samples.\n\n    For a more detailed example, see :ref:`hypothesis_friedmanchisquare`.\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 8830, "code": "def combine_pvalues(pvalues, method='fisher', weights=None, *, axis=0):\n    xp = array_namespace(pvalues, weights)\n    pvalues, weights = xp_promote(pvalues, weights, broadcast=True,\n                                  force_floating=True, xp=xp)\n    if xp_size(pvalues) == 0:\n        NaN = _get_nan(pvalues)\n        return SignificanceResult(NaN, NaN)\n    n = _length_nonmasked(pvalues, axis)\n    n = xp.asarray(n, dtype=pvalues.dtype, device=xp_device(pvalues))\n    if method == 'fisher':\n        statistic = -2 * xp.sum(xp.log(pvalues), axis=axis)", "documentation": "    \"\"\"\n    Combine p-values from independent tests that bear upon the same hypothesis.\n\n    These methods are intended only for combining p-values from hypothesis\n    tests based upon continuous distributions.\n\n    Each method assumes that under the null hypothesis, the p-values are\n    sampled independently and uniformly from the interval [0, 1]. A test\n    statistic (different for each method) is computed and a combined\n    p-value is calculated based upon the distribution of this test statistic\n    under the null hypothesis.\n\n    Parameters\n    ----------\n    pvalues : array_like\n        Array of p-values assumed to come from independent tests based on\n        continuous distributions.\n    method : {'fisher', 'pearson', 'tippett', 'stouffer', 'mudholkar_george'}\n\n        Name of method to use to combine p-values.\n\n        The available methods are (see Notes for details):\n\n        * 'fisher': Fisher's method (Fisher's combined probability test)\n        * 'pearson': Pearson's method\n        * 'mudholkar_george': Mudholkar's and George's method\n        * 'tippett': Tippett's method\n        * 'stouffer': Stouffer's Z-score method\n\n    weights : array_like, optional\n        Optional array of weights used only for Stouffer's Z-score method.\n        Ignored by other methods.\n\n    Returns\n    -------\n    res : SignificanceResult\n        An object containing attributes:\n\n        statistic : float\n            The statistic calculated by the specified method.\n        pvalue : float\n            The combined p-value.\n\n    Examples\n    --------\n    Suppose we wish to combine p-values from four independent tests\n    of the same null hypothesis using Fisher's method (default).\n\n    >>> from scipy.stats import combine_pvalues\n    >>> pvalues = [0.1, 0.05, 0.02, 0.3]\n    >>> combine_pvalues(pvalues)\n    SignificanceResult(statistic=20.828626352604235, pvalue=0.007616871850449092)\n\n    When the individual p-values carry different weights, consider Stouffer's\n    method.\n\n    >>> weights = [1, 2, 3, 4]\n    >>> res = combine_pvalues(pvalues, method='stouffer', weights=weights)\n    >>> res.pvalue\n    0.009578891494533616\n\n    Notes\n    -----\n    If this function is applied to tests with a discrete statistics such as\n    any rank test or contingency-table test, it will yield systematically\n    wrong results, e.g. Fisher's method will systematically overestimate the\n    p-value [1]_. This problem becomes less severe for large sample sizes\n    when the discrete distributions become approximately continuous.\n\n    The differences between the methods can be best illustrated by their\n    statistics and what aspects of a combination of p-values they emphasise\n    when considering significance [2]_. For example, methods emphasising large\n    p-values are more sensitive to strong false and true negatives; conversely\n    methods focussing on small p-values are sensitive to positives.\n\n    * The statistics of Fisher's method (also known as Fisher's combined\n      probability test) [3]_ is :math:`-2\\\\sum_i \\\\log(p_i)`, which is\n      equivalent (as a test statistics) to the product of individual p-values:\n      :math:`\\\\prod_i p_i`. Under the null hypothesis, this statistics follows\n      a :math:`\\\\chi^2` distribution. This method emphasises small p-values.\n    * Pearson's method uses :math:`-2\\\\sum_i\\\\log(1-p_i)`, which is equivalent\n      to :math:`\\\\prod_i \\\\frac{1}{1-p_i}` [2]_.\n      It thus emphasises large p-values.\n    * Mudholkar and George compromise between Fisher's and Pearson's method by\n      averaging their statistics [4]_. Their method emphasises extreme\n      p-values, both close to 1 and 0.\n    * Stouffer's method [5]_ uses Z-scores and the statistic:\n      :math:`\\\\sum_i \\\\Phi^{-1} (p_i)`, where :math:`\\\\Phi` is the CDF of the\n      standard normal distribution. The advantage of this method is that it is\n      straightforward to introduce weights, which can make Stouffer's method\n      more powerful than Fisher's method when the p-values are from studies\n      of different size [6]_ [7]_.\n    * Tippett's method uses the smallest p-value as a statistic.\n      (Mind that this minimum is not the combined p-value.)\n\n    Fisher's method may be extended to combine p-values from dependent tests\n    [8]_. Extensions such as Brown's method and Kost's method are not currently\n    implemented.\n\n    .. versionadded:: 0.15.0\n\n    References\n    ----------\n    .. [1] Kincaid, W. M., \"The Combination of Tests Based on Discrete\n           Distributions.\" Journal of the American Statistical Association 57,\n           no. 297 (1962), 10-19.\n    .. [2] Heard, N. and Rubin-Delanchey, P. \"Choosing between methods of\n           combining p-values.\"  Biometrika 105.1 (2018): 239-246.\n    .. [3] https://en.wikipedia.org/wiki/Fisher%27s_method\n    .. [4] George, E. O., and G. S. Mudholkar. \"On the convolution of logistic\n           random variables.\" Metrika 30.1 (1983): 1-13.\n    .. [5] https://en.wikipedia.org/wiki/Fisher%27s_method#Relation_to_Stouffer.27s_Z-score_method\n    .. [6] Whitlock, M. C. \"Combining probability from independent tests: the\n           weighted Z-method is superior to Fisher's approach.\" Journal of\n           Evolutionary Biology 18, no. 5 (2005): 1368-1373.\n    .. [7] Zaykin, Dmitri V. \"Optimally weighted Z-test is a powerful method\n           for combining probabilities in meta-analysis.\" Journal of\n           Evolutionary Biology 24, no. 8 (2011): 1836-1841.\n    .. [8] https://en.wikipedia.org/wiki/Extensions_of_Fisher%27s_method\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 9043, "code": "    def confidence_interval(self, confidence_level=0.95):\n        alternative = self._alternative\n        p = self._p\n        x = np.sort(self._x)\n        n = len(x)\n        bd = stats.binom(n, p)\n        if confidence_level <= 0 or confidence_level >= 1:\n            message = \"`confidence_level` must be a number between 0 and 1.\"\n            raise ValueError(message)\n        low_index = np.nan\n        high_index = np.nan", "documentation": "        \"\"\"\n        Compute the confidence interval of the quantile.\n\n        Parameters\n        ----------\n        confidence_level : float, default: 0.95\n            Confidence level for the computed confidence interval\n            of the quantile. Default is 0.95.\n\n        Returns\n        -------\n        ci : ``ConfidenceInterval`` object\n            The object has attributes ``low`` and ``high`` that hold the\n            lower and upper bounds of the confidence interval.\n\n        Examples\n        --------\n        >>> import numpy as np\n        >>> import scipy.stats as stats\n        >>> p = 0.75  # quantile of interest\n        >>> q = 0  # hypothesized value of the quantile\n        >>> x = np.exp(np.arange(0, 1.01, 0.01))\n        >>> res = stats.quantile_test(x, q=q, p=p, alternative='less')\n        >>> lb, ub = res.confidence_interval()\n        >>> lb, ub\n        (-inf, 2.293318740264183)\n        >>> res = stats.quantile_test(x, q=q, p=p, alternative='two-sided')\n        >>> lb, ub = res.confidence_interval(0.9)\n        >>> lb, ub\n        (1.9542373206359396, 2.293318740264183)\n        \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 9945, "code": "def _validate_distribution(values, weights):\n    values = np.asarray(values, dtype=float)\n    if len(values) == 0:\n        raise ValueError(\"Distribution can't be empty.\")\n    if weights is not None:\n        weights = np.asarray(weights, dtype=float)\n        if len(weights) != len(values):\n            raise ValueError('Value and weight array-likes for the same '\n                             'empirical distribution must be of the same size.')\n        if np.any(weights < 0):\n            raise ValueError('All weights must be non-negative.')", "documentation": "    \"\"\"\n    Validate the values and weights from a distribution input of `cdf_distance`\n    and return them as ndarray objects.\n\n    Parameters\n    ----------\n    values : array_like\n        Values observed in the (empirical) distribution.\n    weights : array_like\n        Weight for each value.\n\n    Returns\n    -------\n    values : ndarray\n        Values as ndarray.\n    weights : ndarray\n        Weights as ndarray.\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 9990, "code": "def rankdata(a, method='average', *, axis=None, nan_policy='propagate'):\n    methods = ('average', 'min', 'max', 'dense', 'ordinal')\n    if method not in methods:\n        raise ValueError(f'unknown method \"{method}\"')\n    xp = array_namespace(a)\n    x = xp.asarray(a)\n    if axis is None:\n        x = xp_ravel(x)\n        axis = -1\n    if xp_size(x) == 0:\n        dtype = xp.asarray(1.).dtype if method == 'average' else xp.asarray(1).dtype", "documentation": "    \"\"\"Assign ranks to data, dealing with ties appropriately.\n\n    By default (``axis=None``), the data array is first flattened, and a flat\n    array of ranks is returned. Separately reshape the rank array to the\n    shape of the data array if desired (see Examples).\n\n    Ranks begin at 1.  The `method` argument controls how ranks are assigned\n    to equal values.  See [1]_ for further discussion of ranking methods.\n\n    Parameters\n    ----------\n    a : array_like\n        The array of values to be ranked.\n    method : {'average', 'min', 'max', 'dense', 'ordinal'}, optional\n        The method used to assign ranks to tied elements.\n        The following methods are available (default is 'average'):\n\n        * 'average': The average of the ranks that would have been assigned to\n          all the tied values is assigned to each value.\n        * 'min': The minimum of the ranks that would have been assigned to all\n          the tied values is assigned to each value.  (This is also\n          referred to as \"competition\" ranking.)\n        * 'max': The maximum of the ranks that would have been assigned to all\n          the tied values is assigned to each value.\n        * 'dense': Like 'min', but the rank of the next highest element is\n          assigned the rank immediately after those assigned to the tied\n          elements.\n        * 'ordinal': All values are given a distinct rank, corresponding to\n          the order that the values occur in `a`.\n\n    axis : {None, int}, optional\n        Axis along which to perform the ranking. If ``None``, the data array\n        is first flattened.\n    nan_policy : {'propagate', 'omit', 'raise'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': propagates nans through the rank calculation\n        * 'omit': performs the calculations ignoring nan values\n        * 'raise': raises an error\n\n        .. note::\n\n            When `nan_policy` is 'propagate', the output is an array of *all*\n            nans because ranks relative to nans in the input are undefined.\n            When `nan_policy` is 'omit', nans in `a` are ignored when ranking\n            the other values, and the corresponding locations of the output\n            are nan.\n\n        .. versionadded:: 1.10\n\n    Returns\n    -------\n    ranks : ndarray\n         An array of size equal to the size of `a`, containing rank\n         scores.\n\n    References\n    ----------\n    .. [1] \"Ranking\", https://en.wikipedia.org/wiki/Ranking\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import rankdata\n    >>> rankdata([0, 2, 3, 2])\n    array([ 1. ,  2.5,  4. ,  2.5])\n    >>> rankdata([0, 2, 3, 2], method='min')\n    array([ 1,  2,  4,  2])\n    >>> rankdata([0, 2, 3, 2], method='max')\n    array([ 1,  3,  4,  3])\n    >>> rankdata([0, 2, 3, 2], method='dense')\n    array([ 1,  2,  3,  2])\n    >>> rankdata([0, 2, 3, 2], method='ordinal')\n    array([ 1,  2,  4,  3])\n    >>> rankdata([[0, 2], [3, 2]]).reshape(2,2)\n    array([[1. , 2.5],\n          [4. , 2.5]])\n    >>> rankdata([[0, 2, 2], [3, 2, 5]], axis=1)\n    array([[1. , 2.5, 2.5],\n           [2. , 1. , 3. ]])\n    >>> rankdata([0, 2, 3, np.nan, -2, np.nan], nan_policy=\"propagate\")\n    array([nan, nan, nan, nan, nan, nan])\n    >>> rankdata([0, 2, 3, np.nan, -2, np.nan], nan_policy=\"omit\")\n    array([ 2.,  3.,  4., nan,  1., nan])\n\n    \"\"\""}, {"filename": "scipy/stats/_stats_py.py", "start_line": 10536, "code": "def linregress(x, y, alternative='two-sided', *, axis=0):\n    xp = array_namespace(x, y)\n    x, y = xp_promote(x, y, force_floating=True, xp=xp)\n    TINY = 1.0e-20\n    n = x.shape[-1]\n    xmean = xp.mean(x, axis=-1, keepdims=True)\n    ymean = xp.mean(y, axis=-1, keepdims=True)\n    x_ = _demean(x, xmean, axis=-1, xp=xp)\n    y_ = _demean(y, ymean, axis=-1, xp=xp, precision_warning=False)\n    xmean = xp.squeeze(xmean, axis=-1)\n    ymean = xp.squeeze(ymean, axis=-1)", "documentation": "    \"\"\"\n    Calculate a linear least-squares regression for two sets of measurements.\n\n    Parameters\n    ----------\n    x, y : array_like\n        Two sets of measurements.  Both arrays should have the same length N.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n        The following options are available:\n\n        * 'two-sided': the slope of the regression line is nonzero\n        * 'less': the slope of the regression line is less than zero\n        * 'greater':  the slope of the regression line is greater than zero\n\n        .. versionadded:: 1.7.0\n    axis : int or None, default: 0\n        If an int, the axis of the input along which to compute the statistic.\n        The statistic of each axis-slice (e.g. row) of the input will appear in a\n        corresponding element of the output.\n        If ``None``, the input will be raveled before computing the statistic.\n\n    Returns\n    -------\n    result : ``LinregressResult`` instance\n        The return value is an object with the following attributes:\n\n        slope : float\n            Slope of the regression line.\n        intercept : float\n            Intercept of the regression line.\n        rvalue : float\n            The Pearson correlation coefficient. The square of ``rvalue``\n            is equal to the coefficient of determination.\n        pvalue : float\n            The p-value for a hypothesis test whose null hypothesis is\n            that the slope is zero, using Wald Test with t-distribution of\n            the test statistic. See `alternative` above for alternative\n            hypotheses.\n        stderr : float\n            Standard error of the estimated slope (gradient), under the\n            assumption of residual normality.\n        intercept_stderr : float\n            Standard error of the estimated intercept, under the assumption\n            of residual normality.\n\n    See Also\n    --------\n    scipy.optimize.curve_fit :\n        Use non-linear least squares to fit a function to data.\n    scipy.optimize.leastsq :\n        Minimize the sum of squares of a set of equations.\n\n    Notes\n    -----\n    For compatibility with older versions of SciPy, the return value acts\n    like a ``namedtuple`` of length 5, with fields ``slope``, ``intercept``,\n    ``rvalue``, ``pvalue`` and ``stderr``, so one can continue to write::\n\n        slope, intercept, r, p, se = linregress(x, y)\n\n    With that style, however, the standard error of the intercept is not\n    available.  To have access to all the computed values, including the\n    standard error of the intercept, use the return value as an object\n    with attributes, e.g.::\n\n        result = linregress(x, y)\n        print(result.intercept, result.intercept_stderr)\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n\n    Generate some data:\n\n    >>> x = rng.random(10)\n    >>> y = 1.6*x + rng.random(10)\n\n    Perform the linear regression:\n\n    >>> res = stats.linregress(x, y)\n\n    Coefficient of determination (R-squared):\n\n    >>> print(f\"R-squared: {res.rvalue**2:.6f}\")\n    R-squared: 0.717533\n\n    Plot the data along with the fitted line:\n\n    >>> plt.plot(x, y, 'o', label='original data')\n    >>> plt.plot(x, res.intercept + res.slope*x, 'r', label='fitted line')\n    >>> plt.legend()\n    >>> plt.show()\n\n    Calculate 95% confidence interval on slope and intercept:\n\n    >>> # Two-sided inverse Students t-distribution\n    >>> # p - probability, df - degrees of freedom\n    >>> from scipy.stats import t\n    >>> tinv = lambda p, df: abs(t.ppf(p/2, df))\n\n    >>> ts = tinv(0.05, len(x)-2)\n    >>> print(f\"slope (95%): {res.slope:.6f} +/- {ts*res.stderr:.6f}\")\n    slope (95%): 1.453392 +/- 0.743465\n    >>> print(f\"intercept (95%): {res.intercept:.6f}\"\n    ...       f\" +/- {ts*res.intercept_stderr:.6f}\")\n    intercept (95%): 0.616950 +/- 0.544475\n\n    \"\"\""}]}
{"repository": "scipy/scipy", "commit_sha": "3093d3ead614a538612285cfff265db430a8e07a", "commit_message": "ENH: stats.ContinuousDistribution.lmoment: add population L-moments (#23766)\n\n* ENH: stats.ContinuousDistribution: add l-moments\n\n* ENH: stats.ContinuousDistribution: add l-moments from integral over icdf\n\n* MAINT: stats.ContinuousDistribution: simplify l-moment ratio implementation\n\n* ENH: stats.ContinuousDistribution: draft lmoment for custom and shifted/scaled distributions\n\n* DOC: stats.ContinuousDistribution.lmoment: draft documentation\n\n* TST: stats.ContinuousDistribution.lmoment: add tests\n\n* ENH: stats.ContinuousDistribution.lmoment: override for built-in distributions\n\n* MAINT: stats.ContinuousDistribution.lmoment: fixups\n\n* DOC: stats.lmoment: update docs per new default standardize=True\"", "commit_date": "2026-01-31T09:21:12+00:00", "author": "Matt Haberland", "file": "scipy/stats/tests/test_continuous.py", "patch": "@@ -216,6 +216,7 @@ def test_support_moments_sample(self, family, data, seed):\n         with np.errstate(invalid='ignore', divide='ignore'):\n             check_support(dist)\n             check_moment_funcs(dist, result_shape)  # this needs to get split up\n+            check_lmoment_funcs(dist, result_shape)\n             check_sample_shape_NaNs(dist, 'sample', sample_shape, result_shape, rng)\n             qrng = qmc.Halton(d=1, seed=rng)\n             check_sample_shape_NaNs(dist, 'sample', sample_shape, result_shape, qrng)\n@@ -773,23 +774,55 @@ def has_formula(order, kind):\n             check(i, 'standardized', 'general', ref, success=i <= 2)\n         check(i, 'standardized', 'normalize', ref)\n \n-    if isinstance(dist, ShiftedScaledDistribution):\n-        # logmoment is not fully fleshed out; no need to test\n-        # ShiftedScaledDistribution here\n+    dist.reset_cache()\n+\n+\n+def check_lmoment_funcs(dist, result_shape):\n+    # Perform consistency check for L-moments similar to check_moment_funcs above\n+\n+    if not isinstance(dist, ContinuousDistribution):\n+        message = \"L-moments are currently available only for continuous...\"\n+        with pytest.raises(NotImplementedError, match=message):\n+            dist.lmoment(1)\n         return\n \n-    # logmoment is not very accuate, and it's not public, so skip for now\n-    # ### Check Against _logmoment ###\n-    # logmean = dist._logmoment(1, logcenter=-np.inf)\n-    # for i in range(6):\n-    #     ref = np.exp(dist._logmoment(i, logcenter=-np.inf))\n-    #     assert_allclose(dist.moment(i, 'raw'), ref, atol=atol*10**i)\n-    #\n-    #     ref = np.exp(dist._logmoment(i, logcenter=logmean))\n-    #     assert_allclose(dist.moment(i, 'central'), ref, atol=atol*10**i)\n-    #\n-    #     ref = np.exp(dist._logmoment(i, logcenter=logmean, standardized=True))\n-    #     assert_allclose(dist.moment(i, 'standardized'), ref, atol=atol*10**i)\n+    atol = 2e-9\n+\n+    def check(order, standardize=False, method=None, ref=None, success=True):\n+        if success:\n+            res = dist.lmoment(order, standardize=standardize, method=method)\n+            assert_allclose(res, ref, atol=atol)\n+            assert res.shape == ref.shape\n+        else:\n+            with pytest.raises(NotImplementedError):\n+                dist.lmoment(order, standardize=standardize, method=method)\n+\n+    ### Check L-Moments ###\n+\n+    standardize = False\n+    for i in range(1, 6):\n+        check(i, standardize, 'cache', success=standardize)  # not cached yet\n+        ref = dist.lmoment(i, standardize=standardize, method='order_statistics')\n+        check_nans_and_edges(dist, 'lmoment', None, ref)\n+        assert ref.shape == result_shape\n+        check(i, standardize, 'cache', ref, success=True)  # cached now\n+        check(i, standardize, 'formula', ref,\n+              success=dist._overrides('_lmoment_formula')\n+                      and (i < 5 or dist.__class__.__name__ == \"Uniform\"))\n+        check(i, standardize, 'general', ref, success=(i == 1))\n+        if dist._overrides('_icdf_formula'):\n+            check(i, standardize, 'quadrature_icdf', ref, success=True)\n+\n+    standardize=True\n+    for i in range(3, 6):\n+        ref = dist.lmoment(i, standardize=standardize, method='order_statistics')\n+        assert ref.shape == result_shape\n+        check(i, standardize, 'formula', ref,\n+              success=dist._overrides('_lmoment_formula')\n+                      and (i < 5 or dist.__class__.__name__ == \"Uniform\"))\n+        check(i, standardize, 'general', ref, success=False)\n+        if dist._overrides('_icdf_formula'):\n+            check(i, standardize, 'quadrature_icdf', ref, success=True)\n \n \n @pytest.mark.parametrize('family', (Normal,))\n@@ -914,12 +947,17 @@ class Test(ContinuousDistribution):\n         Test(tol=-1)\n \n     message = (\"Argument `order` of `Test.moment` must be a \"\n-               \"finite, positive integer.\")\n+               \"finite integer greater than or equal to 0.\")\n     with pytest.raises(ValueError, match=message):\n         Test().moment(-1)\n     with pytest.raises(ValueError, match=message):\n         Test().moment(np.inf)\n \n+    message = (\"Argument `order` of `Test.lmoment` must be a \"\n+               \"finite integer greater than or equal to 1.\")\n+    with pytest.raises(ValueError, match=message):\n+        Test().lmoment(0)\n+\n     message = \"Argument `kind` of `Test.moment` must be one of...\"\n     with pytest.raises(ValueError, match=message):\n         Test().moment(2, kind='coconut')\n@@ -1233,6 +1271,16 @@ def moment(self, order, kind='raw', *, a, b):\n                     # can tell the difference between the two\n                     return (b - a) / np.log(b/a) + 1e-10\n \n+            def lmoment(self, order, *, a, b):\n+                s = np.log(b/a)\n+                l1 = (b - a) / s\n+                l2 = ((s - 2) * b + (s + 2) * a) / s**2\n+                l3 = ((s**2 - 6*s + 12) * b - (s**2 + 6*s + 12) * a) / s ** 3\n+                l4 = ((s**3 - 12*s**2 + 60*s - 120) * b\n+                      + (s**3 + 12*s**2 + 60*s + 120) * a) / s ** 4\n+                ls = {1: l1, 2: l2, 3: l3, 4: l4}\n+                return ls.get(int(order), None)\n+\n         LogUniform = stats.make_distribution(MyLogUniform())\n \n         X = LogUniform(a=1., b=np.e)\n@@ -1259,6 +1307,12 @@ def moment(self, order, kind='raw', *, a, b):\n             for order in range(5):\n                 assert_allclose(X.moment(order, kind=kind),\n                                 Y.moment(order, kind=kind))\n+        for standardize in [False, True]:\n+            for order in range(1, 5):\n+                if standardize and order < 3:\n+                    continue\n+                assert_allclose(X.lmoment(order, standardize=standardize),\n+                                Y.lmoment(order, standardize=standardize))\n \n         # Confirm that the `sample` and `moment` methods are overriden as expected\n         sample_formula = X.sample(shape=10, rng=0, method='formula')\n@@ -1269,6 +1323,10 @@ def moment(self, order, kind='raw', *, a, b):\n         assert_allclose(X.mean(method='formula'), X.mean(method='quadrature'))\n         assert not X.mean(method='formula') == X.mean(method='quadrature')\n \n+        assert_allclose(X.lmoment(method='formula'),\n+                        X.lmoment(method='quadrature_icdf'))\n+        assert not X.lmoment(method='formula') == X.lmoment(method='quadrature_icdf')\n+\n     # pdf and cdf formulas below can warn on boundary of support in some cases.\n     # See https://github.com/scipy/scipy/pull/22560#discussion_r1962763840.\n     @pytest.mark.slow\n@@ -1506,6 +1564,7 @@ def __init__(self, *args, **kwargs):\n         scale = dist.scale\n         dist0 = StandardNormal()\n         dist_ref = stats.norm(loc=loc, scale=scale)\n+        dist_ref_lmoment = Normal(mu=loc, sigma=scale)\n \n         x0 = (x - loc) / scale\n         y0 = (y - loc) / scale\n@@ -1545,6 +1604,12 @@ def __init__(self, *args, **kwargs):\n                                 dist0.moment(i, 'central') * scale**i)\n                 assert_allclose(dist.moment(i, 'standardized'),\n                                 dist0.moment(i, 'standardized') * np.sign(scale)**i)\n+            for i in range(1, 5):\n+                assert_allclose(dist.lmoment(i), dist_ref_lmoment.lmoment(i), atol=1e-8)\n+                if i >= 3:\n+                    assert_allclose(dist.lmoment(i, standardize=True),\n+                                    dist_ref_lmoment.lmoment(i, standardize=True),\n+                                    atol=1e-8)\n \n         # Transform back to the original distribution using all arithmetic\n         # operations; check that it behaves as expected.\n@@ -1582,6 +1647,11 @@ def __init__(self, *args, **kwargs):\n                 assert_allclose(dist.moment(i, 'central'), dist0.moment(i, 'central'))\n                 assert_allclose(dist.moment(i, 'standardized'),\n                                 dist0.moment(i, 'standardized'))\n+            for i in range(1, 5):\n+                assert_allclose(dist.lmoment(i), dist0.lmoment(i))\n+                if i >= 3:\n+                    assert_allclose(dist.lmoment(i, standardize=True),\n+                                    dist0.lmoment(i, standardize=True))\n \n             # These are tough to compare because of the way the shape works\n             # rng = np.random.default_rng(seed)\n@@ -2128,6 +2198,9 @@ def assert_allclose(res, ref, **kwargs):\n                 assert_allclose(X.moment(order, kind=kind),\n                                 Y.moment(order, kind=kind),\n                                 atol=1e-15)\n+        message = \"L-moments are not currently available...\"\n+        with pytest.raises(NotImplementedError, match=message):\n+            X.lmoment(1)\n \n         # weak test of `sample`\n         shape = (10, 20, 5)", "before_segments": [], "after_segments": []}
{"repository": "scipy/scipy", "commit_sha": "0ef7c86a1760fc153f979c56eeab6b484ab7131e", "commit_message": "DOC: integrate.nsum: correct documentation of `tolerances` argument (#24347)\n\n* DOC: integrate.nsum: correct documentation of tolerances argument", "commit_date": "2026-01-12T20:26:13+00:00", "author": "Matt Haberland", "file": "scipy/integrate/_tanhsinh.py", "patch": "@@ -1018,12 +1018,12 @@ def nsum(f, a, b, *, step=1, args=(), log=False, maxterms=int(2**20), tolerances\n         The maximum number of terms to evaluate for direct summation.\n         Additional function evaluations may be performed for input\n         validation and integral evaluation.\n-    atol, rtol : float, optional\n-        Absolute termination tolerance (default: 0) and relative termination\n-        tolerance (default: ``eps**0.5``, where ``eps`` is the precision of\n-        the result dtype), respectively. Must be non-negative\n-        and finite if `log` is False, and must be expressed as the log of a\n-        non-negative and finite number if `log` is True.\n+    tolerances : dict, optional\n+        Dictionary with recognized keys ``atol`` for absolute termination tolerance\n+        (default: 0) and ``rtol`` for relative termination tolerance (default:\n+        ``eps**0.5``, where ``eps`` is the precision of the result dtype), respectively.\n+        Values must be non-negative and finite if `log` is False, and must be expressed\n+        as the log of a non-negative and finite number if `log` is True.\n \n     Returns\n     -------", "before_segments": [{"filename": "scipy/integrate/_tanhsinh.py", "start_line": 438, "code": "    def check_termination(work):\n        stop = xp.zeros(work.Sn.shape, dtype=bool)\n        if work.nit == 0:\n            i = xp_ravel(work.a == work.b)  # ravel singleton dimension\n            zero = xp.asarray(-xp.inf if log else 0.)\n            zero = xp.full(work.Sn.shape, zero, dtype=Sn.dtype)\n            zero[xp.isnan(Sn)] = xp.nan\n            work.Sn[i] = zero[i]\n            work.aerr[i] = zero[i]\n            work.status[i] = eim._ECONVERGED\n            stop[i] = True", "documentation": "        \"\"\"Terminate due to convergence or encountering non-finite values\"\"\""}], "after_segments": [{"filename": "scipy/integrate/_tanhsinh.py", "start_line": 438, "code": "    def check_termination(work):\n        stop = xp.zeros(work.Sn.shape, dtype=bool)\n        if work.nit == 0:\n            i = xp_ravel(work.a == work.b)  # ravel singleton dimension\n            zero = xp.asarray(-xp.inf if log else 0.)\n            zero = xp.full(work.Sn.shape, zero, dtype=Sn.dtype)\n            zero[xp.isnan(Sn)] = xp.nan\n            work.Sn[i] = zero[i]\n            work.aerr[i] = zero[i]\n            work.status[i] = eim._ECONVERGED\n            stop[i] = True", "documentation": "        \"\"\"Terminate due to convergence or encountering non-finite values\"\"\""}]}
{"repository": "scipy/scipy", "commit_sha": "8b32878c0bdf7716c6c58973f55cbe8c2e0b5c3d", "commit_message": "DOC: ``signal.place_poles``: fix docs for returned ``gain_matrix`` shape (#24294)", "commit_date": "2026-01-07T14:38:33+00:00", "author": "Joren Hammudoglu", "file": "scipy/signal/_ltisys.py", "patch": "@@ -2735,7 +2735,7 @@ def place_poles(A, B, poles, method=\"YT\", rtol=1e-3, maxiter=30):\n     -------\n     full_state_feedback : Bunch object\n         full_state_feedback is composed of:\n-            gain_matrix : 1-D ndarray\n+            gain_matrix : 2-D ndarray\n                 The closed loop matrix K such as the eigenvalues of ``A-BK``\n                 are as close as possible to the requested poles.\n             computed_poles : 1-D ndarray", "before_segments": [{"filename": "scipy/signal/_ltisys.py", "start_line": 50, "code": "    def __new__(cls, *system, **kwargs):\n        if cls is LinearTimeInvariant:\n            raise NotImplementedError('The LinearTimeInvariant class is not '\n                                      'meant to be used directly, use `lti` '\n                                      'or `dlti` instead.')\n        return super().__new__(cls)", "documentation": "        \"\"\"Create a new object, don't allow direct instances.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 58, "code": "    def __init__(self):\n        super().__init__()\n        self.inputs = None\n        self.outputs = None\n        self._dt = None\n    @property", "documentation": "        \"\"\"\n        Initialize the `lti` baseclass.\n\n        The heavy lifting is done by the subclasses.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 71, "code": "    def dt(self):\n        return self._dt\n    @property", "documentation": "        \"\"\"Return the sampling time of the system, `None` for `lti` systems.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 83, "code": "    def zeros(self):\n        return self.to_zpk().zeros\n    @property", "documentation": "        \"\"\"Zeros of the system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 88, "code": "    def poles(self):\n        return self.to_zpk().poles", "documentation": "        \"\"\"Poles of the system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 92, "code": "    def _as_ss(self):\n        if isinstance(self, StateSpace):\n            return self\n        else:\n            return self.to_ss()", "documentation": "        \"\"\"Convert to `StateSpace` system, without copying.\n\n        Returns\n        -------\n        sys: StateSpace\n            The `StateSpace` system. If the class is already an instance of\n            `StateSpace` then this instance is returned.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 106, "code": "    def _as_zpk(self):\n        if isinstance(self, ZerosPolesGain):\n            return self\n        else:\n            return self.to_zpk()", "documentation": "        \"\"\"Convert to `ZerosPolesGain` system, without copying.\n\n        Returns\n        -------\n        sys: ZerosPolesGain\n            The `ZerosPolesGain` system. If the class is already an instance of\n            `ZerosPolesGain` then this instance is returned.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 120, "code": "    def _as_tf(self):\n        if isinstance(self, TransferFunction):\n            return self\n        else:\n            return self.to_tf()", "documentation": "        \"\"\"Convert to `TransferFunction` system, without copying.\n\n        Returns\n        -------\n        sys: ZerosPolesGain\n            The `TransferFunction` system. If the class is already an instance of\n            `TransferFunction` then this instance is returned.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 207, "code": "    def __new__(cls, *system):\n        if cls is lti:\n            N = len(system)\n            if N == 2:\n                return TransferFunctionContinuous.__new__(\n                    TransferFunctionContinuous, *system)\n            elif N == 3:\n                return ZerosPolesGainContinuous.__new__(\n                    ZerosPolesGainContinuous, *system)\n            elif N == 4:\n                return StateSpaceContinuous.__new__(StateSpaceContinuous,", "documentation": "        \"\"\"Create an instance of the appropriate subclass.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 226, "code": "    def __init__(self, *system):\n        super().__init__(*system)", "documentation": "        \"\"\"\n        Initialize the `lti` baseclass.\n\n        The heavy lifting is done by the subclasses.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 234, "code": "    def impulse(self, X0=None, T=None, N=None):\n        return impulse(self, X0=X0, T=T, N=N)", "documentation": "        \"\"\"\n        Return the impulse response of a continuous-time system.\n        See `impulse` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 241, "code": "    def step(self, X0=None, T=None, N=None):\n        return step(self, X0=X0, T=T, N=N)", "documentation": "        \"\"\"\n        Return the step response of a continuous-time system.\n        See `step` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 248, "code": "    def output(self, U, T, X0=None):\n        return lsim(self, U, T, X0=X0)", "documentation": "        \"\"\"\n        Return the response of a continuous-time system to input `U`.\n        See `lsim` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 255, "code": "    def bode(self, w=None, n=100):\n        return bode(self, w=w, n=n)", "documentation": "        \"\"\"\n        Calculate Bode magnitude and phase data of a continuous-time system.\n\n        Returns a 3-tuple containing arrays of frequencies [rad/s], magnitude\n        [dB] and phase [deg]. See `bode` for details.\n\n        Examples\n        --------\n        >>> from scipy import signal\n        >>> import matplotlib.pyplot as plt\n\n        >>> sys = signal.TransferFunction([1], [1, 1])\n        >>> w, mag, phase = sys.bode()\n\n        >>> plt.figure()\n        >>> plt.semilogx(w, mag)    # Bode magnitude plot\n        >>> plt.figure()\n        >>> plt.semilogx(w, phase)  # Bode phase plot\n        >>> plt.show()\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 279, "code": "    def freqresp(self, w=None, n=10000):\n        return freqresp(self, w=w, n=n)", "documentation": "        \"\"\"\n        Calculate the frequency response of a continuous-time system.\n\n        Returns a 2-tuple containing arrays of frequencies [rad/s] and\n        complex magnitude.\n        See `freqresp` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 289, "code": "    def to_discrete(self, dt, method='zoh', alpha=None):\n        raise NotImplementedError('to_discrete is not implemented for this '\n                                  'system class.')", "documentation": "        \"\"\"Return a discretized version of the current system.\n\n        Parameters: See `cont2discrete` for details.\n\n        Returns\n        -------\n        sys: instance of `dlti`\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 391, "code": "    def __new__(cls, *system, **kwargs):\n        if cls is dlti:\n            N = len(system)\n            if N == 2:\n                return TransferFunctionDiscrete.__new__(\n                    TransferFunctionDiscrete, *system, **kwargs)\n            elif N == 3:\n                return ZerosPolesGainDiscrete.__new__(ZerosPolesGainDiscrete,\n                                                      *system, **kwargs)\n            elif N == 4:\n                return StateSpaceDiscrete.__new__(StateSpaceDiscrete, *system,", "documentation": "        \"\"\"Create an instance of the appropriate subclass.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 410, "code": "    def __init__(self, *system, **kwargs):\n        dt = kwargs.pop('dt', True)\n        super().__init__(*system, **kwargs)\n        self.dt = dt\n    @property", "documentation": "        \"\"\"\n        Initialize the `lti` baseclass.\n\n        The heavy lifting is done by the subclasses.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 422, "code": "    def dt(self):\n        return self._dt\n    @dt.setter", "documentation": "        \"\"\"Return the sampling time of the system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 430, "code": "    def impulse(self, x0=None, t=None, n=None):\n        return dimpulse(self, x0=x0, t=t, n=n)", "documentation": "        \"\"\"\n        Return the impulse response of the discrete-time `dlti` system.\n        See `dimpulse` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 437, "code": "    def step(self, x0=None, t=None, n=None):\n        return dstep(self, x0=x0, t=t, n=n)", "documentation": "        \"\"\"\n        Return the step response of the discrete-time `dlti` system.\n        See `dstep` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 444, "code": "    def output(self, u, t, x0=None):\n        return dlsim(self, u, t, x0=x0)", "documentation": "        \"\"\"\n        Return the response of the discrete-time system to input `u`.\n        See `dlsim` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 481, "code": "    def freqresp(self, w=None, n=10000, whole=False):\n        return dfreqresp(self, w=w, n=n, whole=whole)", "documentation": "        \"\"\"\n        Calculate the frequency response of a discrete-time system.\n\n        Returns a 2-tuple containing arrays of frequencies [rad/s] and\n        complex magnitude.\n        See `dfreqresp` for details.\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 569, "code": "    def __new__(cls, *system, **kwargs):\n        if len(system) == 1 and isinstance(system[0], LinearTimeInvariant):\n            return system[0].to_tf()\n        if cls is TransferFunction:\n            if kwargs.get('dt') is None:\n                return TransferFunctionContinuous.__new__(\n                    TransferFunctionContinuous,\n                    *system,\n                    **kwargs)\n            else:\n                return TransferFunctionDiscrete.__new__(", "documentation": "        \"\"\"Handle object conversion if input is an instance of lti.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 590, "code": "    def __init__(self, *system, **kwargs):\n        if isinstance(system[0], LinearTimeInvariant):\n            return\n        super().__init__(**kwargs)\n        self._num = None\n        self._den = None\n        self.num, self.den = normalize(*system)", "documentation": "        \"\"\"Initialize the state space LTI system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 604, "code": "    def __repr__(self):\n        return (\n            f'{self.__class__.__name__}(\\n'\n            f'{repr(self.num)},\\n'\n            f'{repr(self.den)},\\n'\n            f'dt: {repr(self.dt)}\\n)'\n        )\n    @property", "documentation": "        \"\"\"Return representation of the system's transfer function\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 614, "code": "    def num(self):\n        return self._num\n    @num.setter", "documentation": "        \"\"\"Numerator of the `TransferFunction` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 630, "code": "    def den(self):\n        return self._den\n    @den.setter", "documentation": "        \"\"\"Denominator of the `TransferFunction` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 638, "code": "    def _copy(self, system):\n        self.num = system.num\n        self.den = system.den", "documentation": "        \"\"\"\n        Copy the parameters of another `TransferFunction` object\n\n        Parameters\n        ----------\n        system : `TransferFunction`\n            The `StateSpace` system that is to be copied\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 651, "code": "    def to_tf(self):\n        return copy.deepcopy(self)", "documentation": "        \"\"\"\n        Return a copy of the current `TransferFunction` system.\n\n        Returns\n        -------\n        sys : instance of `TransferFunction`\n            The current system (copy)\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 663, "code": "    def to_zpk(self):\n        return ZerosPolesGain(*tf2zpk(self.num, self.den),\n                              **self._dt_dict)", "documentation": "        \"\"\"\n        Convert system representation to `ZerosPolesGain`.\n\n        Returns\n        -------\n        sys : instance of `ZerosPolesGain`\n            Zeros, poles, gain representation of the current system\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 676, "code": "    def to_ss(self):\n        return StateSpace(*tf2ss(self.num, self.den),\n                          **self._dt_dict)\n    @staticmethod", "documentation": "        \"\"\"\n        Convert system representation to `StateSpace`.\n\n        Returns\n        -------\n        sys : instance of `StateSpace`\n            State space model of the current system\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 690, "code": "    def _z_to_zinv(num, den):\n        diff = len(num) - len(den)\n        if diff > 0:\n            den = np.hstack((np.zeros(diff), den))\n        elif diff < 0:\n            num = np.hstack((np.zeros(-diff), num))\n        return num, den\n    @staticmethod", "documentation": "        \"\"\"Change a transfer function from the variable `z` to `z**-1`.\n\n        Parameters\n        ----------\n        num, den: 1d array_like\n            Sequences representing the coefficients of the numerator and\n            denominator polynomials, in order of descending degree of 'z'.\n            That is, ``5z**2 + 3z + 2`` is presented as ``[5, 3, 2]``.\n\n        Returns\n        -------\n        num, den: 1d array_like\n            Sequences representing the coefficients of the numerator and\n            denominator polynomials, in order of ascending degree of 'z**-1'.\n            That is, ``5 + 3 z**-1 + 2 z**-2`` is presented as ``[5, 3, 2]``.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 715, "code": "    def _zinv_to_z(num, den):\n        diff = len(num) - len(den)\n        if diff > 0:\n            den = np.hstack((den, np.zeros(diff)))\n        elif diff < 0:\n            num = np.hstack((num, np.zeros(-diff)))\n        return num, den", "documentation": "        \"\"\"Change a transfer function from the variable `z` to `z**-1`.\n\n        Parameters\n        ----------\n        num, den: 1d array_like\n            Sequences representing the coefficients of the numerator and\n            denominator polynomials, in order of ascending degree of 'z**-1'.\n            That is, ``5 + 3 z**-1 + 2 z**-2`` is presented as ``[5, 3, 2]``.\n\n        Returns\n        -------\n        num, den: 1d array_like\n            Sequences representing the coefficients of the numerator and\n            denominator polynomials, in order of descending degree of 'z'.\n            That is, ``5z**2 + 3z + 2`` is presented as ``[5, 3, 2]``.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 800, "code": "    def to_discrete(self, dt, method='zoh', alpha=None):\n        return TransferFunction(*cont2discrete((self.num, self.den),\n                                               dt,\n                                               method=method,\n                                               alpha=alpha)[:-1],\n                                dt=dt)", "documentation": "        \"\"\"\n        Returns the discretized `TransferFunction` system.\n\n        Parameters: See `cont2discrete` for details.\n\n        Returns\n        -------\n        sys: instance of `dlti` and `StateSpace`\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 951, "code": "    def __new__(cls, *system, **kwargs):\n        if len(system) == 1 and isinstance(system[0], LinearTimeInvariant):\n            return system[0].to_zpk()\n        if cls is ZerosPolesGain:\n            if kwargs.get('dt') is None:\n                return ZerosPolesGainContinuous.__new__(\n                    ZerosPolesGainContinuous,\n                    *system,\n                    **kwargs)\n            else:\n                return ZerosPolesGainDiscrete.__new__(", "documentation": "        \"\"\"Handle object conversion if input is an instance of `lti`\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 973, "code": "    def __init__(self, *system, **kwargs):\n        if isinstance(system[0], LinearTimeInvariant):\n            return\n        super().__init__(**kwargs)\n        self._zeros = None\n        self._poles = None\n        self._gain = None\n        self.zeros, self.poles, self.gain = system", "documentation": "        \"\"\"Initialize the zeros, poles, gain system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 987, "code": "    def __repr__(self):\n        return (\n            f'{self.__class__.__name__}(\\n'\n            f'{repr(self.zeros)},\\n'\n            f'{repr(self.poles)},\\n'\n            f'{repr(self.gain)},\\n'\n            f'dt: {repr(self.dt)}\\n)'\n        )\n    @property", "documentation": "        \"\"\"Return representation of the `ZerosPolesGain` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 998, "code": "    def zeros(self):\n        return self._zeros\n    @zeros.setter", "documentation": "        \"\"\"Zeros of the `ZerosPolesGain` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1014, "code": "    def poles(self):\n        return self._poles\n    @poles.setter", "documentation": "        \"\"\"Poles of the `ZerosPolesGain` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1023, "code": "    def gain(self):\n        return self._gain\n    @gain.setter", "documentation": "        \"\"\"Gain of the `ZerosPolesGain` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1031, "code": "    def _copy(self, system):\n        self.poles = system.poles\n        self.zeros = system.zeros\n        self.gain = system.gain", "documentation": "        \"\"\"\n        Copy the parameters of another `ZerosPolesGain` system.\n\n        Parameters\n        ----------\n        system : instance of `ZerosPolesGain`\n            The zeros, poles gain system that is to be copied\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1045, "code": "    def to_tf(self):\n        return TransferFunction(*zpk2tf(self.zeros, self.poles, self.gain),\n                                **self._dt_dict)", "documentation": "        \"\"\"\n        Convert system representation to `TransferFunction`.\n\n        Returns\n        -------\n        sys : instance of `TransferFunction`\n            Transfer function of the current system\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1058, "code": "    def to_zpk(self):\n        return copy.deepcopy(self)", "documentation": "        \"\"\"\n        Return a copy of the current 'ZerosPolesGain' system.\n\n        Returns\n        -------\n        sys : instance of `ZerosPolesGain`\n            The current system (copy)\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1070, "code": "    def to_ss(self):\n        return StateSpace(*zpk2ss(self.zeros, self.poles, self.gain),\n                          **self._dt_dict)", "documentation": "        \"\"\"\n        Convert system representation to `StateSpace`.\n\n        Returns\n        -------\n        sys : instance of `StateSpace`\n            State space model of the current system\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1136, "code": "    def to_discrete(self, dt, method='zoh', alpha=None):\n        return ZerosPolesGain(\n            *cont2discrete((self.zeros, self.poles, self.gain),\n                           dt,\n                           method=method,\n                           alpha=alpha)[:-1],\n            dt=dt)", "documentation": "        \"\"\"\n        Returns the discretized `ZerosPolesGain` system.\n\n        Parameters: See `cont2discrete` for details.\n\n        Returns\n        -------\n        sys: instance of `dlti` and `ZerosPolesGain`\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1325, "code": "    def __new__(cls, *system, **kwargs):\n        if len(system) == 1 and isinstance(system[0], LinearTimeInvariant):\n            return system[0].to_ss()\n        if cls is StateSpace:\n            if kwargs.get('dt') is None:\n                return StateSpaceContinuous.__new__(StateSpaceContinuous,\n                                                    *system, **kwargs)\n            else:\n                return StateSpaceDiscrete.__new__(StateSpaceDiscrete,\n                                                  *system, **kwargs)\n        return super().__new__(cls)", "documentation": "        \"\"\"Create new StateSpace object and settle inheritance.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1343, "code": "    def __init__(self, *system, **kwargs):\n        if isinstance(system[0], LinearTimeInvariant):\n            return\n        super().__init__(**kwargs)\n        self._A = None\n        self._B = None\n        self._C = None\n        self._D = None\n        self.A, self.B, self.C, self.D = abcd_normalize(*system)", "documentation": "        \"\"\"Initialize the state space lti/dlti system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1360, "code": "    def __repr__(self):\n        return (\n            f'{self.__class__.__name__}(\\n'\n            f'{repr(self.A)},\\n'\n            f'{repr(self.B)},\\n'\n            f'{repr(self.C)},\\n'\n            f'{repr(self.D)},\\n'\n            f'dt: {repr(self.dt)}\\n)'\n        )", "documentation": "        \"\"\"Return representation of the `StateSpace` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1375, "code": "    def __mul__(self, other):\n        if not self._check_binop_other(other):\n            return NotImplemented\n        if isinstance(other, StateSpace):\n            if type(other) is not type(self):\n                return NotImplemented\n            if self.dt != other.dt:\n                raise TypeError('Cannot multiply systems with different `dt`.')\n            n1 = self.A.shape[0]\n            n2 = other.A.shape[0]\n            a = np.vstack((np.hstack((self.A, np.dot(self.B, other.C))),", "documentation": "        \"\"\"\n        Post-multiply another system or a scalar\n\n        Handles multiplication of systems in the sense of a frequency domain\n        multiplication. That means, given two systems E1(s) and E2(s), their\n        multiplication, H(s) = E1(s) * E2(s), means that applying H(s) to U(s)\n        is equivalent to first applying E2(s), and then E1(s).\n\n        Notes\n        -----\n        For SISO systems the order of system application does not matter.\n        However, for MIMO systems, where the two systems are matrices, the\n        order above ensures standard Matrix multiplication rules apply.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1435, "code": "    def __rmul__(self, other):\n        if not self._check_binop_other(other) or isinstance(other, StateSpace):\n            return NotImplemented\n        a = self.A\n        b = self.B\n        c = np.dot(other, self.C)\n        d = np.dot(other, self.D)\n        common_dtype = np.result_type(a.dtype, b.dtype, c.dtype, d.dtype)\n        return StateSpace(np.asarray(a, dtype=common_dtype),\n                          np.asarray(b, dtype=common_dtype),\n                          np.asarray(c, dtype=common_dtype),", "documentation": "        \"\"\"Pre-multiply a scalar or matrix (but not StateSpace)\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1453, "code": "    def __neg__(self):\n        return StateSpace(self.A, self.B, -self.C, -self.D, **self._dt_dict)", "documentation": "        \"\"\"Negate the system (equivalent to pre-multiplying by -1).\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1457, "code": "    def __add__(self, other):\n        if not self._check_binop_other(other):\n            return NotImplemented\n        if isinstance(other, StateSpace):\n            if type(other) is not type(self):\n                raise TypeError(f'Cannot add {type(self)} and {type(other)}')\n            if self.dt != other.dt:\n                raise TypeError('Cannot add systems with different `dt`.')\n            a = linalg.block_diag(self.A, other.A)\n            b = np.vstack((self.B, other.B))\n            c = np.hstack((self.C, other.C))", "documentation": "        \"\"\"\n        Adds two systems in the sense of frequency domain addition.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1524, "code": "    def __truediv__(self, other):\n        if not self._check_binop_other(other) or isinstance(other, StateSpace):\n            return NotImplemented\n        if isinstance(other, np.ndarray) and other.ndim > 0:\n            raise ValueError(\"Cannot divide StateSpace by non-scalar numpy arrays\")\n        return self.__mul__(1/other)\n    @property", "documentation": "        \"\"\"\n        Divide by a scalar\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1539, "code": "    def A(self):\n        return self._A\n    @A.setter", "documentation": "        \"\"\"State matrix of the `StateSpace` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1548, "code": "    def B(self):\n        return self._B\n    @B.setter", "documentation": "        \"\"\"Input matrix of the `StateSpace` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1558, "code": "    def C(self):\n        return self._C\n    @C.setter", "documentation": "        \"\"\"Output matrix of the `StateSpace` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1568, "code": "    def D(self):\n        return self._D\n    @D.setter", "documentation": "        \"\"\"Feedthrough matrix of the `StateSpace` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1576, "code": "    def _copy(self, system):\n        self.A = system.A\n        self.B = system.B\n        self.C = system.C\n        self.D = system.D", "documentation": "        \"\"\"\n        Copy the parameters of another `StateSpace` system.\n\n        Parameters\n        ----------\n        system : instance of `StateSpace`\n            The state-space system that is to be copied\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1591, "code": "    def to_tf(self, **kwargs):\n        return TransferFunction(*ss2tf(self._A, self._B, self._C, self._D,\n                                       **kwargs), **self._dt_dict)", "documentation": "        \"\"\"\n        Convert system representation to `TransferFunction`.\n\n        Parameters\n        ----------\n        kwargs : dict, optional\n            Additional keywords passed to `ss2zpk`\n\n        Returns\n        -------\n        sys : instance of `TransferFunction`\n            Transfer function of the current system\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1609, "code": "    def to_zpk(self, **kwargs):\n        return ZerosPolesGain(*ss2zpk(self._A, self._B, self._C, self._D,\n                                      **kwargs), **self._dt_dict)", "documentation": "        \"\"\"\n        Convert system representation to `ZerosPolesGain`.\n\n        Parameters\n        ----------\n        kwargs : dict, optional\n            Additional keywords passed to `ss2zpk`\n\n        Returns\n        -------\n        sys : instance of `ZerosPolesGain`\n            Zeros, poles, gain representation of the current system\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1627, "code": "    def to_ss(self):\n        return copy.deepcopy(self)", "documentation": "        \"\"\"\n        Return a copy of the current `StateSpace` system.\n\n        Returns\n        -------\n        sys : instance of `StateSpace`\n            The current system (copy)\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1697, "code": "    def to_discrete(self, dt, method='zoh', alpha=None):\n        return StateSpace(*cont2discrete((self.A, self.B, self.C, self.D),\n                                         dt,\n                                         method=method,\n                                         alpha=alpha)[:-1],\n                          dt=dt)", "documentation": "        \"\"\"\n        Returns the discretized `StateSpace` system.\n\n        Parameters: See `cont2discrete` for details.\n\n        Returns\n        -------\n        sys: instance of `dlti` and `StateSpace`\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1776, "code": "def lsim(system, U, T, X0=None, interp=True):\n    if isinstance(system, lti):\n        sys = system._as_ss()\n    elif isinstance(system, dlti):\n        raise AttributeError('lsim can only be used with continuous-time '\n                             'systems.')\n    else:\n        sys = lti(*system)._as_ss()\n    T = atleast_1d(T)\n    if len(T.shape) != 1:\n        raise ValueError(\"T must be a rank-1 array.\")", "documentation": "    \"\"\"\n    Simulate output of a continuous-time linear system.\n\n    Parameters\n    ----------\n    system : an instance of the LTI class or a tuple describing the system.\n        The following gives the number of elements in the tuple and\n        the interpretation:\n\n        * 1: (instance of `lti`)\n        * 2: (num, den)\n        * 3: (zeros, poles, gain)\n        * 4: (A, B, C, D)\n\n    U : array_like\n        An input array describing the input at each time `T`\n        (interpolation is assumed between given times).  If there are\n        multiple inputs, then each column of the rank-2 array\n        represents an input.  If U = 0 or None, a zero input is used.\n    T : array_like\n        The time steps at which the input is defined and at which the\n        output is desired.  Must be nonnegative, increasing, and equally spaced.\n    X0 : array_like, optional\n        The initial conditions on the state vector (zero by default).\n    interp : bool, optional\n        Whether to use linear (True, the default) or zero-order-hold (False)\n        interpolation for the input array.\n\n    Returns\n    -------\n    T : 1D ndarray\n        Time values for the output.\n    yout : 1D ndarray\n        System response.\n    xout : ndarray\n        Time evolution of the state vector.\n\n    Notes\n    -----\n    If (num, den) is passed in for ``system``, coefficients for both the\n    numerator and denominator should be specified in descending exponent\n    order (e.g. ``s^2 + 3s + 5`` would be represented as ``[1, 3, 5]``).\n\n    Examples\n    --------\n    We'll use `lsim` to simulate an analog Bessel filter applied to\n    a signal.\n\n    >>> import numpy as np\n    >>> from scipy.signal import bessel, lsim\n    >>> import matplotlib.pyplot as plt\n\n    Create a low-pass Bessel filter with a cutoff of 12 Hz.\n\n    >>> b, a = bessel(N=5, Wn=2*np.pi*12, btype='lowpass', analog=True)\n\n    Generate data to which the filter is applied.\n\n    >>> t = np.linspace(0, 1.25, 500, endpoint=False)\n\n    The input signal is the sum of three sinusoidal curves, with\n    frequencies 4 Hz, 40 Hz, and 80 Hz.  The filter should mostly\n    eliminate the 40 Hz and 80 Hz components, leaving just the 4 Hz signal.\n\n    >>> u = (np.cos(2*np.pi*4*t) + 0.6*np.sin(2*np.pi*40*t) +\n    ...      0.5*np.cos(2*np.pi*80*t))\n\n    Simulate the filter with `lsim`.\n\n    >>> tout, yout, xout = lsim((b, a), U=u, T=t)\n\n    Plot the result.\n\n    >>> plt.plot(t, u, 'r', alpha=0.5, linewidth=1, label='input')\n    >>> plt.plot(tout, yout, 'k', linewidth=1.5, label='output')\n    >>> plt.legend(loc='best', shadow=True, framealpha=1)\n    >>> plt.grid(alpha=0.3)\n    >>> plt.xlabel('t')\n    >>> plt.show()\n\n    In a second example, we simulate a double integrator ``y'' = u``, with\n    a constant input ``u = 1``.  We'll use the state space representation\n    of the integrator.\n\n    >>> from scipy.signal import lti\n    >>> A = np.array([[0.0, 1.0], [0.0, 0.0]])\n    >>> B = np.array([[0.0], [1.0]])\n    >>> C = np.array([[1.0, 0.0]])\n    >>> D = 0.0\n    >>> system = lti(A, B, C, D)\n\n    `t` and `u` define the time and input signal for the system to\n    be simulated.\n\n    >>> t = np.linspace(0, 5, num=50)\n    >>> u = np.ones_like(t)\n\n    Compute the simulation, and then plot `y`.  As expected, the plot shows\n    the curve ``y = 0.5*t**2``.\n\n    >>> tout, y, x = lsim(system, u, t)\n    >>> plt.plot(t, y)\n    >>> plt.grid(alpha=0.3)\n    >>> plt.xlabel('t')\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1991, "code": "def _default_response_times(A, n):\n    vals = linalg.eigvals(A)\n    r = min(abs(real(vals)))\n    if r == 0.0:\n        r = 1.0\n    tc = 1.0 / r\n    t = linspace(0.0, 7 * tc, n)\n    return t", "documentation": "    \"\"\"Compute a reasonable set of time samples for the response time.\n\n    This function is used by `impulse` and `step`  to compute the response time\n    when the `T` argument to the function is None.\n\n    Parameters\n    ----------\n    A : array_like\n        The system matrix, which is square.\n    n : int\n        The number of time samples to generate.\n\n    Returns\n    -------\n    t : ndarray\n        The 1-D array of length `n` of time samples at which the response\n        is to be computed.\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2022, "code": "def impulse(system, X0=None, T=None, N=None):\n    if isinstance(system, lti):\n        sys = system._as_ss()\n    elif isinstance(system, dlti):\n        raise AttributeError('impulse can only be used with continuous-time '\n                             'systems.')\n    else:\n        sys = lti(*system)._as_ss()\n    if X0 is None:\n        X = squeeze(sys.B)\n    else:", "documentation": "    \"\"\"Impulse response of continuous-time system.\n\n    Parameters\n    ----------\n    system : an instance of the LTI class or a tuple of array_like\n        describing the system.\n        The following gives the number of elements in the tuple and\n        the interpretation:\n\n            * 1 (instance of `lti`)\n            * 2 (num, den)\n            * 3 (zeros, poles, gain)\n            * 4 (A, B, C, D)\n\n    X0 : array_like, optional\n        Initial state-vector.  Defaults to zero.\n    T : array_like, optional\n        Time points.  Computed if not given.\n    N : int, optional\n        The number of time points to compute (if `T` is not given).\n\n    Returns\n    -------\n    T : ndarray\n        A 1-D array of time points.\n    yout : ndarray\n        A 1-D array containing the impulse response of the system (except for\n        singularities at zero).\n\n    Notes\n    -----\n    If (num, den) is passed in for ``system``, coefficients for both the\n    numerator and denominator should be specified in descending exponent\n    order (e.g. ``s^2 + 3s + 5`` would be represented as ``[1, 3, 5]``).\n\n    Examples\n    --------\n    Compute the impulse response of a second order system with a repeated\n    root: ``x''(t) + 2*x'(t) + x(t) = u(t)``\n\n    >>> from scipy import signal\n    >>> system = ([1.0], [1.0, 2.0, 1.0])\n    >>> t, y = signal.impulse(system)\n    >>> import matplotlib.pyplot as plt\n    >>> plt.plot(t, y)\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2092, "code": "def step(system, X0=None, T=None, N=None):\n    if isinstance(system, lti):\n        sys = system._as_ss()\n    elif isinstance(system, dlti):\n        raise AttributeError('step can only be used with continuous-time '\n                             'systems.')\n    else:\n        sys = lti(*system)._as_ss()\n    if N is None:\n        N = 100\n    if T is None:", "documentation": "    \"\"\"Step response of continuous-time system.\n\n    Parameters\n    ----------\n    system : an instance of the LTI class or a tuple of array_like\n        describing the system.\n        The following gives the number of elements in the tuple and\n        the interpretation:\n\n            * 1 (instance of `lti`)\n            * 2 (num, den)\n            * 3 (zeros, poles, gain)\n            * 4 (A, B, C, D)\n\n    X0 : array_like, optional\n        Initial state-vector (default is zero).\n    T : array_like, optional\n        Time points (computed if not given).\n    N : int, optional\n        Number of time points to compute if `T` is not given.\n\n    Returns\n    -------\n    T : 1D ndarray\n        Output time points.\n    yout : 1D ndarray\n        Step response of system.\n\n\n    Notes\n    -----\n    If (num, den) is passed in for ``system``, coefficients for both the\n    numerator and denominator should be specified in descending exponent\n    order (e.g. ``s^2 + 3s + 5`` would be represented as ``[1, 3, 5]``).\n\n    Examples\n    --------\n    >>> from scipy import signal\n    >>> import matplotlib.pyplot as plt\n    >>> lti = signal.lti([1.0], [1.0, 1.0])\n    >>> t, y = signal.step(lti)\n    >>> plt.plot(t, y)\n    >>> plt.xlabel('Time [s]')\n    >>> plt.ylabel('Amplitude')\n    >>> plt.title('Step response for 1. Order Lowpass')\n    >>> plt.grid()\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2159, "code": "def bode(system, w=None, n=100):\n    w, y = freqresp(system, w=w, n=n)\n    mag = 20.0 * np.log10(abs(y))\n    phase = np.unwrap(np.arctan2(y.imag, y.real)) * 180.0 / np.pi\n    return w, mag, phase", "documentation": "    \"\"\"\n    Calculate Bode magnitude and phase data of a continuous-time system.\n\n    Parameters\n    ----------\n    system : an instance of the LTI class or a tuple describing the system.\n        The following gives the number of elements in the tuple and\n        the interpretation:\n\n            * 1 (instance of `lti`)\n            * 2 (num, den)\n            * 3 (zeros, poles, gain)\n            * 4 (A, B, C, D)\n\n    w : array_like, optional\n        Array of frequencies (in rad/s). Magnitude and phase data is calculated\n        for every value in this array. If not given a reasonable set will be\n        calculated.\n    n : int, optional\n        Number of frequency points to compute if `w` is not given. The `n`\n        frequencies are logarithmically spaced in an interval chosen to\n        include the influence of the poles and zeros of the system.\n\n    Returns\n    -------\n    w : 1D ndarray\n        Frequency array [rad/s]\n    mag : 1D ndarray\n        Magnitude array [dB]\n    phase : 1D ndarray\n        Phase array [deg]\n\n    Notes\n    -----\n    If (num, den) is passed in for ``system``, coefficients for both the\n    numerator and denominator should be specified in descending exponent\n    order (e.g. ``s^2 + 3s + 5`` would be represented as ``[1, 3, 5]``).\n\n    .. versionadded:: 0.11.0\n\n    Examples\n    --------\n    >>> from scipy import signal\n    >>> import matplotlib.pyplot as plt\n\n    >>> sys = signal.TransferFunction([1], [1, 1])\n    >>> w, mag, phase = signal.bode(sys)\n\n    >>> plt.figure()\n    >>> plt.semilogx(w, mag)    # Bode magnitude plot\n    >>> plt.figure()\n    >>> plt.semilogx(w, phase)  # Bode phase plot\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2315, "code": "def _valid_inputs(A, B, poles, method, rtol, maxiter):\n    poles = np.asarray(poles)\n    if poles.ndim > 1:\n        raise ValueError(\"Poles must be a 1D array like.\")\n    poles = _order_complex_poles(poles)\n    if A.ndim > 2:\n        raise ValueError(\"A must be a 2D array/matrix.\")\n    if B.ndim > 2:\n        raise ValueError(\"B must be a 2D array/matrix\")\n    if A.shape[0] != A.shape[1]:\n        raise ValueError(\"A must be square\")", "documentation": "    \"\"\"\n    Check the poles come in complex conjugate pairs\n    Check shapes of A, B and poles are compatible.\n    Check the method chosen is compatible with provided poles\n    Return update method to use and ordered poles\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2368, "code": "def _order_complex_poles(poles):\n    ordered_poles = np.sort(poles[np.isreal(poles)])\n    im_poles = []\n    for p in np.sort(poles[np.imag(poles) < 0]):\n        if np.conj(p) in poles:\n            im_poles.extend((p, np.conj(p)))\n    ordered_poles = np.hstack((ordered_poles, im_poles))\n    if poles.shape[0] != len(ordered_poles):\n        raise ValueError(\"Complex poles must come with their conjugates\")\n    return ordered_poles", "documentation": "    \"\"\"\n    Check we have complex conjugates pairs and reorder P according to YT, ie\n    real_poles, complex_i, conjugate complex_i, ....\n    The lexicographic sort on the complex poles is added to help the user to\n    compare sets of poles.\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2388, "code": "def _KNV0(B, ker_pole, transfer_matrix, j, poles):\n    transfer_matrix_not_j = np.delete(transfer_matrix, j, axis=1)\n    Q, R = s_qr(transfer_matrix_not_j, mode=\"full\")\n    mat_ker_pj = np.dot(ker_pole[j], ker_pole[j].T)\n    yj = np.dot(mat_ker_pj, Q[:, -1])\n    if not np.allclose(yj, 0):\n        xj = yj/np.linalg.norm(yj)\n        transfer_matrix[:, j] = xj", "documentation": "    \"\"\"\n    Algorithm \"KNV0\" Kautsky et Al. Robust pole\n    assignment in linear state feedback, Int journal of Control\n    1985, vol 41 p 1129->1155\n    https://la.epfl.ch/files/content/sites/la/files/\n        users/105941/public/KautskyNicholsDooren\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2435, "code": "def _YT_real(ker_pole, Q, transfer_matrix, i, j):\n    u = Q[:, -2, np.newaxis]\n    v = Q[:, -1, np.newaxis]\n    m = np.dot(np.dot(ker_pole[i].T, np.dot(u, v.T) -\n        np.dot(v, u.T)), ker_pole[j])\n    um, sm, vm = np.linalg.svd(m)\n    mu1, mu2 = um.T[:2, :, np.newaxis]\n    nu1, nu2 = vm[:2, :, np.newaxis]\n    transfer_matrix_j_mo_transfer_matrix_j = np.vstack((\n            transfer_matrix[:, i, np.newaxis],\n            transfer_matrix[:, j, np.newaxis]))", "documentation": "    \"\"\"\n    Applies algorithm from YT section 6.1 page 19 related to real pairs\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2500, "code": "def _YT_complex(ker_pole, Q, transfer_matrix, i, j):\n    ur = np.sqrt(2)*Q[:, -2, np.newaxis]\n    ui = np.sqrt(2)*Q[:, -1, np.newaxis]\n    u = ur + 1j*ui\n    ker_pole_ij = ker_pole[i]\n    m = np.dot(np.dot(np.conj(ker_pole_ij.T), np.dot(u, np.conj(u).T) -\n               np.dot(np.conj(u), u.T)), ker_pole_ij)\n    e_val, e_vec = np.linalg.eig(m)\n    e_val_idx = np.argsort(np.abs(e_val))\n    mu1 = e_vec[:, e_val_idx[-1], np.newaxis]\n    mu2 = e_vec[:, e_val_idx[-2], np.newaxis]", "documentation": "    \"\"\"\n    Applies algorithm from YT section 6.2 page 20 related to complex pairs\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2551, "code": "def _YT_loop(ker_pole, transfer_matrix, poles, B, maxiter, rtol):\n    nb_real = poles[np.isreal(poles)].shape[0]\n    hnb = nb_real // 2\n    if nb_real > 0:\n        update_order = [[nb_real], [1]]\n    else:\n        update_order = [[],[]]\n    r_comp = np.arange(nb_real+1, len(poles)+1, 2)\n    r_p = np.arange(1, hnb+nb_real % 2)\n    update_order[0].extend(2*r_p)\n    update_order[1].extend(2*r_p+1)", "documentation": "    \"\"\"\n    Algorithm \"YT\" Tits, Yang. Globally Convergent\n    Algorithms for Robust Pole Assignment by State Feedback\n    https://hdl.handle.net/1903/5598\n    The poles P have to be sorted accordingly to section 6.2 page 20\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2674, "code": "def _KNV0_loop(ker_pole, transfer_matrix, poles, B, maxiter, rtol):\n    stop = False\n    nb_try = 0\n    while nb_try < maxiter and not stop:\n        det_transfer_matrixb = np.abs(np.linalg.det(transfer_matrix))\n        for j in range(B.shape[0]):\n            _KNV0(B, ker_pole, transfer_matrix, j, poles)\n        det_transfer_matrix = np.max((np.sqrt(np.spacing(1)),\n                                  np.abs(np.linalg.det(transfer_matrix))))\n        cur_rtol = np.abs((det_transfer_matrix - det_transfer_matrixb) /\n                       det_transfer_matrix)", "documentation": "    \"\"\"\n    Loop over all poles one by one and apply KNV method 0 algorithm\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2700, "code": "def place_poles(A, B, poles, method=\"YT\", rtol=1e-3, maxiter=30):\n    update_loop, poles = _valid_inputs(A, B, poles, method, rtol, maxiter)\n    cur_rtol = 0\n    nb_iter = 0\n    u, z = s_qr(B, mode=\"full\")\n    rankB = np.linalg.matrix_rank(B)\n    u0 = u[:, :rankB]\n    u1 = u[:, rankB:]\n    z = z[:rankB, :]\n    if B.shape[0] == rankB:\n        diag_poles = np.zeros(A.shape)", "documentation": "    \"\"\"\n    Compute K such that eigenvalues (A - dot(B, K))=poles.\n\n    K is the gain matrix such as the plant described by the linear system\n    ``AX+BU`` will have its closed-loop poles, i.e the eigenvalues ``A - B*K``,\n    as close as possible to those asked for in poles.\n\n    SISO, MISO and MIMO systems are supported.\n\n    Parameters\n    ----------\n    A, B : ndarray\n        State-space representation of linear system ``AX + BU``.\n    poles : array_like\n        Desired real poles and/or complex conjugates poles.\n        Complex poles are only supported with ``method=\"YT\"`` (default).\n    method: {'YT', 'KNV0'}, optional\n        Which method to choose to find the gain matrix K. One of:\n\n            - 'YT': Yang Tits\n            - 'KNV0': Kautsky, Nichols, Van Dooren update method 0\n\n        See References and Notes for details on the algorithms.\n    rtol: float, optional\n        After each iteration the determinant of the eigenvectors of\n        ``A - B*K`` is compared to its previous value, when the relative\n        error between these two values becomes lower than `rtol` the algorithm\n        stops.  Default is 1e-3.\n    maxiter: int, optional\n        Maximum number of iterations to compute the gain matrix.\n        Default is 30.\n\n    Returns\n    -------\n    full_state_feedback : Bunch object\n        full_state_feedback is composed of:\n            gain_matrix : 1-D ndarray\n                The closed loop matrix K such as the eigenvalues of ``A-BK``\n                are as close as possible to the requested poles.\n            computed_poles : 1-D ndarray\n                The poles corresponding to ``A-BK`` sorted as first the real\n                poles in increasing order, then the complex conjugates in\n                lexicographic order.\n            requested_poles : 1-D ndarray\n                The poles the algorithm was asked to place sorted as above,\n                they may differ from what was achieved.\n            X : 2-D ndarray\n                The transfer matrix such as ``X * diag(poles) = (A - B*K)*X``\n                (see Notes)\n            rtol : float\n                The relative tolerance achieved on ``det(X)`` (see Notes).\n                `rtol` will be NaN if it is possible to solve the system\n                ``diag(poles) = (A - B*K)``, or 0 when the optimization\n                algorithms can't do anything i.e when ``B.shape[1] == 1``.\n            nb_iter : int\n                The number of iterations performed before converging.\n                `nb_iter` will be NaN if it is possible to solve the system\n                ``diag(poles) = (A - B*K)``, or 0 when the optimization\n                algorithms can't do anything i.e when ``B.shape[1] == 1``.\n\n    Notes\n    -----\n    The Tits and Yang (YT), [2]_ paper is an update of the original Kautsky et\n    al. (KNV) paper [1]_.  KNV relies on rank-1 updates to find the transfer\n    matrix X such that ``X * diag(poles) = (A - B*K)*X``, whereas YT uses\n    rank-2 updates. This yields on average more robust solutions (see [2]_\n    pp 21-22), furthermore the YT algorithm supports complex poles whereas KNV\n    does not in its original version.  Only update method 0 proposed by KNV has\n    been implemented here, hence the name ``'KNV0'``.\n\n    KNV extended to complex poles is used in Matlab's ``place`` function, YT is\n    distributed under a non-free licence by Slicot under the name ``robpole``.\n    It is unclear and undocumented how KNV0 has been extended to complex poles\n    (Tits and Yang claim on page 14 of their paper that their method can not be\n    used to extend KNV to complex poles), therefore only YT supports them in\n    this implementation.\n\n    As the solution to the problem of pole placement is not unique for MIMO\n    systems, both methods start with a tentative transfer matrix which is\n    altered in various way to increase its determinant.  Both methods have been\n    proven to converge to a stable solution, however depending on the way the\n    initial transfer matrix is chosen they will converge to different\n    solutions and therefore there is absolutely no guarantee that using\n    ``'KNV0'`` will yield results similar to Matlab's or any other\n    implementation of these algorithms.\n\n    Using the default method ``'YT'`` should be fine in most cases; ``'KNV0'``\n    is only provided because it is needed by ``'YT'`` in some specific cases.\n    Furthermore ``'YT'`` gives on average more robust results than ``'KNV0'``\n    when ``abs(det(X))`` is used as a robustness indicator.\n\n    [2]_ is available as a technical report on the following URL:\n    https://hdl.handle.net/1903/5598\n\n    References\n    ----------\n    .. [1] J. Kautsky, N.K. Nichols and P. van Dooren, \"Robust pole assignment\n           in linear state feedback\", International Journal of Control, Vol. 41\n           pp. 1129-1155, 1985.\n    .. [2] A.L. Tits and Y. Yang, \"Globally convergent algorithms for robust\n           pole assignment by state feedback\", IEEE Transactions on Automatic\n           Control, Vol. 41, pp. 1432-1452, 1996.\n\n    Examples\n    --------\n    A simple example demonstrating real pole placement using both KNV and YT\n    algorithms.  This is example number 1 from section 4 of the reference KNV\n    publication ([1]_):\n\n    >>> import numpy as np\n    >>> from scipy import signal\n    >>> import matplotlib.pyplot as plt\n\n    >>> A = np.array([[ 1.380,  -0.2077,  6.715, -5.676  ],\n    ...               [-0.5814, -4.290,   0,      0.6750 ],\n    ...               [ 1.067,   4.273,  -6.654,  5.893  ],\n    ...               [ 0.0480,  4.273,   1.343, -2.104  ]])\n    >>> B = np.array([[ 0,      5.679 ],\n    ...               [ 1.136,  1.136 ],\n    ...               [ 0,      0,    ],\n    ...               [-3.146,  0     ]])\n    >>> P = np.array([-0.2, -0.5, -5.0566, -8.6659])\n\n    Now compute K with KNV method 0, with the default YT method and with the YT\n    method while forcing 100 iterations of the algorithm and print some results\n    after each call.\n\n    >>> fsf1 = signal.place_poles(A, B, P, method='KNV0')\n    >>> fsf1.gain_matrix\n    array([[ 0.20071427, -0.96665799,  0.24066128, -0.10279785],\n           [ 0.50587268,  0.57779091,  0.51795763, -0.41991442]])\n\n    >>> fsf2 = signal.place_poles(A, B, P)  # uses YT method\n    >>> fsf2.computed_poles\n    array([-8.6659, -5.0566, -0.5   , -0.2   ])\n\n    >>> fsf3 = signal.place_poles(A, B, P, rtol=-1, maxiter=100)\n    >>> fsf3.X\n    array([[ 0.52072442+0.j, -0.08409372+0.j, -0.56847937+0.j,  0.74823657+0.j],\n           [-0.04977751+0.j, -0.80872954+0.j,  0.13566234+0.j, -0.29322906+0.j],\n           [-0.82266932+0.j, -0.19168026+0.j, -0.56348322+0.j, -0.43815060+0.j],\n           [ 0.22267347+0.j,  0.54967577+0.j, -0.58387806+0.j, -0.40271926+0.j]])\n\n    The absolute value of the determinant of X is a good indicator to check the\n    robustness of the results, both ``'KNV0'`` and ``'YT'`` aim at maximizing\n    it.  Below a comparison of the robustness of the results above:\n\n    >>> abs(np.linalg.det(fsf1.X)) < abs(np.linalg.det(fsf2.X))\n    True\n    >>> abs(np.linalg.det(fsf2.X)) < abs(np.linalg.det(fsf3.X))\n    True\n\n    Now a simple example for complex poles:\n\n    >>> A = np.array([[ 0,  7/3.,  0,   0   ],\n    ...               [ 0,   0,    0,  7/9. ],\n    ...               [ 0,   0,    0,   0   ],\n    ...               [ 0,   0,    0,   0   ]])\n    >>> B = np.array([[ 0,  0 ],\n    ...               [ 0,  0 ],\n    ...               [ 1,  0 ],\n    ...               [ 0,  1 ]])\n    >>> P = np.array([-3, -1, -2-1j, -2+1j]) / 3.\n    >>> fsf = signal.place_poles(A, B, P, method='YT')\n\n    We can plot the desired and computed poles in the complex plane:\n\n    >>> t = np.linspace(0, 2*np.pi, 401)\n    >>> plt.plot(np.cos(t), np.sin(t), 'k--')  # unit circle\n    >>> plt.plot(fsf.requested_poles.real, fsf.requested_poles.imag,\n    ...          'wo', label='Desired')\n    >>> plt.plot(fsf.computed_poles.real, fsf.computed_poles.imag, 'bx',\n    ...          label='Placed')\n    >>> plt.grid()\n    >>> plt.axis('image')\n    >>> plt.axis([-1.1, 1.1, -1.1, 1.1])\n    >>> plt.legend(bbox_to_anchor=(1.05, 1), loc=2, numpoints=1)\n\n    \"\"\""}], "after_segments": [{"filename": "scipy/signal/_ltisys.py", "start_line": 50, "code": "    def __new__(cls, *system, **kwargs):\n        if cls is LinearTimeInvariant:\n            raise NotImplementedError('The LinearTimeInvariant class is not '\n                                      'meant to be used directly, use `lti` '\n                                      'or `dlti` instead.')\n        return super().__new__(cls)", "documentation": "        \"\"\"Create a new object, don't allow direct instances.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 58, "code": "    def __init__(self):\n        super().__init__()\n        self.inputs = None\n        self.outputs = None\n        self._dt = None\n    @property", "documentation": "        \"\"\"\n        Initialize the `lti` baseclass.\n\n        The heavy lifting is done by the subclasses.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 71, "code": "    def dt(self):\n        return self._dt\n    @property", "documentation": "        \"\"\"Return the sampling time of the system, `None` for `lti` systems.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 83, "code": "    def zeros(self):\n        return self.to_zpk().zeros\n    @property", "documentation": "        \"\"\"Zeros of the system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 88, "code": "    def poles(self):\n        return self.to_zpk().poles", "documentation": "        \"\"\"Poles of the system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 92, "code": "    def _as_ss(self):\n        if isinstance(self, StateSpace):\n            return self\n        else:\n            return self.to_ss()", "documentation": "        \"\"\"Convert to `StateSpace` system, without copying.\n\n        Returns\n        -------\n        sys: StateSpace\n            The `StateSpace` system. If the class is already an instance of\n            `StateSpace` then this instance is returned.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 106, "code": "    def _as_zpk(self):\n        if isinstance(self, ZerosPolesGain):\n            return self\n        else:\n            return self.to_zpk()", "documentation": "        \"\"\"Convert to `ZerosPolesGain` system, without copying.\n\n        Returns\n        -------\n        sys: ZerosPolesGain\n            The `ZerosPolesGain` system. If the class is already an instance of\n            `ZerosPolesGain` then this instance is returned.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 120, "code": "    def _as_tf(self):\n        if isinstance(self, TransferFunction):\n            return self\n        else:\n            return self.to_tf()", "documentation": "        \"\"\"Convert to `TransferFunction` system, without copying.\n\n        Returns\n        -------\n        sys: ZerosPolesGain\n            The `TransferFunction` system. If the class is already an instance of\n            `TransferFunction` then this instance is returned.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 207, "code": "    def __new__(cls, *system):\n        if cls is lti:\n            N = len(system)\n            if N == 2:\n                return TransferFunctionContinuous.__new__(\n                    TransferFunctionContinuous, *system)\n            elif N == 3:\n                return ZerosPolesGainContinuous.__new__(\n                    ZerosPolesGainContinuous, *system)\n            elif N == 4:\n                return StateSpaceContinuous.__new__(StateSpaceContinuous,", "documentation": "        \"\"\"Create an instance of the appropriate subclass.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 226, "code": "    def __init__(self, *system):\n        super().__init__(*system)", "documentation": "        \"\"\"\n        Initialize the `lti` baseclass.\n\n        The heavy lifting is done by the subclasses.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 234, "code": "    def impulse(self, X0=None, T=None, N=None):\n        return impulse(self, X0=X0, T=T, N=N)", "documentation": "        \"\"\"\n        Return the impulse response of a continuous-time system.\n        See `impulse` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 241, "code": "    def step(self, X0=None, T=None, N=None):\n        return step(self, X0=X0, T=T, N=N)", "documentation": "        \"\"\"\n        Return the step response of a continuous-time system.\n        See `step` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 248, "code": "    def output(self, U, T, X0=None):\n        return lsim(self, U, T, X0=X0)", "documentation": "        \"\"\"\n        Return the response of a continuous-time system to input `U`.\n        See `lsim` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 255, "code": "    def bode(self, w=None, n=100):\n        return bode(self, w=w, n=n)", "documentation": "        \"\"\"\n        Calculate Bode magnitude and phase data of a continuous-time system.\n\n        Returns a 3-tuple containing arrays of frequencies [rad/s], magnitude\n        [dB] and phase [deg]. See `bode` for details.\n\n        Examples\n        --------\n        >>> from scipy import signal\n        >>> import matplotlib.pyplot as plt\n\n        >>> sys = signal.TransferFunction([1], [1, 1])\n        >>> w, mag, phase = sys.bode()\n\n        >>> plt.figure()\n        >>> plt.semilogx(w, mag)    # Bode magnitude plot\n        >>> plt.figure()\n        >>> plt.semilogx(w, phase)  # Bode phase plot\n        >>> plt.show()\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 279, "code": "    def freqresp(self, w=None, n=10000):\n        return freqresp(self, w=w, n=n)", "documentation": "        \"\"\"\n        Calculate the frequency response of a continuous-time system.\n\n        Returns a 2-tuple containing arrays of frequencies [rad/s] and\n        complex magnitude.\n        See `freqresp` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 289, "code": "    def to_discrete(self, dt, method='zoh', alpha=None):\n        raise NotImplementedError('to_discrete is not implemented for this '\n                                  'system class.')", "documentation": "        \"\"\"Return a discretized version of the current system.\n\n        Parameters: See `cont2discrete` for details.\n\n        Returns\n        -------\n        sys: instance of `dlti`\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 391, "code": "    def __new__(cls, *system, **kwargs):\n        if cls is dlti:\n            N = len(system)\n            if N == 2:\n                return TransferFunctionDiscrete.__new__(\n                    TransferFunctionDiscrete, *system, **kwargs)\n            elif N == 3:\n                return ZerosPolesGainDiscrete.__new__(ZerosPolesGainDiscrete,\n                                                      *system, **kwargs)\n            elif N == 4:\n                return StateSpaceDiscrete.__new__(StateSpaceDiscrete, *system,", "documentation": "        \"\"\"Create an instance of the appropriate subclass.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 410, "code": "    def __init__(self, *system, **kwargs):\n        dt = kwargs.pop('dt', True)\n        super().__init__(*system, **kwargs)\n        self.dt = dt\n    @property", "documentation": "        \"\"\"\n        Initialize the `lti` baseclass.\n\n        The heavy lifting is done by the subclasses.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 422, "code": "    def dt(self):\n        return self._dt\n    @dt.setter", "documentation": "        \"\"\"Return the sampling time of the system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 430, "code": "    def impulse(self, x0=None, t=None, n=None):\n        return dimpulse(self, x0=x0, t=t, n=n)", "documentation": "        \"\"\"\n        Return the impulse response of the discrete-time `dlti` system.\n        See `dimpulse` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 437, "code": "    def step(self, x0=None, t=None, n=None):\n        return dstep(self, x0=x0, t=t, n=n)", "documentation": "        \"\"\"\n        Return the step response of the discrete-time `dlti` system.\n        See `dstep` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 444, "code": "    def output(self, u, t, x0=None):\n        return dlsim(self, u, t, x0=x0)", "documentation": "        \"\"\"\n        Return the response of the discrete-time system to input `u`.\n        See `dlsim` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 481, "code": "    def freqresp(self, w=None, n=10000, whole=False):\n        return dfreqresp(self, w=w, n=n, whole=whole)", "documentation": "        \"\"\"\n        Calculate the frequency response of a discrete-time system.\n\n        Returns a 2-tuple containing arrays of frequencies [rad/s] and\n        complex magnitude.\n        See `dfreqresp` for details.\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 569, "code": "    def __new__(cls, *system, **kwargs):\n        if len(system) == 1 and isinstance(system[0], LinearTimeInvariant):\n            return system[0].to_tf()\n        if cls is TransferFunction:\n            if kwargs.get('dt') is None:\n                return TransferFunctionContinuous.__new__(\n                    TransferFunctionContinuous,\n                    *system,\n                    **kwargs)\n            else:\n                return TransferFunctionDiscrete.__new__(", "documentation": "        \"\"\"Handle object conversion if input is an instance of lti.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 590, "code": "    def __init__(self, *system, **kwargs):\n        if isinstance(system[0], LinearTimeInvariant):\n            return\n        super().__init__(**kwargs)\n        self._num = None\n        self._den = None\n        self.num, self.den = normalize(*system)", "documentation": "        \"\"\"Initialize the state space LTI system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 604, "code": "    def __repr__(self):\n        return (\n            f'{self.__class__.__name__}(\\n'\n            f'{repr(self.num)},\\n'\n            f'{repr(self.den)},\\n'\n            f'dt: {repr(self.dt)}\\n)'\n        )\n    @property", "documentation": "        \"\"\"Return representation of the system's transfer function\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 614, "code": "    def num(self):\n        return self._num\n    @num.setter", "documentation": "        \"\"\"Numerator of the `TransferFunction` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 630, "code": "    def den(self):\n        return self._den\n    @den.setter", "documentation": "        \"\"\"Denominator of the `TransferFunction` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 638, "code": "    def _copy(self, system):\n        self.num = system.num\n        self.den = system.den", "documentation": "        \"\"\"\n        Copy the parameters of another `TransferFunction` object\n\n        Parameters\n        ----------\n        system : `TransferFunction`\n            The `StateSpace` system that is to be copied\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 651, "code": "    def to_tf(self):\n        return copy.deepcopy(self)", "documentation": "        \"\"\"\n        Return a copy of the current `TransferFunction` system.\n\n        Returns\n        -------\n        sys : instance of `TransferFunction`\n            The current system (copy)\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 663, "code": "    def to_zpk(self):\n        return ZerosPolesGain(*tf2zpk(self.num, self.den),\n                              **self._dt_dict)", "documentation": "        \"\"\"\n        Convert system representation to `ZerosPolesGain`.\n\n        Returns\n        -------\n        sys : instance of `ZerosPolesGain`\n            Zeros, poles, gain representation of the current system\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 676, "code": "    def to_ss(self):\n        return StateSpace(*tf2ss(self.num, self.den),\n                          **self._dt_dict)\n    @staticmethod", "documentation": "        \"\"\"\n        Convert system representation to `StateSpace`.\n\n        Returns\n        -------\n        sys : instance of `StateSpace`\n            State space model of the current system\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 690, "code": "    def _z_to_zinv(num, den):\n        diff = len(num) - len(den)\n        if diff > 0:\n            den = np.hstack((np.zeros(diff), den))\n        elif diff < 0:\n            num = np.hstack((np.zeros(-diff), num))\n        return num, den\n    @staticmethod", "documentation": "        \"\"\"Change a transfer function from the variable `z` to `z**-1`.\n\n        Parameters\n        ----------\n        num, den: 1d array_like\n            Sequences representing the coefficients of the numerator and\n            denominator polynomials, in order of descending degree of 'z'.\n            That is, ``5z**2 + 3z + 2`` is presented as ``[5, 3, 2]``.\n\n        Returns\n        -------\n        num, den: 1d array_like\n            Sequences representing the coefficients of the numerator and\n            denominator polynomials, in order of ascending degree of 'z**-1'.\n            That is, ``5 + 3 z**-1 + 2 z**-2`` is presented as ``[5, 3, 2]``.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 715, "code": "    def _zinv_to_z(num, den):\n        diff = len(num) - len(den)\n        if diff > 0:\n            den = np.hstack((den, np.zeros(diff)))\n        elif diff < 0:\n            num = np.hstack((num, np.zeros(-diff)))\n        return num, den", "documentation": "        \"\"\"Change a transfer function from the variable `z` to `z**-1`.\n\n        Parameters\n        ----------\n        num, den: 1d array_like\n            Sequences representing the coefficients of the numerator and\n            denominator polynomials, in order of ascending degree of 'z**-1'.\n            That is, ``5 + 3 z**-1 + 2 z**-2`` is presented as ``[5, 3, 2]``.\n\n        Returns\n        -------\n        num, den: 1d array_like\n            Sequences representing the coefficients of the numerator and\n            denominator polynomials, in order of descending degree of 'z'.\n            That is, ``5z**2 + 3z + 2`` is presented as ``[5, 3, 2]``.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 800, "code": "    def to_discrete(self, dt, method='zoh', alpha=None):\n        return TransferFunction(*cont2discrete((self.num, self.den),\n                                               dt,\n                                               method=method,\n                                               alpha=alpha)[:-1],\n                                dt=dt)", "documentation": "        \"\"\"\n        Returns the discretized `TransferFunction` system.\n\n        Parameters: See `cont2discrete` for details.\n\n        Returns\n        -------\n        sys: instance of `dlti` and `StateSpace`\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 951, "code": "    def __new__(cls, *system, **kwargs):\n        if len(system) == 1 and isinstance(system[0], LinearTimeInvariant):\n            return system[0].to_zpk()\n        if cls is ZerosPolesGain:\n            if kwargs.get('dt') is None:\n                return ZerosPolesGainContinuous.__new__(\n                    ZerosPolesGainContinuous,\n                    *system,\n                    **kwargs)\n            else:\n                return ZerosPolesGainDiscrete.__new__(", "documentation": "        \"\"\"Handle object conversion if input is an instance of `lti`\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 973, "code": "    def __init__(self, *system, **kwargs):\n        if isinstance(system[0], LinearTimeInvariant):\n            return\n        super().__init__(**kwargs)\n        self._zeros = None\n        self._poles = None\n        self._gain = None\n        self.zeros, self.poles, self.gain = system", "documentation": "        \"\"\"Initialize the zeros, poles, gain system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 987, "code": "    def __repr__(self):\n        return (\n            f'{self.__class__.__name__}(\\n'\n            f'{repr(self.zeros)},\\n'\n            f'{repr(self.poles)},\\n'\n            f'{repr(self.gain)},\\n'\n            f'dt: {repr(self.dt)}\\n)'\n        )\n    @property", "documentation": "        \"\"\"Return representation of the `ZerosPolesGain` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 998, "code": "    def zeros(self):\n        return self._zeros\n    @zeros.setter", "documentation": "        \"\"\"Zeros of the `ZerosPolesGain` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1014, "code": "    def poles(self):\n        return self._poles\n    @poles.setter", "documentation": "        \"\"\"Poles of the `ZerosPolesGain` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1023, "code": "    def gain(self):\n        return self._gain\n    @gain.setter", "documentation": "        \"\"\"Gain of the `ZerosPolesGain` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1031, "code": "    def _copy(self, system):\n        self.poles = system.poles\n        self.zeros = system.zeros\n        self.gain = system.gain", "documentation": "        \"\"\"\n        Copy the parameters of another `ZerosPolesGain` system.\n\n        Parameters\n        ----------\n        system : instance of `ZerosPolesGain`\n            The zeros, poles gain system that is to be copied\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1045, "code": "    def to_tf(self):\n        return TransferFunction(*zpk2tf(self.zeros, self.poles, self.gain),\n                                **self._dt_dict)", "documentation": "        \"\"\"\n        Convert system representation to `TransferFunction`.\n\n        Returns\n        -------\n        sys : instance of `TransferFunction`\n            Transfer function of the current system\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1058, "code": "    def to_zpk(self):\n        return copy.deepcopy(self)", "documentation": "        \"\"\"\n        Return a copy of the current 'ZerosPolesGain' system.\n\n        Returns\n        -------\n        sys : instance of `ZerosPolesGain`\n            The current system (copy)\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1070, "code": "    def to_ss(self):\n        return StateSpace(*zpk2ss(self.zeros, self.poles, self.gain),\n                          **self._dt_dict)", "documentation": "        \"\"\"\n        Convert system representation to `StateSpace`.\n\n        Returns\n        -------\n        sys : instance of `StateSpace`\n            State space model of the current system\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1136, "code": "    def to_discrete(self, dt, method='zoh', alpha=None):\n        return ZerosPolesGain(\n            *cont2discrete((self.zeros, self.poles, self.gain),\n                           dt,\n                           method=method,\n                           alpha=alpha)[:-1],\n            dt=dt)", "documentation": "        \"\"\"\n        Returns the discretized `ZerosPolesGain` system.\n\n        Parameters: See `cont2discrete` for details.\n\n        Returns\n        -------\n        sys: instance of `dlti` and `ZerosPolesGain`\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1325, "code": "    def __new__(cls, *system, **kwargs):\n        if len(system) == 1 and isinstance(system[0], LinearTimeInvariant):\n            return system[0].to_ss()\n        if cls is StateSpace:\n            if kwargs.get('dt') is None:\n                return StateSpaceContinuous.__new__(StateSpaceContinuous,\n                                                    *system, **kwargs)\n            else:\n                return StateSpaceDiscrete.__new__(StateSpaceDiscrete,\n                                                  *system, **kwargs)\n        return super().__new__(cls)", "documentation": "        \"\"\"Create new StateSpace object and settle inheritance.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1343, "code": "    def __init__(self, *system, **kwargs):\n        if isinstance(system[0], LinearTimeInvariant):\n            return\n        super().__init__(**kwargs)\n        self._A = None\n        self._B = None\n        self._C = None\n        self._D = None\n        self.A, self.B, self.C, self.D = abcd_normalize(*system)", "documentation": "        \"\"\"Initialize the state space lti/dlti system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1360, "code": "    def __repr__(self):\n        return (\n            f'{self.__class__.__name__}(\\n'\n            f'{repr(self.A)},\\n'\n            f'{repr(self.B)},\\n'\n            f'{repr(self.C)},\\n'\n            f'{repr(self.D)},\\n'\n            f'dt: {repr(self.dt)}\\n)'\n        )", "documentation": "        \"\"\"Return representation of the `StateSpace` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1375, "code": "    def __mul__(self, other):\n        if not self._check_binop_other(other):\n            return NotImplemented\n        if isinstance(other, StateSpace):\n            if type(other) is not type(self):\n                return NotImplemented\n            if self.dt != other.dt:\n                raise TypeError('Cannot multiply systems with different `dt`.')\n            n1 = self.A.shape[0]\n            n2 = other.A.shape[0]\n            a = np.vstack((np.hstack((self.A, np.dot(self.B, other.C))),", "documentation": "        \"\"\"\n        Post-multiply another system or a scalar\n\n        Handles multiplication of systems in the sense of a frequency domain\n        multiplication. That means, given two systems E1(s) and E2(s), their\n        multiplication, H(s) = E1(s) * E2(s), means that applying H(s) to U(s)\n        is equivalent to first applying E2(s), and then E1(s).\n\n        Notes\n        -----\n        For SISO systems the order of system application does not matter.\n        However, for MIMO systems, where the two systems are matrices, the\n        order above ensures standard Matrix multiplication rules apply.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1435, "code": "    def __rmul__(self, other):\n        if not self._check_binop_other(other) or isinstance(other, StateSpace):\n            return NotImplemented\n        a = self.A\n        b = self.B\n        c = np.dot(other, self.C)\n        d = np.dot(other, self.D)\n        common_dtype = np.result_type(a.dtype, b.dtype, c.dtype, d.dtype)\n        return StateSpace(np.asarray(a, dtype=common_dtype),\n                          np.asarray(b, dtype=common_dtype),\n                          np.asarray(c, dtype=common_dtype),", "documentation": "        \"\"\"Pre-multiply a scalar or matrix (but not StateSpace)\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1453, "code": "    def __neg__(self):\n        return StateSpace(self.A, self.B, -self.C, -self.D, **self._dt_dict)", "documentation": "        \"\"\"Negate the system (equivalent to pre-multiplying by -1).\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1457, "code": "    def __add__(self, other):\n        if not self._check_binop_other(other):\n            return NotImplemented\n        if isinstance(other, StateSpace):\n            if type(other) is not type(self):\n                raise TypeError(f'Cannot add {type(self)} and {type(other)}')\n            if self.dt != other.dt:\n                raise TypeError('Cannot add systems with different `dt`.')\n            a = linalg.block_diag(self.A, other.A)\n            b = np.vstack((self.B, other.B))\n            c = np.hstack((self.C, other.C))", "documentation": "        \"\"\"\n        Adds two systems in the sense of frequency domain addition.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1524, "code": "    def __truediv__(self, other):\n        if not self._check_binop_other(other) or isinstance(other, StateSpace):\n            return NotImplemented\n        if isinstance(other, np.ndarray) and other.ndim > 0:\n            raise ValueError(\"Cannot divide StateSpace by non-scalar numpy arrays\")\n        return self.__mul__(1/other)\n    @property", "documentation": "        \"\"\"\n        Divide by a scalar\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1539, "code": "    def A(self):\n        return self._A\n    @A.setter", "documentation": "        \"\"\"State matrix of the `StateSpace` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1548, "code": "    def B(self):\n        return self._B\n    @B.setter", "documentation": "        \"\"\"Input matrix of the `StateSpace` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1558, "code": "    def C(self):\n        return self._C\n    @C.setter", "documentation": "        \"\"\"Output matrix of the `StateSpace` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1568, "code": "    def D(self):\n        return self._D\n    @D.setter", "documentation": "        \"\"\"Feedthrough matrix of the `StateSpace` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1576, "code": "    def _copy(self, system):\n        self.A = system.A\n        self.B = system.B\n        self.C = system.C\n        self.D = system.D", "documentation": "        \"\"\"\n        Copy the parameters of another `StateSpace` system.\n\n        Parameters\n        ----------\n        system : instance of `StateSpace`\n            The state-space system that is to be copied\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1591, "code": "    def to_tf(self, **kwargs):\n        return TransferFunction(*ss2tf(self._A, self._B, self._C, self._D,\n                                       **kwargs), **self._dt_dict)", "documentation": "        \"\"\"\n        Convert system representation to `TransferFunction`.\n\n        Parameters\n        ----------\n        kwargs : dict, optional\n            Additional keywords passed to `ss2zpk`\n\n        Returns\n        -------\n        sys : instance of `TransferFunction`\n            Transfer function of the current system\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1609, "code": "    def to_zpk(self, **kwargs):\n        return ZerosPolesGain(*ss2zpk(self._A, self._B, self._C, self._D,\n                                      **kwargs), **self._dt_dict)", "documentation": "        \"\"\"\n        Convert system representation to `ZerosPolesGain`.\n\n        Parameters\n        ----------\n        kwargs : dict, optional\n            Additional keywords passed to `ss2zpk`\n\n        Returns\n        -------\n        sys : instance of `ZerosPolesGain`\n            Zeros, poles, gain representation of the current system\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1627, "code": "    def to_ss(self):\n        return copy.deepcopy(self)", "documentation": "        \"\"\"\n        Return a copy of the current `StateSpace` system.\n\n        Returns\n        -------\n        sys : instance of `StateSpace`\n            The current system (copy)\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1697, "code": "    def to_discrete(self, dt, method='zoh', alpha=None):\n        return StateSpace(*cont2discrete((self.A, self.B, self.C, self.D),\n                                         dt,\n                                         method=method,\n                                         alpha=alpha)[:-1],\n                          dt=dt)", "documentation": "        \"\"\"\n        Returns the discretized `StateSpace` system.\n\n        Parameters: See `cont2discrete` for details.\n\n        Returns\n        -------\n        sys: instance of `dlti` and `StateSpace`\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1776, "code": "def lsim(system, U, T, X0=None, interp=True):\n    if isinstance(system, lti):\n        sys = system._as_ss()\n    elif isinstance(system, dlti):\n        raise AttributeError('lsim can only be used with continuous-time '\n                             'systems.')\n    else:\n        sys = lti(*system)._as_ss()\n    T = atleast_1d(T)\n    if len(T.shape) != 1:\n        raise ValueError(\"T must be a rank-1 array.\")", "documentation": "    \"\"\"\n    Simulate output of a continuous-time linear system.\n\n    Parameters\n    ----------\n    system : an instance of the LTI class or a tuple describing the system.\n        The following gives the number of elements in the tuple and\n        the interpretation:\n\n        * 1: (instance of `lti`)\n        * 2: (num, den)\n        * 3: (zeros, poles, gain)\n        * 4: (A, B, C, D)\n\n    U : array_like\n        An input array describing the input at each time `T`\n        (interpolation is assumed between given times).  If there are\n        multiple inputs, then each column of the rank-2 array\n        represents an input.  If U = 0 or None, a zero input is used.\n    T : array_like\n        The time steps at which the input is defined and at which the\n        output is desired.  Must be nonnegative, increasing, and equally spaced.\n    X0 : array_like, optional\n        The initial conditions on the state vector (zero by default).\n    interp : bool, optional\n        Whether to use linear (True, the default) or zero-order-hold (False)\n        interpolation for the input array.\n\n    Returns\n    -------\n    T : 1D ndarray\n        Time values for the output.\n    yout : 1D ndarray\n        System response.\n    xout : ndarray\n        Time evolution of the state vector.\n\n    Notes\n    -----\n    If (num, den) is passed in for ``system``, coefficients for both the\n    numerator and denominator should be specified in descending exponent\n    order (e.g. ``s^2 + 3s + 5`` would be represented as ``[1, 3, 5]``).\n\n    Examples\n    --------\n    We'll use `lsim` to simulate an analog Bessel filter applied to\n    a signal.\n\n    >>> import numpy as np\n    >>> from scipy.signal import bessel, lsim\n    >>> import matplotlib.pyplot as plt\n\n    Create a low-pass Bessel filter with a cutoff of 12 Hz.\n\n    >>> b, a = bessel(N=5, Wn=2*np.pi*12, btype='lowpass', analog=True)\n\n    Generate data to which the filter is applied.\n\n    >>> t = np.linspace(0, 1.25, 500, endpoint=False)\n\n    The input signal is the sum of three sinusoidal curves, with\n    frequencies 4 Hz, 40 Hz, and 80 Hz.  The filter should mostly\n    eliminate the 40 Hz and 80 Hz components, leaving just the 4 Hz signal.\n\n    >>> u = (np.cos(2*np.pi*4*t) + 0.6*np.sin(2*np.pi*40*t) +\n    ...      0.5*np.cos(2*np.pi*80*t))\n\n    Simulate the filter with `lsim`.\n\n    >>> tout, yout, xout = lsim((b, a), U=u, T=t)\n\n    Plot the result.\n\n    >>> plt.plot(t, u, 'r', alpha=0.5, linewidth=1, label='input')\n    >>> plt.plot(tout, yout, 'k', linewidth=1.5, label='output')\n    >>> plt.legend(loc='best', shadow=True, framealpha=1)\n    >>> plt.grid(alpha=0.3)\n    >>> plt.xlabel('t')\n    >>> plt.show()\n\n    In a second example, we simulate a double integrator ``y'' = u``, with\n    a constant input ``u = 1``.  We'll use the state space representation\n    of the integrator.\n\n    >>> from scipy.signal import lti\n    >>> A = np.array([[0.0, 1.0], [0.0, 0.0]])\n    >>> B = np.array([[0.0], [1.0]])\n    >>> C = np.array([[1.0, 0.0]])\n    >>> D = 0.0\n    >>> system = lti(A, B, C, D)\n\n    `t` and `u` define the time and input signal for the system to\n    be simulated.\n\n    >>> t = np.linspace(0, 5, num=50)\n    >>> u = np.ones_like(t)\n\n    Compute the simulation, and then plot `y`.  As expected, the plot shows\n    the curve ``y = 0.5*t**2``.\n\n    >>> tout, y, x = lsim(system, u, t)\n    >>> plt.plot(t, y)\n    >>> plt.grid(alpha=0.3)\n    >>> plt.xlabel('t')\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1991, "code": "def _default_response_times(A, n):\n    vals = linalg.eigvals(A)\n    r = min(abs(real(vals)))\n    if r == 0.0:\n        r = 1.0\n    tc = 1.0 / r\n    t = linspace(0.0, 7 * tc, n)\n    return t", "documentation": "    \"\"\"Compute a reasonable set of time samples for the response time.\n\n    This function is used by `impulse` and `step`  to compute the response time\n    when the `T` argument to the function is None.\n\n    Parameters\n    ----------\n    A : array_like\n        The system matrix, which is square.\n    n : int\n        The number of time samples to generate.\n\n    Returns\n    -------\n    t : ndarray\n        The 1-D array of length `n` of time samples at which the response\n        is to be computed.\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2022, "code": "def impulse(system, X0=None, T=None, N=None):\n    if isinstance(system, lti):\n        sys = system._as_ss()\n    elif isinstance(system, dlti):\n        raise AttributeError('impulse can only be used with continuous-time '\n                             'systems.')\n    else:\n        sys = lti(*system)._as_ss()\n    if X0 is None:\n        X = squeeze(sys.B)\n    else:", "documentation": "    \"\"\"Impulse response of continuous-time system.\n\n    Parameters\n    ----------\n    system : an instance of the LTI class or a tuple of array_like\n        describing the system.\n        The following gives the number of elements in the tuple and\n        the interpretation:\n\n            * 1 (instance of `lti`)\n            * 2 (num, den)\n            * 3 (zeros, poles, gain)\n            * 4 (A, B, C, D)\n\n    X0 : array_like, optional\n        Initial state-vector.  Defaults to zero.\n    T : array_like, optional\n        Time points.  Computed if not given.\n    N : int, optional\n        The number of time points to compute (if `T` is not given).\n\n    Returns\n    -------\n    T : ndarray\n        A 1-D array of time points.\n    yout : ndarray\n        A 1-D array containing the impulse response of the system (except for\n        singularities at zero).\n\n    Notes\n    -----\n    If (num, den) is passed in for ``system``, coefficients for both the\n    numerator and denominator should be specified in descending exponent\n    order (e.g. ``s^2 + 3s + 5`` would be represented as ``[1, 3, 5]``).\n\n    Examples\n    --------\n    Compute the impulse response of a second order system with a repeated\n    root: ``x''(t) + 2*x'(t) + x(t) = u(t)``\n\n    >>> from scipy import signal\n    >>> system = ([1.0], [1.0, 2.0, 1.0])\n    >>> t, y = signal.impulse(system)\n    >>> import matplotlib.pyplot as plt\n    >>> plt.plot(t, y)\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2092, "code": "def step(system, X0=None, T=None, N=None):\n    if isinstance(system, lti):\n        sys = system._as_ss()\n    elif isinstance(system, dlti):\n        raise AttributeError('step can only be used with continuous-time '\n                             'systems.')\n    else:\n        sys = lti(*system)._as_ss()\n    if N is None:\n        N = 100\n    if T is None:", "documentation": "    \"\"\"Step response of continuous-time system.\n\n    Parameters\n    ----------\n    system : an instance of the LTI class or a tuple of array_like\n        describing the system.\n        The following gives the number of elements in the tuple and\n        the interpretation:\n\n            * 1 (instance of `lti`)\n            * 2 (num, den)\n            * 3 (zeros, poles, gain)\n            * 4 (A, B, C, D)\n\n    X0 : array_like, optional\n        Initial state-vector (default is zero).\n    T : array_like, optional\n        Time points (computed if not given).\n    N : int, optional\n        Number of time points to compute if `T` is not given.\n\n    Returns\n    -------\n    T : 1D ndarray\n        Output time points.\n    yout : 1D ndarray\n        Step response of system.\n\n\n    Notes\n    -----\n    If (num, den) is passed in for ``system``, coefficients for both the\n    numerator and denominator should be specified in descending exponent\n    order (e.g. ``s^2 + 3s + 5`` would be represented as ``[1, 3, 5]``).\n\n    Examples\n    --------\n    >>> from scipy import signal\n    >>> import matplotlib.pyplot as plt\n    >>> lti = signal.lti([1.0], [1.0, 1.0])\n    >>> t, y = signal.step(lti)\n    >>> plt.plot(t, y)\n    >>> plt.xlabel('Time [s]')\n    >>> plt.ylabel('Amplitude')\n    >>> plt.title('Step response for 1. Order Lowpass')\n    >>> plt.grid()\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2159, "code": "def bode(system, w=None, n=100):\n    w, y = freqresp(system, w=w, n=n)\n    mag = 20.0 * np.log10(abs(y))\n    phase = np.unwrap(np.arctan2(y.imag, y.real)) * 180.0 / np.pi\n    return w, mag, phase", "documentation": "    \"\"\"\n    Calculate Bode magnitude and phase data of a continuous-time system.\n\n    Parameters\n    ----------\n    system : an instance of the LTI class or a tuple describing the system.\n        The following gives the number of elements in the tuple and\n        the interpretation:\n\n            * 1 (instance of `lti`)\n            * 2 (num, den)\n            * 3 (zeros, poles, gain)\n            * 4 (A, B, C, D)\n\n    w : array_like, optional\n        Array of frequencies (in rad/s). Magnitude and phase data is calculated\n        for every value in this array. If not given a reasonable set will be\n        calculated.\n    n : int, optional\n        Number of frequency points to compute if `w` is not given. The `n`\n        frequencies are logarithmically spaced in an interval chosen to\n        include the influence of the poles and zeros of the system.\n\n    Returns\n    -------\n    w : 1D ndarray\n        Frequency array [rad/s]\n    mag : 1D ndarray\n        Magnitude array [dB]\n    phase : 1D ndarray\n        Phase array [deg]\n\n    Notes\n    -----\n    If (num, den) is passed in for ``system``, coefficients for both the\n    numerator and denominator should be specified in descending exponent\n    order (e.g. ``s^2 + 3s + 5`` would be represented as ``[1, 3, 5]``).\n\n    .. versionadded:: 0.11.0\n\n    Examples\n    --------\n    >>> from scipy import signal\n    >>> import matplotlib.pyplot as plt\n\n    >>> sys = signal.TransferFunction([1], [1, 1])\n    >>> w, mag, phase = signal.bode(sys)\n\n    >>> plt.figure()\n    >>> plt.semilogx(w, mag)    # Bode magnitude plot\n    >>> plt.figure()\n    >>> plt.semilogx(w, phase)  # Bode phase plot\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2315, "code": "def _valid_inputs(A, B, poles, method, rtol, maxiter):\n    poles = np.asarray(poles)\n    if poles.ndim > 1:\n        raise ValueError(\"Poles must be a 1D array like.\")\n    poles = _order_complex_poles(poles)\n    if A.ndim > 2:\n        raise ValueError(\"A must be a 2D array/matrix.\")\n    if B.ndim > 2:\n        raise ValueError(\"B must be a 2D array/matrix\")\n    if A.shape[0] != A.shape[1]:\n        raise ValueError(\"A must be square\")", "documentation": "    \"\"\"\n    Check the poles come in complex conjugate pairs\n    Check shapes of A, B and poles are compatible.\n    Check the method chosen is compatible with provided poles\n    Return update method to use and ordered poles\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2368, "code": "def _order_complex_poles(poles):\n    ordered_poles = np.sort(poles[np.isreal(poles)])\n    im_poles = []\n    for p in np.sort(poles[np.imag(poles) < 0]):\n        if np.conj(p) in poles:\n            im_poles.extend((p, np.conj(p)))\n    ordered_poles = np.hstack((ordered_poles, im_poles))\n    if poles.shape[0] != len(ordered_poles):\n        raise ValueError(\"Complex poles must come with their conjugates\")\n    return ordered_poles", "documentation": "    \"\"\"\n    Check we have complex conjugates pairs and reorder P according to YT, ie\n    real_poles, complex_i, conjugate complex_i, ....\n    The lexicographic sort on the complex poles is added to help the user to\n    compare sets of poles.\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2388, "code": "def _KNV0(B, ker_pole, transfer_matrix, j, poles):\n    transfer_matrix_not_j = np.delete(transfer_matrix, j, axis=1)\n    Q, R = s_qr(transfer_matrix_not_j, mode=\"full\")\n    mat_ker_pj = np.dot(ker_pole[j], ker_pole[j].T)\n    yj = np.dot(mat_ker_pj, Q[:, -1])\n    if not np.allclose(yj, 0):\n        xj = yj/np.linalg.norm(yj)\n        transfer_matrix[:, j] = xj", "documentation": "    \"\"\"\n    Algorithm \"KNV0\" Kautsky et Al. Robust pole\n    assignment in linear state feedback, Int journal of Control\n    1985, vol 41 p 1129->1155\n    https://la.epfl.ch/files/content/sites/la/files/\n        users/105941/public/KautskyNicholsDooren\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2435, "code": "def _YT_real(ker_pole, Q, transfer_matrix, i, j):\n    u = Q[:, -2, np.newaxis]\n    v = Q[:, -1, np.newaxis]\n    m = np.dot(np.dot(ker_pole[i].T, np.dot(u, v.T) -\n        np.dot(v, u.T)), ker_pole[j])\n    um, sm, vm = np.linalg.svd(m)\n    mu1, mu2 = um.T[:2, :, np.newaxis]\n    nu1, nu2 = vm[:2, :, np.newaxis]\n    transfer_matrix_j_mo_transfer_matrix_j = np.vstack((\n            transfer_matrix[:, i, np.newaxis],\n            transfer_matrix[:, j, np.newaxis]))", "documentation": "    \"\"\"\n    Applies algorithm from YT section 6.1 page 19 related to real pairs\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2500, "code": "def _YT_complex(ker_pole, Q, transfer_matrix, i, j):\n    ur = np.sqrt(2)*Q[:, -2, np.newaxis]\n    ui = np.sqrt(2)*Q[:, -1, np.newaxis]\n    u = ur + 1j*ui\n    ker_pole_ij = ker_pole[i]\n    m = np.dot(np.dot(np.conj(ker_pole_ij.T), np.dot(u, np.conj(u).T) -\n               np.dot(np.conj(u), u.T)), ker_pole_ij)\n    e_val, e_vec = np.linalg.eig(m)\n    e_val_idx = np.argsort(np.abs(e_val))\n    mu1 = e_vec[:, e_val_idx[-1], np.newaxis]\n    mu2 = e_vec[:, e_val_idx[-2], np.newaxis]", "documentation": "    \"\"\"\n    Applies algorithm from YT section 6.2 page 20 related to complex pairs\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2551, "code": "def _YT_loop(ker_pole, transfer_matrix, poles, B, maxiter, rtol):\n    nb_real = poles[np.isreal(poles)].shape[0]\n    hnb = nb_real // 2\n    if nb_real > 0:\n        update_order = [[nb_real], [1]]\n    else:\n        update_order = [[],[]]\n    r_comp = np.arange(nb_real+1, len(poles)+1, 2)\n    r_p = np.arange(1, hnb+nb_real % 2)\n    update_order[0].extend(2*r_p)\n    update_order[1].extend(2*r_p+1)", "documentation": "    \"\"\"\n    Algorithm \"YT\" Tits, Yang. Globally Convergent\n    Algorithms for Robust Pole Assignment by State Feedback\n    https://hdl.handle.net/1903/5598\n    The poles P have to be sorted accordingly to section 6.2 page 20\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2674, "code": "def _KNV0_loop(ker_pole, transfer_matrix, poles, B, maxiter, rtol):\n    stop = False\n    nb_try = 0\n    while nb_try < maxiter and not stop:\n        det_transfer_matrixb = np.abs(np.linalg.det(transfer_matrix))\n        for j in range(B.shape[0]):\n            _KNV0(B, ker_pole, transfer_matrix, j, poles)\n        det_transfer_matrix = np.max((np.sqrt(np.spacing(1)),\n                                  np.abs(np.linalg.det(transfer_matrix))))\n        cur_rtol = np.abs((det_transfer_matrix - det_transfer_matrixb) /\n                       det_transfer_matrix)", "documentation": "    \"\"\"\n    Loop over all poles one by one and apply KNV method 0 algorithm\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2700, "code": "def place_poles(A, B, poles, method=\"YT\", rtol=1e-3, maxiter=30):\n    update_loop, poles = _valid_inputs(A, B, poles, method, rtol, maxiter)\n    cur_rtol = 0\n    nb_iter = 0\n    u, z = s_qr(B, mode=\"full\")\n    rankB = np.linalg.matrix_rank(B)\n    u0 = u[:, :rankB]\n    u1 = u[:, rankB:]\n    z = z[:rankB, :]\n    if B.shape[0] == rankB:\n        diag_poles = np.zeros(A.shape)", "documentation": "    \"\"\"\n    Compute K such that eigenvalues (A - dot(B, K))=poles.\n\n    K is the gain matrix such as the plant described by the linear system\n    ``AX+BU`` will have its closed-loop poles, i.e the eigenvalues ``A - B*K``,\n    as close as possible to those asked for in poles.\n\n    SISO, MISO and MIMO systems are supported.\n\n    Parameters\n    ----------\n    A, B : ndarray\n        State-space representation of linear system ``AX + BU``.\n    poles : array_like\n        Desired real poles and/or complex conjugates poles.\n        Complex poles are only supported with ``method=\"YT\"`` (default).\n    method: {'YT', 'KNV0'}, optional\n        Which method to choose to find the gain matrix K. One of:\n\n            - 'YT': Yang Tits\n            - 'KNV0': Kautsky, Nichols, Van Dooren update method 0\n\n        See References and Notes for details on the algorithms.\n    rtol: float, optional\n        After each iteration the determinant of the eigenvectors of\n        ``A - B*K`` is compared to its previous value, when the relative\n        error between these two values becomes lower than `rtol` the algorithm\n        stops.  Default is 1e-3.\n    maxiter: int, optional\n        Maximum number of iterations to compute the gain matrix.\n        Default is 30.\n\n    Returns\n    -------\n    full_state_feedback : Bunch object\n        full_state_feedback is composed of:\n            gain_matrix : 2-D ndarray\n                The closed loop matrix K such as the eigenvalues of ``A-BK``\n                are as close as possible to the requested poles.\n            computed_poles : 1-D ndarray\n                The poles corresponding to ``A-BK`` sorted as first the real\n                poles in increasing order, then the complex conjugates in\n                lexicographic order.\n            requested_poles : 1-D ndarray\n                The poles the algorithm was asked to place sorted as above,\n                they may differ from what was achieved.\n            X : 2-D ndarray\n                The transfer matrix such as ``X * diag(poles) = (A - B*K)*X``\n                (see Notes)\n            rtol : float\n                The relative tolerance achieved on ``det(X)`` (see Notes).\n                `rtol` will be NaN if it is possible to solve the system\n                ``diag(poles) = (A - B*K)``, or 0 when the optimization\n                algorithms can't do anything i.e when ``B.shape[1] == 1``.\n            nb_iter : int\n                The number of iterations performed before converging.\n                `nb_iter` will be NaN if it is possible to solve the system\n                ``diag(poles) = (A - B*K)``, or 0 when the optimization\n                algorithms can't do anything i.e when ``B.shape[1] == 1``.\n\n    Notes\n    -----\n    The Tits and Yang (YT), [2]_ paper is an update of the original Kautsky et\n    al. (KNV) paper [1]_.  KNV relies on rank-1 updates to find the transfer\n    matrix X such that ``X * diag(poles) = (A - B*K)*X``, whereas YT uses\n    rank-2 updates. This yields on average more robust solutions (see [2]_\n    pp 21-22), furthermore the YT algorithm supports complex poles whereas KNV\n    does not in its original version.  Only update method 0 proposed by KNV has\n    been implemented here, hence the name ``'KNV0'``.\n\n    KNV extended to complex poles is used in Matlab's ``place`` function, YT is\n    distributed under a non-free licence by Slicot under the name ``robpole``.\n    It is unclear and undocumented how KNV0 has been extended to complex poles\n    (Tits and Yang claim on page 14 of their paper that their method can not be\n    used to extend KNV to complex poles), therefore only YT supports them in\n    this implementation.\n\n    As the solution to the problem of pole placement is not unique for MIMO\n    systems, both methods start with a tentative transfer matrix which is\n    altered in various way to increase its determinant.  Both methods have been\n    proven to converge to a stable solution, however depending on the way the\n    initial transfer matrix is chosen they will converge to different\n    solutions and therefore there is absolutely no guarantee that using\n    ``'KNV0'`` will yield results similar to Matlab's or any other\n    implementation of these algorithms.\n\n    Using the default method ``'YT'`` should be fine in most cases; ``'KNV0'``\n    is only provided because it is needed by ``'YT'`` in some specific cases.\n    Furthermore ``'YT'`` gives on average more robust results than ``'KNV0'``\n    when ``abs(det(X))`` is used as a robustness indicator.\n\n    [2]_ is available as a technical report on the following URL:\n    https://hdl.handle.net/1903/5598\n\n    References\n    ----------\n    .. [1] J. Kautsky, N.K. Nichols and P. van Dooren, \"Robust pole assignment\n           in linear state feedback\", International Journal of Control, Vol. 41\n           pp. 1129-1155, 1985.\n    .. [2] A.L. Tits and Y. Yang, \"Globally convergent algorithms for robust\n           pole assignment by state feedback\", IEEE Transactions on Automatic\n           Control, Vol. 41, pp. 1432-1452, 1996.\n\n    Examples\n    --------\n    A simple example demonstrating real pole placement using both KNV and YT\n    algorithms.  This is example number 1 from section 4 of the reference KNV\n    publication ([1]_):\n\n    >>> import numpy as np\n    >>> from scipy import signal\n    >>> import matplotlib.pyplot as plt\n\n    >>> A = np.array([[ 1.380,  -0.2077,  6.715, -5.676  ],\n    ...               [-0.5814, -4.290,   0,      0.6750 ],\n    ...               [ 1.067,   4.273,  -6.654,  5.893  ],\n    ...               [ 0.0480,  4.273,   1.343, -2.104  ]])\n    >>> B = np.array([[ 0,      5.679 ],\n    ...               [ 1.136,  1.136 ],\n    ...               [ 0,      0,    ],\n    ...               [-3.146,  0     ]])\n    >>> P = np.array([-0.2, -0.5, -5.0566, -8.6659])\n\n    Now compute K with KNV method 0, with the default YT method and with the YT\n    method while forcing 100 iterations of the algorithm and print some results\n    after each call.\n\n    >>> fsf1 = signal.place_poles(A, B, P, method='KNV0')\n    >>> fsf1.gain_matrix\n    array([[ 0.20071427, -0.96665799,  0.24066128, -0.10279785],\n           [ 0.50587268,  0.57779091,  0.51795763, -0.41991442]])\n\n    >>> fsf2 = signal.place_poles(A, B, P)  # uses YT method\n    >>> fsf2.computed_poles\n    array([-8.6659, -5.0566, -0.5   , -0.2   ])\n\n    >>> fsf3 = signal.place_poles(A, B, P, rtol=-1, maxiter=100)\n    >>> fsf3.X\n    array([[ 0.52072442+0.j, -0.08409372+0.j, -0.56847937+0.j,  0.74823657+0.j],\n           [-0.04977751+0.j, -0.80872954+0.j,  0.13566234+0.j, -0.29322906+0.j],\n           [-0.82266932+0.j, -0.19168026+0.j, -0.56348322+0.j, -0.43815060+0.j],\n           [ 0.22267347+0.j,  0.54967577+0.j, -0.58387806+0.j, -0.40271926+0.j]])\n\n    The absolute value of the determinant of X is a good indicator to check the\n    robustness of the results, both ``'KNV0'`` and ``'YT'`` aim at maximizing\n    it.  Below a comparison of the robustness of the results above:\n\n    >>> abs(np.linalg.det(fsf1.X)) < abs(np.linalg.det(fsf2.X))\n    True\n    >>> abs(np.linalg.det(fsf2.X)) < abs(np.linalg.det(fsf3.X))\n    True\n\n    Now a simple example for complex poles:\n\n    >>> A = np.array([[ 0,  7/3.,  0,   0   ],\n    ...               [ 0,   0,    0,  7/9. ],\n    ...               [ 0,   0,    0,   0   ],\n    ...               [ 0,   0,    0,   0   ]])\n    >>> B = np.array([[ 0,  0 ],\n    ...               [ 0,  0 ],\n    ...               [ 1,  0 ],\n    ...               [ 0,  1 ]])\n    >>> P = np.array([-3, -1, -2-1j, -2+1j]) / 3.\n    >>> fsf = signal.place_poles(A, B, P, method='YT')\n\n    We can plot the desired and computed poles in the complex plane:\n\n    >>> t = np.linspace(0, 2*np.pi, 401)\n    >>> plt.plot(np.cos(t), np.sin(t), 'k--')  # unit circle\n    >>> plt.plot(fsf.requested_poles.real, fsf.requested_poles.imag,\n    ...          'wo', label='Desired')\n    >>> plt.plot(fsf.computed_poles.real, fsf.computed_poles.imag, 'bx',\n    ...          label='Placed')\n    >>> plt.grid()\n    >>> plt.axis('image')\n    >>> plt.axis([-1.1, 1.1, -1.1, 1.1])\n    >>> plt.legend(bbox_to_anchor=(1.05, 1), loc=2, numpoints=1)\n\n    \"\"\""}]}
{"repository": "scipy/scipy", "commit_sha": "328ad15aceb98de0f42a5f744195eebed34f601c", "commit_message": "MAINT: signal: make abcd_normalize respect input dtype and remove dtype kwarg (#24232)\n\n* MAINT: Make abcd_normalize respect input dtype with xp_promote\n\n* TST: Update tests in response to abcd_normalize change\n\n* MAINT: explicitly set dtype in xp.zeros\n\n* MAINT: Add dtypes test for abcd_normalize\n\n* TST: xfail some torch float32 tests\n\n* MAINT: Change abcd_normalize signature back in delegators\n\n* DOC: remove parts of abcd_normalize docstring involving dtype\n\n* TST: use xp_result_type in abcd_normalize dtype test\n\n* DOC: update docstrings on abcd_normalize dtype behavior\n\n* MAINT: add missing comma", "commit_date": "2026-01-05T18:27:34+00:00", "author": "Albert Steppi", "file": "scipy/signal/_delegators.py", "patch": "@@ -58,7 +58,7 @@ def _skip_if_poly1d(arg):\n \n ###################\n \n-def abcd_normalize_signature(A=None, B=None, C=None, D=None, *, dtype=None):\n+def abcd_normalize_signature(A=None, B=None, C=None, D=None):\n     return array_namespace(A, B, C, D)\n \n def argrelextrema_signature(data, *args, **kwds):", "before_segments": [{"filename": "scipy/signal/_delegators.py", "start_line": 33, "code": "def _skip_if_lti(arg):\n    if isinstance(arg, tuple):\n        return arg\n    else:\n        return (None,)", "documentation": "    \"\"\"Handle `system` arg overloads.\n\n    ATM, only pass tuples through. Consider updating when cupyx.lti class\n    is supported.\n    \"\"\""}, {"filename": "scipy/signal/_delegators.py", "start_line": 45, "code": "def _skip_if_str_or_tuple(window):\n    if isinstance(window, str) or isinstance(window, tuple) or callable(window):\n        return None\n    else:\n        return window", "documentation": "    \"\"\"Handle `window` being a str or a tuple or an array-like.\n    \"\"\""}], "after_segments": [{"filename": "scipy/signal/_delegators.py", "start_line": 33, "code": "def _skip_if_lti(arg):\n    if isinstance(arg, tuple):\n        return arg\n    else:\n        return (None,)", "documentation": "    \"\"\"Handle `system` arg overloads.\n\n    ATM, only pass tuples through. Consider updating when cupyx.lti class\n    is supported.\n    \"\"\""}, {"filename": "scipy/signal/_delegators.py", "start_line": 45, "code": "def _skip_if_str_or_tuple(window):\n    if isinstance(window, str) or isinstance(window, tuple) or callable(window):\n        return None\n    else:\n        return window", "documentation": "    \"\"\"Handle `window` being a str or a tuple or an array-like.\n    \"\"\""}]}
{"repository": "scipy/scipy", "commit_sha": "328ad15aceb98de0f42a5f744195eebed34f601c", "commit_message": "MAINT: signal: make abcd_normalize respect input dtype and remove dtype kwarg (#24232)\n\n* MAINT: Make abcd_normalize respect input dtype with xp_promote\n\n* TST: Update tests in response to abcd_normalize change\n\n* MAINT: explicitly set dtype in xp.zeros\n\n* MAINT: Add dtypes test for abcd_normalize\n\n* TST: xfail some torch float32 tests\n\n* MAINT: Change abcd_normalize signature back in delegators\n\n* DOC: remove parts of abcd_normalize docstring involving dtype\n\n* TST: use xp_result_type in abcd_normalize dtype test\n\n* DOC: update docstrings on abcd_normalize dtype behavior\n\n* MAINT: add missing comma", "commit_date": "2026-01-05T18:27:34+00:00", "author": "Albert Steppi", "file": "scipy/signal/_lti_conversion.py", "patch": "@@ -8,7 +8,8 @@\n                    asarray, zeros, array, outer)\n from scipy import linalg\n \n-from scipy._lib._array_api import array_namespace, xp_size\n+from scipy._lib._array_api import (array_namespace, xp_size, xp_promote,\n+                                   xp_result_type)\n import scipy._lib.array_api_extra as xpx\n from ._filter_design import tf2zpk, zpk2tf, normalize\n \n@@ -114,7 +115,7 @@ def tf2ss(num, den):\n     return A, B, C, D\n \n \n-def abcd_normalize(A=None, B=None, C=None, D=None, *, dtype=None):\n+def abcd_normalize(A=None, B=None, C=None, D=None):\n     r\"\"\"Check state-space matrices compatibility and ensure they are 2d arrays.\n \n     First, the input matrices are converted into two-dimensional arrays with\n@@ -134,20 +135,15 @@ def abcd_normalize(A=None, B=None, C=None, D=None, *, dtype=None):\n         Two-dimensional array of shape (q, n).\n     D : array_like, optional\n         Two-dimensional array of shape (q, p).\n-    dtype : dtype | None, optional\n-        Cast all matrices to the specified dtype. If set to ``None`` (default), their\n-        dtypes will be \"complex128\" if any of the matrices are complex-valued.\n-        Otherwise, they will be of the type \"float64\".\n-\n-        .. versionadded:: 1.18.0\n-\n-            With this new parameter, all return values have identical dtypes.\n-            In previous versions the dtype of the input was preserved.\n \n     Returns\n     -------\n     A, B, C, D : array\n         State-space matrices as two-dimensional arrays with identical dtype.\n+        The result dtype is determined based on the standard\n+        `dtype promotion rules <https://numpy.org/doc/2.3/reference/arrays.promotion.html>`_\n+        except for when the inputs are all of integer dtype, in which case the returned\n+        arrays will have the default floating point dtype of ``float64``.\n \n     Raises\n     ------\n@@ -193,20 +189,6 @@ def abcd_normalize(A=None, B=None, C=None, D=None, *, dtype=None):\n     >>> CC\n     array([[0., 0.]])\n \n-    The following snippet shows the effect of the `dtype` parameter:\n-\n-    >>> import numpy as np\n-    >>> from scipy.signal import abcd_normalize\n-    >>> A, D = [[1, 2], [3, 4]], 2.5\n-    ...\n-    >>> AA, BB, CC, DD = abcd_normalize(A=A, D=D)  # default type casting\n-    >>> print(f\" AA: {AA.dtype}, BB: {BB.dtype}\\n CC: {CC.dtype}, DD: {DD.dtype}\")\n-     AA: float64, BB: float64\n-     CC: float64, DD: float64\n-    >>> AA, BB, CC, DD = abcd_normalize(A=A, D=D, dtype=np.float32)  # Explicit dtype\n-    >>> print(f\" AA: {AA.dtype}, BB: {BB.dtype}\\n CC: {CC.dtype}, DD: {DD.dtype}\")\n-     AA: float32, BB: float32\n-     CC: float32, DD: float32\n     \"\"\"\n     if A is None and B is None and C is None:\n         raise ValueError(\"Dimension n is undefined for parameters A = B = C = None!\")\n@@ -216,34 +198,25 @@ def abcd_normalize(A=None, B=None, C=None, D=None, *, dtype=None):\n         raise ValueError(\"Dimension q is undefined for parameters C = D = None!\")\n \n     xp = array_namespace(A, B, C, D)\n+    A, B, C, D = xp_promote(A, B, C, D, xp=xp, force_floating=True)\n+    dtype = xp_result_type(A, B, C, D, xp=xp)\n \n     # convert inputs into 2d arrays (zero-size 2d array if None):\n-    A, B, C, D = (xpx.atleast_nd(xp.asarray(M_), ndim=2, xp=xp)\n-                  if M_ is not None else xp.zeros((0, 0)) for M_ in (A, B, C, D))\n-\n-    if dtype is None:\n-        to_comp = any(xp.isdtype(M_.dtype, 'complex floating') for M_ in (A, B, C, D))\n-        dtype = xp.complex128 if to_comp else xp.float64\n-    else:\n-        try:\n-            is_numeric = xp.isdtype(dtype, 'numeric')\n-        except Exception as dtype_error:\n-            err_msg = f\"Parameter {dtype=} must be None or a numeric dtype!\"\n-            raise ValueError(err_msg) from dtype_error\n-        if not is_numeric:\n-            raise ValueError(f\"Parameter {dtype=} is not a numeric dtype!\")\n+    A, B, C, D = (\n+        xpx.atleast_nd(xp.asarray(M_), ndim=2, xp=xp)\n+        if M_ is not None else xp.zeros((0, 0), dtype=dtype)\n+        for M_ in (A, B, C, D)\n+    )\n \n     n = A.shape[0] or B.shape[0] or C.shape[1] # try finding non-zero dimensions\n     p = B.shape[1] or D.shape[1]\n     q = C.shape[0] or D.shape[0]\n \n     # Create zero matrices as needed:\n-    A = xp.zeros((n, n)) if xp_size(A) == 0 else A\n-    B = xp.zeros((n, p)) if xp_size(B) == 0 else B\n-    C = xp.zeros((q, n)) if xp_size(C) == 0 else C\n-    D = xp.zeros((q, p)) if xp_size(D) == 0 else D\n-\n-    A, B, C, D = (xp.astype(M_, dtype, copy=False) for M_ in (A, B, C, D))\n+    A = xp.zeros((n, n), dtype=dtype) if xp_size(A) == 0 else A\n+    B = xp.zeros((n, p), dtype=dtype) if xp_size(B) == 0 else B\n+    C = xp.zeros((q, n), dtype=dtype) if xp_size(C) == 0 else C\n+    D = xp.zeros((q, p), dtype=dtype) if xp_size(D) == 0 else D\n \n     if A.shape != (n, n):\n         raise ValueError(f\"Parameter A has shape {A.shape} but should be ({n}, {n})!\")\n@@ -289,9 +262,10 @@ def ss2tf(A, B, C, D, input=0):\n     Notes\n     -----\n     Before calculating `num` and `den`, the function `abcd_normalize` is called to\n-    convert the parameters `A`, `B`, `C`, `D` into two-dimesional arrays. Their dtypes\n-    will be \"complex128\" if any of the matrices are complex-valued. Otherwise, they\n-    will be of type \"float64\".\n+    convert the parameters `A`, `B`, `C`, `D` into two-dimesional arrays of the\n+    same dtype. The resulting dtype will be based on NumPy's dtype promotion rules,\n+    except in the case where each of `A`, `B`, `C`, and `D` has integer dtype, in which\n+    case the resulting dtype will be the default floating point dtype of ``float64``.\n \n     The :ref:`tutorial_signal_state_space_representation` section of the\n     :ref:`user_guide` presents the corresponding definitions of continuous-time and", "before_segments": [{"filename": "scipy/signal/_lti_conversion.py", "start_line": 359, "code": "def zpk2ss(z, p, k):\n    return tf2ss(*zpk2tf(z, p, k))", "documentation": "    \"\"\"Zero-pole-gain representation to state-space representation\n\n    Parameters\n    ----------\n    z, p : sequence\n        Zeros and poles.\n    k : float\n        System gain.\n\n    Returns\n    -------\n    A, B, C, D : ndarray\n        State space representation of the system, in controller canonical\n        form.\n\n    \"\"\""}, {"filename": "scipy/signal/_lti_conversion.py", "start_line": 379, "code": "def ss2zpk(A, B, C, D, input=0):\n    return tf2zpk(*ss2tf(A, B, C, D, input=input))", "documentation": "    \"\"\"State-space representation to zero-pole-gain representation.\n\n    A, B, C, D defines a linear state-space system with `p` inputs,\n    `q` outputs, and `n` state variables.\n\n    Parameters\n    ----------\n    A : array_like\n        State (or system) matrix of shape ``(n, n)``\n    B : array_like\n        Input matrix of shape ``(n, p)``\n    C : array_like\n        Output matrix of shape ``(q, n)``\n    D : array_like\n        Feedthrough (or feedforward) matrix of shape ``(q, p)``\n    input : int, optional\n        For multiple-input systems, the index of the input to use.\n\n    Returns\n    -------\n    z, p : sequence\n        Zeros and poles.\n    k : float\n        System gain.\n\n    \"\"\""}, {"filename": "scipy/signal/_lti_conversion.py", "start_line": 409, "code": "def cont2discrete(system, dt, method=\"zoh\", alpha=None):\n    if hasattr(system, 'to_discrete') and callable(system.to_discrete):\n        return system.to_discrete(dt=dt, method=method, alpha=alpha)\n    if len(system) == 2:\n        sysd = cont2discrete(tf2ss(system[0], system[1]), dt, method=method,\n                             alpha=alpha)\n        return ss2tf(sysd[0], sysd[1], sysd[2], sysd[3]) + (dt,)\n    elif len(system) == 3:\n        sysd = cont2discrete(zpk2ss(system[0], system[1], system[2]), dt,\n                             method=method, alpha=alpha)\n        return ss2zpk(sysd[0], sysd[1], sysd[2], sysd[3]) + (dt,)", "documentation": "    \"\"\"\n    Transform a continuous to a discrete state-space system.\n\n    Parameters\n    ----------\n    system : a tuple describing the system or an instance of `lti`\n        The following gives the number of elements in the tuple and\n        the interpretation:\n\n            * 1: (instance of `lti`)\n            * 2: (num, den)\n            * 3: (zeros, poles, gain)\n            * 4: (A, B, C, D)\n\n    dt : float\n        The discretization time step.\n    method : str, optional\n        Which method to use:\n\n            * gbt: generalized bilinear transformation\n            * bilinear: Tustin's approximation (\"gbt\" with alpha=0.5)\n            * euler: Euler (or forward differencing) method (\"gbt\" with alpha=0)\n            * backward_diff: Backwards differencing (\"gbt\" with alpha=1.0)\n            * zoh: zero-order hold (default)\n            * foh: first-order hold (*versionadded: 1.3.0*)\n            * impulse: equivalent impulse response (*versionadded: 1.3.0*)\n\n    alpha : float within [0, 1], optional\n        The generalized bilinear transformation weighting parameter, which\n        should only be specified with method=\"gbt\", and is ignored otherwise\n\n    Returns\n    -------\n    sysd : tuple containing the discrete system\n        Based on the input type, the output will be of the form\n\n        * (num, den, dt)   for transfer function input\n        * (zeros, poles, gain, dt)   for zeros-poles-gain input\n        * (A, B, C, D, dt) for state-space system input\n\n    Notes\n    -----\n    By default, the routine uses a Zero-Order Hold (zoh) method to perform\n    the transformation. Alternatively, a generalized bilinear transformation\n    may be used, which includes the common Tustin's bilinear approximation,\n    an Euler's method technique, or a backwards differencing technique.\n\n    The Zero-Order Hold (zoh) method is based on [1]_, the generalized bilinear\n    approximation is based on [2]_ and [3]_, the First-Order Hold (foh) method\n    is based on [4]_.\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/Discretization#Discretization_of_linear_state_space_models\n\n    .. [2] http://techteach.no/publications/discretetime_signals_systems/discrete.pdf\n\n    .. [3] G. Zhang, X. Chen, and T. Chen, Digital redesign via the generalized\n        bilinear transformation, Int. J. Control, vol. 82, no. 4, pp. 741-754,\n        2009.\n        (https://www.mypolyuweb.hk/~magzhang/Research/ZCC09_IJC.pdf)\n\n    .. [4] G. F. Franklin, J. D. Powell, and M. L. Workman, Digital control\n        of dynamic systems, 3rd ed. Menlo Park, Calif: Addison-Wesley,\n        pp. 204-206, 1998.\n\n    Examples\n    --------\n    We can transform a continuous state-space system to a discrete one:\n\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy.signal import cont2discrete, lti, dlti, dstep\n\n    Define a continuous state-space system.\n\n    >>> A = np.array([[0, 1],[-10., -3]])\n    >>> B = np.array([[0],[10.]])\n    >>> C = np.array([[1., 0]])\n    >>> D = np.array([[0.]])\n    >>> l_system = lti(A, B, C, D)\n    >>> t, x = l_system.step(T=np.linspace(0, 5, 100))\n    >>> fig, ax = plt.subplots()\n    >>> ax.plot(t, x, label='Continuous', linewidth=3)\n\n    Transform it to a discrete state-space system using several methods.\n\n    >>> dt = 0.1\n    >>> for method in ['zoh', 'bilinear', 'euler', 'backward_diff', 'foh', 'impulse']:\n    ...    d_system = cont2discrete((A, B, C, D), dt, method=method)\n    ...    s, x_d = dstep(d_system)\n    ...    ax.step(s, np.squeeze(x_d), label=method, where='post')\n    >>> ax.axis([t[0], t[-1], x[0], 1.4])\n    >>> ax.legend(loc='best')\n    >>> fig.tight_layout()\n    >>> plt.show()\n\n    \"\"\""}], "after_segments": [{"filename": "scipy/signal/_lti_conversion.py", "start_line": 333, "code": "def zpk2ss(z, p, k):\n    return tf2ss(*zpk2tf(z, p, k))", "documentation": "    \"\"\"Zero-pole-gain representation to state-space representation\n\n    Parameters\n    ----------\n    z, p : sequence\n        Zeros and poles.\n    k : float\n        System gain.\n\n    Returns\n    -------\n    A, B, C, D : ndarray\n        State space representation of the system, in controller canonical\n        form.\n\n    \"\"\""}, {"filename": "scipy/signal/_lti_conversion.py", "start_line": 353, "code": "def ss2zpk(A, B, C, D, input=0):\n    return tf2zpk(*ss2tf(A, B, C, D, input=input))", "documentation": "    \"\"\"State-space representation to zero-pole-gain representation.\n\n    A, B, C, D defines a linear state-space system with `p` inputs,\n    `q` outputs, and `n` state variables.\n\n    Parameters\n    ----------\n    A : array_like\n        State (or system) matrix of shape ``(n, n)``\n    B : array_like\n        Input matrix of shape ``(n, p)``\n    C : array_like\n        Output matrix of shape ``(q, n)``\n    D : array_like\n        Feedthrough (or feedforward) matrix of shape ``(q, p)``\n    input : int, optional\n        For multiple-input systems, the index of the input to use.\n\n    Returns\n    -------\n    z, p : sequence\n        Zeros and poles.\n    k : float\n        System gain.\n\n    \"\"\""}, {"filename": "scipy/signal/_lti_conversion.py", "start_line": 383, "code": "def cont2discrete(system, dt, method=\"zoh\", alpha=None):\n    if hasattr(system, 'to_discrete') and callable(system.to_discrete):\n        return system.to_discrete(dt=dt, method=method, alpha=alpha)\n    if len(system) == 2:\n        sysd = cont2discrete(tf2ss(system[0], system[1]), dt, method=method,\n                             alpha=alpha)\n        return ss2tf(sysd[0], sysd[1], sysd[2], sysd[3]) + (dt,)\n    elif len(system) == 3:\n        sysd = cont2discrete(zpk2ss(system[0], system[1], system[2]), dt,\n                             method=method, alpha=alpha)\n        return ss2zpk(sysd[0], sysd[1], sysd[2], sysd[3]) + (dt,)", "documentation": "    \"\"\"\n    Transform a continuous to a discrete state-space system.\n\n    Parameters\n    ----------\n    system : a tuple describing the system or an instance of `lti`\n        The following gives the number of elements in the tuple and\n        the interpretation:\n\n            * 1: (instance of `lti`)\n            * 2: (num, den)\n            * 3: (zeros, poles, gain)\n            * 4: (A, B, C, D)\n\n    dt : float\n        The discretization time step.\n    method : str, optional\n        Which method to use:\n\n            * gbt: generalized bilinear transformation\n            * bilinear: Tustin's approximation (\"gbt\" with alpha=0.5)\n            * euler: Euler (or forward differencing) method (\"gbt\" with alpha=0)\n            * backward_diff: Backwards differencing (\"gbt\" with alpha=1.0)\n            * zoh: zero-order hold (default)\n            * foh: first-order hold (*versionadded: 1.3.0*)\n            * impulse: equivalent impulse response (*versionadded: 1.3.0*)\n\n    alpha : float within [0, 1], optional\n        The generalized bilinear transformation weighting parameter, which\n        should only be specified with method=\"gbt\", and is ignored otherwise\n\n    Returns\n    -------\n    sysd : tuple containing the discrete system\n        Based on the input type, the output will be of the form\n\n        * (num, den, dt)   for transfer function input\n        * (zeros, poles, gain, dt)   for zeros-poles-gain input\n        * (A, B, C, D, dt) for state-space system input\n\n    Notes\n    -----\n    By default, the routine uses a Zero-Order Hold (zoh) method to perform\n    the transformation. Alternatively, a generalized bilinear transformation\n    may be used, which includes the common Tustin's bilinear approximation,\n    an Euler's method technique, or a backwards differencing technique.\n\n    The Zero-Order Hold (zoh) method is based on [1]_, the generalized bilinear\n    approximation is based on [2]_ and [3]_, the First-Order Hold (foh) method\n    is based on [4]_.\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/Discretization#Discretization_of_linear_state_space_models\n\n    .. [2] http://techteach.no/publications/discretetime_signals_systems/discrete.pdf\n\n    .. [3] G. Zhang, X. Chen, and T. Chen, Digital redesign via the generalized\n        bilinear transformation, Int. J. Control, vol. 82, no. 4, pp. 741-754,\n        2009.\n        (https://www.mypolyuweb.hk/~magzhang/Research/ZCC09_IJC.pdf)\n\n    .. [4] G. F. Franklin, J. D. Powell, and M. L. Workman, Digital control\n        of dynamic systems, 3rd ed. Menlo Park, Calif: Addison-Wesley,\n        pp. 204-206, 1998.\n\n    Examples\n    --------\n    We can transform a continuous state-space system to a discrete one:\n\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy.signal import cont2discrete, lti, dlti, dstep\n\n    Define a continuous state-space system.\n\n    >>> A = np.array([[0, 1],[-10., -3]])\n    >>> B = np.array([[0],[10.]])\n    >>> C = np.array([[1., 0]])\n    >>> D = np.array([[0.]])\n    >>> l_system = lti(A, B, C, D)\n    >>> t, x = l_system.step(T=np.linspace(0, 5, 100))\n    >>> fig, ax = plt.subplots()\n    >>> ax.plot(t, x, label='Continuous', linewidth=3)\n\n    Transform it to a discrete state-space system using several methods.\n\n    >>> dt = 0.1\n    >>> for method in ['zoh', 'bilinear', 'euler', 'backward_diff', 'foh', 'impulse']:\n    ...    d_system = cont2discrete((A, B, C, D), dt, method=method)\n    ...    s, x_d = dstep(d_system)\n    ...    ax.step(s, np.squeeze(x_d), label=method, where='post')\n    >>> ax.axis([t[0], t[-1], x[0], 1.4])\n    >>> ax.legend(loc='best')\n    >>> fig.tight_layout()\n    >>> plt.show()\n\n    \"\"\""}]}
{"repository": "scipy/scipy", "commit_sha": "328ad15aceb98de0f42a5f744195eebed34f601c", "commit_message": "MAINT: signal: make abcd_normalize respect input dtype and remove dtype kwarg (#24232)\n\n* MAINT: Make abcd_normalize respect input dtype with xp_promote\n\n* TST: Update tests in response to abcd_normalize change\n\n* MAINT: explicitly set dtype in xp.zeros\n\n* MAINT: Add dtypes test for abcd_normalize\n\n* TST: xfail some torch float32 tests\n\n* MAINT: Change abcd_normalize signature back in delegators\n\n* DOC: remove parts of abcd_normalize docstring involving dtype\n\n* TST: use xp_result_type in abcd_normalize dtype test\n\n* DOC: update docstrings on abcd_normalize dtype behavior\n\n* MAINT: add missing comma", "commit_date": "2026-01-05T18:27:34+00:00", "author": "Albert Steppi", "file": "scipy/signal/_ltisys.py", "patch": "@@ -1255,9 +1255,11 @@ class StateSpace(LinearTimeInvariant):\n     Notes\n     -----\n     If the parameter `system` is a tuple (A, B, C, D) with four state space matrices,\n-    then those matrices are converted into two-dimensional arrays by calling\n-    `abcd_normalize`. Their dtypes will be \"complex128\" if any of the matrices are\n-    complex-valued. Otherwise, they will be of type \"float64\".\n+    then those matrices are converted into two-dimensional arrays of the same dtype\n+    by calling `abcd_normalize`. The resulting dtype will be based on NumPy's dtype\n+    promotion rules, except in the case where each of `A`, `B`, `C`, and `D` has\n+    integer dtype, in which case the resulting dtype will be the default floating point\n+    dtype of ``float64``.\n \n     Changing the value of properties that are not part of the\n     `StateSpace` system representation (such as `zeros` or `poles`) is very", "before_segments": [{"filename": "scipy/signal/_ltisys.py", "start_line": 50, "code": "    def __new__(cls, *system, **kwargs):\n        if cls is LinearTimeInvariant:\n            raise NotImplementedError('The LinearTimeInvariant class is not '\n                                      'meant to be used directly, use `lti` '\n                                      'or `dlti` instead.')\n        return super().__new__(cls)", "documentation": "        \"\"\"Create a new object, don't allow direct instances.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 58, "code": "    def __init__(self):\n        super().__init__()\n        self.inputs = None\n        self.outputs = None\n        self._dt = None\n    @property", "documentation": "        \"\"\"\n        Initialize the `lti` baseclass.\n\n        The heavy lifting is done by the subclasses.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 71, "code": "    def dt(self):\n        return self._dt\n    @property", "documentation": "        \"\"\"Return the sampling time of the system, `None` for `lti` systems.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 83, "code": "    def zeros(self):\n        return self.to_zpk().zeros\n    @property", "documentation": "        \"\"\"Zeros of the system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 88, "code": "    def poles(self):\n        return self.to_zpk().poles", "documentation": "        \"\"\"Poles of the system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 92, "code": "    def _as_ss(self):\n        if isinstance(self, StateSpace):\n            return self\n        else:\n            return self.to_ss()", "documentation": "        \"\"\"Convert to `StateSpace` system, without copying.\n\n        Returns\n        -------\n        sys: StateSpace\n            The `StateSpace` system. If the class is already an instance of\n            `StateSpace` then this instance is returned.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 106, "code": "    def _as_zpk(self):\n        if isinstance(self, ZerosPolesGain):\n            return self\n        else:\n            return self.to_zpk()", "documentation": "        \"\"\"Convert to `ZerosPolesGain` system, without copying.\n\n        Returns\n        -------\n        sys: ZerosPolesGain\n            The `ZerosPolesGain` system. If the class is already an instance of\n            `ZerosPolesGain` then this instance is returned.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 120, "code": "    def _as_tf(self):\n        if isinstance(self, TransferFunction):\n            return self\n        else:\n            return self.to_tf()", "documentation": "        \"\"\"Convert to `TransferFunction` system, without copying.\n\n        Returns\n        -------\n        sys: ZerosPolesGain\n            The `TransferFunction` system. If the class is already an instance of\n            `TransferFunction` then this instance is returned.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 207, "code": "    def __new__(cls, *system):\n        if cls is lti:\n            N = len(system)\n            if N == 2:\n                return TransferFunctionContinuous.__new__(\n                    TransferFunctionContinuous, *system)\n            elif N == 3:\n                return ZerosPolesGainContinuous.__new__(\n                    ZerosPolesGainContinuous, *system)\n            elif N == 4:\n                return StateSpaceContinuous.__new__(StateSpaceContinuous,", "documentation": "        \"\"\"Create an instance of the appropriate subclass.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 226, "code": "    def __init__(self, *system):\n        super().__init__(*system)", "documentation": "        \"\"\"\n        Initialize the `lti` baseclass.\n\n        The heavy lifting is done by the subclasses.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 234, "code": "    def impulse(self, X0=None, T=None, N=None):\n        return impulse(self, X0=X0, T=T, N=N)", "documentation": "        \"\"\"\n        Return the impulse response of a continuous-time system.\n        See `impulse` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 241, "code": "    def step(self, X0=None, T=None, N=None):\n        return step(self, X0=X0, T=T, N=N)", "documentation": "        \"\"\"\n        Return the step response of a continuous-time system.\n        See `step` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 248, "code": "    def output(self, U, T, X0=None):\n        return lsim(self, U, T, X0=X0)", "documentation": "        \"\"\"\n        Return the response of a continuous-time system to input `U`.\n        See `lsim` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 255, "code": "    def bode(self, w=None, n=100):\n        return bode(self, w=w, n=n)", "documentation": "        \"\"\"\n        Calculate Bode magnitude and phase data of a continuous-time system.\n\n        Returns a 3-tuple containing arrays of frequencies [rad/s], magnitude\n        [dB] and phase [deg]. See `bode` for details.\n\n        Examples\n        --------\n        >>> from scipy import signal\n        >>> import matplotlib.pyplot as plt\n\n        >>> sys = signal.TransferFunction([1], [1, 1])\n        >>> w, mag, phase = sys.bode()\n\n        >>> plt.figure()\n        >>> plt.semilogx(w, mag)    # Bode magnitude plot\n        >>> plt.figure()\n        >>> plt.semilogx(w, phase)  # Bode phase plot\n        >>> plt.show()\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 279, "code": "    def freqresp(self, w=None, n=10000):\n        return freqresp(self, w=w, n=n)", "documentation": "        \"\"\"\n        Calculate the frequency response of a continuous-time system.\n\n        Returns a 2-tuple containing arrays of frequencies [rad/s] and\n        complex magnitude.\n        See `freqresp` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 289, "code": "    def to_discrete(self, dt, method='zoh', alpha=None):\n        raise NotImplementedError('to_discrete is not implemented for this '\n                                  'system class.')", "documentation": "        \"\"\"Return a discretized version of the current system.\n\n        Parameters: See `cont2discrete` for details.\n\n        Returns\n        -------\n        sys: instance of `dlti`\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 391, "code": "    def __new__(cls, *system, **kwargs):\n        if cls is dlti:\n            N = len(system)\n            if N == 2:\n                return TransferFunctionDiscrete.__new__(\n                    TransferFunctionDiscrete, *system, **kwargs)\n            elif N == 3:\n                return ZerosPolesGainDiscrete.__new__(ZerosPolesGainDiscrete,\n                                                      *system, **kwargs)\n            elif N == 4:\n                return StateSpaceDiscrete.__new__(StateSpaceDiscrete, *system,", "documentation": "        \"\"\"Create an instance of the appropriate subclass.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 410, "code": "    def __init__(self, *system, **kwargs):\n        dt = kwargs.pop('dt', True)\n        super().__init__(*system, **kwargs)\n        self.dt = dt\n    @property", "documentation": "        \"\"\"\n        Initialize the `lti` baseclass.\n\n        The heavy lifting is done by the subclasses.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 422, "code": "    def dt(self):\n        return self._dt\n    @dt.setter", "documentation": "        \"\"\"Return the sampling time of the system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 430, "code": "    def impulse(self, x0=None, t=None, n=None):\n        return dimpulse(self, x0=x0, t=t, n=n)", "documentation": "        \"\"\"\n        Return the impulse response of the discrete-time `dlti` system.\n        See `dimpulse` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 437, "code": "    def step(self, x0=None, t=None, n=None):\n        return dstep(self, x0=x0, t=t, n=n)", "documentation": "        \"\"\"\n        Return the step response of the discrete-time `dlti` system.\n        See `dstep` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 444, "code": "    def output(self, u, t, x0=None):\n        return dlsim(self, u, t, x0=x0)", "documentation": "        \"\"\"\n        Return the response of the discrete-time system to input `u`.\n        See `dlsim` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 481, "code": "    def freqresp(self, w=None, n=10000, whole=False):\n        return dfreqresp(self, w=w, n=n, whole=whole)", "documentation": "        \"\"\"\n        Calculate the frequency response of a discrete-time system.\n\n        Returns a 2-tuple containing arrays of frequencies [rad/s] and\n        complex magnitude.\n        See `dfreqresp` for details.\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 569, "code": "    def __new__(cls, *system, **kwargs):\n        if len(system) == 1 and isinstance(system[0], LinearTimeInvariant):\n            return system[0].to_tf()\n        if cls is TransferFunction:\n            if kwargs.get('dt') is None:\n                return TransferFunctionContinuous.__new__(\n                    TransferFunctionContinuous,\n                    *system,\n                    **kwargs)\n            else:\n                return TransferFunctionDiscrete.__new__(", "documentation": "        \"\"\"Handle object conversion if input is an instance of lti.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 590, "code": "    def __init__(self, *system, **kwargs):\n        if isinstance(system[0], LinearTimeInvariant):\n            return\n        super().__init__(**kwargs)\n        self._num = None\n        self._den = None\n        self.num, self.den = normalize(*system)", "documentation": "        \"\"\"Initialize the state space LTI system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 604, "code": "    def __repr__(self):\n        return (\n            f'{self.__class__.__name__}(\\n'\n            f'{repr(self.num)},\\n'\n            f'{repr(self.den)},\\n'\n            f'dt: {repr(self.dt)}\\n)'\n        )\n    @property", "documentation": "        \"\"\"Return representation of the system's transfer function\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 614, "code": "    def num(self):\n        return self._num\n    @num.setter", "documentation": "        \"\"\"Numerator of the `TransferFunction` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 630, "code": "    def den(self):\n        return self._den\n    @den.setter", "documentation": "        \"\"\"Denominator of the `TransferFunction` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 638, "code": "    def _copy(self, system):\n        self.num = system.num\n        self.den = system.den", "documentation": "        \"\"\"\n        Copy the parameters of another `TransferFunction` object\n\n        Parameters\n        ----------\n        system : `TransferFunction`\n            The `StateSpace` system that is to be copied\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 651, "code": "    def to_tf(self):\n        return copy.deepcopy(self)", "documentation": "        \"\"\"\n        Return a copy of the current `TransferFunction` system.\n\n        Returns\n        -------\n        sys : instance of `TransferFunction`\n            The current system (copy)\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 663, "code": "    def to_zpk(self):\n        return ZerosPolesGain(*tf2zpk(self.num, self.den),\n                              **self._dt_dict)", "documentation": "        \"\"\"\n        Convert system representation to `ZerosPolesGain`.\n\n        Returns\n        -------\n        sys : instance of `ZerosPolesGain`\n            Zeros, poles, gain representation of the current system\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 676, "code": "    def to_ss(self):\n        return StateSpace(*tf2ss(self.num, self.den),\n                          **self._dt_dict)\n    @staticmethod", "documentation": "        \"\"\"\n        Convert system representation to `StateSpace`.\n\n        Returns\n        -------\n        sys : instance of `StateSpace`\n            State space model of the current system\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 690, "code": "    def _z_to_zinv(num, den):\n        diff = len(num) - len(den)\n        if diff > 0:\n            den = np.hstack((np.zeros(diff), den))\n        elif diff < 0:\n            num = np.hstack((np.zeros(-diff), num))\n        return num, den\n    @staticmethod", "documentation": "        \"\"\"Change a transfer function from the variable `z` to `z**-1`.\n\n        Parameters\n        ----------\n        num, den: 1d array_like\n            Sequences representing the coefficients of the numerator and\n            denominator polynomials, in order of descending degree of 'z'.\n            That is, ``5z**2 + 3z + 2`` is presented as ``[5, 3, 2]``.\n\n        Returns\n        -------\n        num, den: 1d array_like\n            Sequences representing the coefficients of the numerator and\n            denominator polynomials, in order of ascending degree of 'z**-1'.\n            That is, ``5 + 3 z**-1 + 2 z**-2`` is presented as ``[5, 3, 2]``.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 715, "code": "    def _zinv_to_z(num, den):\n        diff = len(num) - len(den)\n        if diff > 0:\n            den = np.hstack((den, np.zeros(diff)))\n        elif diff < 0:\n            num = np.hstack((num, np.zeros(-diff)))\n        return num, den", "documentation": "        \"\"\"Change a transfer function from the variable `z` to `z**-1`.\n\n        Parameters\n        ----------\n        num, den: 1d array_like\n            Sequences representing the coefficients of the numerator and\n            denominator polynomials, in order of ascending degree of 'z**-1'.\n            That is, ``5 + 3 z**-1 + 2 z**-2`` is presented as ``[5, 3, 2]``.\n\n        Returns\n        -------\n        num, den: 1d array_like\n            Sequences representing the coefficients of the numerator and\n            denominator polynomials, in order of descending degree of 'z'.\n            That is, ``5z**2 + 3z + 2`` is presented as ``[5, 3, 2]``.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 800, "code": "    def to_discrete(self, dt, method='zoh', alpha=None):\n        return TransferFunction(*cont2discrete((self.num, self.den),\n                                               dt,\n                                               method=method,\n                                               alpha=alpha)[:-1],\n                                dt=dt)", "documentation": "        \"\"\"\n        Returns the discretized `TransferFunction` system.\n\n        Parameters: See `cont2discrete` for details.\n\n        Returns\n        -------\n        sys: instance of `dlti` and `StateSpace`\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 951, "code": "    def __new__(cls, *system, **kwargs):\n        if len(system) == 1 and isinstance(system[0], LinearTimeInvariant):\n            return system[0].to_zpk()\n        if cls is ZerosPolesGain:\n            if kwargs.get('dt') is None:\n                return ZerosPolesGainContinuous.__new__(\n                    ZerosPolesGainContinuous,\n                    *system,\n                    **kwargs)\n            else:\n                return ZerosPolesGainDiscrete.__new__(", "documentation": "        \"\"\"Handle object conversion if input is an instance of `lti`\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 973, "code": "    def __init__(self, *system, **kwargs):\n        if isinstance(system[0], LinearTimeInvariant):\n            return\n        super().__init__(**kwargs)\n        self._zeros = None\n        self._poles = None\n        self._gain = None\n        self.zeros, self.poles, self.gain = system", "documentation": "        \"\"\"Initialize the zeros, poles, gain system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 987, "code": "    def __repr__(self):\n        return (\n            f'{self.__class__.__name__}(\\n'\n            f'{repr(self.zeros)},\\n'\n            f'{repr(self.poles)},\\n'\n            f'{repr(self.gain)},\\n'\n            f'dt: {repr(self.dt)}\\n)'\n        )\n    @property", "documentation": "        \"\"\"Return representation of the `ZerosPolesGain` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 998, "code": "    def zeros(self):\n        return self._zeros\n    @zeros.setter", "documentation": "        \"\"\"Zeros of the `ZerosPolesGain` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1014, "code": "    def poles(self):\n        return self._poles\n    @poles.setter", "documentation": "        \"\"\"Poles of the `ZerosPolesGain` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1023, "code": "    def gain(self):\n        return self._gain\n    @gain.setter", "documentation": "        \"\"\"Gain of the `ZerosPolesGain` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1031, "code": "    def _copy(self, system):\n        self.poles = system.poles\n        self.zeros = system.zeros\n        self.gain = system.gain", "documentation": "        \"\"\"\n        Copy the parameters of another `ZerosPolesGain` system.\n\n        Parameters\n        ----------\n        system : instance of `ZerosPolesGain`\n            The zeros, poles gain system that is to be copied\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1045, "code": "    def to_tf(self):\n        return TransferFunction(*zpk2tf(self.zeros, self.poles, self.gain),\n                                **self._dt_dict)", "documentation": "        \"\"\"\n        Convert system representation to `TransferFunction`.\n\n        Returns\n        -------\n        sys : instance of `TransferFunction`\n            Transfer function of the current system\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1058, "code": "    def to_zpk(self):\n        return copy.deepcopy(self)", "documentation": "        \"\"\"\n        Return a copy of the current 'ZerosPolesGain' system.\n\n        Returns\n        -------\n        sys : instance of `ZerosPolesGain`\n            The current system (copy)\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1070, "code": "    def to_ss(self):\n        return StateSpace(*zpk2ss(self.zeros, self.poles, self.gain),\n                          **self._dt_dict)", "documentation": "        \"\"\"\n        Convert system representation to `StateSpace`.\n\n        Returns\n        -------\n        sys : instance of `StateSpace`\n            State space model of the current system\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1136, "code": "    def to_discrete(self, dt, method='zoh', alpha=None):\n        return ZerosPolesGain(\n            *cont2discrete((self.zeros, self.poles, self.gain),\n                           dt,\n                           method=method,\n                           alpha=alpha)[:-1],\n            dt=dt)", "documentation": "        \"\"\"\n        Returns the discretized `ZerosPolesGain` system.\n\n        Parameters: See `cont2discrete` for details.\n\n        Returns\n        -------\n        sys: instance of `dlti` and `ZerosPolesGain`\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1323, "code": "    def __new__(cls, *system, **kwargs):\n        if len(system) == 1 and isinstance(system[0], LinearTimeInvariant):\n            return system[0].to_ss()\n        if cls is StateSpace:\n            if kwargs.get('dt') is None:\n                return StateSpaceContinuous.__new__(StateSpaceContinuous,\n                                                    *system, **kwargs)\n            else:\n                return StateSpaceDiscrete.__new__(StateSpaceDiscrete,\n                                                  *system, **kwargs)\n        return super().__new__(cls)", "documentation": "        \"\"\"Create new StateSpace object and settle inheritance.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1341, "code": "    def __init__(self, *system, **kwargs):\n        if isinstance(system[0], LinearTimeInvariant):\n            return\n        super().__init__(**kwargs)\n        self._A = None\n        self._B = None\n        self._C = None\n        self._D = None\n        self.A, self.B, self.C, self.D = abcd_normalize(*system)", "documentation": "        \"\"\"Initialize the state space lti/dlti system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1358, "code": "    def __repr__(self):\n        return (\n            f'{self.__class__.__name__}(\\n'\n            f'{repr(self.A)},\\n'\n            f'{repr(self.B)},\\n'\n            f'{repr(self.C)},\\n'\n            f'{repr(self.D)},\\n'\n            f'dt: {repr(self.dt)}\\n)'\n        )", "documentation": "        \"\"\"Return representation of the `StateSpace` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1373, "code": "    def __mul__(self, other):\n        if not self._check_binop_other(other):\n            return NotImplemented\n        if isinstance(other, StateSpace):\n            if type(other) is not type(self):\n                return NotImplemented\n            if self.dt != other.dt:\n                raise TypeError('Cannot multiply systems with different `dt`.')\n            n1 = self.A.shape[0]\n            n2 = other.A.shape[0]\n            a = np.vstack((np.hstack((self.A, np.dot(self.B, other.C))),", "documentation": "        \"\"\"\n        Post-multiply another system or a scalar\n\n        Handles multiplication of systems in the sense of a frequency domain\n        multiplication. That means, given two systems E1(s) and E2(s), their\n        multiplication, H(s) = E1(s) * E2(s), means that applying H(s) to U(s)\n        is equivalent to first applying E2(s), and then E1(s).\n\n        Notes\n        -----\n        For SISO systems the order of system application does not matter.\n        However, for MIMO systems, where the two systems are matrices, the\n        order above ensures standard Matrix multiplication rules apply.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1433, "code": "    def __rmul__(self, other):\n        if not self._check_binop_other(other) or isinstance(other, StateSpace):\n            return NotImplemented\n        a = self.A\n        b = self.B\n        c = np.dot(other, self.C)\n        d = np.dot(other, self.D)\n        common_dtype = np.result_type(a.dtype, b.dtype, c.dtype, d.dtype)\n        return StateSpace(np.asarray(a, dtype=common_dtype),\n                          np.asarray(b, dtype=common_dtype),\n                          np.asarray(c, dtype=common_dtype),", "documentation": "        \"\"\"Pre-multiply a scalar or matrix (but not StateSpace)\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1451, "code": "    def __neg__(self):\n        return StateSpace(self.A, self.B, -self.C, -self.D, **self._dt_dict)", "documentation": "        \"\"\"Negate the system (equivalent to pre-multiplying by -1).\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1455, "code": "    def __add__(self, other):\n        if not self._check_binop_other(other):\n            return NotImplemented\n        if isinstance(other, StateSpace):\n            if type(other) is not type(self):\n                raise TypeError(f'Cannot add {type(self)} and {type(other)}')\n            if self.dt != other.dt:\n                raise TypeError('Cannot add systems with different `dt`.')\n            a = linalg.block_diag(self.A, other.A)\n            b = np.vstack((self.B, other.B))\n            c = np.hstack((self.C, other.C))", "documentation": "        \"\"\"\n        Adds two systems in the sense of frequency domain addition.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1522, "code": "    def __truediv__(self, other):\n        if not self._check_binop_other(other) or isinstance(other, StateSpace):\n            return NotImplemented\n        if isinstance(other, np.ndarray) and other.ndim > 0:\n            raise ValueError(\"Cannot divide StateSpace by non-scalar numpy arrays\")\n        return self.__mul__(1/other)\n    @property", "documentation": "        \"\"\"\n        Divide by a scalar\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1537, "code": "    def A(self):\n        return self._A\n    @A.setter", "documentation": "        \"\"\"State matrix of the `StateSpace` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1546, "code": "    def B(self):\n        return self._B\n    @B.setter", "documentation": "        \"\"\"Input matrix of the `StateSpace` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1556, "code": "    def C(self):\n        return self._C\n    @C.setter", "documentation": "        \"\"\"Output matrix of the `StateSpace` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1566, "code": "    def D(self):\n        return self._D\n    @D.setter", "documentation": "        \"\"\"Feedthrough matrix of the `StateSpace` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1574, "code": "    def _copy(self, system):\n        self.A = system.A\n        self.B = system.B\n        self.C = system.C\n        self.D = system.D", "documentation": "        \"\"\"\n        Copy the parameters of another `StateSpace` system.\n\n        Parameters\n        ----------\n        system : instance of `StateSpace`\n            The state-space system that is to be copied\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1589, "code": "    def to_tf(self, **kwargs):\n        return TransferFunction(*ss2tf(self._A, self._B, self._C, self._D,\n                                       **kwargs), **self._dt_dict)", "documentation": "        \"\"\"\n        Convert system representation to `TransferFunction`.\n\n        Parameters\n        ----------\n        kwargs : dict, optional\n            Additional keywords passed to `ss2zpk`\n\n        Returns\n        -------\n        sys : instance of `TransferFunction`\n            Transfer function of the current system\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1607, "code": "    def to_zpk(self, **kwargs):\n        return ZerosPolesGain(*ss2zpk(self._A, self._B, self._C, self._D,\n                                      **kwargs), **self._dt_dict)", "documentation": "        \"\"\"\n        Convert system representation to `ZerosPolesGain`.\n\n        Parameters\n        ----------\n        kwargs : dict, optional\n            Additional keywords passed to `ss2zpk`\n\n        Returns\n        -------\n        sys : instance of `ZerosPolesGain`\n            Zeros, poles, gain representation of the current system\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1625, "code": "    def to_ss(self):\n        return copy.deepcopy(self)", "documentation": "        \"\"\"\n        Return a copy of the current `StateSpace` system.\n\n        Returns\n        -------\n        sys : instance of `StateSpace`\n            The current system (copy)\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1695, "code": "    def to_discrete(self, dt, method='zoh', alpha=None):\n        return StateSpace(*cont2discrete((self.A, self.B, self.C, self.D),\n                                         dt,\n                                         method=method,\n                                         alpha=alpha)[:-1],\n                          dt=dt)", "documentation": "        \"\"\"\n        Returns the discretized `StateSpace` system.\n\n        Parameters: See `cont2discrete` for details.\n\n        Returns\n        -------\n        sys: instance of `dlti` and `StateSpace`\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1774, "code": "def lsim(system, U, T, X0=None, interp=True):\n    if isinstance(system, lti):\n        sys = system._as_ss()\n    elif isinstance(system, dlti):\n        raise AttributeError('lsim can only be used with continuous-time '\n                             'systems.')\n    else:\n        sys = lti(*system)._as_ss()\n    T = atleast_1d(T)\n    if len(T.shape) != 1:\n        raise ValueError(\"T must be a rank-1 array.\")", "documentation": "    \"\"\"\n    Simulate output of a continuous-time linear system.\n\n    Parameters\n    ----------\n    system : an instance of the LTI class or a tuple describing the system.\n        The following gives the number of elements in the tuple and\n        the interpretation:\n\n        * 1: (instance of `lti`)\n        * 2: (num, den)\n        * 3: (zeros, poles, gain)\n        * 4: (A, B, C, D)\n\n    U : array_like\n        An input array describing the input at each time `T`\n        (interpolation is assumed between given times).  If there are\n        multiple inputs, then each column of the rank-2 array\n        represents an input.  If U = 0 or None, a zero input is used.\n    T : array_like\n        The time steps at which the input is defined and at which the\n        output is desired.  Must be nonnegative, increasing, and equally spaced.\n    X0 : array_like, optional\n        The initial conditions on the state vector (zero by default).\n    interp : bool, optional\n        Whether to use linear (True, the default) or zero-order-hold (False)\n        interpolation for the input array.\n\n    Returns\n    -------\n    T : 1D ndarray\n        Time values for the output.\n    yout : 1D ndarray\n        System response.\n    xout : ndarray\n        Time evolution of the state vector.\n\n    Notes\n    -----\n    If (num, den) is passed in for ``system``, coefficients for both the\n    numerator and denominator should be specified in descending exponent\n    order (e.g. ``s^2 + 3s + 5`` would be represented as ``[1, 3, 5]``).\n\n    Examples\n    --------\n    We'll use `lsim` to simulate an analog Bessel filter applied to\n    a signal.\n\n    >>> import numpy as np\n    >>> from scipy.signal import bessel, lsim\n    >>> import matplotlib.pyplot as plt\n\n    Create a low-pass Bessel filter with a cutoff of 12 Hz.\n\n    >>> b, a = bessel(N=5, Wn=2*np.pi*12, btype='lowpass', analog=True)\n\n    Generate data to which the filter is applied.\n\n    >>> t = np.linspace(0, 1.25, 500, endpoint=False)\n\n    The input signal is the sum of three sinusoidal curves, with\n    frequencies 4 Hz, 40 Hz, and 80 Hz.  The filter should mostly\n    eliminate the 40 Hz and 80 Hz components, leaving just the 4 Hz signal.\n\n    >>> u = (np.cos(2*np.pi*4*t) + 0.6*np.sin(2*np.pi*40*t) +\n    ...      0.5*np.cos(2*np.pi*80*t))\n\n    Simulate the filter with `lsim`.\n\n    >>> tout, yout, xout = lsim((b, a), U=u, T=t)\n\n    Plot the result.\n\n    >>> plt.plot(t, u, 'r', alpha=0.5, linewidth=1, label='input')\n    >>> plt.plot(tout, yout, 'k', linewidth=1.5, label='output')\n    >>> plt.legend(loc='best', shadow=True, framealpha=1)\n    >>> plt.grid(alpha=0.3)\n    >>> plt.xlabel('t')\n    >>> plt.show()\n\n    In a second example, we simulate a double integrator ``y'' = u``, with\n    a constant input ``u = 1``.  We'll use the state space representation\n    of the integrator.\n\n    >>> from scipy.signal import lti\n    >>> A = np.array([[0.0, 1.0], [0.0, 0.0]])\n    >>> B = np.array([[0.0], [1.0]])\n    >>> C = np.array([[1.0, 0.0]])\n    >>> D = 0.0\n    >>> system = lti(A, B, C, D)\n\n    `t` and `u` define the time and input signal for the system to\n    be simulated.\n\n    >>> t = np.linspace(0, 5, num=50)\n    >>> u = np.ones_like(t)\n\n    Compute the simulation, and then plot `y`.  As expected, the plot shows\n    the curve ``y = 0.5*t**2``.\n\n    >>> tout, y, x = lsim(system, u, t)\n    >>> plt.plot(t, y)\n    >>> plt.grid(alpha=0.3)\n    >>> plt.xlabel('t')\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1989, "code": "def _default_response_times(A, n):\n    vals = linalg.eigvals(A)\n    r = min(abs(real(vals)))\n    if r == 0.0:\n        r = 1.0\n    tc = 1.0 / r\n    t = linspace(0.0, 7 * tc, n)\n    return t", "documentation": "    \"\"\"Compute a reasonable set of time samples for the response time.\n\n    This function is used by `impulse` and `step`  to compute the response time\n    when the `T` argument to the function is None.\n\n    Parameters\n    ----------\n    A : array_like\n        The system matrix, which is square.\n    n : int\n        The number of time samples to generate.\n\n    Returns\n    -------\n    t : ndarray\n        The 1-D array of length `n` of time samples at which the response\n        is to be computed.\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2020, "code": "def impulse(system, X0=None, T=None, N=None):\n    if isinstance(system, lti):\n        sys = system._as_ss()\n    elif isinstance(system, dlti):\n        raise AttributeError('impulse can only be used with continuous-time '\n                             'systems.')\n    else:\n        sys = lti(*system)._as_ss()\n    if X0 is None:\n        X = squeeze(sys.B)\n    else:", "documentation": "    \"\"\"Impulse response of continuous-time system.\n\n    Parameters\n    ----------\n    system : an instance of the LTI class or a tuple of array_like\n        describing the system.\n        The following gives the number of elements in the tuple and\n        the interpretation:\n\n            * 1 (instance of `lti`)\n            * 2 (num, den)\n            * 3 (zeros, poles, gain)\n            * 4 (A, B, C, D)\n\n    X0 : array_like, optional\n        Initial state-vector.  Defaults to zero.\n    T : array_like, optional\n        Time points.  Computed if not given.\n    N : int, optional\n        The number of time points to compute (if `T` is not given).\n\n    Returns\n    -------\n    T : ndarray\n        A 1-D array of time points.\n    yout : ndarray\n        A 1-D array containing the impulse response of the system (except for\n        singularities at zero).\n\n    Notes\n    -----\n    If (num, den) is passed in for ``system``, coefficients for both the\n    numerator and denominator should be specified in descending exponent\n    order (e.g. ``s^2 + 3s + 5`` would be represented as ``[1, 3, 5]``).\n\n    Examples\n    --------\n    Compute the impulse response of a second order system with a repeated\n    root: ``x''(t) + 2*x'(t) + x(t) = u(t)``\n\n    >>> from scipy import signal\n    >>> system = ([1.0], [1.0, 2.0, 1.0])\n    >>> t, y = signal.impulse(system)\n    >>> import matplotlib.pyplot as plt\n    >>> plt.plot(t, y)\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2090, "code": "def step(system, X0=None, T=None, N=None):\n    if isinstance(system, lti):\n        sys = system._as_ss()\n    elif isinstance(system, dlti):\n        raise AttributeError('step can only be used with continuous-time '\n                             'systems.')\n    else:\n        sys = lti(*system)._as_ss()\n    if N is None:\n        N = 100\n    if T is None:", "documentation": "    \"\"\"Step response of continuous-time system.\n\n    Parameters\n    ----------\n    system : an instance of the LTI class or a tuple of array_like\n        describing the system.\n        The following gives the number of elements in the tuple and\n        the interpretation:\n\n            * 1 (instance of `lti`)\n            * 2 (num, den)\n            * 3 (zeros, poles, gain)\n            * 4 (A, B, C, D)\n\n    X0 : array_like, optional\n        Initial state-vector (default is zero).\n    T : array_like, optional\n        Time points (computed if not given).\n    N : int, optional\n        Number of time points to compute if `T` is not given.\n\n    Returns\n    -------\n    T : 1D ndarray\n        Output time points.\n    yout : 1D ndarray\n        Step response of system.\n\n\n    Notes\n    -----\n    If (num, den) is passed in for ``system``, coefficients for both the\n    numerator and denominator should be specified in descending exponent\n    order (e.g. ``s^2 + 3s + 5`` would be represented as ``[1, 3, 5]``).\n\n    Examples\n    --------\n    >>> from scipy import signal\n    >>> import matplotlib.pyplot as plt\n    >>> lti = signal.lti([1.0], [1.0, 1.0])\n    >>> t, y = signal.step(lti)\n    >>> plt.plot(t, y)\n    >>> plt.xlabel('Time [s]')\n    >>> plt.ylabel('Amplitude')\n    >>> plt.title('Step response for 1. Order Lowpass')\n    >>> plt.grid()\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2157, "code": "def bode(system, w=None, n=100):\n    w, y = freqresp(system, w=w, n=n)\n    mag = 20.0 * np.log10(abs(y))\n    phase = np.unwrap(np.arctan2(y.imag, y.real)) * 180.0 / np.pi\n    return w, mag, phase", "documentation": "    \"\"\"\n    Calculate Bode magnitude and phase data of a continuous-time system.\n\n    Parameters\n    ----------\n    system : an instance of the LTI class or a tuple describing the system.\n        The following gives the number of elements in the tuple and\n        the interpretation:\n\n            * 1 (instance of `lti`)\n            * 2 (num, den)\n            * 3 (zeros, poles, gain)\n            * 4 (A, B, C, D)\n\n    w : array_like, optional\n        Array of frequencies (in rad/s). Magnitude and phase data is calculated\n        for every value in this array. If not given a reasonable set will be\n        calculated.\n    n : int, optional\n        Number of frequency points to compute if `w` is not given. The `n`\n        frequencies are logarithmically spaced in an interval chosen to\n        include the influence of the poles and zeros of the system.\n\n    Returns\n    -------\n    w : 1D ndarray\n        Frequency array [rad/s]\n    mag : 1D ndarray\n        Magnitude array [dB]\n    phase : 1D ndarray\n        Phase array [deg]\n\n    Notes\n    -----\n    If (num, den) is passed in for ``system``, coefficients for both the\n    numerator and denominator should be specified in descending exponent\n    order (e.g. ``s^2 + 3s + 5`` would be represented as ``[1, 3, 5]``).\n\n    .. versionadded:: 0.11.0\n\n    Examples\n    --------\n    >>> from scipy import signal\n    >>> import matplotlib.pyplot as plt\n\n    >>> sys = signal.TransferFunction([1], [1, 1])\n    >>> w, mag, phase = signal.bode(sys)\n\n    >>> plt.figure()\n    >>> plt.semilogx(w, mag)    # Bode magnitude plot\n    >>> plt.figure()\n    >>> plt.semilogx(w, phase)  # Bode phase plot\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2313, "code": "def _valid_inputs(A, B, poles, method, rtol, maxiter):\n    poles = np.asarray(poles)\n    if poles.ndim > 1:\n        raise ValueError(\"Poles must be a 1D array like.\")\n    poles = _order_complex_poles(poles)\n    if A.ndim > 2:\n        raise ValueError(\"A must be a 2D array/matrix.\")\n    if B.ndim > 2:\n        raise ValueError(\"B must be a 2D array/matrix\")\n    if A.shape[0] != A.shape[1]:\n        raise ValueError(\"A must be square\")", "documentation": "    \"\"\"\n    Check the poles come in complex conjugate pairs\n    Check shapes of A, B and poles are compatible.\n    Check the method chosen is compatible with provided poles\n    Return update method to use and ordered poles\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2366, "code": "def _order_complex_poles(poles):\n    ordered_poles = np.sort(poles[np.isreal(poles)])\n    im_poles = []\n    for p in np.sort(poles[np.imag(poles) < 0]):\n        if np.conj(p) in poles:\n            im_poles.extend((p, np.conj(p)))\n    ordered_poles = np.hstack((ordered_poles, im_poles))\n    if poles.shape[0] != len(ordered_poles):\n        raise ValueError(\"Complex poles must come with their conjugates\")\n    return ordered_poles", "documentation": "    \"\"\"\n    Check we have complex conjugates pairs and reorder P according to YT, ie\n    real_poles, complex_i, conjugate complex_i, ....\n    The lexicographic sort on the complex poles is added to help the user to\n    compare sets of poles.\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2386, "code": "def _KNV0(B, ker_pole, transfer_matrix, j, poles):\n    transfer_matrix_not_j = np.delete(transfer_matrix, j, axis=1)\n    Q, R = s_qr(transfer_matrix_not_j, mode=\"full\")\n    mat_ker_pj = np.dot(ker_pole[j], ker_pole[j].T)\n    yj = np.dot(mat_ker_pj, Q[:, -1])\n    if not np.allclose(yj, 0):\n        xj = yj/np.linalg.norm(yj)\n        transfer_matrix[:, j] = xj", "documentation": "    \"\"\"\n    Algorithm \"KNV0\" Kautsky et Al. Robust pole\n    assignment in linear state feedback, Int journal of Control\n    1985, vol 41 p 1129->1155\n    https://la.epfl.ch/files/content/sites/la/files/\n        users/105941/public/KautskyNicholsDooren\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2433, "code": "def _YT_real(ker_pole, Q, transfer_matrix, i, j):\n    u = Q[:, -2, np.newaxis]\n    v = Q[:, -1, np.newaxis]\n    m = np.dot(np.dot(ker_pole[i].T, np.dot(u, v.T) -\n        np.dot(v, u.T)), ker_pole[j])\n    um, sm, vm = np.linalg.svd(m)\n    mu1, mu2 = um.T[:2, :, np.newaxis]\n    nu1, nu2 = vm[:2, :, np.newaxis]\n    transfer_matrix_j_mo_transfer_matrix_j = np.vstack((\n            transfer_matrix[:, i, np.newaxis],\n            transfer_matrix[:, j, np.newaxis]))", "documentation": "    \"\"\"\n    Applies algorithm from YT section 6.1 page 19 related to real pairs\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2498, "code": "def _YT_complex(ker_pole, Q, transfer_matrix, i, j):\n    ur = np.sqrt(2)*Q[:, -2, np.newaxis]\n    ui = np.sqrt(2)*Q[:, -1, np.newaxis]\n    u = ur + 1j*ui\n    ker_pole_ij = ker_pole[i]\n    m = np.dot(np.dot(np.conj(ker_pole_ij.T), np.dot(u, np.conj(u).T) -\n               np.dot(np.conj(u), u.T)), ker_pole_ij)\n    e_val, e_vec = np.linalg.eig(m)\n    e_val_idx = np.argsort(np.abs(e_val))\n    mu1 = e_vec[:, e_val_idx[-1], np.newaxis]\n    mu2 = e_vec[:, e_val_idx[-2], np.newaxis]", "documentation": "    \"\"\"\n    Applies algorithm from YT section 6.2 page 20 related to complex pairs\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2549, "code": "def _YT_loop(ker_pole, transfer_matrix, poles, B, maxiter, rtol):\n    nb_real = poles[np.isreal(poles)].shape[0]\n    hnb = nb_real // 2\n    if nb_real > 0:\n        update_order = [[nb_real], [1]]\n    else:\n        update_order = [[],[]]\n    r_comp = np.arange(nb_real+1, len(poles)+1, 2)\n    r_p = np.arange(1, hnb+nb_real % 2)\n    update_order[0].extend(2*r_p)\n    update_order[1].extend(2*r_p+1)", "documentation": "    \"\"\"\n    Algorithm \"YT\" Tits, Yang. Globally Convergent\n    Algorithms for Robust Pole Assignment by State Feedback\n    https://hdl.handle.net/1903/5598\n    The poles P have to be sorted accordingly to section 6.2 page 20\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2672, "code": "def _KNV0_loop(ker_pole, transfer_matrix, poles, B, maxiter, rtol):\n    stop = False\n    nb_try = 0\n    while nb_try < maxiter and not stop:\n        det_transfer_matrixb = np.abs(np.linalg.det(transfer_matrix))\n        for j in range(B.shape[0]):\n            _KNV0(B, ker_pole, transfer_matrix, j, poles)\n        det_transfer_matrix = np.max((np.sqrt(np.spacing(1)),\n                                  np.abs(np.linalg.det(transfer_matrix))))\n        cur_rtol = np.abs((det_transfer_matrix - det_transfer_matrixb) /\n                       det_transfer_matrix)", "documentation": "    \"\"\"\n    Loop over all poles one by one and apply KNV method 0 algorithm\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2698, "code": "def place_poles(A, B, poles, method=\"YT\", rtol=1e-3, maxiter=30):\n    update_loop, poles = _valid_inputs(A, B, poles, method, rtol, maxiter)\n    cur_rtol = 0\n    nb_iter = 0\n    u, z = s_qr(B, mode=\"full\")\n    rankB = np.linalg.matrix_rank(B)\n    u0 = u[:, :rankB]\n    u1 = u[:, rankB:]\n    z = z[:rankB, :]\n    if B.shape[0] == rankB:\n        diag_poles = np.zeros(A.shape)", "documentation": "    \"\"\"\n    Compute K such that eigenvalues (A - dot(B, K))=poles.\n\n    K is the gain matrix such as the plant described by the linear system\n    ``AX+BU`` will have its closed-loop poles, i.e the eigenvalues ``A - B*K``,\n    as close as possible to those asked for in poles.\n\n    SISO, MISO and MIMO systems are supported.\n\n    Parameters\n    ----------\n    A, B : ndarray\n        State-space representation of linear system ``AX + BU``.\n    poles : array_like\n        Desired real poles and/or complex conjugates poles.\n        Complex poles are only supported with ``method=\"YT\"`` (default).\n    method: {'YT', 'KNV0'}, optional\n        Which method to choose to find the gain matrix K. One of:\n\n            - 'YT': Yang Tits\n            - 'KNV0': Kautsky, Nichols, Van Dooren update method 0\n\n        See References and Notes for details on the algorithms.\n    rtol: float, optional\n        After each iteration the determinant of the eigenvectors of\n        ``A - B*K`` is compared to its previous value, when the relative\n        error between these two values becomes lower than `rtol` the algorithm\n        stops.  Default is 1e-3.\n    maxiter: int, optional\n        Maximum number of iterations to compute the gain matrix.\n        Default is 30.\n\n    Returns\n    -------\n    full_state_feedback : Bunch object\n        full_state_feedback is composed of:\n            gain_matrix : 1-D ndarray\n                The closed loop matrix K such as the eigenvalues of ``A-BK``\n                are as close as possible to the requested poles.\n            computed_poles : 1-D ndarray\n                The poles corresponding to ``A-BK`` sorted as first the real\n                poles in increasing order, then the complex conjugates in\n                lexicographic order.\n            requested_poles : 1-D ndarray\n                The poles the algorithm was asked to place sorted as above,\n                they may differ from what was achieved.\n            X : 2-D ndarray\n                The transfer matrix such as ``X * diag(poles) = (A - B*K)*X``\n                (see Notes)\n            rtol : float\n                The relative tolerance achieved on ``det(X)`` (see Notes).\n                `rtol` will be NaN if it is possible to solve the system\n                ``diag(poles) = (A - B*K)``, or 0 when the optimization\n                algorithms can't do anything i.e when ``B.shape[1] == 1``.\n            nb_iter : int\n                The number of iterations performed before converging.\n                `nb_iter` will be NaN if it is possible to solve the system\n                ``diag(poles) = (A - B*K)``, or 0 when the optimization\n                algorithms can't do anything i.e when ``B.shape[1] == 1``.\n\n    Notes\n    -----\n    The Tits and Yang (YT), [2]_ paper is an update of the original Kautsky et\n    al. (KNV) paper [1]_.  KNV relies on rank-1 updates to find the transfer\n    matrix X such that ``X * diag(poles) = (A - B*K)*X``, whereas YT uses\n    rank-2 updates. This yields on average more robust solutions (see [2]_\n    pp 21-22), furthermore the YT algorithm supports complex poles whereas KNV\n    does not in its original version.  Only update method 0 proposed by KNV has\n    been implemented here, hence the name ``'KNV0'``.\n\n    KNV extended to complex poles is used in Matlab's ``place`` function, YT is\n    distributed under a non-free licence by Slicot under the name ``robpole``.\n    It is unclear and undocumented how KNV0 has been extended to complex poles\n    (Tits and Yang claim on page 14 of their paper that their method can not be\n    used to extend KNV to complex poles), therefore only YT supports them in\n    this implementation.\n\n    As the solution to the problem of pole placement is not unique for MIMO\n    systems, both methods start with a tentative transfer matrix which is\n    altered in various way to increase its determinant.  Both methods have been\n    proven to converge to a stable solution, however depending on the way the\n    initial transfer matrix is chosen they will converge to different\n    solutions and therefore there is absolutely no guarantee that using\n    ``'KNV0'`` will yield results similar to Matlab's or any other\n    implementation of these algorithms.\n\n    Using the default method ``'YT'`` should be fine in most cases; ``'KNV0'``\n    is only provided because it is needed by ``'YT'`` in some specific cases.\n    Furthermore ``'YT'`` gives on average more robust results than ``'KNV0'``\n    when ``abs(det(X))`` is used as a robustness indicator.\n\n    [2]_ is available as a technical report on the following URL:\n    https://hdl.handle.net/1903/5598\n\n    References\n    ----------\n    .. [1] J. Kautsky, N.K. Nichols and P. van Dooren, \"Robust pole assignment\n           in linear state feedback\", International Journal of Control, Vol. 41\n           pp. 1129-1155, 1985.\n    .. [2] A.L. Tits and Y. Yang, \"Globally convergent algorithms for robust\n           pole assignment by state feedback\", IEEE Transactions on Automatic\n           Control, Vol. 41, pp. 1432-1452, 1996.\n\n    Examples\n    --------\n    A simple example demonstrating real pole placement using both KNV and YT\n    algorithms.  This is example number 1 from section 4 of the reference KNV\n    publication ([1]_):\n\n    >>> import numpy as np\n    >>> from scipy import signal\n    >>> import matplotlib.pyplot as plt\n\n    >>> A = np.array([[ 1.380,  -0.2077,  6.715, -5.676  ],\n    ...               [-0.5814, -4.290,   0,      0.6750 ],\n    ...               [ 1.067,   4.273,  -6.654,  5.893  ],\n    ...               [ 0.0480,  4.273,   1.343, -2.104  ]])\n    >>> B = np.array([[ 0,      5.679 ],\n    ...               [ 1.136,  1.136 ],\n    ...               [ 0,      0,    ],\n    ...               [-3.146,  0     ]])\n    >>> P = np.array([-0.2, -0.5, -5.0566, -8.6659])\n\n    Now compute K with KNV method 0, with the default YT method and with the YT\n    method while forcing 100 iterations of the algorithm and print some results\n    after each call.\n\n    >>> fsf1 = signal.place_poles(A, B, P, method='KNV0')\n    >>> fsf1.gain_matrix\n    array([[ 0.20071427, -0.96665799,  0.24066128, -0.10279785],\n           [ 0.50587268,  0.57779091,  0.51795763, -0.41991442]])\n\n    >>> fsf2 = signal.place_poles(A, B, P)  # uses YT method\n    >>> fsf2.computed_poles\n    array([-8.6659, -5.0566, -0.5   , -0.2   ])\n\n    >>> fsf3 = signal.place_poles(A, B, P, rtol=-1, maxiter=100)\n    >>> fsf3.X\n    array([[ 0.52072442+0.j, -0.08409372+0.j, -0.56847937+0.j,  0.74823657+0.j],\n           [-0.04977751+0.j, -0.80872954+0.j,  0.13566234+0.j, -0.29322906+0.j],\n           [-0.82266932+0.j, -0.19168026+0.j, -0.56348322+0.j, -0.43815060+0.j],\n           [ 0.22267347+0.j,  0.54967577+0.j, -0.58387806+0.j, -0.40271926+0.j]])\n\n    The absolute value of the determinant of X is a good indicator to check the\n    robustness of the results, both ``'KNV0'`` and ``'YT'`` aim at maximizing\n    it.  Below a comparison of the robustness of the results above:\n\n    >>> abs(np.linalg.det(fsf1.X)) < abs(np.linalg.det(fsf2.X))\n    True\n    >>> abs(np.linalg.det(fsf2.X)) < abs(np.linalg.det(fsf3.X))\n    True\n\n    Now a simple example for complex poles:\n\n    >>> A = np.array([[ 0,  7/3.,  0,   0   ],\n    ...               [ 0,   0,    0,  7/9. ],\n    ...               [ 0,   0,    0,   0   ],\n    ...               [ 0,   0,    0,   0   ]])\n    >>> B = np.array([[ 0,  0 ],\n    ...               [ 0,  0 ],\n    ...               [ 1,  0 ],\n    ...               [ 0,  1 ]])\n    >>> P = np.array([-3, -1, -2-1j, -2+1j]) / 3.\n    >>> fsf = signal.place_poles(A, B, P, method='YT')\n\n    We can plot the desired and computed poles in the complex plane:\n\n    >>> t = np.linspace(0, 2*np.pi, 401)\n    >>> plt.plot(np.cos(t), np.sin(t), 'k--')  # unit circle\n    >>> plt.plot(fsf.requested_poles.real, fsf.requested_poles.imag,\n    ...          'wo', label='Desired')\n    >>> plt.plot(fsf.computed_poles.real, fsf.computed_poles.imag, 'bx',\n    ...          label='Placed')\n    >>> plt.grid()\n    >>> plt.axis('image')\n    >>> plt.axis([-1.1, 1.1, -1.1, 1.1])\n    >>> plt.legend(bbox_to_anchor=(1.05, 1), loc=2, numpoints=1)\n\n    \"\"\""}], "after_segments": [{"filename": "scipy/signal/_ltisys.py", "start_line": 50, "code": "    def __new__(cls, *system, **kwargs):\n        if cls is LinearTimeInvariant:\n            raise NotImplementedError('The LinearTimeInvariant class is not '\n                                      'meant to be used directly, use `lti` '\n                                      'or `dlti` instead.')\n        return super().__new__(cls)", "documentation": "        \"\"\"Create a new object, don't allow direct instances.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 58, "code": "    def __init__(self):\n        super().__init__()\n        self.inputs = None\n        self.outputs = None\n        self._dt = None\n    @property", "documentation": "        \"\"\"\n        Initialize the `lti` baseclass.\n\n        The heavy lifting is done by the subclasses.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 71, "code": "    def dt(self):\n        return self._dt\n    @property", "documentation": "        \"\"\"Return the sampling time of the system, `None` for `lti` systems.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 83, "code": "    def zeros(self):\n        return self.to_zpk().zeros\n    @property", "documentation": "        \"\"\"Zeros of the system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 88, "code": "    def poles(self):\n        return self.to_zpk().poles", "documentation": "        \"\"\"Poles of the system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 92, "code": "    def _as_ss(self):\n        if isinstance(self, StateSpace):\n            return self\n        else:\n            return self.to_ss()", "documentation": "        \"\"\"Convert to `StateSpace` system, without copying.\n\n        Returns\n        -------\n        sys: StateSpace\n            The `StateSpace` system. If the class is already an instance of\n            `StateSpace` then this instance is returned.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 106, "code": "    def _as_zpk(self):\n        if isinstance(self, ZerosPolesGain):\n            return self\n        else:\n            return self.to_zpk()", "documentation": "        \"\"\"Convert to `ZerosPolesGain` system, without copying.\n\n        Returns\n        -------\n        sys: ZerosPolesGain\n            The `ZerosPolesGain` system. If the class is already an instance of\n            `ZerosPolesGain` then this instance is returned.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 120, "code": "    def _as_tf(self):\n        if isinstance(self, TransferFunction):\n            return self\n        else:\n            return self.to_tf()", "documentation": "        \"\"\"Convert to `TransferFunction` system, without copying.\n\n        Returns\n        -------\n        sys: ZerosPolesGain\n            The `TransferFunction` system. If the class is already an instance of\n            `TransferFunction` then this instance is returned.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 207, "code": "    def __new__(cls, *system):\n        if cls is lti:\n            N = len(system)\n            if N == 2:\n                return TransferFunctionContinuous.__new__(\n                    TransferFunctionContinuous, *system)\n            elif N == 3:\n                return ZerosPolesGainContinuous.__new__(\n                    ZerosPolesGainContinuous, *system)\n            elif N == 4:\n                return StateSpaceContinuous.__new__(StateSpaceContinuous,", "documentation": "        \"\"\"Create an instance of the appropriate subclass.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 226, "code": "    def __init__(self, *system):\n        super().__init__(*system)", "documentation": "        \"\"\"\n        Initialize the `lti` baseclass.\n\n        The heavy lifting is done by the subclasses.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 234, "code": "    def impulse(self, X0=None, T=None, N=None):\n        return impulse(self, X0=X0, T=T, N=N)", "documentation": "        \"\"\"\n        Return the impulse response of a continuous-time system.\n        See `impulse` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 241, "code": "    def step(self, X0=None, T=None, N=None):\n        return step(self, X0=X0, T=T, N=N)", "documentation": "        \"\"\"\n        Return the step response of a continuous-time system.\n        See `step` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 248, "code": "    def output(self, U, T, X0=None):\n        return lsim(self, U, T, X0=X0)", "documentation": "        \"\"\"\n        Return the response of a continuous-time system to input `U`.\n        See `lsim` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 255, "code": "    def bode(self, w=None, n=100):\n        return bode(self, w=w, n=n)", "documentation": "        \"\"\"\n        Calculate Bode magnitude and phase data of a continuous-time system.\n\n        Returns a 3-tuple containing arrays of frequencies [rad/s], magnitude\n        [dB] and phase [deg]. See `bode` for details.\n\n        Examples\n        --------\n        >>> from scipy import signal\n        >>> import matplotlib.pyplot as plt\n\n        >>> sys = signal.TransferFunction([1], [1, 1])\n        >>> w, mag, phase = sys.bode()\n\n        >>> plt.figure()\n        >>> plt.semilogx(w, mag)    # Bode magnitude plot\n        >>> plt.figure()\n        >>> plt.semilogx(w, phase)  # Bode phase plot\n        >>> plt.show()\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 279, "code": "    def freqresp(self, w=None, n=10000):\n        return freqresp(self, w=w, n=n)", "documentation": "        \"\"\"\n        Calculate the frequency response of a continuous-time system.\n\n        Returns a 2-tuple containing arrays of frequencies [rad/s] and\n        complex magnitude.\n        See `freqresp` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 289, "code": "    def to_discrete(self, dt, method='zoh', alpha=None):\n        raise NotImplementedError('to_discrete is not implemented for this '\n                                  'system class.')", "documentation": "        \"\"\"Return a discretized version of the current system.\n\n        Parameters: See `cont2discrete` for details.\n\n        Returns\n        -------\n        sys: instance of `dlti`\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 391, "code": "    def __new__(cls, *system, **kwargs):\n        if cls is dlti:\n            N = len(system)\n            if N == 2:\n                return TransferFunctionDiscrete.__new__(\n                    TransferFunctionDiscrete, *system, **kwargs)\n            elif N == 3:\n                return ZerosPolesGainDiscrete.__new__(ZerosPolesGainDiscrete,\n                                                      *system, **kwargs)\n            elif N == 4:\n                return StateSpaceDiscrete.__new__(StateSpaceDiscrete, *system,", "documentation": "        \"\"\"Create an instance of the appropriate subclass.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 410, "code": "    def __init__(self, *system, **kwargs):\n        dt = kwargs.pop('dt', True)\n        super().__init__(*system, **kwargs)\n        self.dt = dt\n    @property", "documentation": "        \"\"\"\n        Initialize the `lti` baseclass.\n\n        The heavy lifting is done by the subclasses.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 422, "code": "    def dt(self):\n        return self._dt\n    @dt.setter", "documentation": "        \"\"\"Return the sampling time of the system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 430, "code": "    def impulse(self, x0=None, t=None, n=None):\n        return dimpulse(self, x0=x0, t=t, n=n)", "documentation": "        \"\"\"\n        Return the impulse response of the discrete-time `dlti` system.\n        See `dimpulse` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 437, "code": "    def step(self, x0=None, t=None, n=None):\n        return dstep(self, x0=x0, t=t, n=n)", "documentation": "        \"\"\"\n        Return the step response of the discrete-time `dlti` system.\n        See `dstep` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 444, "code": "    def output(self, u, t, x0=None):\n        return dlsim(self, u, t, x0=x0)", "documentation": "        \"\"\"\n        Return the response of the discrete-time system to input `u`.\n        See `dlsim` for details.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 481, "code": "    def freqresp(self, w=None, n=10000, whole=False):\n        return dfreqresp(self, w=w, n=n, whole=whole)", "documentation": "        \"\"\"\n        Calculate the frequency response of a discrete-time system.\n\n        Returns a 2-tuple containing arrays of frequencies [rad/s] and\n        complex magnitude.\n        See `dfreqresp` for details.\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 569, "code": "    def __new__(cls, *system, **kwargs):\n        if len(system) == 1 and isinstance(system[0], LinearTimeInvariant):\n            return system[0].to_tf()\n        if cls is TransferFunction:\n            if kwargs.get('dt') is None:\n                return TransferFunctionContinuous.__new__(\n                    TransferFunctionContinuous,\n                    *system,\n                    **kwargs)\n            else:\n                return TransferFunctionDiscrete.__new__(", "documentation": "        \"\"\"Handle object conversion if input is an instance of lti.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 590, "code": "    def __init__(self, *system, **kwargs):\n        if isinstance(system[0], LinearTimeInvariant):\n            return\n        super().__init__(**kwargs)\n        self._num = None\n        self._den = None\n        self.num, self.den = normalize(*system)", "documentation": "        \"\"\"Initialize the state space LTI system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 604, "code": "    def __repr__(self):\n        return (\n            f'{self.__class__.__name__}(\\n'\n            f'{repr(self.num)},\\n'\n            f'{repr(self.den)},\\n'\n            f'dt: {repr(self.dt)}\\n)'\n        )\n    @property", "documentation": "        \"\"\"Return representation of the system's transfer function\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 614, "code": "    def num(self):\n        return self._num\n    @num.setter", "documentation": "        \"\"\"Numerator of the `TransferFunction` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 630, "code": "    def den(self):\n        return self._den\n    @den.setter", "documentation": "        \"\"\"Denominator of the `TransferFunction` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 638, "code": "    def _copy(self, system):\n        self.num = system.num\n        self.den = system.den", "documentation": "        \"\"\"\n        Copy the parameters of another `TransferFunction` object\n\n        Parameters\n        ----------\n        system : `TransferFunction`\n            The `StateSpace` system that is to be copied\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 651, "code": "    def to_tf(self):\n        return copy.deepcopy(self)", "documentation": "        \"\"\"\n        Return a copy of the current `TransferFunction` system.\n\n        Returns\n        -------\n        sys : instance of `TransferFunction`\n            The current system (copy)\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 663, "code": "    def to_zpk(self):\n        return ZerosPolesGain(*tf2zpk(self.num, self.den),\n                              **self._dt_dict)", "documentation": "        \"\"\"\n        Convert system representation to `ZerosPolesGain`.\n\n        Returns\n        -------\n        sys : instance of `ZerosPolesGain`\n            Zeros, poles, gain representation of the current system\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 676, "code": "    def to_ss(self):\n        return StateSpace(*tf2ss(self.num, self.den),\n                          **self._dt_dict)\n    @staticmethod", "documentation": "        \"\"\"\n        Convert system representation to `StateSpace`.\n\n        Returns\n        -------\n        sys : instance of `StateSpace`\n            State space model of the current system\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 690, "code": "    def _z_to_zinv(num, den):\n        diff = len(num) - len(den)\n        if diff > 0:\n            den = np.hstack((np.zeros(diff), den))\n        elif diff < 0:\n            num = np.hstack((np.zeros(-diff), num))\n        return num, den\n    @staticmethod", "documentation": "        \"\"\"Change a transfer function from the variable `z` to `z**-1`.\n\n        Parameters\n        ----------\n        num, den: 1d array_like\n            Sequences representing the coefficients of the numerator and\n            denominator polynomials, in order of descending degree of 'z'.\n            That is, ``5z**2 + 3z + 2`` is presented as ``[5, 3, 2]``.\n\n        Returns\n        -------\n        num, den: 1d array_like\n            Sequences representing the coefficients of the numerator and\n            denominator polynomials, in order of ascending degree of 'z**-1'.\n            That is, ``5 + 3 z**-1 + 2 z**-2`` is presented as ``[5, 3, 2]``.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 715, "code": "    def _zinv_to_z(num, den):\n        diff = len(num) - len(den)\n        if diff > 0:\n            den = np.hstack((den, np.zeros(diff)))\n        elif diff < 0:\n            num = np.hstack((num, np.zeros(-diff)))\n        return num, den", "documentation": "        \"\"\"Change a transfer function from the variable `z` to `z**-1`.\n\n        Parameters\n        ----------\n        num, den: 1d array_like\n            Sequences representing the coefficients of the numerator and\n            denominator polynomials, in order of ascending degree of 'z**-1'.\n            That is, ``5 + 3 z**-1 + 2 z**-2`` is presented as ``[5, 3, 2]``.\n\n        Returns\n        -------\n        num, den: 1d array_like\n            Sequences representing the coefficients of the numerator and\n            denominator polynomials, in order of descending degree of 'z'.\n            That is, ``5z**2 + 3z + 2`` is presented as ``[5, 3, 2]``.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 800, "code": "    def to_discrete(self, dt, method='zoh', alpha=None):\n        return TransferFunction(*cont2discrete((self.num, self.den),\n                                               dt,\n                                               method=method,\n                                               alpha=alpha)[:-1],\n                                dt=dt)", "documentation": "        \"\"\"\n        Returns the discretized `TransferFunction` system.\n\n        Parameters: See `cont2discrete` for details.\n\n        Returns\n        -------\n        sys: instance of `dlti` and `StateSpace`\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 951, "code": "    def __new__(cls, *system, **kwargs):\n        if len(system) == 1 and isinstance(system[0], LinearTimeInvariant):\n            return system[0].to_zpk()\n        if cls is ZerosPolesGain:\n            if kwargs.get('dt') is None:\n                return ZerosPolesGainContinuous.__new__(\n                    ZerosPolesGainContinuous,\n                    *system,\n                    **kwargs)\n            else:\n                return ZerosPolesGainDiscrete.__new__(", "documentation": "        \"\"\"Handle object conversion if input is an instance of `lti`\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 973, "code": "    def __init__(self, *system, **kwargs):\n        if isinstance(system[0], LinearTimeInvariant):\n            return\n        super().__init__(**kwargs)\n        self._zeros = None\n        self._poles = None\n        self._gain = None\n        self.zeros, self.poles, self.gain = system", "documentation": "        \"\"\"Initialize the zeros, poles, gain system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 987, "code": "    def __repr__(self):\n        return (\n            f'{self.__class__.__name__}(\\n'\n            f'{repr(self.zeros)},\\n'\n            f'{repr(self.poles)},\\n'\n            f'{repr(self.gain)},\\n'\n            f'dt: {repr(self.dt)}\\n)'\n        )\n    @property", "documentation": "        \"\"\"Return representation of the `ZerosPolesGain` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 998, "code": "    def zeros(self):\n        return self._zeros\n    @zeros.setter", "documentation": "        \"\"\"Zeros of the `ZerosPolesGain` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1014, "code": "    def poles(self):\n        return self._poles\n    @poles.setter", "documentation": "        \"\"\"Poles of the `ZerosPolesGain` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1023, "code": "    def gain(self):\n        return self._gain\n    @gain.setter", "documentation": "        \"\"\"Gain of the `ZerosPolesGain` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1031, "code": "    def _copy(self, system):\n        self.poles = system.poles\n        self.zeros = system.zeros\n        self.gain = system.gain", "documentation": "        \"\"\"\n        Copy the parameters of another `ZerosPolesGain` system.\n\n        Parameters\n        ----------\n        system : instance of `ZerosPolesGain`\n            The zeros, poles gain system that is to be copied\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1045, "code": "    def to_tf(self):\n        return TransferFunction(*zpk2tf(self.zeros, self.poles, self.gain),\n                                **self._dt_dict)", "documentation": "        \"\"\"\n        Convert system representation to `TransferFunction`.\n\n        Returns\n        -------\n        sys : instance of `TransferFunction`\n            Transfer function of the current system\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1058, "code": "    def to_zpk(self):\n        return copy.deepcopy(self)", "documentation": "        \"\"\"\n        Return a copy of the current 'ZerosPolesGain' system.\n\n        Returns\n        -------\n        sys : instance of `ZerosPolesGain`\n            The current system (copy)\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1070, "code": "    def to_ss(self):\n        return StateSpace(*zpk2ss(self.zeros, self.poles, self.gain),\n                          **self._dt_dict)", "documentation": "        \"\"\"\n        Convert system representation to `StateSpace`.\n\n        Returns\n        -------\n        sys : instance of `StateSpace`\n            State space model of the current system\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1136, "code": "    def to_discrete(self, dt, method='zoh', alpha=None):\n        return ZerosPolesGain(\n            *cont2discrete((self.zeros, self.poles, self.gain),\n                           dt,\n                           method=method,\n                           alpha=alpha)[:-1],\n            dt=dt)", "documentation": "        \"\"\"\n        Returns the discretized `ZerosPolesGain` system.\n\n        Parameters: See `cont2discrete` for details.\n\n        Returns\n        -------\n        sys: instance of `dlti` and `ZerosPolesGain`\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1325, "code": "    def __new__(cls, *system, **kwargs):\n        if len(system) == 1 and isinstance(system[0], LinearTimeInvariant):\n            return system[0].to_ss()\n        if cls is StateSpace:\n            if kwargs.get('dt') is None:\n                return StateSpaceContinuous.__new__(StateSpaceContinuous,\n                                                    *system, **kwargs)\n            else:\n                return StateSpaceDiscrete.__new__(StateSpaceDiscrete,\n                                                  *system, **kwargs)\n        return super().__new__(cls)", "documentation": "        \"\"\"Create new StateSpace object and settle inheritance.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1343, "code": "    def __init__(self, *system, **kwargs):\n        if isinstance(system[0], LinearTimeInvariant):\n            return\n        super().__init__(**kwargs)\n        self._A = None\n        self._B = None\n        self._C = None\n        self._D = None\n        self.A, self.B, self.C, self.D = abcd_normalize(*system)", "documentation": "        \"\"\"Initialize the state space lti/dlti system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1360, "code": "    def __repr__(self):\n        return (\n            f'{self.__class__.__name__}(\\n'\n            f'{repr(self.A)},\\n'\n            f'{repr(self.B)},\\n'\n            f'{repr(self.C)},\\n'\n            f'{repr(self.D)},\\n'\n            f'dt: {repr(self.dt)}\\n)'\n        )", "documentation": "        \"\"\"Return representation of the `StateSpace` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1375, "code": "    def __mul__(self, other):\n        if not self._check_binop_other(other):\n            return NotImplemented\n        if isinstance(other, StateSpace):\n            if type(other) is not type(self):\n                return NotImplemented\n            if self.dt != other.dt:\n                raise TypeError('Cannot multiply systems with different `dt`.')\n            n1 = self.A.shape[0]\n            n2 = other.A.shape[0]\n            a = np.vstack((np.hstack((self.A, np.dot(self.B, other.C))),", "documentation": "        \"\"\"\n        Post-multiply another system or a scalar\n\n        Handles multiplication of systems in the sense of a frequency domain\n        multiplication. That means, given two systems E1(s) and E2(s), their\n        multiplication, H(s) = E1(s) * E2(s), means that applying H(s) to U(s)\n        is equivalent to first applying E2(s), and then E1(s).\n\n        Notes\n        -----\n        For SISO systems the order of system application does not matter.\n        However, for MIMO systems, where the two systems are matrices, the\n        order above ensures standard Matrix multiplication rules apply.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1435, "code": "    def __rmul__(self, other):\n        if not self._check_binop_other(other) or isinstance(other, StateSpace):\n            return NotImplemented\n        a = self.A\n        b = self.B\n        c = np.dot(other, self.C)\n        d = np.dot(other, self.D)\n        common_dtype = np.result_type(a.dtype, b.dtype, c.dtype, d.dtype)\n        return StateSpace(np.asarray(a, dtype=common_dtype),\n                          np.asarray(b, dtype=common_dtype),\n                          np.asarray(c, dtype=common_dtype),", "documentation": "        \"\"\"Pre-multiply a scalar or matrix (but not StateSpace)\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1453, "code": "    def __neg__(self):\n        return StateSpace(self.A, self.B, -self.C, -self.D, **self._dt_dict)", "documentation": "        \"\"\"Negate the system (equivalent to pre-multiplying by -1).\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1457, "code": "    def __add__(self, other):\n        if not self._check_binop_other(other):\n            return NotImplemented\n        if isinstance(other, StateSpace):\n            if type(other) is not type(self):\n                raise TypeError(f'Cannot add {type(self)} and {type(other)}')\n            if self.dt != other.dt:\n                raise TypeError('Cannot add systems with different `dt`.')\n            a = linalg.block_diag(self.A, other.A)\n            b = np.vstack((self.B, other.B))\n            c = np.hstack((self.C, other.C))", "documentation": "        \"\"\"\n        Adds two systems in the sense of frequency domain addition.\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1524, "code": "    def __truediv__(self, other):\n        if not self._check_binop_other(other) or isinstance(other, StateSpace):\n            return NotImplemented\n        if isinstance(other, np.ndarray) and other.ndim > 0:\n            raise ValueError(\"Cannot divide StateSpace by non-scalar numpy arrays\")\n        return self.__mul__(1/other)\n    @property", "documentation": "        \"\"\"\n        Divide by a scalar\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1539, "code": "    def A(self):\n        return self._A\n    @A.setter", "documentation": "        \"\"\"State matrix of the `StateSpace` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1548, "code": "    def B(self):\n        return self._B\n    @B.setter", "documentation": "        \"\"\"Input matrix of the `StateSpace` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1558, "code": "    def C(self):\n        return self._C\n    @C.setter", "documentation": "        \"\"\"Output matrix of the `StateSpace` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1568, "code": "    def D(self):\n        return self._D\n    @D.setter", "documentation": "        \"\"\"Feedthrough matrix of the `StateSpace` system.\"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1576, "code": "    def _copy(self, system):\n        self.A = system.A\n        self.B = system.B\n        self.C = system.C\n        self.D = system.D", "documentation": "        \"\"\"\n        Copy the parameters of another `StateSpace` system.\n\n        Parameters\n        ----------\n        system : instance of `StateSpace`\n            The state-space system that is to be copied\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1591, "code": "    def to_tf(self, **kwargs):\n        return TransferFunction(*ss2tf(self._A, self._B, self._C, self._D,\n                                       **kwargs), **self._dt_dict)", "documentation": "        \"\"\"\n        Convert system representation to `TransferFunction`.\n\n        Parameters\n        ----------\n        kwargs : dict, optional\n            Additional keywords passed to `ss2zpk`\n\n        Returns\n        -------\n        sys : instance of `TransferFunction`\n            Transfer function of the current system\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1609, "code": "    def to_zpk(self, **kwargs):\n        return ZerosPolesGain(*ss2zpk(self._A, self._B, self._C, self._D,\n                                      **kwargs), **self._dt_dict)", "documentation": "        \"\"\"\n        Convert system representation to `ZerosPolesGain`.\n\n        Parameters\n        ----------\n        kwargs : dict, optional\n            Additional keywords passed to `ss2zpk`\n\n        Returns\n        -------\n        sys : instance of `ZerosPolesGain`\n            Zeros, poles, gain representation of the current system\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1627, "code": "    def to_ss(self):\n        return copy.deepcopy(self)", "documentation": "        \"\"\"\n        Return a copy of the current `StateSpace` system.\n\n        Returns\n        -------\n        sys : instance of `StateSpace`\n            The current system (copy)\n\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1697, "code": "    def to_discrete(self, dt, method='zoh', alpha=None):\n        return StateSpace(*cont2discrete((self.A, self.B, self.C, self.D),\n                                         dt,\n                                         method=method,\n                                         alpha=alpha)[:-1],\n                          dt=dt)", "documentation": "        \"\"\"\n        Returns the discretized `StateSpace` system.\n\n        Parameters: See `cont2discrete` for details.\n\n        Returns\n        -------\n        sys: instance of `dlti` and `StateSpace`\n        \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1776, "code": "def lsim(system, U, T, X0=None, interp=True):\n    if isinstance(system, lti):\n        sys = system._as_ss()\n    elif isinstance(system, dlti):\n        raise AttributeError('lsim can only be used with continuous-time '\n                             'systems.')\n    else:\n        sys = lti(*system)._as_ss()\n    T = atleast_1d(T)\n    if len(T.shape) != 1:\n        raise ValueError(\"T must be a rank-1 array.\")", "documentation": "    \"\"\"\n    Simulate output of a continuous-time linear system.\n\n    Parameters\n    ----------\n    system : an instance of the LTI class or a tuple describing the system.\n        The following gives the number of elements in the tuple and\n        the interpretation:\n\n        * 1: (instance of `lti`)\n        * 2: (num, den)\n        * 3: (zeros, poles, gain)\n        * 4: (A, B, C, D)\n\n    U : array_like\n        An input array describing the input at each time `T`\n        (interpolation is assumed between given times).  If there are\n        multiple inputs, then each column of the rank-2 array\n        represents an input.  If U = 0 or None, a zero input is used.\n    T : array_like\n        The time steps at which the input is defined and at which the\n        output is desired.  Must be nonnegative, increasing, and equally spaced.\n    X0 : array_like, optional\n        The initial conditions on the state vector (zero by default).\n    interp : bool, optional\n        Whether to use linear (True, the default) or zero-order-hold (False)\n        interpolation for the input array.\n\n    Returns\n    -------\n    T : 1D ndarray\n        Time values for the output.\n    yout : 1D ndarray\n        System response.\n    xout : ndarray\n        Time evolution of the state vector.\n\n    Notes\n    -----\n    If (num, den) is passed in for ``system``, coefficients for both the\n    numerator and denominator should be specified in descending exponent\n    order (e.g. ``s^2 + 3s + 5`` would be represented as ``[1, 3, 5]``).\n\n    Examples\n    --------\n    We'll use `lsim` to simulate an analog Bessel filter applied to\n    a signal.\n\n    >>> import numpy as np\n    >>> from scipy.signal import bessel, lsim\n    >>> import matplotlib.pyplot as plt\n\n    Create a low-pass Bessel filter with a cutoff of 12 Hz.\n\n    >>> b, a = bessel(N=5, Wn=2*np.pi*12, btype='lowpass', analog=True)\n\n    Generate data to which the filter is applied.\n\n    >>> t = np.linspace(0, 1.25, 500, endpoint=False)\n\n    The input signal is the sum of three sinusoidal curves, with\n    frequencies 4 Hz, 40 Hz, and 80 Hz.  The filter should mostly\n    eliminate the 40 Hz and 80 Hz components, leaving just the 4 Hz signal.\n\n    >>> u = (np.cos(2*np.pi*4*t) + 0.6*np.sin(2*np.pi*40*t) +\n    ...      0.5*np.cos(2*np.pi*80*t))\n\n    Simulate the filter with `lsim`.\n\n    >>> tout, yout, xout = lsim((b, a), U=u, T=t)\n\n    Plot the result.\n\n    >>> plt.plot(t, u, 'r', alpha=0.5, linewidth=1, label='input')\n    >>> plt.plot(tout, yout, 'k', linewidth=1.5, label='output')\n    >>> plt.legend(loc='best', shadow=True, framealpha=1)\n    >>> plt.grid(alpha=0.3)\n    >>> plt.xlabel('t')\n    >>> plt.show()\n\n    In a second example, we simulate a double integrator ``y'' = u``, with\n    a constant input ``u = 1``.  We'll use the state space representation\n    of the integrator.\n\n    >>> from scipy.signal import lti\n    >>> A = np.array([[0.0, 1.0], [0.0, 0.0]])\n    >>> B = np.array([[0.0], [1.0]])\n    >>> C = np.array([[1.0, 0.0]])\n    >>> D = 0.0\n    >>> system = lti(A, B, C, D)\n\n    `t` and `u` define the time and input signal for the system to\n    be simulated.\n\n    >>> t = np.linspace(0, 5, num=50)\n    >>> u = np.ones_like(t)\n\n    Compute the simulation, and then plot `y`.  As expected, the plot shows\n    the curve ``y = 0.5*t**2``.\n\n    >>> tout, y, x = lsim(system, u, t)\n    >>> plt.plot(t, y)\n    >>> plt.grid(alpha=0.3)\n    >>> plt.xlabel('t')\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 1991, "code": "def _default_response_times(A, n):\n    vals = linalg.eigvals(A)\n    r = min(abs(real(vals)))\n    if r == 0.0:\n        r = 1.0\n    tc = 1.0 / r\n    t = linspace(0.0, 7 * tc, n)\n    return t", "documentation": "    \"\"\"Compute a reasonable set of time samples for the response time.\n\n    This function is used by `impulse` and `step`  to compute the response time\n    when the `T` argument to the function is None.\n\n    Parameters\n    ----------\n    A : array_like\n        The system matrix, which is square.\n    n : int\n        The number of time samples to generate.\n\n    Returns\n    -------\n    t : ndarray\n        The 1-D array of length `n` of time samples at which the response\n        is to be computed.\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2022, "code": "def impulse(system, X0=None, T=None, N=None):\n    if isinstance(system, lti):\n        sys = system._as_ss()\n    elif isinstance(system, dlti):\n        raise AttributeError('impulse can only be used with continuous-time '\n                             'systems.')\n    else:\n        sys = lti(*system)._as_ss()\n    if X0 is None:\n        X = squeeze(sys.B)\n    else:", "documentation": "    \"\"\"Impulse response of continuous-time system.\n\n    Parameters\n    ----------\n    system : an instance of the LTI class or a tuple of array_like\n        describing the system.\n        The following gives the number of elements in the tuple and\n        the interpretation:\n\n            * 1 (instance of `lti`)\n            * 2 (num, den)\n            * 3 (zeros, poles, gain)\n            * 4 (A, B, C, D)\n\n    X0 : array_like, optional\n        Initial state-vector.  Defaults to zero.\n    T : array_like, optional\n        Time points.  Computed if not given.\n    N : int, optional\n        The number of time points to compute (if `T` is not given).\n\n    Returns\n    -------\n    T : ndarray\n        A 1-D array of time points.\n    yout : ndarray\n        A 1-D array containing the impulse response of the system (except for\n        singularities at zero).\n\n    Notes\n    -----\n    If (num, den) is passed in for ``system``, coefficients for both the\n    numerator and denominator should be specified in descending exponent\n    order (e.g. ``s^2 + 3s + 5`` would be represented as ``[1, 3, 5]``).\n\n    Examples\n    --------\n    Compute the impulse response of a second order system with a repeated\n    root: ``x''(t) + 2*x'(t) + x(t) = u(t)``\n\n    >>> from scipy import signal\n    >>> system = ([1.0], [1.0, 2.0, 1.0])\n    >>> t, y = signal.impulse(system)\n    >>> import matplotlib.pyplot as plt\n    >>> plt.plot(t, y)\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2092, "code": "def step(system, X0=None, T=None, N=None):\n    if isinstance(system, lti):\n        sys = system._as_ss()\n    elif isinstance(system, dlti):\n        raise AttributeError('step can only be used with continuous-time '\n                             'systems.')\n    else:\n        sys = lti(*system)._as_ss()\n    if N is None:\n        N = 100\n    if T is None:", "documentation": "    \"\"\"Step response of continuous-time system.\n\n    Parameters\n    ----------\n    system : an instance of the LTI class or a tuple of array_like\n        describing the system.\n        The following gives the number of elements in the tuple and\n        the interpretation:\n\n            * 1 (instance of `lti`)\n            * 2 (num, den)\n            * 3 (zeros, poles, gain)\n            * 4 (A, B, C, D)\n\n    X0 : array_like, optional\n        Initial state-vector (default is zero).\n    T : array_like, optional\n        Time points (computed if not given).\n    N : int, optional\n        Number of time points to compute if `T` is not given.\n\n    Returns\n    -------\n    T : 1D ndarray\n        Output time points.\n    yout : 1D ndarray\n        Step response of system.\n\n\n    Notes\n    -----\n    If (num, den) is passed in for ``system``, coefficients for both the\n    numerator and denominator should be specified in descending exponent\n    order (e.g. ``s^2 + 3s + 5`` would be represented as ``[1, 3, 5]``).\n\n    Examples\n    --------\n    >>> from scipy import signal\n    >>> import matplotlib.pyplot as plt\n    >>> lti = signal.lti([1.0], [1.0, 1.0])\n    >>> t, y = signal.step(lti)\n    >>> plt.plot(t, y)\n    >>> plt.xlabel('Time [s]')\n    >>> plt.ylabel('Amplitude')\n    >>> plt.title('Step response for 1. Order Lowpass')\n    >>> plt.grid()\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2159, "code": "def bode(system, w=None, n=100):\n    w, y = freqresp(system, w=w, n=n)\n    mag = 20.0 * np.log10(abs(y))\n    phase = np.unwrap(np.arctan2(y.imag, y.real)) * 180.0 / np.pi\n    return w, mag, phase", "documentation": "    \"\"\"\n    Calculate Bode magnitude and phase data of a continuous-time system.\n\n    Parameters\n    ----------\n    system : an instance of the LTI class or a tuple describing the system.\n        The following gives the number of elements in the tuple and\n        the interpretation:\n\n            * 1 (instance of `lti`)\n            * 2 (num, den)\n            * 3 (zeros, poles, gain)\n            * 4 (A, B, C, D)\n\n    w : array_like, optional\n        Array of frequencies (in rad/s). Magnitude and phase data is calculated\n        for every value in this array. If not given a reasonable set will be\n        calculated.\n    n : int, optional\n        Number of frequency points to compute if `w` is not given. The `n`\n        frequencies are logarithmically spaced in an interval chosen to\n        include the influence of the poles and zeros of the system.\n\n    Returns\n    -------\n    w : 1D ndarray\n        Frequency array [rad/s]\n    mag : 1D ndarray\n        Magnitude array [dB]\n    phase : 1D ndarray\n        Phase array [deg]\n\n    Notes\n    -----\n    If (num, den) is passed in for ``system``, coefficients for both the\n    numerator and denominator should be specified in descending exponent\n    order (e.g. ``s^2 + 3s + 5`` would be represented as ``[1, 3, 5]``).\n\n    .. versionadded:: 0.11.0\n\n    Examples\n    --------\n    >>> from scipy import signal\n    >>> import matplotlib.pyplot as plt\n\n    >>> sys = signal.TransferFunction([1], [1, 1])\n    >>> w, mag, phase = signal.bode(sys)\n\n    >>> plt.figure()\n    >>> plt.semilogx(w, mag)    # Bode magnitude plot\n    >>> plt.figure()\n    >>> plt.semilogx(w, phase)  # Bode phase plot\n    >>> plt.show()\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2315, "code": "def _valid_inputs(A, B, poles, method, rtol, maxiter):\n    poles = np.asarray(poles)\n    if poles.ndim > 1:\n        raise ValueError(\"Poles must be a 1D array like.\")\n    poles = _order_complex_poles(poles)\n    if A.ndim > 2:\n        raise ValueError(\"A must be a 2D array/matrix.\")\n    if B.ndim > 2:\n        raise ValueError(\"B must be a 2D array/matrix\")\n    if A.shape[0] != A.shape[1]:\n        raise ValueError(\"A must be square\")", "documentation": "    \"\"\"\n    Check the poles come in complex conjugate pairs\n    Check shapes of A, B and poles are compatible.\n    Check the method chosen is compatible with provided poles\n    Return update method to use and ordered poles\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2368, "code": "def _order_complex_poles(poles):\n    ordered_poles = np.sort(poles[np.isreal(poles)])\n    im_poles = []\n    for p in np.sort(poles[np.imag(poles) < 0]):\n        if np.conj(p) in poles:\n            im_poles.extend((p, np.conj(p)))\n    ordered_poles = np.hstack((ordered_poles, im_poles))\n    if poles.shape[0] != len(ordered_poles):\n        raise ValueError(\"Complex poles must come with their conjugates\")\n    return ordered_poles", "documentation": "    \"\"\"\n    Check we have complex conjugates pairs and reorder P according to YT, ie\n    real_poles, complex_i, conjugate complex_i, ....\n    The lexicographic sort on the complex poles is added to help the user to\n    compare sets of poles.\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2388, "code": "def _KNV0(B, ker_pole, transfer_matrix, j, poles):\n    transfer_matrix_not_j = np.delete(transfer_matrix, j, axis=1)\n    Q, R = s_qr(transfer_matrix_not_j, mode=\"full\")\n    mat_ker_pj = np.dot(ker_pole[j], ker_pole[j].T)\n    yj = np.dot(mat_ker_pj, Q[:, -1])\n    if not np.allclose(yj, 0):\n        xj = yj/np.linalg.norm(yj)\n        transfer_matrix[:, j] = xj", "documentation": "    \"\"\"\n    Algorithm \"KNV0\" Kautsky et Al. Robust pole\n    assignment in linear state feedback, Int journal of Control\n    1985, vol 41 p 1129->1155\n    https://la.epfl.ch/files/content/sites/la/files/\n        users/105941/public/KautskyNicholsDooren\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2435, "code": "def _YT_real(ker_pole, Q, transfer_matrix, i, j):\n    u = Q[:, -2, np.newaxis]\n    v = Q[:, -1, np.newaxis]\n    m = np.dot(np.dot(ker_pole[i].T, np.dot(u, v.T) -\n        np.dot(v, u.T)), ker_pole[j])\n    um, sm, vm = np.linalg.svd(m)\n    mu1, mu2 = um.T[:2, :, np.newaxis]\n    nu1, nu2 = vm[:2, :, np.newaxis]\n    transfer_matrix_j_mo_transfer_matrix_j = np.vstack((\n            transfer_matrix[:, i, np.newaxis],\n            transfer_matrix[:, j, np.newaxis]))", "documentation": "    \"\"\"\n    Applies algorithm from YT section 6.1 page 19 related to real pairs\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2500, "code": "def _YT_complex(ker_pole, Q, transfer_matrix, i, j):\n    ur = np.sqrt(2)*Q[:, -2, np.newaxis]\n    ui = np.sqrt(2)*Q[:, -1, np.newaxis]\n    u = ur + 1j*ui\n    ker_pole_ij = ker_pole[i]\n    m = np.dot(np.dot(np.conj(ker_pole_ij.T), np.dot(u, np.conj(u).T) -\n               np.dot(np.conj(u), u.T)), ker_pole_ij)\n    e_val, e_vec = np.linalg.eig(m)\n    e_val_idx = np.argsort(np.abs(e_val))\n    mu1 = e_vec[:, e_val_idx[-1], np.newaxis]\n    mu2 = e_vec[:, e_val_idx[-2], np.newaxis]", "documentation": "    \"\"\"\n    Applies algorithm from YT section 6.2 page 20 related to complex pairs\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2551, "code": "def _YT_loop(ker_pole, transfer_matrix, poles, B, maxiter, rtol):\n    nb_real = poles[np.isreal(poles)].shape[0]\n    hnb = nb_real // 2\n    if nb_real > 0:\n        update_order = [[nb_real], [1]]\n    else:\n        update_order = [[],[]]\n    r_comp = np.arange(nb_real+1, len(poles)+1, 2)\n    r_p = np.arange(1, hnb+nb_real % 2)\n    update_order[0].extend(2*r_p)\n    update_order[1].extend(2*r_p+1)", "documentation": "    \"\"\"\n    Algorithm \"YT\" Tits, Yang. Globally Convergent\n    Algorithms for Robust Pole Assignment by State Feedback\n    https://hdl.handle.net/1903/5598\n    The poles P have to be sorted accordingly to section 6.2 page 20\n\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2674, "code": "def _KNV0_loop(ker_pole, transfer_matrix, poles, B, maxiter, rtol):\n    stop = False\n    nb_try = 0\n    while nb_try < maxiter and not stop:\n        det_transfer_matrixb = np.abs(np.linalg.det(transfer_matrix))\n        for j in range(B.shape[0]):\n            _KNV0(B, ker_pole, transfer_matrix, j, poles)\n        det_transfer_matrix = np.max((np.sqrt(np.spacing(1)),\n                                  np.abs(np.linalg.det(transfer_matrix))))\n        cur_rtol = np.abs((det_transfer_matrix - det_transfer_matrixb) /\n                       det_transfer_matrix)", "documentation": "    \"\"\"\n    Loop over all poles one by one and apply KNV method 0 algorithm\n    \"\"\""}, {"filename": "scipy/signal/_ltisys.py", "start_line": 2700, "code": "def place_poles(A, B, poles, method=\"YT\", rtol=1e-3, maxiter=30):\n    update_loop, poles = _valid_inputs(A, B, poles, method, rtol, maxiter)\n    cur_rtol = 0\n    nb_iter = 0\n    u, z = s_qr(B, mode=\"full\")\n    rankB = np.linalg.matrix_rank(B)\n    u0 = u[:, :rankB]\n    u1 = u[:, rankB:]\n    z = z[:rankB, :]\n    if B.shape[0] == rankB:\n        diag_poles = np.zeros(A.shape)", "documentation": "    \"\"\"\n    Compute K such that eigenvalues (A - dot(B, K))=poles.\n\n    K is the gain matrix such as the plant described by the linear system\n    ``AX+BU`` will have its closed-loop poles, i.e the eigenvalues ``A - B*K``,\n    as close as possible to those asked for in poles.\n\n    SISO, MISO and MIMO systems are supported.\n\n    Parameters\n    ----------\n    A, B : ndarray\n        State-space representation of linear system ``AX + BU``.\n    poles : array_like\n        Desired real poles and/or complex conjugates poles.\n        Complex poles are only supported with ``method=\"YT\"`` (default).\n    method: {'YT', 'KNV0'}, optional\n        Which method to choose to find the gain matrix K. One of:\n\n            - 'YT': Yang Tits\n            - 'KNV0': Kautsky, Nichols, Van Dooren update method 0\n\n        See References and Notes for details on the algorithms.\n    rtol: float, optional\n        After each iteration the determinant of the eigenvectors of\n        ``A - B*K`` is compared to its previous value, when the relative\n        error between these two values becomes lower than `rtol` the algorithm\n        stops.  Default is 1e-3.\n    maxiter: int, optional\n        Maximum number of iterations to compute the gain matrix.\n        Default is 30.\n\n    Returns\n    -------\n    full_state_feedback : Bunch object\n        full_state_feedback is composed of:\n            gain_matrix : 1-D ndarray\n                The closed loop matrix K such as the eigenvalues of ``A-BK``\n                are as close as possible to the requested poles.\n            computed_poles : 1-D ndarray\n                The poles corresponding to ``A-BK`` sorted as first the real\n                poles in increasing order, then the complex conjugates in\n                lexicographic order.\n            requested_poles : 1-D ndarray\n                The poles the algorithm was asked to place sorted as above,\n                they may differ from what was achieved.\n            X : 2-D ndarray\n                The transfer matrix such as ``X * diag(poles) = (A - B*K)*X``\n                (see Notes)\n            rtol : float\n                The relative tolerance achieved on ``det(X)`` (see Notes).\n                `rtol` will be NaN if it is possible to solve the system\n                ``diag(poles) = (A - B*K)``, or 0 when the optimization\n                algorithms can't do anything i.e when ``B.shape[1] == 1``.\n            nb_iter : int\n                The number of iterations performed before converging.\n                `nb_iter` will be NaN if it is possible to solve the system\n                ``diag(poles) = (A - B*K)``, or 0 when the optimization\n                algorithms can't do anything i.e when ``B.shape[1] == 1``.\n\n    Notes\n    -----\n    The Tits and Yang (YT), [2]_ paper is an update of the original Kautsky et\n    al. (KNV) paper [1]_.  KNV relies on rank-1 updates to find the transfer\n    matrix X such that ``X * diag(poles) = (A - B*K)*X``, whereas YT uses\n    rank-2 updates. This yields on average more robust solutions (see [2]_\n    pp 21-22), furthermore the YT algorithm supports complex poles whereas KNV\n    does not in its original version.  Only update method 0 proposed by KNV has\n    been implemented here, hence the name ``'KNV0'``.\n\n    KNV extended to complex poles is used in Matlab's ``place`` function, YT is\n    distributed under a non-free licence by Slicot under the name ``robpole``.\n    It is unclear and undocumented how KNV0 has been extended to complex poles\n    (Tits and Yang claim on page 14 of their paper that their method can not be\n    used to extend KNV to complex poles), therefore only YT supports them in\n    this implementation.\n\n    As the solution to the problem of pole placement is not unique for MIMO\n    systems, both methods start with a tentative transfer matrix which is\n    altered in various way to increase its determinant.  Both methods have been\n    proven to converge to a stable solution, however depending on the way the\n    initial transfer matrix is chosen they will converge to different\n    solutions and therefore there is absolutely no guarantee that using\n    ``'KNV0'`` will yield results similar to Matlab's or any other\n    implementation of these algorithms.\n\n    Using the default method ``'YT'`` should be fine in most cases; ``'KNV0'``\n    is only provided because it is needed by ``'YT'`` in some specific cases.\n    Furthermore ``'YT'`` gives on average more robust results than ``'KNV0'``\n    when ``abs(det(X))`` is used as a robustness indicator.\n\n    [2]_ is available as a technical report on the following URL:\n    https://hdl.handle.net/1903/5598\n\n    References\n    ----------\n    .. [1] J. Kautsky, N.K. Nichols and P. van Dooren, \"Robust pole assignment\n           in linear state feedback\", International Journal of Control, Vol. 41\n           pp. 1129-1155, 1985.\n    .. [2] A.L. Tits and Y. Yang, \"Globally convergent algorithms for robust\n           pole assignment by state feedback\", IEEE Transactions on Automatic\n           Control, Vol. 41, pp. 1432-1452, 1996.\n\n    Examples\n    --------\n    A simple example demonstrating real pole placement using both KNV and YT\n    algorithms.  This is example number 1 from section 4 of the reference KNV\n    publication ([1]_):\n\n    >>> import numpy as np\n    >>> from scipy import signal\n    >>> import matplotlib.pyplot as plt\n\n    >>> A = np.array([[ 1.380,  -0.2077,  6.715, -5.676  ],\n    ...               [-0.5814, -4.290,   0,      0.6750 ],\n    ...               [ 1.067,   4.273,  -6.654,  5.893  ],\n    ...               [ 0.0480,  4.273,   1.343, -2.104  ]])\n    >>> B = np.array([[ 0,      5.679 ],\n    ...               [ 1.136,  1.136 ],\n    ...               [ 0,      0,    ],\n    ...               [-3.146,  0     ]])\n    >>> P = np.array([-0.2, -0.5, -5.0566, -8.6659])\n\n    Now compute K with KNV method 0, with the default YT method and with the YT\n    method while forcing 100 iterations of the algorithm and print some results\n    after each call.\n\n    >>> fsf1 = signal.place_poles(A, B, P, method='KNV0')\n    >>> fsf1.gain_matrix\n    array([[ 0.20071427, -0.96665799,  0.24066128, -0.10279785],\n           [ 0.50587268,  0.57779091,  0.51795763, -0.41991442]])\n\n    >>> fsf2 = signal.place_poles(A, B, P)  # uses YT method\n    >>> fsf2.computed_poles\n    array([-8.6659, -5.0566, -0.5   , -0.2   ])\n\n    >>> fsf3 = signal.place_poles(A, B, P, rtol=-1, maxiter=100)\n    >>> fsf3.X\n    array([[ 0.52072442+0.j, -0.08409372+0.j, -0.56847937+0.j,  0.74823657+0.j],\n           [-0.04977751+0.j, -0.80872954+0.j,  0.13566234+0.j, -0.29322906+0.j],\n           [-0.82266932+0.j, -0.19168026+0.j, -0.56348322+0.j, -0.43815060+0.j],\n           [ 0.22267347+0.j,  0.54967577+0.j, -0.58387806+0.j, -0.40271926+0.j]])\n\n    The absolute value of the determinant of X is a good indicator to check the\n    robustness of the results, both ``'KNV0'`` and ``'YT'`` aim at maximizing\n    it.  Below a comparison of the robustness of the results above:\n\n    >>> abs(np.linalg.det(fsf1.X)) < abs(np.linalg.det(fsf2.X))\n    True\n    >>> abs(np.linalg.det(fsf2.X)) < abs(np.linalg.det(fsf3.X))\n    True\n\n    Now a simple example for complex poles:\n\n    >>> A = np.array([[ 0,  7/3.,  0,   0   ],\n    ...               [ 0,   0,    0,  7/9. ],\n    ...               [ 0,   0,    0,   0   ],\n    ...               [ 0,   0,    0,   0   ]])\n    >>> B = np.array([[ 0,  0 ],\n    ...               [ 0,  0 ],\n    ...               [ 1,  0 ],\n    ...               [ 0,  1 ]])\n    >>> P = np.array([-3, -1, -2-1j, -2+1j]) / 3.\n    >>> fsf = signal.place_poles(A, B, P, method='YT')\n\n    We can plot the desired and computed poles in the complex plane:\n\n    >>> t = np.linspace(0, 2*np.pi, 401)\n    >>> plt.plot(np.cos(t), np.sin(t), 'k--')  # unit circle\n    >>> plt.plot(fsf.requested_poles.real, fsf.requested_poles.imag,\n    ...          'wo', label='Desired')\n    >>> plt.plot(fsf.computed_poles.real, fsf.computed_poles.imag, 'bx',\n    ...          label='Placed')\n    >>> plt.grid()\n    >>> plt.axis('image')\n    >>> plt.axis([-1.1, 1.1, -1.1, 1.1])\n    >>> plt.legend(bbox_to_anchor=(1.05, 1), loc=2, numpoints=1)\n\n    \"\"\""}]}
{"repository": "scipy/scipy", "commit_sha": "328ad15aceb98de0f42a5f744195eebed34f601c", "commit_message": "MAINT: signal: make abcd_normalize respect input dtype and remove dtype kwarg (#24232)\n\n* MAINT: Make abcd_normalize respect input dtype with xp_promote\n\n* TST: Update tests in response to abcd_normalize change\n\n* MAINT: explicitly set dtype in xp.zeros\n\n* MAINT: Add dtypes test for abcd_normalize\n\n* TST: xfail some torch float32 tests\n\n* MAINT: Change abcd_normalize signature back in delegators\n\n* DOC: remove parts of abcd_normalize docstring involving dtype\n\n* TST: use xp_result_type in abcd_normalize dtype test\n\n* DOC: update docstrings on abcd_normalize dtype behavior\n\n* MAINT: add missing comma", "commit_date": "2026-01-05T18:27:34+00:00", "author": "Albert Steppi", "file": "scipy/signal/_support_alternative_backends.py", "patch": "@@ -154,7 +154,14 @@ def get_default_capabilities(func_name, delegator):\n \n     \"\"\"\n \n+abcd_normalize_extra_note = \\\n+    \"\"\"The result dtype when all array inputs are of integer dtype is the\n+    backend's current default floating point dtype.\n+\n+    \"\"\"\n+\n capabilities_overrides = {\n+    \"abcd_normalize\": xp_capabilities(extra_note=abcd_normalize_extra_note),\n     \"bessel\": xp_capabilities(cpu_only=True, jax_jit=False, allow_dask_compute=True),\n     \"bilinear\": xp_capabilities(cpu_only=True, exceptions=[\"cupy\"],\n                                 jax_jit=False, allow_dask_compute=True,", "before_segments": [], "after_segments": []}
{"repository": "scipy/scipy", "commit_sha": "328ad15aceb98de0f42a5f744195eebed34f601c", "commit_message": "MAINT: signal: make abcd_normalize respect input dtype and remove dtype kwarg (#24232)\n\n* MAINT: Make abcd_normalize respect input dtype with xp_promote\n\n* TST: Update tests in response to abcd_normalize change\n\n* MAINT: explicitly set dtype in xp.zeros\n\n* MAINT: Add dtypes test for abcd_normalize\n\n* TST: xfail some torch float32 tests\n\n* MAINT: Change abcd_normalize signature back in delegators\n\n* DOC: remove parts of abcd_normalize docstring involving dtype\n\n* TST: use xp_result_type in abcd_normalize dtype test\n\n* DOC: update docstrings on abcd_normalize dtype behavior\n\n* MAINT: add missing comma", "commit_date": "2026-01-05T18:27:34+00:00", "author": "Albert Steppi", "file": "scipy/signal/tests/test_cont2discrete.py", "patch": "@@ -354,9 +354,9 @@ def test_multioutput(self):\n         xp_assert_close(den, den1, rtol=1e-13)\n         xp_assert_close(den, den2, rtol=1e-13)\n \n-@skip_xp_backends(np_only=True, reason=\"lti currently not supported\")\n+\n class TestC2dLti:\n-    def test_c2d_ss(self, xp):\n+    def test_c2d_ss(self):\n         # StateSpace\n         A = np.array([[-0.3, 0.1], [0.2, -0.7]])\n         B = np.array([[0], [1]])\n@@ -367,8 +367,7 @@ def test_c2d_ss(self, xp):\n         A_res = np.array([[0.985136404135682, 0.004876671474795],\n                           [0.009753342949590, 0.965629718236502]])\n         B_res = np.array([[0.000122937599964], [0.049135527547844]])\n-        # Resulting A, B, C, D are arrays of the same dtype due to abcd_normalize() use:\n-        C_res = np.astype(C, A_res.dtype)\n+        C_res = np.astype(C, np.float64)\n \n         sys_ssc = lti(A, B, C, D)\n         sys_ssd = sys_ssc.to_discrete(dt=dt)\n@@ -385,7 +384,7 @@ def test_c2d_ss(self, xp):\n         xp_assert_close(sys_ssd2.C, C_res)\n         xp_assert_close(sys_ssd2.D, np.zeros_like(sys_ssd.D))\n \n-    def test_c2d_tf(self, xp):\n+    def test_c2d_tf(self):\n \n         sys = lti([0.5, 0.3], [1.0, 0.4])\n         sys = sys.to_discrete(0.005)", "before_segments": [{"filename": "scipy/signal/tests/test_cont2discrete.py", "start_line": 239, "code": "    def test_gbt_with_sio_tf_and_zpk(self, xp):\n        A = -1.0\n        B = 1.0\n        C = 1.0\n        D = 0.5\n        cnum, cden = ss2tf(A, B, C, D)\n        cz, cp, ck = ss2zpk(A, B, C, D)\n        h = 1.0\n        alpha = 0.25\n        Ad = (1 + (1 - alpha) * h * A) / (1 - alpha * h * A)\n        Bd = h * B / (1 - alpha * h * A)", "documentation": "        \"\"\"Test method='gbt' with alpha=0.25 for tf and zpk cases.\"\"\""}, {"filename": "scipy/signal/tests/test_cont2discrete.py", "start_line": 281, "code": "    def test_discrete_approx(self, xp):", "documentation": "        \"\"\"\n        Test that the solution to the discrete approximation of a continuous\n        system actually approximates the solution to the continuous system.\n        This is an indirect test of the correctness of the implementation\n        of cont2discrete.\n        \"\"\""}], "after_segments": [{"filename": "scipy/signal/tests/test_cont2discrete.py", "start_line": 239, "code": "    def test_gbt_with_sio_tf_and_zpk(self, xp):\n        A = -1.0\n        B = 1.0\n        C = 1.0\n        D = 0.5\n        cnum, cden = ss2tf(A, B, C, D)\n        cz, cp, ck = ss2zpk(A, B, C, D)\n        h = 1.0\n        alpha = 0.25\n        Ad = (1 + (1 - alpha) * h * A) / (1 - alpha * h * A)\n        Bd = h * B / (1 - alpha * h * A)", "documentation": "        \"\"\"Test method='gbt' with alpha=0.25 for tf and zpk cases.\"\"\""}, {"filename": "scipy/signal/tests/test_cont2discrete.py", "start_line": 281, "code": "    def test_discrete_approx(self, xp):", "documentation": "        \"\"\"\n        Test that the solution to the discrete approximation of a continuous\n        system actually approximates the solution to the continuous system.\n        This is an indirect test of the correctness of the implementation\n        of cont2discrete.\n        \"\"\""}]}
{"repository": "scipy/scipy", "commit_sha": "328ad15aceb98de0f42a5f744195eebed34f601c", "commit_message": "MAINT: signal: make abcd_normalize respect input dtype and remove dtype kwarg (#24232)\n\n* MAINT: Make abcd_normalize respect input dtype with xp_promote\n\n* TST: Update tests in response to abcd_normalize change\n\n* MAINT: explicitly set dtype in xp.zeros\n\n* MAINT: Add dtypes test for abcd_normalize\n\n* TST: xfail some torch float32 tests\n\n* MAINT: Change abcd_normalize signature back in delegators\n\n* DOC: remove parts of abcd_normalize docstring involving dtype\n\n* TST: use xp_result_type in abcd_normalize dtype test\n\n* DOC: update docstrings on abcd_normalize dtype behavior\n\n* MAINT: add missing comma", "commit_date": "2026-01-05T18:27:34+00:00", "author": "Albert Steppi", "file": "scipy/signal/tests/test_ltisys.py", "patch": "@@ -5,13 +5,15 @@\n import pytest\n from pytest import raises as assert_raises\n from scipy._lib._array_api import(\n-    assert_almost_equal, xp_assert_equal, xp_assert_close, make_xp_test_case\n+    assert_almost_equal, xp_assert_equal, xp_assert_close, make_xp_test_case,\n+    xp_result_type\n )\n \n from scipy.signal import (ss2tf, tf2ss, lti,\n                           dlti, bode, freqresp, lsim, impulse, step,\n                           abcd_normalize, place_poles,\n                           TransferFunction, StateSpace, ZerosPolesGain)\n+\n from scipy.signal._filter_design import BadCoefficients\n import scipy.linalg as linalg\n \n@@ -952,9 +954,11 @@ def test_BD_mismatch_fails(self, xp):\n                       self.A, [-1, 5], self.C, self.D)\n \n     def test_normalized_matrices_unchanged(self, xp):\n-        A_, B_, C_, D_ = map(xp.asarray, (self.A, self.B, self.C, self.D))\n-        # On torch/float32: A_, B_, C_, D_ are of dtype float32 => set dtype:\n-        A, B, C, D = abcd_normalize(A=A_, B=B_, C=C_, D=D_, dtype=A_.dtype)\n+        A_, B_, C_, D_ = map(\n+            lambda t: xp.asarray(t, dtype=xp.float64),\n+            (self.A, self.B, self.C, self.D),\n+        )\n+        A, B, C, D = abcd_normalize(A=A_, B=B_, C=C_, D=D_)\n         xp_assert_equal(A, A_)\n         xp_assert_equal(B, B_)\n         xp_assert_equal(C, C_)\n@@ -969,31 +973,32 @@ def test_shapes(self):\n         assert B.shape[1] == D.shape[1]\n \n     def test_zero_dimension_is_not_none1(self, xp):\n-        A_ = xp.asarray(self.A)\n-        B_ = xp.zeros((2, 0))\n-        D_ = xp.zeros((0, 0))\n-        # On torch/float32: A_, B_, C_, D_ are of dtype float32 => set dtype:\n-        A, B, C, D = abcd_normalize(A=A_, B=B_, D=D_, dtype=A_.dtype)\n+        A_ = xp.asarray(self.A, dtype=xp.float64)\n+        B_ = xp.zeros((2, 0), dtype=xp.float64)\n+        D_ = xp.zeros((0, 0), dtype=xp.float64)\n+        A, B, C, D = abcd_normalize(A=A_, B=B_, D=D_)\n         xp_assert_equal(A, A_)\n         xp_assert_equal(B, B_)\n         xp_assert_equal(D, D_)\n         assert C.shape[0] == D_.shape[0]\n         assert C.shape[1] == A_.shape[0]\n \n     def test_zero_dimension_is_not_none2(self, xp):\n-        A_ = xp.asarray(self.A)\n-        B_ = xp.zeros((2, 0))\n-        C_ = xp.zeros((0, 2))\n-        # On torch/float32: A_, B_, C_, D_ are of dtype float32 => set dtype:\n-        A, B, C, D = abcd_normalize(A=A_, B=B_, C=C_, dtype=A_.dtype)\n+        A_ = xp.asarray(self.A, dtype=xp.float64)\n+        B_ = xp.zeros((2, 0), dtype=xp.float64)\n+        C_ = xp.zeros((0, 2), dtype=xp.float64)\n+        A, B, C, D = abcd_normalize(A=A_, B=B_, C=C_)\n         xp_assert_equal(A, A_)\n         xp_assert_equal(B, B_)\n         xp_assert_equal(C, C_)\n         assert D.shape[0] == C_.shape[0]\n         assert D.shape[1] == B_.shape[1]\n \n     def test_missing_A(self, xp):\n-        B_, C_, D_ = map(xp.asarray, (self.B, self.C, self.D))\n+        B_, C_, D_ = map(\n+            lambda t: xp.asarray(t, dtype=xp.float64),\n+            (self.B, self.C, self.D),\n+        )\n         A, B, C, D = abcd_normalize(B=B_, C=C_, D=D_)\n         assert A.shape[0] == A.shape[1]\n         assert A.shape[0] == B.shape[0]\n@@ -1007,7 +1012,10 @@ def test_missing_B(self, xp):\n         assert B.shape == (A_.shape[0], D_.shape[1])\n \n     def test_missing_C(self, xp):\n-        A_, B_, D_ = map(xp.asarray, (self.A, self.B, self.D))\n+        A_, B_, D_ = map(\n+            lambda t: xp.asarray(t, dtype=xp.float64),\n+            (self.A, self.B, self.D),\n+        )\n         A, B, C, D = abcd_normalize(A=A_, B=B_, D=D_)\n         assert C.shape[0] == D.shape[0]\n         assert C.shape[1] == A.shape[0]\n@@ -1070,25 +1078,41 @@ def test_missing_CD_fails(self, xp):\n         A, B = xp.asarray(self.A), xp.asarray(self.B)\n         assert_raises(ValueError, abcd_normalize, A=A, B=B)\n \n-    def test_param_dtype_exceptions(self):\n-        with pytest.raises(ValueError, match=\"^Parameter dtype='INVALID' must be\"):\n-            abcd_normalize(self.A, self.B, self.C, self.D, dtype='INVALID')\n-        with pytest.raises(ValueError, match=\"^Parameter dtype=<class 'str'>\"):\n-            abcd_normalize(self.A, self.B, self.C, self.D, dtype=str)\n-        with pytest.raises(ValueError, match=\"^Parameter dtype=\"):\n-            abcd_normalize(self.A, self.B, self.C, self.D, dtype=np.datetime64)\n-\n-    def test_param_dtype(self, xp):\n-        A, D = xp.asarray(self.A), xp.asarray(self.D)\n-        AA, BB, CC, DD = abcd_normalize(A=A, D=D)\n-        assert AA.dtype == BB.dtype == CC.dtype == DD.dtype == xp.float64\n-\n-        AA, BB, CC, DD = abcd_normalize(A=A, D=D, dtype=xp.int64)\n-        assert AA.dtype == BB.dtype == CC.dtype == DD.dtype == xp.int64\n-\n-        DD_ = 1 + 2j  # converts to complex128\n-        AA, BB, CC, DD = abcd_normalize(A=A, D=DD_)\n-        assert AA.dtype == BB.dtype == CC.dtype == DD.dtype == xp.complex128\n+\n+    @pytest.mark.parametrize(\n+        \"A_dtype\",\n+        # A_dtype=None here means to pass None as the value for A\n+        [None, \"int64\", \"float32\", \"float64\", \"complex64\", \"complex128\"],\n+    )\n+    @pytest.mark.parametrize(\n+        \"B_dtype\",\n+        [None, \"int64\", \"float32\", \"float64\", \"complex64\", \"complex128\"],\n+    )\n+    @pytest.mark.parametrize(\n+        \"C_dtype\",\n+        [\"int64\", \"float32\", \"float64\", \"complex64\", \"complex128\"],\n+    )\n+    @pytest.mark.parametrize(\n+        \"D_dtype\",\n+        [\"int64\", \"float32\", \"float64\", \"complex64\", \"complex128\"],\n+    )\n+    # Also check case where one input array has size zero.\n+    @pytest.mark.parametrize(\"D\", [[[2.5]], [[]]])\n+    def test_dtypes(self, D, A_dtype, B_dtype, C_dtype, D_dtype, xp):\n+       \n+        args = []\n+        for X, X_dtype in zip(\n+                [self.A, self.B, self.C, D],\n+                [A_dtype, B_dtype, C_dtype, D_dtype]\n+        ):\n+            args.append(\n+                xp.asarray(X, dtype=getattr(xp, X_dtype))\n+                if X_dtype is not None else None\n+            )\n+        A, B, C, D = args\n+        expected_dtype = xp_result_type(A, B, C, D, xp=xp, force_floating=True)\n+        A, B, C, D = abcd_normalize(A=A, B=B, C=C, D=D)\n+        assert A.dtype == B.dtype == C.dtype == D.dtype == expected_dtype\n \n \n class Test_bode:", "before_segments": [{"filename": "scipy/signal/tests/test_ltisys.py", "start_line": 18, "code": "def _assert_poles_close(P1,P2, rtol=1e-8, atol=1e-8):\n    P2 = P2.copy()\n    for p1 in P1:\n        found = False\n        for p2_idx in range(P2.shape[0]):\n            if np.allclose([np.real(p1), np.imag(p1)],\n                           [np.real(P2[p2_idx]), np.imag(P2[p2_idx])],\n                           rtol, atol):\n                found = True\n                np.delete(P2, p2_idx)\n                break", "documentation": "    \"\"\"\n    Check each pole in P1 is close to a pole in P2 with a 1e-8\n    relative tolerance or 1e-8 absolute tolerance (useful for zero poles).\n    These tolerances are very strict but the systems tested are known to\n    accept these poles so we should not be far from what is requested.\n    \"\"\""}, {"filename": "scipy/signal/tests/test_ltisys.py", "start_line": 41, "code": "    def _check(self, A, B, P, **kwargs):\n        fsf = place_poles(A, B, P, **kwargs)\n        expected, _ = np.linalg.eig(A - np.dot(B, fsf.gain_matrix))\n        _assert_poles_close(expected, fsf.requested_poles)\n        _assert_poles_close(expected, fsf.computed_poles)\n        _assert_poles_close(P,fsf.requested_poles)\n        return fsf", "documentation": "        \"\"\"\n        Perform the most common tests on the poles computed by place_poles\n        and return the Bunch object for further specific tests\n        \"\"\""}], "after_segments": [{"filename": "scipy/signal/tests/test_ltisys.py", "start_line": 20, "code": "def _assert_poles_close(P1,P2, rtol=1e-8, atol=1e-8):\n    P2 = P2.copy()\n    for p1 in P1:\n        found = False\n        for p2_idx in range(P2.shape[0]):\n            if np.allclose([np.real(p1), np.imag(p1)],\n                           [np.real(P2[p2_idx]), np.imag(P2[p2_idx])],\n                           rtol, atol):\n                found = True\n                np.delete(P2, p2_idx)\n                break", "documentation": "    \"\"\"\n    Check each pole in P1 is close to a pole in P2 with a 1e-8\n    relative tolerance or 1e-8 absolute tolerance (useful for zero poles).\n    These tolerances are very strict but the systems tested are known to\n    accept these poles so we should not be far from what is requested.\n    \"\"\""}, {"filename": "scipy/signal/tests/test_ltisys.py", "start_line": 43, "code": "    def _check(self, A, B, P, **kwargs):\n        fsf = place_poles(A, B, P, **kwargs)\n        expected, _ = np.linalg.eig(A - np.dot(B, fsf.gain_matrix))\n        _assert_poles_close(expected, fsf.requested_poles)\n        _assert_poles_close(expected, fsf.computed_poles)\n        _assert_poles_close(P,fsf.requested_poles)\n        return fsf", "documentation": "        \"\"\"\n        Perform the most common tests on the poles computed by place_poles\n        and return the Bunch object for further specific tests\n        \"\"\""}]}
{"repository": "scipy/scipy", "commit_sha": "633b82d5545a94a5d271e690d5ef883b2d96a78a", "commit_message": "DOC: special.airy: Fix Documentation (#24110)\n\n---------\n\nCo-authored-by: Jake Bowhay <60778417+j-bowhay@users.noreply.github.com>", "commit_date": "2025-12-08T09:36:09+00:00", "author": "Florian Bourgey", "file": "scipy/special/_special_ufuncs_docs.cpp", "patch": "@@ -442,28 +442,29 @@ const char *airy_doc = R\"(\n \n     Notes\n     -----\n-    The Airy functions Ai and Bi are two independent solutions of\n+    The Airy functions :math:`\\operatorname{Ai}` and :math:`\\operatorname{Bi}` are two \n+    independent solutions of\n \n-    .. math:: y''(x) = x y(x).\n+    .. math:: y''(z) = z y(z).\n \n-    For real `z` in [-10, 10], the computation is carried out by calling\n+    For real :math:`z` in :math:`[-10, 10]`, the computation is carried out by calling\n     the Cephes [1]_ `airy` routine, which uses power series summation\n-    for small `z` and rational minimax approximations for large `z`.\n+    for small :math:`z` and rational minimax approximations for large :math:`z`.\n \n     Outside this range, the AMOS [2]_ `zairy` and `zbiry` routines are\n     employed.  They are computed using power series for :math:`|z| < 1` and\n-    the following relations to modified Bessel functions for larger `z`\n+    the following relations to modified Bessel functions for larger :math:`z`\n     (where :math:`t \\equiv 2 z^{3/2}/3`):\n \n     .. math::\n \n-        Ai(z) = \\frac{1}{\\pi \\sqrt{3}} K_{1/3}(t)\n-\n-        Ai'(z) = -\\frac{z}{\\pi \\sqrt{3}} K_{2/3}(t)\n-\n-        Bi(z) = \\sqrt{\\frac{z}{3}} \\left(I_{-1/3}(t) + I_{1/3}(t) \\right)\n-\n-        Bi'(z) = \\frac{z}{\\sqrt{3}} \\left(I_{-2/3}(t) + I_{2/3}(t)\\right)\n+        \\operatorname{Ai}(z) = \\frac{1}{\\pi}\\sqrt{\\frac{z}{3}} \\, K_{1/3}(t)\n+        \n+        \\operatorname{Ai}'(z) = -\\frac{z}{\\pi \\sqrt{3}} \\, K_{2/3}(t)\n+        \n+        \\operatorname{Bi}(z) = \\sqrt{\\frac{z}{3}} \\left(I_{-1/3}(t) + I_{1/3}(t)\\right)\n+        \n+        \\operatorname{Bi}'(z) = \\frac{z}{\\sqrt{3}} \\left(I_{-2/3}(t) + I_{2/3}(t)\\right)\n \n     References\n     ----------\n@@ -475,14 +476,14 @@ const char *airy_doc = R\"(\n \n     Examples\n     --------\n-    Compute the Airy functions on the interval [-15, 5].\n+    Compute the Airy functions on the interval :math:`[-15, 5]`.\n \n     >>> import numpy as np\n     >>> from scipy import special\n     >>> x = np.linspace(-15, 5, 201)\n     >>> ai, aip, bi, bip = special.airy(x)\n \n-    Plot Ai(x) and Bi(x).\n+    Plot :math:`\\operatorname{Ai}(x)` and :math:`\\operatorname{Bi}(x)`.\n \n     >>> import matplotlib.pyplot as plt\n     >>> plt.plot(x, ai, 'r', label='Ai(x)')", "before_segments": [], "after_segments": []}
{"repository": "scipy/scipy", "commit_sha": "2270279deadd7ebf5a0c888a46a3afa6c7c77255", "commit_message": "TST: Update comment on filter function dtypes in tests", "commit_date": "2025-11-03T16:57:12+00:00", "author": "steppi", "file": "scipy/signal/tests/test_filter_design.py", "patch": "@@ -2958,10 +2958,11 @@ def test_fs_param(self):\n                                     xp_assert_close(ba1_, ba2_)\n \n \n-# Currently the filter functions below all return float64 (or complex128) output\n-# regardless of input dtype. Therefore reference arrays below are all given an\n-# explicit 64 bit dtype, because the output will not match the xp_default_dtype\n-# when the default dtype is float32. Although the output arrays and all internal\n+# Currently the filter functions tested below (butter, cheby1, cheby2, ellip,\n+# and bessel) all return float64 (or complex128) output regardless of input\n+# dtype. Therefore reference arrays in these tests are all given an explicit 64\n+# bit dtype, because the output will not match the xp_default_dtype when the\n+# default dtype is float32. Although the output arrays and all internal\n # calculations are in 64 bit precision, tolerances are still loosened for the\n # float32 case when results are impacted by reduced precision in the inputs.\n ", "before_segments": [{"filename": "scipy/signal/tests/test_filter_design.py", "start_line": 224, "code": "    def test_identity(self, xp):\n        z = xp.asarray([])\n        p = xp.asarray([])\n        k = 1.\n        b, a = zpk2tf(z, p, k)\n        b_r = xp.asarray([1.])  # desired result\n        a_r = xp.asarray([1.])  # desired result\n        xp_assert_equal(b, b_r)\n        xp_assert_equal(a, a_r)\n        if is_numpy(xp):\n            assert isinstance(b, np.ndarray)", "documentation": "        \"\"\"Test the identity transfer function.\"\"\""}, {"filename": "scipy/signal/tests/test_filter_design.py", "start_line": 337, "code": "    def test_fewer_zeros(self, xp):\n        sos = butter(3, 0.1, output='sos')\n        z, p, k = sos2zpk(xp.asarray(sos))\n        assert z.shape[0] == 4\n        assert p.shape[0] == 4\n        sos = butter(12, [5., 30.], 'bandpass', fs=1200., analog=False, output='sos')\n        e = BadCoefficients\n        if is_cupy(xp):\n            e = scipy_namespace_for(xp).signal.BadCoefficients\n        with pytest.warns(e, match='Badly conditioned'):\n            z, p, k = sos2zpk(xp.asarray(sos))", "documentation": "        \"\"\"Test not the expected number of p/z (effectively at origin).\"\"\""}, {"filename": "scipy/signal/tests/test_filter_design.py", "start_line": 757, "code": "    def test_ticket1441(self, xp):\n        N = 100000\n        w, h = freqz(xp.asarray([1.0]), worN=N)\n        assert w.shape == (N,)", "documentation": "        \"\"\"Regression test for ticket 1441.\"\"\""}, {"filename": "scipy/signal/tests/test_filter_design.py", "start_line": 1371, "code": "    def test_ticket1441(self, xp):\n        N = 100000\n        w, h = freqz_zpk(xp.asarray([0.5]), xp.asarray([0.5]), 1.0, worN=N)\n        assert w.shape == (N,)", "documentation": "        \"\"\"Regression test for ticket 1441.\"\"\""}, {"filename": "scipy/signal/tests/test_filter_design.py", "start_line": 1491, "code": "    def test_allclose(self, xp):\n        b_matlab = xp.asarray([2.150733144728282e-11, 1.720586515782626e-10,\n                               6.022052805239190e-10, 1.204410561047838e-09,\n                               1.505513201309798e-09, 1.204410561047838e-09,\n                               6.022052805239190e-10, 1.720586515782626e-10,\n                               2.150733144728282e-11])\n        a_matlab = xp.asarray([1.000000000000000e+00, -7.782402035027959e+00,\n                               2.654354569747454e+01, -5.182182531666387e+01,\n                               6.334127355102684e+01, -4.963358186631157e+01,\n                               2.434862182949389e+01, -6.836925348604676e+00,\n                               8.412934944449140e-01])", "documentation": "        \"\"\"Test for false positive on allclose in normalize() in\n        filter_design.py\"\"\""}, {"filename": "scipy/signal/tests/test_filter_design.py", "start_line": 1537, "code": "    def test_errors(self):\n        assert_raises(ValueError, normalize, [1, 2], 0)\n        assert_raises(ValueError, normalize, [1, 2], [[1]])\n        assert_raises(ValueError, normalize, [[[1, 2]]], 1)", "documentation": "        \"\"\"Test the error cases.\"\"\""}, {"filename": "scipy/signal/tests/test_filter_design.py", "start_line": 1603, "code": "class TestBilinear:", "documentation": "    \"\"\"Tests for function `signal.bilinear`. \"\"\""}, {"filename": "scipy/signal/tests/test_filter_design.py", "start_line": 1606, "code": "    def test_exceptions(self):\n        with pytest.raises(ValueError, match=\"Parameter a is not .*\"):\n            bilinear(1., np.array([[1, 2, 3]]))\n        with pytest.raises(ValueError, match=\"Parameter b is not .*\"):\n            bilinear(np.ones((2,3)), 1. )\n    @pytest.mark.xfail(DEFAULT_F32, reason=\"wrong answer with torch/float32\")", "documentation": "        \"\"\"Raise all exceptions in `bilinear()`. \"\"\""}], "after_segments": [{"filename": "scipy/signal/tests/test_filter_design.py", "start_line": 224, "code": "    def test_identity(self, xp):\n        z = xp.asarray([])\n        p = xp.asarray([])\n        k = 1.\n        b, a = zpk2tf(z, p, k)\n        b_r = xp.asarray([1.])  # desired result\n        a_r = xp.asarray([1.])  # desired result\n        xp_assert_equal(b, b_r)\n        xp_assert_equal(a, a_r)\n        if is_numpy(xp):\n            assert isinstance(b, np.ndarray)", "documentation": "        \"\"\"Test the identity transfer function.\"\"\""}, {"filename": "scipy/signal/tests/test_filter_design.py", "start_line": 337, "code": "    def test_fewer_zeros(self, xp):\n        sos = butter(3, 0.1, output='sos')\n        z, p, k = sos2zpk(xp.asarray(sos))\n        assert z.shape[0] == 4\n        assert p.shape[0] == 4\n        sos = butter(12, [5., 30.], 'bandpass', fs=1200., analog=False, output='sos')\n        e = BadCoefficients\n        if is_cupy(xp):\n            e = scipy_namespace_for(xp).signal.BadCoefficients\n        with pytest.warns(e, match='Badly conditioned'):\n            z, p, k = sos2zpk(xp.asarray(sos))", "documentation": "        \"\"\"Test not the expected number of p/z (effectively at origin).\"\"\""}, {"filename": "scipy/signal/tests/test_filter_design.py", "start_line": 757, "code": "    def test_ticket1441(self, xp):\n        N = 100000\n        w, h = freqz(xp.asarray([1.0]), worN=N)\n        assert w.shape == (N,)", "documentation": "        \"\"\"Regression test for ticket 1441.\"\"\""}, {"filename": "scipy/signal/tests/test_filter_design.py", "start_line": 1371, "code": "    def test_ticket1441(self, xp):\n        N = 100000\n        w, h = freqz_zpk(xp.asarray([0.5]), xp.asarray([0.5]), 1.0, worN=N)\n        assert w.shape == (N,)", "documentation": "        \"\"\"Regression test for ticket 1441.\"\"\""}, {"filename": "scipy/signal/tests/test_filter_design.py", "start_line": 1491, "code": "    def test_allclose(self, xp):\n        b_matlab = xp.asarray([2.150733144728282e-11, 1.720586515782626e-10,\n                               6.022052805239190e-10, 1.204410561047838e-09,\n                               1.505513201309798e-09, 1.204410561047838e-09,\n                               6.022052805239190e-10, 1.720586515782626e-10,\n                               2.150733144728282e-11])\n        a_matlab = xp.asarray([1.000000000000000e+00, -7.782402035027959e+00,\n                               2.654354569747454e+01, -5.182182531666387e+01,\n                               6.334127355102684e+01, -4.963358186631157e+01,\n                               2.434862182949389e+01, -6.836925348604676e+00,\n                               8.412934944449140e-01])", "documentation": "        \"\"\"Test for false positive on allclose in normalize() in\n        filter_design.py\"\"\""}, {"filename": "scipy/signal/tests/test_filter_design.py", "start_line": 1537, "code": "    def test_errors(self):\n        assert_raises(ValueError, normalize, [1, 2], 0)\n        assert_raises(ValueError, normalize, [1, 2], [[1]])\n        assert_raises(ValueError, normalize, [[[1, 2]]], 1)", "documentation": "        \"\"\"Test the error cases.\"\"\""}, {"filename": "scipy/signal/tests/test_filter_design.py", "start_line": 1603, "code": "class TestBilinear:", "documentation": "    \"\"\"Tests for function `signal.bilinear`. \"\"\""}, {"filename": "scipy/signal/tests/test_filter_design.py", "start_line": 1606, "code": "    def test_exceptions(self):\n        with pytest.raises(ValueError, match=\"Parameter a is not .*\"):\n            bilinear(1., np.array([[1, 2, 3]]))\n        with pytest.raises(ValueError, match=\"Parameter b is not .*\"):\n            bilinear(np.ones((2,3)), 1. )\n    @pytest.mark.xfail(DEFAULT_F32, reason=\"wrong answer with torch/float32\")", "documentation": "        \"\"\"Raise all exceptions in `bilinear()`. \"\"\""}]}
